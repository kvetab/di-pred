{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46de4b61-9432-4737-9f84-d2b352774630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from proteinbert import OutputType, OutputSpec, FinetuningModelGenerator, load_pretrained_model, finetune, evaluate_by_len\n",
    "from proteinbert.conv_and_global_attention_model import get_model_with_hidden_layers_as_outputs\n",
    "from os import path\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7207ca03-77c3-4e21-9daf-e4dc0b007d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "170e33c4-f43d-4107-ac8e-f49f7b8639b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca433177-b772-4f06-b299-d0881b7f12a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_TYPE = OutputType(False, 'binary')\n",
    "UNIQUE_LABELS = [0, 1]\n",
    "OUTPUT_SPEC = OutputSpec(OUTPUT_TYPE, UNIQUE_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59b9bb79-e747-445a-985d-68f32f922316",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_generator, input_encoder = load_pretrained_model(\"../../data/protein_bert/\", \"epoch_92400_sample_23500000.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15c7d9ab-2013-4cdd-82b1-44157cab56ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrained_model_generator, input_encoder = load_pretrained_model(\"../../data/protein_bert/\", \"checkpoint_2022_01_19.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e89d8432-0860-431c-8b30-f9e44686f9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "112bf82b-2866-43b8-9bea-2a300fdf1dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(patience = 1, factor = 0.25, min_lr = 1e-05, verbose = 1),\n",
    "    keras.callbacks.EarlyStopping(patience = 2, restore_best_weights = True),\n",
    "    #WandbCallback()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6fd92e3-8ed7-43da-9da1-a7454c11a571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antibody_ID</th>\n",
       "      <th>heavy</th>\n",
       "      <th>light</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2073</th>\n",
       "      <td>6aod</td>\n",
       "      <td>EVQLVQSGAEVKKPGASVKVSCKASGYTFTGYYMHWVRQAPGQGLE...</td>\n",
       "      <td>DIVMTKSPSSLSASVGDRVTITCRASQGIRNDLGWYQQKPGKAPKR...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>4yny</td>\n",
       "      <td>EVQLVESGGGLVQPGRSLKLSCAASGFTFSNYGMAWVRQTPTKGLE...</td>\n",
       "      <td>EFVLTQPNSVSTNLGSTVKLSCKRSTGNIGSNYVNWYQQHEGRSPT...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>5xcv</td>\n",
       "      <td>EVQLVESGGGLVQPGRSLKLSCAASGFTFSNYGMAWVRQTPTKGLE...</td>\n",
       "      <td>QFVLTQPNSVSTNLGSTVKLSCKRSTGNIGSNYVNWYQQHEGRSPT...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2070</th>\n",
       "      <td>6and</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGYEFSRSWMNWVRQAPGKGLE...</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCRSSQSIVHSVGNTFLEWYQQKPG...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>2xqy</td>\n",
       "      <td>QVQLQQPGAELVKPGASVKMSCKASGYSFTSYWMNWVKQRPGRGLE...</td>\n",
       "      <td>DIVLTQSPASLALSLGQRATISCRASKSVSTSGYSYMYWYQQKPGQ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Antibody_ID                                              heavy  \\\n",
       "2073        6aod  EVQLVQSGAEVKKPGASVKVSCKASGYTFTGYYMHWVRQAPGQGLE...   \n",
       "1517        4yny  EVQLVESGGGLVQPGRSLKLSCAASGFTFSNYGMAWVRQTPTKGLE...   \n",
       "2025        5xcv  EVQLVESGGGLVQPGRSLKLSCAASGFTFSNYGMAWVRQTPTKGLE...   \n",
       "2070        6and  EVQLVESGGGLVQPGGSLRLSCAASGYEFSRSWMNWVRQAPGKGLE...   \n",
       "666         2xqy  QVQLQQPGAELVKPGASVKMSCKASGYSFTSYWMNWVKQRPGRGLE...   \n",
       "\n",
       "                                                  light  Y  \n",
       "2073  DIVMTKSPSSLSASVGDRVTITCRASQGIRNDLGWYQQKPGKAPKR...  0  \n",
       "1517  EFVLTQPNSVSTNLGSTVKLSCKRSTGNIGSNYVNWYQQHEGRSPT...  1  \n",
       "2025  QFVLTQPNSVSTNLGSTVKLSCKRSTGNIGSNYVNWYQQHEGRSPT...  1  \n",
       "2070  DIQMTQSPSSLSASVGDRVTITCRSSQSIVHSVGNTFLEWYQQKPG...  1  \n",
       "666   DIVLTQSPASLALSLGQRATISCRASKSVSTSGYSYMYWYQQKPGQ...  0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_train_data.csv\"), index_col=0)\n",
    "valid_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_valid_data.csv\"), index_col=0)\n",
    "test_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_test_data.csv\"), index_col=0)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13d3a01f-a233-4f90-8b8c-6409bafdcaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"seq\"] = train_data[\"heavy\"] + train_data[\"light\"]\n",
    "valid_data[\"seq\"] = valid_data[\"heavy\"] + valid_data[\"light\"]\n",
    "test_data[\"seq\"] = test_data[\"heavy\"] + test_data[\"light\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ceab599-bc63-4238-b996-8fb6d70409c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config = {\n",
    "  \"learning_rate\": 1e-04,\n",
    "  \"epochs\": 65,\n",
    "  \"batch_size\": 32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "513d219a-b643-406f-ac1d-966ecdf7293c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_01_25-16:32:40] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_01_25-16:32:40] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_01_25-16:32:40] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "11/11 [==============================] - 97s 8s/step - loss: 0.7128 - val_loss: 0.5163 - lr: 0.0100\n",
      "Epoch 2/40\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5223\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "11/11 [==============================] - 87s 8s/step - loss: 0.5223 - val_loss: 0.5215 - lr: 0.0100\n",
      "Epoch 3/40\n",
      "11/11 [==============================] - 88s 8s/step - loss: 0.4733 - val_loss: 0.4881 - lr: 0.0025\n",
      "Epoch 4/40\n",
      "11/11 [==============================] - 88s 8s/step - loss: 0.4678 - val_loss: 0.4809 - lr: 0.0025\n",
      "Epoch 5/40\n",
      "11/11 [==============================] - 87s 8s/step - loss: 0.4583 - val_loss: 0.4767 - lr: 0.0025\n",
      "Epoch 6/40\n",
      "11/11 [==============================] - 88s 8s/step - loss: 0.4537 - val_loss: 0.4747 - lr: 0.0025\n",
      "Epoch 7/40\n",
      "11/11 [==============================] - 88s 8s/step - loss: 0.4525 - val_loss: 0.4691 - lr: 0.0025\n",
      "Epoch 8/40\n",
      " 3/11 [=======>......................] - ETA: 1:02 - loss: 0.4423"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/pbs.35869.lich-compute.vscht.cz/ipykernel_15301/2377173464.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data['seq'], train_data['Y'], valid_data['seq'], valid_data['Y'], \\\n\u001b[1;32m      2\u001b[0m         \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs_per_stage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-04\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbegin_with_frozen_pretrained_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = 1e-05, callbacks = training_callbacks)\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/proteinbert/finetuning.py\u001b[0m in \u001b[0;36mfinetune\u001b[0;34m(model_generator, input_encoder, output_spec, train_seqs, train_raw_Y, valid_seqs, valid_raw_Y, seq_len, batch_size, max_epochs_per_stage, lr, begin_with_frozen_pretrained_layers, lr_with_frozen_pretrained_layers, n_final_epochs, final_seq_len, final_lr, callbacks)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training with frozen pretrained layers...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         model_generator.train(encoded_train_set, encoded_valid_set, seq_len, batch_size, max_epochs_per_stage, lr = lr_with_frozen_pretrained_layers, \\\n\u001b[0;32m---> 53\u001b[0;31m                 callbacks = callbacks, freeze_pretrained_layers = True)\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training the entire fine-tuned model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/proteinbert/model_generation.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, encoded_train_set, encoded_valid_set, seq_len, batch_size, n_epochs, lr, callbacks, **create_model_kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         model.fit(train_X, train_Y, sample_weight = train_sample_weigths, batch_size = batch_size, epochs = n_epochs, validation_data = encoded_valid_set, \\\n\u001b[0;32m---> 30\u001b[0;31m                 callbacks = callbacks)\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/wandb/integration/keras/keras.py\u001b[0m in \u001b[0;36mnew_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0mset_wandb_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mold_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0mtraining_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_fit_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data['seq'], train_data['Y'], valid_data['seq'], valid_data['Y'], \\\n",
    "        seq_len = 512, batch_size = 128, max_epochs_per_stage = 40, lr = 1e-04, begin_with_frozen_pretrained_layers = True, \\\n",
    "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = 1e-05, callbacks = training_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ca778fa-d9a6-41d1-b4ab-8b280a05c590",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "        start_seq_len = 512, start_batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75e592d1-75bb-490a-a91e-9a6d3f4f6972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-set performance:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># records</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model seq len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>119</td>\n",
       "      <td>0.936368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>119</td>\n",
       "      <td>0.936368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               # records       AUC\n",
       "Model seq len                     \n",
       "512                  119  0.936368\n",
       "All                  119  0.936368"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  96   0\n",
       "1  13  10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Test-set performance:')\n",
    "display(results)\n",
    "\n",
    "print('Confusion matrix:')\n",
    "display(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3cedffd7-4882-4f29-879c-9d05936995a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6060606060606061"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = 10 / (10 + 0.5* (13 + 0))\n",
    "# TN / (TN + 0.5 * (FP + FN))\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f263713-8459-4eee-a37e-5d15fe8a784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = model_generator.create_model(seq_len = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5a9eafe-dcef-4be1-a375-8d200ae7a922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-25 15:36:30.188640: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/batch_32_lr_1e-4_2022_01_25.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/batch_32_lr_1e-4_2022_01_25.pkl/assets\n",
      "/home/brazdilv/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/home/brazdilv/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    }
   ],
   "source": [
    "mod.save(path.join(DATA_DIR, \"protein_bert/batch_32_lr_1e-4_2022_01_25.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbfa05eb-f86e-4053-95c7-6cd3be27019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(i, lr, epochs, batch_size):\n",
    "    wandb.init(project=f\"ProteinBERT_{i}\", entity=\"kvetab\")\n",
    "    model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)\n",
    "    training_callbacks = [\n",
    "        keras.callbacks.ReduceLROnPlateau(patience = 1, factor = 0.25, min_lr = 1e-05, verbose = 1),\n",
    "        keras.callbacks.EarlyStopping(patience = 2, restore_best_weights = True),\n",
    "        WandbCallback()\n",
    "    ]\n",
    "    \n",
    "    wandb.config = {\n",
    "      \"learning_rate\": lr,\n",
    "      \"epochs\": epochs * 2,\n",
    "      \"batch_size\": batch_size\n",
    "    }\n",
    "    \n",
    "    finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data['seq'], train_data['Y'], valid_data['seq'], valid_data['Y'], \\\n",
    "        seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epochs, lr = lr, begin_with_frozen_pretrained_layers = True, \\\n",
    "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 512, final_lr = lr / 10, callbacks = training_callbacks)\n",
    "    results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "        start_seq_len = 512, start_batch_size = batch_size)\n",
    "    print(f\"Model number {i} trained with learning rate {lr}:\")\n",
    "    print('Test-set performance:')\n",
    "    display(results)\n",
    "\n",
    "    print('Confusion matrix:')\n",
    "    display(confusion_matrix)\n",
    "    mod = model_generator.create_model(seq_len = 512)\n",
    "    mod.save(path.join(DATA_DIR, f\"protein_bert/{i}_batch_{batch_size}_lr_{lr}_2022_01_25.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67973463-5237-43c2-a3f2-aeacae1fa8be",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_01_26-17:23:10] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_01_26-17:23:11] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_01_26-17:23:11] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-26 17:23:11.044361: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "/home/brazdilv/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "21/21 [==============================] - 113s 5s/step - loss: 0.7342 - val_loss: 0.6132 - lr: 0.0100\n",
      "Epoch 2/50\n",
      "21/21 [==============================] - 90s 4s/step - loss: 0.5042 - val_loss: 0.5167 - lr: 0.0100\n",
      "Epoch 3/50\n",
      "21/21 [==============================] - 90s 4s/step - loss: 0.5016 - val_loss: 0.4712 - lr: 0.0100\n",
      "Epoch 4/50\n",
      "21/21 [==============================] - 90s 4s/step - loss: 0.4621 - val_loss: 0.4606 - lr: 0.0100\n",
      "Epoch 5/50\n",
      "21/21 [==============================] - 100s 5s/step - loss: 0.4341 - val_loss: 0.4351 - lr: 0.0100\n",
      "Epoch 6/50\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.4177 - val_loss: 0.4287 - lr: 0.0100\n",
      "Epoch 7/50\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.4200 - val_loss: 0.4166 - lr: 0.0100\n",
      "Epoch 8/50\n",
      "21/21 [==============================] - 97s 5s/step - loss: 0.4277 - val_loss: 0.4164 - lr: 0.0100\n",
      "Epoch 9/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4395\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "21/21 [==============================] - 88s 4s/step - loss: 0.4395 - val_loss: 0.4215 - lr: 0.0100\n",
      "Epoch 10/50\n",
      "21/21 [==============================] - 87s 4s/step - loss: 0.3935 - val_loss: 0.4155 - lr: 0.0025\n",
      "Epoch 11/50\n",
      "21/21 [==============================] - 97s 5s/step - loss: 0.4053 - val_loss: 0.4090 - lr: 0.0025\n",
      "Epoch 12/50\n",
      "21/21 [==============================] - 95s 5s/step - loss: 0.3905 - val_loss: 0.4084 - lr: 0.0025\n",
      "Epoch 13/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3811\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "21/21 [==============================] - 89s 4s/step - loss: 0.3811 - val_loss: 0.4146 - lr: 0.0025\n",
      "Epoch 14/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3890\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "21/21 [==============================] - 90s 4s/step - loss: 0.3890 - val_loss: 0.4085 - lr: 6.2500e-04\n",
      "[2022_01_26-17:45:08] Training the entire fine-tuned model...\n",
      "[2022_01_26-17:45:19] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/50\n",
      "21/21 [==============================] - 284s 13s/step - loss: 0.3841 - val_loss: 0.4073 - lr: 1.0000e-05\n",
      "Epoch 2/50\n",
      "21/21 [==============================] - 281s 13s/step - loss: 0.3752 - val_loss: 0.4066 - lr: 1.0000e-05\n",
      "Epoch 3/50\n",
      "21/21 [==============================] - 276s 13s/step - loss: 0.3825 - val_loss: 0.4076 - lr: 1.0000e-05\n",
      "Epoch 4/50\n",
      "21/21 [==============================] - 268s 13s/step - loss: 0.3896 - val_loss: 0.4047 - lr: 1.0000e-05\n",
      "Epoch 5/50\n",
      "21/21 [==============================] - 254s 12s/step - loss: 0.3799 - val_loss: 0.4026 - lr: 1.0000e-05\n",
      "Epoch 6/50\n",
      "21/21 [==============================] - 274s 13s/step - loss: 0.3789 - val_loss: 0.4017 - lr: 1.0000e-05\n",
      "Epoch 7/50\n",
      "21/21 [==============================] - 276s 13s/step - loss: 0.3746 - val_loss: 0.4012 - lr: 1.0000e-05\n",
      "Epoch 8/50\n",
      "21/21 [==============================] - 280s 13s/step - loss: 0.3711 - val_loss: 0.4000 - lr: 1.0000e-05\n",
      "Epoch 9/50\n",
      "21/21 [==============================] - 271s 13s/step - loss: 0.3818 - val_loss: 0.4003 - lr: 1.0000e-05\n",
      "Epoch 10/50\n",
      "21/21 [==============================] - 286s 14s/step - loss: 0.3743 - val_loss: 0.3990 - lr: 1.0000e-05\n",
      "Epoch 11/50\n",
      "21/21 [==============================] - 268s 13s/step - loss: 0.3778 - val_loss: 0.3980 - lr: 1.0000e-05\n",
      "Epoch 12/50\n",
      "21/21 [==============================] - 268s 13s/step - loss: 0.3718 - val_loss: 0.3981 - lr: 1.0000e-05\n",
      "Epoch 13/50\n",
      "21/21 [==============================] - 271s 13s/step - loss: 0.3744 - val_loss: 0.3965 - lr: 1.0000e-05\n",
      "Epoch 14/50\n",
      "21/21 [==============================] - 266s 13s/step - loss: 0.3566 - val_loss: 0.3955 - lr: 1.0000e-05\n",
      "Epoch 15/50\n",
      "21/21 [==============================] - 290s 14s/step - loss: 0.3573 - val_loss: 0.4018 - lr: 1.0000e-05\n",
      "Epoch 16/50\n",
      "21/21 [==============================] - 278s 13s/step - loss: 0.3654 - val_loss: 0.3937 - lr: 1.0000e-05\n",
      "Epoch 17/50\n",
      "21/21 [==============================] - 285s 14s/step - loss: 0.3642 - val_loss: 0.3936 - lr: 1.0000e-05\n",
      "Epoch 18/50\n",
      "21/21 [==============================] - 280s 13s/step - loss: 0.3574 - val_loss: 0.3967 - lr: 1.0000e-05\n",
      "Epoch 19/50\n",
      "21/21 [==============================] - 313s 15s/step - loss: 0.3603 - val_loss: 0.3916 - lr: 1.0000e-05\n",
      "Epoch 20/50\n",
      "21/21 [==============================] - 287s 14s/step - loss: 0.3631 - val_loss: 0.3990 - lr: 1.0000e-05\n",
      "Epoch 21/50\n",
      "21/21 [==============================] - 278s 13s/step - loss: 0.3576 - val_loss: 0.3901 - lr: 1.0000e-05\n",
      "Epoch 22/50\n",
      "21/21 [==============================] - 277s 13s/step - loss: 0.3627 - val_loss: 0.3888 - lr: 1.0000e-05\n",
      "Epoch 23/50\n",
      "21/21 [==============================] - 265s 13s/step - loss: 0.3572 - val_loss: 0.3871 - lr: 1.0000e-05\n",
      "Epoch 24/50\n",
      "21/21 [==============================] - 268s 13s/step - loss: 0.3516 - val_loss: 0.3919 - lr: 1.0000e-05\n",
      "Epoch 25/50\n",
      "21/21 [==============================] - 274s 13s/step - loss: 0.3455 - val_loss: 0.3874 - lr: 1.0000e-05\n",
      "[2022_01_26-19:40:35] Training on final epochs of sequence length 512...\n",
      "[2022_01_26-19:40:35] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_01_26-19:40:51] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "21/21 [==============================] - 290s 13s/step - loss: 0.3502 - val_loss: 0.3871 - lr: 1.0000e-06\n",
      "Model number 00 trained with learning rate 1e-05:\n",
      "Test-set performance:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># records</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model seq len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>119</td>\n",
       "      <td>0.939538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>119</td>\n",
       "      <td>0.939538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               # records       AUC\n",
       "Model seq len                     \n",
       "512                  119  0.939538\n",
       "All                  119  0.939538"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  96   0\n",
       "1  12  11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "2022-01-26 19:46:14.901117: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/00_batch_64_lr_1e-05_2022_01_25.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/00_batch_64_lr_1e-05_2022_01_25.pkl/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_01_26-19:46:27] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/home/brazdilv/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_01_26-19:46:27] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_01_26-19:46:27] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "21/21 [==============================] - 111s 5s/step - loss: 0.6730 - val_loss: 0.6027 - lr: 0.0100\n",
      "Epoch 2/50\n",
      "21/21 [==============================] - 95s 4s/step - loss: 0.5148 - val_loss: 0.4726 - lr: 0.0100\n",
      "Epoch 3/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4569\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "21/21 [==============================] - 88s 4s/step - loss: 0.4569 - val_loss: 0.5062 - lr: 0.0100\n",
      "Epoch 4/50\n",
      "21/21 [==============================] - 94s 5s/step - loss: 0.4348 - val_loss: 0.4506 - lr: 0.0025\n",
      "Epoch 5/50\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.4316 - val_loss: 0.4473 - lr: 0.0025\n",
      "Epoch 6/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4187\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "21/21 [==============================] - 106s 5s/step - loss: 0.4187 - val_loss: 0.4534 - lr: 0.0025\n",
      "Epoch 7/50\n",
      "21/21 [==============================] - 104s 5s/step - loss: 0.4246 - val_loss: 0.4435 - lr: 6.2500e-04\n",
      "Epoch 8/50\n",
      "21/21 [==============================] - 100s 5s/step - loss: 0.4132 - val_loss: 0.4429 - lr: 6.2500e-04\n",
      "Epoch 9/50\n",
      "21/21 [==============================] - 96s 5s/step - loss: 0.4180 - val_loss: 0.4415 - lr: 6.2500e-04\n",
      "Epoch 10/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4223\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "21/21 [==============================] - 104s 5s/step - loss: 0.4223 - val_loss: 0.4415 - lr: 6.2500e-04\n",
      "Epoch 11/50\n",
      "21/21 [==============================] - 93s 4s/step - loss: 0.4158 - val_loss: 0.4402 - lr: 1.5625e-04\n",
      "Epoch 12/50\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.4191 - val_loss: 0.4399 - lr: 1.5625e-04\n",
      "Epoch 13/50\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.4167 - val_loss: 0.4397 - lr: 1.5625e-04\n",
      "Epoch 14/50\n",
      "21/21 [==============================] - 95s 4s/step - loss: 0.4146 - val_loss: 0.4394 - lr: 1.5625e-04\n",
      "Epoch 15/50\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.4188 - val_loss: 0.4391 - lr: 1.5625e-04\n",
      "Epoch 16/50\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.4076 - val_loss: 0.4389 - lr: 1.5625e-04\n",
      "Epoch 17/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4246\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "21/21 [==============================] - 88s 4s/step - loss: 0.4246 - val_loss: 0.4389 - lr: 1.5625e-04\n",
      "Epoch 18/50\n",
      "21/21 [==============================] - 93s 4s/step - loss: 0.4181 - val_loss: 0.4387 - lr: 3.9062e-05\n",
      "Epoch 19/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4179\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.4179 - val_loss: 0.4387 - lr: 3.9062e-05\n",
      "Epoch 20/50\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.4112 - val_loss: 0.4387 - lr: 1.0000e-05\n",
      "Epoch 21/50\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.4171 - val_loss: 0.4387 - lr: 1.0000e-05\n",
      "Epoch 22/50\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.4168 - val_loss: 0.4386 - lr: 1.0000e-05\n",
      "Epoch 23/50\n",
      "21/21 [==============================] - 86s 4s/step - loss: 0.4113 - val_loss: 0.4386 - lr: 1.0000e-05\n",
      "Epoch 24/50\n",
      "21/21 [==============================] - 95s 5s/step - loss: 0.4190 - val_loss: 0.4386 - lr: 1.0000e-05\n",
      "Epoch 25/50\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.4181 - val_loss: 0.4386 - lr: 1.0000e-05\n",
      "Epoch 26/50\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.4190 - val_loss: 0.4386 - lr: 1.0000e-05\n",
      "Epoch 27/50\n",
      "21/21 [==============================] - 104s 5s/step - loss: 0.4119 - val_loss: 0.4386 - lr: 1.0000e-05\n",
      "Epoch 28/50\n",
      "21/21 [==============================] - 94s 4s/step - loss: 0.4211 - val_loss: 0.4385 - lr: 1.0000e-05\n",
      "Epoch 29/50\n",
      "21/21 [==============================] - 93s 4s/step - loss: 0.4096 - val_loss: 0.4385 - lr: 1.0000e-05\n",
      "Epoch 30/50\n",
      "21/21 [==============================] - 86s 4s/step - loss: 0.4195 - val_loss: 0.4385 - lr: 1.0000e-05\n",
      "Epoch 31/50\n",
      "21/21 [==============================] - 93s 4s/step - loss: 0.4102 - val_loss: 0.4385 - lr: 1.0000e-05\n",
      "Epoch 32/50\n",
      "21/21 [==============================] - 100s 5s/step - loss: 0.4136 - val_loss: 0.4385 - lr: 1.0000e-05\n",
      "Epoch 33/50\n",
      "21/21 [==============================] - 89s 4s/step - loss: 0.4209 - val_loss: 0.4385 - lr: 1.0000e-05\n",
      "Epoch 34/50\n",
      "21/21 [==============================] - 89s 4s/step - loss: 0.4055 - val_loss: 0.4385 - lr: 1.0000e-05\n",
      "Epoch 35/50\n",
      "21/21 [==============================] - 89s 4s/step - loss: 0.4101 - val_loss: 0.4384 - lr: 1.0000e-05\n",
      "Epoch 36/50\n",
      "21/21 [==============================] - 90s 4s/step - loss: 0.4071 - val_loss: 0.4384 - lr: 1.0000e-05\n",
      "Epoch 37/50\n",
      "21/21 [==============================] - 95s 4s/step - loss: 0.4146 - val_loss: 0.4384 - lr: 1.0000e-05\n",
      "Epoch 38/50\n",
      "21/21 [==============================] - 95s 5s/step - loss: 0.4105 - val_loss: 0.4384 - lr: 1.0000e-05\n",
      "Epoch 39/50\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.4175 - val_loss: 0.4384 - lr: 1.0000e-05\n",
      "Epoch 40/50\n",
      "21/21 [==============================] - 100s 5s/step - loss: 0.4194 - val_loss: 0.4384 - lr: 1.0000e-05\n",
      "Epoch 41/50\n",
      "21/21 [==============================] - 90s 4s/step - loss: 0.4137 - val_loss: 0.4383 - lr: 1.0000e-05\n",
      "Epoch 42/50\n",
      "21/21 [==============================] - 88s 4s/step - loss: 0.4153 - val_loss: 0.4383 - lr: 1.0000e-05\n",
      "Epoch 43/50\n",
      "21/21 [==============================] - 97s 5s/step - loss: 0.4150 - val_loss: 0.4383 - lr: 1.0000e-05\n",
      "Epoch 44/50\n",
      "21/21 [==============================] - 89s 4s/step - loss: 0.4160 - val_loss: 0.4383 - lr: 1.0000e-05\n",
      "Epoch 45/50\n",
      "21/21 [==============================] - 86s 4s/step - loss: 0.4199 - val_loss: 0.4383 - lr: 1.0000e-05\n",
      "Epoch 46/50\n",
      "21/21 [==============================] - 90s 4s/step - loss: 0.4109 - val_loss: 0.4383 - lr: 1.0000e-05\n",
      "Epoch 47/50\n",
      "21/21 [==============================] - 94s 5s/step - loss: 0.4113 - val_loss: 0.4383 - lr: 1.0000e-05\n",
      "Epoch 48/50\n",
      "21/21 [==============================] - 81s 4s/step - loss: 0.4117 - val_loss: 0.4382 - lr: 1.0000e-05\n",
      "Epoch 49/50\n",
      "21/21 [==============================] - 86s 4s/step - loss: 0.4198 - val_loss: 0.4382 - lr: 1.0000e-05\n",
      "Epoch 50/50\n",
      "21/21 [==============================] - 95s 5s/step - loss: 0.4154 - val_loss: 0.4382 - lr: 1.0000e-05\n",
      "[2022_01_26-21:06:01] Training the entire fine-tuned model...\n",
      "[2022_01_26-21:06:12] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/50\n",
      "21/21 [==============================] - 277s 13s/step - loss: 0.4233 - val_loss: 0.4351 - lr: 5.0000e-05\n",
      "Epoch 2/50\n",
      "21/21 [==============================] - 281s 13s/step - loss: 0.4128 - val_loss: 0.4328 - lr: 5.0000e-05\n",
      "Epoch 3/50\n",
      "21/21 [==============================] - 257s 12s/step - loss: 0.4116 - val_loss: 0.4293 - lr: 5.0000e-05\n",
      "Epoch 4/50\n",
      "21/21 [==============================] - 271s 13s/step - loss: 0.4085 - val_loss: 0.4279 - lr: 5.0000e-05\n",
      "Epoch 5/50\n",
      "21/21 [==============================] - 274s 13s/step - loss: 0.3895 - val_loss: 0.4246 - lr: 5.0000e-05\n",
      "Epoch 6/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3973 \n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "21/21 [==============================] - 277s 13s/step - loss: 0.3973 - val_loss: 0.4326 - lr: 5.0000e-05\n",
      "Epoch 7/50\n",
      "21/21 [==============================] - 269s 13s/step - loss: 0.3872 - val_loss: 0.4189 - lr: 1.2500e-05\n",
      "Epoch 8/50\n",
      "21/21 [==============================] - 274s 13s/step - loss: 0.3760 - val_loss: 0.4176 - lr: 1.2500e-05\n",
      "Epoch 9/50\n",
      "21/21 [==============================] - 275s 13s/step - loss: 0.3833 - val_loss: 0.4169 - lr: 1.2500e-05\n",
      "Epoch 10/50\n",
      "21/21 [==============================] - 280s 13s/step - loss: 0.3797 - val_loss: 0.4162 - lr: 1.2500e-05\n",
      "Epoch 11/50\n",
      "21/21 [==============================] - 264s 13s/step - loss: 0.3810 - val_loss: 0.4144 - lr: 1.2500e-05\n",
      "Epoch 12/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3779 \n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "21/21 [==============================] - 289s 14s/step - loss: 0.3779 - val_loss: 0.4156 - lr: 1.2500e-05\n",
      "Epoch 13/50\n",
      "21/21 [==============================] - 311s 15s/step - loss: 0.3776 - val_loss: 0.4134 - lr: 1.0000e-05\n",
      "Epoch 14/50\n",
      "21/21 [==============================] - 276s 13s/step - loss: 0.3643 - val_loss: 0.4119 - lr: 1.0000e-05\n",
      "Epoch 15/50\n",
      "21/21 [==============================] - 276s 13s/step - loss: 0.3590 - val_loss: 0.4128 - lr: 1.0000e-05\n",
      "Epoch 16/50\n",
      "21/21 [==============================] - 291s 14s/step - loss: 0.3631 - val_loss: 0.4114 - lr: 1.0000e-05\n",
      "Epoch 17/50\n",
      "21/21 [==============================] - 274s 13s/step - loss: 0.3578 - val_loss: 0.4111 - lr: 1.0000e-05\n",
      "Epoch 18/50\n",
      "21/21 [==============================] - 281s 14s/step - loss: 0.3608 - val_loss: 0.4109 - lr: 1.0000e-05\n",
      "Epoch 19/50\n",
      "21/21 [==============================] - 273s 13s/step - loss: 0.3562 - val_loss: 0.4119 - lr: 1.0000e-05\n",
      "Epoch 20/50\n",
      "21/21 [==============================] - 291s 14s/step - loss: 0.3598 - val_loss: 0.4104 - lr: 1.0000e-05\n",
      "Epoch 21/50\n",
      "21/21 [==============================] - 267s 13s/step - loss: 0.3531 - val_loss: 0.4099 - lr: 1.0000e-05\n",
      "Epoch 22/50\n",
      "21/21 [==============================] - 302s 14s/step - loss: 0.3404 - val_loss: 0.4172 - lr: 1.0000e-05\n",
      "Epoch 23/50\n",
      "21/21 [==============================] - 287s 14s/step - loss: 0.3460 - val_loss: 0.4099 - lr: 1.0000e-05\n",
      "Epoch 24/50\n",
      "21/21 [==============================] - 279s 13s/step - loss: 0.3505 - val_loss: 0.4089 - lr: 1.0000e-05\n",
      "Epoch 25/50\n",
      "21/21 [==============================] - 291s 14s/step - loss: 0.3366 - val_loss: 0.4135 - lr: 1.0000e-05\n",
      "Epoch 26/50\n",
      "21/21 [==============================] - 291s 14s/step - loss: 0.3436 - val_loss: 0.4067 - lr: 1.0000e-05\n",
      "Epoch 27/50\n",
      "21/21 [==============================] - 264s 13s/step - loss: 0.3456 - val_loss: 0.4114 - lr: 1.0000e-05\n",
      "Epoch 28/50\n",
      "21/21 [==============================] - 283s 13s/step - loss: 0.3308 - val_loss: 0.4108 - lr: 1.0000e-05\n",
      "[2022_01_26-23:16:37] Training on final epochs of sequence length 512...\n",
      "[2022_01_26-23:16:37] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_01_26-23:16:37] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "21/21 [==============================] - 287s 13s/step - loss: 0.3389 - val_loss: 0.4077 - lr: 5.0000e-06\n",
      "Model number 01 trained with learning rate 5e-05:\n",
      "Test-set performance:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># records</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model seq len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>119</td>\n",
       "      <td>0.938179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>119</td>\n",
       "      <td>0.938179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               # records       AUC\n",
       "Model seq len                     \n",
       "512                  119  0.938179\n",
       "All                  119  0.938179"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  95   1\n",
       "1  13  10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/01_batch_64_lr_5e-05_2022_01_25.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/01_batch_64_lr_5e-05_2022_01_25.pkl/assets\n",
      "/home/brazdilv/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/home/brazdilv/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_01_26-23:22:52] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_01_26-23:22:52] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_01_26-23:22:52] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "21/21 [==============================] - 105s 5s/step - loss: 0.7489 - val_loss: 0.5243 - lr: 0.0100\n",
      "Epoch 2/50\n",
      "21/21 [==============================] - 100s 5s/step - loss: 0.4953 - val_loss: 0.4746 - lr: 0.0100\n",
      "Epoch 3/50\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.4417 - val_loss: 0.4660 - lr: 0.0100\n",
      "Epoch 4/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4679\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "21/21 [==============================] - 96s 5s/step - loss: 0.4679 - val_loss: 0.5355 - lr: 0.0100\n",
      "Epoch 5/50\n",
      "21/21 [==============================] - 101s 5s/step - loss: 0.4519 - val_loss: 0.4452 - lr: 0.0025\n",
      "Epoch 6/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4234\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "21/21 [==============================] - 93s 4s/step - loss: 0.4234 - val_loss: 0.4452 - lr: 0.0025\n",
      "Epoch 7/50\n",
      "21/21 [==============================] - 106s 5s/step - loss: 0.4180 - val_loss: 0.4415 - lr: 6.2500e-04\n",
      "Epoch 8/50\n",
      "21/21 [==============================] - 109s 5s/step - loss: 0.4236 - val_loss: 0.4407 - lr: 6.2500e-04\n",
      "Epoch 9/50\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.4113 - val_loss: 0.4396 - lr: 6.2500e-04\n",
      "Epoch 10/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4076\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "21/21 [==============================] - 94s 5s/step - loss: 0.4076 - val_loss: 0.4400 - lr: 6.2500e-04\n",
      "Epoch 11/50\n",
      "21/21 [==============================] - 106s 5s/step - loss: 0.4108 - val_loss: 0.4385 - lr: 1.5625e-04\n",
      "Epoch 12/50\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.4144 - val_loss: 0.4384 - lr: 1.5625e-04\n",
      "Epoch 13/50\n",
      "21/21 [==============================] - 89s 4s/step - loss: 0.4219 - val_loss: 0.4382 - lr: 1.5625e-04\n",
      "Epoch 14/50\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.4134 - val_loss: 0.4380 - lr: 1.5625e-04\n",
      "Epoch 15/50\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.4127 - val_loss: 0.4377 - lr: 1.5625e-04\n",
      "Epoch 16/50\n",
      "21/21 [==============================] - 95s 5s/step - loss: 0.4134 - val_loss: 0.4376 - lr: 1.5625e-04\n",
      "Epoch 17/50\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.4121 - val_loss: 0.4372 - lr: 1.5625e-04\n",
      "Epoch 18/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4141\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "21/21 [==============================] - 95s 5s/step - loss: 0.4141 - val_loss: 0.4372 - lr: 1.5625e-04\n",
      "Epoch 19/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4103\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.4103 - val_loss: 0.4372 - lr: 3.9062e-05\n",
      "Epoch 20/50\n",
      "21/21 [==============================] - 94s 5s/step - loss: 0.4135 - val_loss: 0.4372 - lr: 1.0000e-05\n",
      "Epoch 21/50\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.4040 - val_loss: 0.4372 - lr: 1.0000e-05\n",
      "Epoch 22/50\n",
      "21/21 [==============================] - 104s 5s/step - loss: 0.4093 - val_loss: 0.4371 - lr: 1.0000e-05\n",
      "Epoch 23/50\n",
      "21/21 [==============================] - 87s 4s/step - loss: 0.4113 - val_loss: 0.4371 - lr: 1.0000e-05\n",
      "Epoch 24/50\n",
      "21/21 [==============================] - 97s 5s/step - loss: 0.4088 - val_loss: 0.4371 - lr: 1.0000e-05\n",
      "Epoch 25/50\n",
      "21/21 [==============================] - 90s 4s/step - loss: 0.4120 - val_loss: 0.4371 - lr: 1.0000e-05\n",
      "Epoch 26/50\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.4111 - val_loss: 0.4371 - lr: 1.0000e-05\n",
      "Epoch 27/50\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.4108 - val_loss: 0.4371 - lr: 1.0000e-05\n",
      "Epoch 28/50\n",
      "21/21 [==============================] - 95s 5s/step - loss: 0.4107 - val_loss: 0.4370 - lr: 1.0000e-05\n",
      "Epoch 29/50\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.4128 - val_loss: 0.4370 - lr: 1.0000e-05\n",
      "Epoch 30/50\n",
      "21/21 [==============================] - 100s 5s/step - loss: 0.4128 - val_loss: 0.4370 - lr: 1.0000e-05\n",
      "Epoch 31/50\n",
      "21/21 [==============================] - 93s 4s/step - loss: 0.4085 - val_loss: 0.4370 - lr: 1.0000e-05\n",
      "Epoch 32/50\n",
      "21/21 [==============================] - 95s 5s/step - loss: 0.4166 - val_loss: 0.4370 - lr: 1.0000e-05\n",
      "Epoch 33/50\n",
      "21/21 [==============================] - 101s 5s/step - loss: 0.4096 - val_loss: 0.4370 - lr: 1.0000e-05\n",
      "Epoch 34/50\n",
      "21/21 [==============================] - 84s 4s/step - loss: 0.4055 - val_loss: 0.4369 - lr: 1.0000e-05\n",
      "Epoch 35/50\n",
      "21/21 [==============================] - 95s 5s/step - loss: 0.4091 - val_loss: 0.4369 - lr: 1.0000e-05\n",
      "Epoch 36/50\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.4073 - val_loss: 0.4369 - lr: 1.0000e-05\n",
      "Epoch 37/50\n",
      "21/21 [==============================] - 94s 4s/step - loss: 0.4077 - val_loss: 0.4369 - lr: 1.0000e-05\n",
      "Epoch 38/50\n",
      "21/21 [==============================] - 97s 5s/step - loss: 0.4103 - val_loss: 0.4369 - lr: 1.0000e-05\n",
      "Epoch 39/50\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.4130 - val_loss: 0.4369 - lr: 1.0000e-05\n",
      "Epoch 40/50\n",
      "21/21 [==============================] - 104s 5s/step - loss: 0.4080 - val_loss: 0.4369 - lr: 1.0000e-05\n",
      "Epoch 41/50\n",
      "21/21 [==============================] - 93s 4s/step - loss: 0.4085 - val_loss: 0.4369 - lr: 1.0000e-05\n",
      "Epoch 42/50\n",
      "21/21 [==============================] - 93s 4s/step - loss: 0.4090 - val_loss: 0.4369 - lr: 1.0000e-05\n",
      "Epoch 43/50\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.4079 - val_loss: 0.4369 - lr: 1.0000e-05\n",
      "Epoch 44/50\n",
      "21/21 [==============================] - 94s 4s/step - loss: 0.4123 - val_loss: 0.4368 - lr: 1.0000e-05\n",
      "Epoch 45/50\n",
      "21/21 [==============================] - 88s 4s/step - loss: 0.4121 - val_loss: 0.4368 - lr: 1.0000e-05\n",
      "Epoch 46/50\n",
      "21/21 [==============================] - 93s 4s/step - loss: 0.4123 - val_loss: 0.4368 - lr: 1.0000e-05\n",
      "Epoch 47/50\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.4100 - val_loss: 0.4368 - lr: 1.0000e-05\n",
      "Epoch 48/50\n",
      "21/21 [==============================] - 86s 4s/step - loss: 0.4088 - val_loss: 0.4368 - lr: 1.0000e-05\n",
      "[2022_01_27-00:39:44] Training the entire fine-tuned model...\n",
      "[2022_01_27-00:40:39] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/50\n",
      "21/21 [==============================] - 290s 13s/step - loss: 0.4097 - val_loss: 0.4421 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4060 \n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "21/21 [==============================] - 280s 13s/step - loss: 0.4060 - val_loss: 0.4439 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "21/21 [==============================] - 295s 14s/step - loss: 0.4095 - val_loss: 0.4223 - lr: 2.5000e-05\n",
      "Epoch 4/50\n",
      "21/21 [==============================] - 272s 13s/step - loss: 0.3949 - val_loss: 0.4189 - lr: 2.5000e-05\n",
      "Epoch 5/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3955 \n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "21/21 [==============================] - 290s 14s/step - loss: 0.3955 - val_loss: 0.4209 - lr: 2.5000e-05\n",
      "Epoch 6/50\n",
      "21/21 [==============================] - 266s 13s/step - loss: 0.3862 - val_loss: 0.4188 - lr: 1.0000e-05\n",
      "Epoch 7/50\n",
      "21/21 [==============================] - 322s 15s/step - loss: 0.3853 - val_loss: 0.4171 - lr: 1.0000e-05\n",
      "Epoch 8/50\n",
      "21/21 [==============================] - 313s 15s/step - loss: 0.3834 - val_loss: 0.4151 - lr: 1.0000e-05\n",
      "Epoch 9/50\n",
      "21/21 [==============================] - 280s 13s/step - loss: 0.3794 - val_loss: 0.4141 - lr: 1.0000e-05\n",
      "Epoch 10/50\n",
      "21/21 [==============================] - 287s 14s/step - loss: 0.3789 - val_loss: 0.4133 - lr: 1.0000e-05\n",
      "Epoch 11/50\n",
      "21/21 [==============================] - 286s 14s/step - loss: 0.3794 - val_loss: 0.4125 - lr: 1.0000e-05\n",
      "Epoch 12/50\n",
      "21/21 [==============================] - 289s 14s/step - loss: 0.3793 - val_loss: 0.4122 - lr: 1.0000e-05\n",
      "Epoch 13/50\n",
      "21/21 [==============================] - 277s 13s/step - loss: 0.3804 - val_loss: 0.4139 - lr: 1.0000e-05\n",
      "Epoch 14/50\n",
      "21/21 [==============================] - 268s 13s/step - loss: 0.3727 - val_loss: 0.4105 - lr: 1.0000e-05\n",
      "Epoch 15/50\n",
      "21/21 [==============================] - 273s 13s/step - loss: 0.3767 - val_loss: 0.4107 - lr: 1.0000e-05\n",
      "Epoch 16/50\n",
      "21/21 [==============================] - 310s 15s/step - loss: 0.3697 - val_loss: 0.4113 - lr: 1.0000e-05\n",
      "[2022_01_27-01:57:20] Training on final epochs of sequence length 512...\n",
      "[2022_01_27-01:57:20] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_01_27-01:58:10] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "21/21 [==============================] - 282s 13s/step - loss: 0.3746 - val_loss: 0.4093 - lr: 1.0000e-05\n",
      "Model number 02 trained with learning rate 0.0001:\n",
      "Test-set performance:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># records</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model seq len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>119</td>\n",
       "      <td>0.922781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>119</td>\n",
       "      <td>0.922781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               # records       AUC\n",
       "Model seq len                     \n",
       "512                  119  0.922781\n",
       "All                  119  0.922781"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  96  0\n",
       "1  14  9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/02_batch_64_lr_0.0001_2022_01_25.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/02_batch_64_lr_0.0001_2022_01_25.pkl/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_01_27-02:04:20] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/home/brazdilv/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_01_27-02:04:20] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_01_27-02:04:20] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "21/21 [==============================] - 108s 5s/step - loss: 0.6782 - val_loss: 0.4937 - lr: 0.0100\n",
      "Epoch 2/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4960\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "21/21 [==============================] - 94s 5s/step - loss: 0.4960 - val_loss: 0.5307 - lr: 0.0100\n",
      "Epoch 3/50\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.4574 - val_loss: 0.4719 - lr: 0.0025\n",
      "Epoch 4/50\n",
      "21/21 [==============================] - 95s 5s/step - loss: 0.4504 - val_loss: 0.4682 - lr: 0.0025\n",
      "Epoch 5/50\n",
      "21/21 [==============================] - 105s 5s/step - loss: 0.4406 - val_loss: 0.4553 - lr: 0.0025\n",
      "Epoch 6/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4353\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "21/21 [==============================] - 105s 5s/step - loss: 0.4353 - val_loss: 0.4581 - lr: 0.0025\n",
      "Epoch 7/50\n",
      "21/21 [==============================] - 101s 5s/step - loss: 0.4296 - val_loss: 0.4494 - lr: 6.2500e-04\n",
      "Epoch 8/50\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.4213 - val_loss: 0.4478 - lr: 6.2500e-04\n",
      "Epoch 9/50\n",
      "21/21 [==============================] - 100s 5s/step - loss: 0.4228 - val_loss: 0.4469 - lr: 6.2500e-04\n",
      "Epoch 10/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4290\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "21/21 [==============================] - 97s 5s/step - loss: 0.4290 - val_loss: 0.4520 - lr: 6.2500e-04\n",
      "Epoch 11/50\n",
      "21/21 [==============================] - 100s 5s/step - loss: 0.4295 - val_loss: 0.4459 - lr: 1.5625e-04\n",
      "Epoch 12/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4270\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "21/21 [==============================] - 95s 5s/step - loss: 0.4270 - val_loss: 0.4458 - lr: 1.5625e-04\n",
      "Epoch 13/50\n",
      "21/21 [==============================] - 106s 5s/step - loss: 0.4181 - val_loss: 0.4455 - lr: 3.9062e-05\n",
      "Epoch 14/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4267\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "21/21 [==============================] - 107s 5s/step - loss: 0.4267 - val_loss: 0.4454 - lr: 3.9062e-05\n",
      "Epoch 15/50\n",
      "21/21 [==============================] - 103s 5s/step - loss: 0.4156 - val_loss: 0.4454 - lr: 1.0000e-05\n",
      "Epoch 16/50\n",
      "21/21 [==============================] - 106s 5s/step - loss: 0.4217 - val_loss: 0.4454 - lr: 1.0000e-05\n",
      "Epoch 17/50\n",
      "21/21 [==============================] - 91s 4s/step - loss: 0.4257 - val_loss: 0.4454 - lr: 1.0000e-05\n",
      "Epoch 18/50\n",
      "21/21 [==============================] - 102s 5s/step - loss: 0.4242 - val_loss: 0.4453 - lr: 1.0000e-05\n",
      "Epoch 19/50\n",
      "21/21 [==============================] - 97s 5s/step - loss: 0.4202 - val_loss: 0.4453 - lr: 1.0000e-05\n",
      "Epoch 20/50\n",
      "21/21 [==============================] - 90s 4s/step - loss: 0.4272 - val_loss: 0.4453 - lr: 1.0000e-05\n",
      "Epoch 21/50\n",
      "21/21 [==============================] - 94s 4s/step - loss: 0.4197 - val_loss: 0.4453 - lr: 1.0000e-05\n",
      "Epoch 22/50\n",
      "21/21 [==============================] - 93s 4s/step - loss: 0.4225 - val_loss: 0.4453 - lr: 1.0000e-05\n",
      "Epoch 23/50\n",
      "21/21 [==============================] - 100s 5s/step - loss: 0.4224 - val_loss: 0.4453 - lr: 1.0000e-05\n",
      "Epoch 24/50\n",
      "21/21 [==============================] - 90s 4s/step - loss: 0.4264 - val_loss: 0.4453 - lr: 1.0000e-05\n",
      "Epoch 25/50\n",
      "21/21 [==============================] - 86s 4s/step - loss: 0.4302 - val_loss: 0.4452 - lr: 1.0000e-05\n",
      "Epoch 26/50\n",
      "21/21 [==============================] - 100s 5s/step - loss: 0.4307 - val_loss: 0.4452 - lr: 1.0000e-05\n",
      "Epoch 27/50\n",
      "21/21 [==============================] - 105s 5s/step - loss: 0.4187 - val_loss: 0.4452 - lr: 1.0000e-05\n",
      "Epoch 28/50\n",
      "21/21 [==============================] - 84s 4s/step - loss: 0.4293 - val_loss: 0.4452 - lr: 1.0000e-05\n",
      "Epoch 29/50\n",
      "21/21 [==============================] - 90s 4s/step - loss: 0.4224 - val_loss: 0.4452 - lr: 1.0000e-05\n",
      "Epoch 30/50\n",
      "21/21 [==============================] - 93s 4s/step - loss: 0.4232 - val_loss: 0.4451 - lr: 1.0000e-05\n",
      "Epoch 31/50\n",
      "21/21 [==============================] - 97s 5s/step - loss: 0.4233 - val_loss: 0.4451 - lr: 1.0000e-05\n",
      "Epoch 32/50\n",
      "21/21 [==============================] - 95s 5s/step - loss: 0.4253 - val_loss: 0.4451 - lr: 1.0000e-05\n",
      "Epoch 33/50\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.4246 - val_loss: 0.4451 - lr: 1.0000e-05\n",
      "Epoch 34/50\n",
      "21/21 [==============================] - 94s 5s/step - loss: 0.4248 - val_loss: 0.4451 - lr: 1.0000e-05\n",
      "Epoch 35/50\n",
      "21/21 [==============================] - 84s 4s/step - loss: 0.4179 - val_loss: 0.4451 - lr: 1.0000e-05\n",
      "Epoch 36/50\n",
      "21/21 [==============================] - 85s 4s/step - loss: 0.4145 - val_loss: 0.4450 - lr: 1.0000e-05\n",
      "Epoch 37/50\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.4287 - val_loss: 0.4450 - lr: 1.0000e-05\n",
      "Epoch 38/50\n",
      "21/21 [==============================] - 107s 5s/step - loss: 0.4213 - val_loss: 0.4450 - lr: 1.0000e-05\n",
      "Epoch 39/50\n",
      "21/21 [==============================] - 89s 4s/step - loss: 0.4321 - val_loss: 0.4450 - lr: 1.0000e-05\n",
      "Epoch 40/50\n",
      "21/21 [==============================] - 99s 5s/step - loss: 0.4166 - val_loss: 0.4450 - lr: 1.0000e-05\n",
      "Epoch 41/50\n",
      "21/21 [==============================] - 93s 4s/step - loss: 0.4204 - val_loss: 0.4450 - lr: 1.0000e-05\n",
      "Epoch 42/50\n",
      "21/21 [==============================] - 104s 5s/step - loss: 0.4221 - val_loss: 0.4450 - lr: 1.0000e-05\n",
      "Epoch 43/50\n",
      "21/21 [==============================] - 93s 4s/step - loss: 0.4230 - val_loss: 0.4449 - lr: 1.0000e-05\n",
      "Epoch 44/50\n",
      "21/21 [==============================] - 93s 4s/step - loss: 0.4163 - val_loss: 0.4449 - lr: 1.0000e-05\n",
      "Epoch 45/50\n",
      "21/21 [==============================] - 100s 5s/step - loss: 0.4216 - val_loss: 0.4449 - lr: 1.0000e-05\n",
      "Epoch 46/50\n",
      "21/21 [==============================] - 100s 5s/step - loss: 0.4212 - val_loss: 0.4449 - lr: 1.0000e-05\n",
      "Epoch 47/50\n",
      "21/21 [==============================] - 106s 5s/step - loss: 0.4196 - val_loss: 0.4449 - lr: 1.0000e-05\n",
      "Epoch 48/50\n",
      "21/21 [==============================] - 98s 5s/step - loss: 0.4223 - val_loss: 0.4449 - lr: 1.0000e-05\n",
      "Epoch 49/50\n",
      "21/21 [==============================] - 97s 5s/step - loss: 0.4284 - val_loss: 0.4448 - lr: 1.0000e-05\n",
      "Epoch 50/50\n",
      "21/21 [==============================] - 95s 5s/step - loss: 0.4194 - val_loss: 0.4448 - lr: 1.0000e-05\n",
      "[2022_01_27-03:25:03] Training the entire fine-tuned model...\n",
      "[2022_01_27-03:25:14] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/50\n",
      "21/21 [==============================] - 306s 14s/step - loss: 0.4777 - val_loss: 0.4397 - lr: 5.0000e-04\n",
      "Epoch 2/50\n",
      "21/21 [==============================] - 278s 13s/step - loss: 0.4366 - val_loss: 0.4343 - lr: 5.0000e-04\n",
      "Epoch 3/50\n",
      "21/21 [==============================] - 285s 14s/step - loss: 0.4107 - val_loss: 0.3974 - lr: 5.0000e-04\n",
      "Epoch 4/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3855 \n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "21/21 [==============================] - 268s 13s/step - loss: 0.3855 - val_loss: 0.4391 - lr: 5.0000e-04\n",
      "Epoch 5/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3267 \n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "21/21 [==============================] - 280s 13s/step - loss: 0.3267 - val_loss: 0.4080 - lr: 1.2500e-04\n",
      "[2022_01_27-03:48:52] Training on final epochs of sequence length 512...\n",
      "[2022_01_27-03:48:52] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_01_27-03:49:45] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "21/21 [==============================] - 306s 14s/step - loss: 0.3645 - val_loss: 0.3906 - lr: 5.0000e-05\n",
      "Model number 03 trained with learning rate 0.0005:\n",
      "Test-set performance:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># records</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model seq len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>119</td>\n",
       "      <td>0.925951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>119</td>\n",
       "      <td>0.925951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               # records       AUC\n",
       "Model seq len                     \n",
       "512                  119  0.925951\n",
       "All                  119  0.925951"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  96  0\n",
       "1  15  8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/03_batch_64_lr_0.0005_2022_01_25.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/03_batch_64_lr_0.0005_2022_01_25.pkl/assets\n",
      "/home/brazdilv/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/home/brazdilv/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "batch_size = 64\n",
    "epoch_num = 50\n",
    "for learning_rate in [1e-5, 5e-5, 1e-4, 5e-4]:\n",
    "    fine_tune(f\"{i:02d}\", learning_rate, epoch_num, batch_size)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99a45ace-6b35-4187-840d-4f9f6f4b88ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5161290322580645"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = 8 / (8 + 0.5* (15 + 0))\n",
    "# TN / (TN + 0.5 * (FP + FN))\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ea7ac88-4084-45a9-8328-32a9bfc76a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkvetab\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "/home/brazdilv/.conda/envs/tf-gpu/lib/python3.7/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/ProteinBERT_04/runs/1i7v6v9d\" target=\"_blank\">sunny-music-2</a></strong> to <a href=\"https://wandb.ai/kvetab/ProteinBERT_04\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_01_27-10:06:31] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_01_27-10:06:31] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_01_27-10:06:31] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-27 10:06:31.462191: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "/home/brazdilv/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7227"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Layer GlobalAttention has arguments ['self', 'n_heads', 'd_key', 'd_value']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m in `__init__` and therefore must override `get_config()`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Example:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m class CustomLayer(keras.layers.Layer):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     def __init__(self, arg1, arg2):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         super().__init__()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         self.arg1 = arg1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         self.arg2 = arg2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     def get_config(self):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         config = super().get_config()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         config.update({\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             \"arg1\": self.arg1,\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             \"arg2\": self.arg2,\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         })\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         return config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 99s 4s/step - loss: 0.7227 - val_loss: 0.5562 - lr: 0.0100\n",
      "Epoch 2/60\n",
      "21/21 [==============================] - 96s 5s/step - loss: 0.4803 - val_loss: 0.4784 - lr: 0.0100\n",
      "Epoch 3/60\n",
      "21/21 [==============================] - 101s 5s/step - loss: 0.4659 - val_loss: 0.4608 - lr: 0.0100\n",
      "Epoch 4/60\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4718\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "21/21 [==============================] - 111s 5s/step - loss: 0.4718 - val_loss: 0.4721 - lr: 0.0100\n",
      "Epoch 5/60\n",
      "21/21 [==============================] - 96s 5s/step - loss: 0.4233 - val_loss: 0.4445 - lr: 0.0025\n",
      "Epoch 6/60\n",
      "21/21 [==============================] - 95s 5s/step - loss: 0.4166 - val_loss: 0.4398 - lr: 0.0025\n",
      "Epoch 7/60\n",
      "21/21 [==============================] - 92s 4s/step - loss: 0.4142 - val_loss: 0.4360 - lr: 0.0025\n",
      "Epoch 8/60\n",
      "21/21 [==============================] - 89s 4s/step - loss: 0.4090 - val_loss: 0.4332 - lr: 0.0025\n",
      "Epoch 9/60\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4200\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "21/21 [==============================] - 89s 4s/step - loss: 0.4200 - val_loss: 0.4349 - lr: 0.0025\n",
      "Epoch 10/60\n",
      "21/21 [==============================] - 96s 5s/step - loss: 0.4189 - val_loss: 0.4315 - lr: 6.2500e-04\n",
      "Epoch 11/60\n",
      "21/21 [==============================] - 104s 5s/step - loss: 0.4060 - val_loss: 0.4300 - lr: 6.2500e-04\n",
      "Epoch 12/60\n",
      "21/21 [==============================] - 96s 5s/step - loss: 0.4068 - val_loss: 0.4291 - lr: 6.2500e-04\n",
      "Epoch 13/60\n",
      "21/21 [==============================] - 85s 4s/step - loss: 0.4042 - val_loss: 0.4283 - lr: 6.2500e-04\n",
      "Epoch 14/60\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4007\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "21/21 [==============================] - 84s 4s/step - loss: 0.4007 - val_loss: 0.4299 - lr: 6.2500e-04\n",
      "Epoch 15/60\n",
      "21/21 [==============================] - 84s 4s/step - loss: 0.4071 - val_loss: 0.4271 - lr: 1.5625e-04\n",
      "Epoch 16/60\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3998\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "21/21 [==============================] - 84s 4s/step - loss: 0.3998 - val_loss: 0.4272 - lr: 1.5625e-04\n",
      "Epoch 17/60\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4080\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "21/21 [==============================] - 84s 4s/step - loss: 0.4080 - val_loss: 0.4272 - lr: 3.9062e-05\n",
      "[2022_01_27-10:32:58] Training the entire fine-tuned model...\n",
      "[2022_01_27-10:33:09] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/60\n",
      "21/21 [==============================] - 253s 12s/step - loss: 0.4031 - val_loss: 0.4295 - lr: 1.0000e-05\n",
      "Epoch 2/60\n",
      "21/21 [==============================] - 240s 11s/step - loss: 0.3983 - val_loss: 0.4256 - lr: 1.0000e-05\n",
      "Epoch 3/60\n",
      "21/21 [==============================] - 242s 12s/step - loss: 0.4050 - val_loss: 0.4252 - lr: 1.0000e-05\n",
      "Epoch 4/60\n",
      "21/21 [==============================] - 240s 11s/step - loss: 0.3985 - val_loss: 0.4227 - lr: 1.0000e-05\n",
      "Epoch 5/60\n",
      "21/21 [==============================] - 241s 11s/step - loss: 0.3978 - val_loss: 0.4215 - lr: 1.0000e-05\n",
      "Epoch 6/60\n",
      "21/21 [==============================] - 241s 11s/step - loss: 0.3965 - val_loss: 0.4210 - lr: 1.0000e-05\n",
      "Epoch 7/60\n",
      "21/21 [==============================] - 243s 12s/step - loss: 0.3931 - val_loss: 0.4198 - lr: 1.0000e-05\n",
      "Epoch 8/60\n",
      "21/21 [==============================] - 241s 11s/step - loss: 0.3916 - val_loss: 0.4195 - lr: 1.0000e-05\n",
      "Epoch 9/60\n",
      "21/21 [==============================] - 241s 12s/step - loss: 0.3925 - val_loss: 0.4177 - lr: 1.0000e-05\n",
      "Epoch 10/60\n",
      "21/21 [==============================] - 241s 11s/step - loss: 0.3927 - val_loss: 0.4164 - lr: 1.0000e-05\n",
      "Epoch 11/60\n",
      "21/21 [==============================] - 243s 12s/step - loss: 0.3869 - val_loss: 0.4210 - lr: 1.0000e-05\n",
      "Epoch 12/60\n",
      "21/21 [==============================] - 244s 12s/step - loss: 0.3883 - val_loss: 0.4156 - lr: 1.0000e-05\n",
      "Epoch 13/60\n",
      "21/21 [==============================] - 241s 12s/step - loss: 0.3836 - val_loss: 0.4135 - lr: 1.0000e-05\n",
      "Epoch 14/60\n",
      "21/21 [==============================] - 240s 11s/step - loss: 0.3870 - val_loss: 0.4148 - lr: 1.0000e-05\n",
      "Epoch 15/60\n",
      "21/21 [==============================] - 240s 11s/step - loss: 0.3817 - val_loss: 0.4129 - lr: 1.0000e-05\n",
      "Epoch 16/60\n",
      "21/21 [==============================] - 239s 11s/step - loss: 0.3822 - val_loss: 0.4115 - lr: 1.0000e-05\n",
      "Epoch 17/60\n",
      "21/21 [==============================] - 241s 12s/step - loss: 0.3749 - val_loss: 0.4114 - lr: 1.0000e-05\n",
      "Epoch 18/60\n",
      "21/21 [==============================] - 239s 11s/step - loss: 0.3807 - val_loss: 0.4087 - lr: 1.0000e-05\n",
      "Epoch 19/60\n",
      "21/21 [==============================] - 239s 11s/step - loss: 0.3700 - val_loss: 0.4099 - lr: 1.0000e-05\n",
      "Epoch 20/60\n",
      "21/21 [==============================] - 238s 11s/step - loss: 0.3700 - val_loss: 0.4088 - lr: 1.0000e-05\n",
      "[2022_01_27-11:53:38] Training on final epochs of sequence length 512...\n",
      "[2022_01_27-11:53:38] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_01_27-11:53:38] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "21/21 [==============================] - 251s 12s/step - loss: 0.3743 - val_loss: 0.4087 - lr: 1.0000e-06\n",
      "Model number 04 trained with learning rate 1e-05:\n",
      "Test-set performance:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># records</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model seq len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>119</td>\n",
       "      <td>0.922781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>119</td>\n",
       "      <td>0.922781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               # records       AUC\n",
       "Model seq len                     \n",
       "512                  119  0.922781\n",
       "All                  119  0.922781"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  96  0\n",
       "1  15  8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "2022-01-27 11:58:35.240804: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/04_batch_64_lr_1e-05_2022_01_25.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/04_batch_64_lr_1e-05_2022_01_25.pkl/assets\n",
      "/home/brazdilv/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/home/brazdilv/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    }
   ],
   "source": [
    "i = 4\n",
    "epoch_num = 60\n",
    "batch_size = 64\n",
    "learning_rate = 1e-5\n",
    "fine_tune(f\"{i:02d}\", learning_rate, epoch_num, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0179ca64-60fd-4c04-8448-a451832b7bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
