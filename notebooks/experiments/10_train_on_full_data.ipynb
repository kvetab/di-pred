{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7475a9ca-3a62-4637-a7e0-8fee065d68a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import path\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21f4364b-eda2-4389-9ea9-ba2d357c08ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "718fbe40-7584-4592-a1fa-e9f59a0dce13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1291\n",
      "260\n"
     ]
    }
   ],
   "source": [
    "chen_train = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_train.csv\"), index_col=0)\n",
    "chen_test = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_test.csv\"), index_col=0)\n",
    "print(len(chen_train))\n",
    "print(len(chen_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b3a2ebc-eb81-489b-8673-6cebd44f8cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tap = pd.read_csv(path.join(DATA_DIR, \"tap/TAP_data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98b9ff48-d793-4383-af82-a1fc8558038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = pd.read_csv(path.join(DATA_DIR, \"evaluations/best_total.csv\"))\n",
    "model_configs = json.load(open(path.join(DATA_DIR, \"evaluations/best_by_model_configs.json\"), \"r\"))\n",
    "data_configs = json.load(open(path.join(DATA_DIR, \"evaluations/best_by_data_configs.json\"), \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c171eae-8ef9-49b8-83fd-d65eaaa77925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(preprocessing, data_name, hp_dir):\n",
    "    filename = path.join(hp_dir, f\"kNN_{data_name}_{preprocessing}.json\")\n",
    "    parameters = json.load(open(filename))\n",
    "    #n_neighbors = int(parameters[\"n_neighbors\"])\n",
    "    model = KNeighborsClassifier(n_neighbors=parameters[\"n_neighbors\"]) \n",
    "    return model, parameters, \"kNN\"\n",
    "\n",
    "def logistic_regression(preprocessing, data_name, hp_dir):\n",
    "    filename = path.join(hp_dir, f\"logistic_regression_{data_name}_{preprocessing}.json\")\n",
    "    parameters = json.load(open(filename))\n",
    "    #C = float(parameters[\"C\"])\n",
    "    lr = LogisticRegression(\n",
    "        class_weight='balanced', max_iter=1000, random_state=42,\n",
    "        C=parameters[\"C\"], penalty=parameters[\"penalty\"], solver=parameters[\"solver\"]\n",
    "    )\n",
    "    return lr, parameters, \"logistic_regression\"\n",
    "\n",
    "def random_forest(preprocessing, data_name, hp_dir):\n",
    "    filename = path.join(hp_dir, f\"random_forest_{data_name}_{preprocessing}.json\")\n",
    "    parameters = json.load(open(filename))\n",
    "    rf = RandomForestClassifier(\n",
    "        random_state=42, n_jobs=-1, class_weight='balanced', n_estimators=int(parameters[\"n_estimators\"]),\n",
    "        max_depth=int(parameters[\"max_depth\"]), max_features=float(parameters[\"max_features\"])\n",
    "    )\n",
    "    return rf, parameters, \"random_forest\"\n",
    "\n",
    "def multilayer_perceptron(preprocessing, data_name, hp_dir):\n",
    "    filename = path.join(hp_dir, f\"multilayer_perceptron_{data_name}_{preprocessing}.json\")\n",
    "    parameters = json.load(open(filename))\n",
    "    mlp = MLPClassifier(\n",
    "        random_state=42, max_iter=int(1000), hidden_layer_sizes=parameters[\"hidden_layer_sizes\"],\n",
    "        activation=parameters[\"activation\"]\n",
    "    )\n",
    "    return mlp, parameters, \"multilayer_perceptron\"\n",
    "\n",
    "def svm(preprocessing, data_name, hp_dir):\n",
    "    filename = path.join(hp_dir, f\"SVM_{data_name}_{preprocessing}.json\")\n",
    "    parameters = json.load(open(filename))\n",
    "    svc = SVC(\n",
    "        max_iter=8000, probability=True, class_weight='balanced', C=parameters[\"C\"],\n",
    "        kernel=parameters[\"kernel\"], gamma=parameters[\"gamma\"]\n",
    "    )\n",
    "    return svc, parameters, \"SVM\"\n",
    "\n",
    "def gradient_boosting(preprocessing, data_name, hp_dir):\n",
    "    filename = path.join(hp_dir, f\"gradient_boosting_{data_name}_{preprocessing}.json\")\n",
    "    parameters = json.load(open(filename))\n",
    "    gb = GradientBoostingClassifier(\n",
    "        random_state=42, n_iter_no_change=70, learning_rate=float(parameters[\"learning_rate\"]),\n",
    "        n_estimators=int(parameters[\"n_estimators\"]), max_depth=int(parameters[\"max_depth\"]),\n",
    "        max_features=float(parameters[\"max_features\"])\n",
    "    )\n",
    "    return gb, parameters, \"gradient_boosting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d4f9f19-c981-4326-9a8d-1b7b6ad2559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_encoded(train_df, test_df):\n",
    "    x_chen = pd.read_csv(path.join(DATA_DIR, \"chen/integer_encoding/chen_integer_encoded.csv\"), index_col=0)\n",
    "    x_chen_train = x_chen.merge(train_df[[\"Antibody_ID\", \"Y\"]].reset_index(), left_on=\"Ab_ID\", right_on=\"Antibody_ID\").set_index('index').drop(\"Antibody_ID\", axis=1)\n",
    "    x_chen_test = x_chen.merge(test_df[[\"Antibody_ID\", \"Y\"]].reset_index(), left_on=\"Ab_ID\", right_on=\"Antibody_ID\").set_index('index').drop(\"Antibody_ID\", axis=1)\n",
    "    x_tap = pd.read_csv(path.join(DATA_DIR, \"tap/integer_encoding/tap_integer_encoded.csv\"))\n",
    "    x_tap.drop(\"Ab_ID\", axis=1, inplace=True)\n",
    "    return x_chen_train, x_chen_test, x_tap\n",
    "\n",
    "\n",
    "def pybiomed(train_df, test_df):\n",
    "    x_chen = pd.read_feather(path.join(DATA_DIR, \"chen/pybiomed/X_data.ftr\"))\n",
    "    x_chen_train = x_chen.merge(train_df[[\"Antibody_ID\", \"Y\"]].reset_index(), left_on=\"Ab_ID\", right_on=\"Antibody_ID\").set_index('index').drop(\"Antibody_ID\", axis=1)\n",
    "    x_chen_test = x_chen.merge(test_df[[\"Antibody_ID\", \"Y\"]].reset_index(), left_on=\"Ab_ID\", right_on=\"Antibody_ID\").set_index('index').drop(\"Antibody_ID\", axis=1)\n",
    "    x_tap = pd.read_feather(path.join(DATA_DIR, \"tap/pybiomed/X_TAP_data.ftr\"))\n",
    "    return x_chen_train, x_chen_test, x_tap\n",
    "\n",
    "\n",
    "def protparam(train_df, test_df):\n",
    "    x_chen = pd.read_csv(path.join(DATA_DIR, \"chen/protparam/protparam_features.csv\"))\n",
    "    x_chen.rename({\"Unnamed: 0\": \"Ab_ID\"}, axis=1, inplace=True)\n",
    "    x_chen = x_chen.drop(\"name\", axis=1)\n",
    "    x_chen_train = x_chen.merge(train_df[[\"Antibody_ID\", \"Y\"]].reset_index(), left_on=\"Ab_ID\", right_on=\"Antibody_ID\").set_index('index').drop(\"Antibody_ID\", axis=1)\n",
    "    x_chen_test = x_chen.merge(test_df[[\"Antibody_ID\", \"Y\"]].reset_index(), left_on=\"Ab_ID\", right_on=\"Antibody_ID\").set_index('index').drop(\"Antibody_ID\", axis=1)\n",
    "    \n",
    "    x_tap = pd.read_csv(path.join(DATA_DIR, \"tap/protparam/protparam_features_tap.csv\"))\n",
    "    x_tap = x_tap.drop(\"Unnamed: 0\", axis=1)\n",
    "    return x_chen_train, x_chen_test, x_tap\n",
    "\n",
    "\n",
    "def bert(train_df, test_df):\n",
    "    x_chen = pd.read_feather(path.join(DATA_DIR, \"chen/embeddings/bert/bert_chen_embeddings.ftr\"))\n",
    "    x_chen_train = x_chen.merge(train_df[[\"Antibody_ID\", \"Y\"]].reset_index(), left_on=\"Ab_ID\", right_on=\"Antibody_ID\").set_index('index').drop(\"Antibody_ID\", axis=1)\n",
    "    x_chen_test = x_chen.merge(test_df[[\"Antibody_ID\", \"Y\"]].reset_index(), left_on=\"Ab_ID\", right_on=\"Antibody_ID\").set_index('index').drop(\"Antibody_ID\", axis=1)\n",
    "    x_tap = pd.read_feather(path.join(DATA_DIR, \"tap/embeddings/bert/bert_tap_embeddings.ftr\"))\n",
    "    x_tap = x_tap.drop(\"Ab_ID\", axis=1)\n",
    "    return x_chen_train, x_chen_test, x_tap\n",
    "\n",
    "\n",
    "def seqvec(train_df, test_df):\n",
    "    x_chen = pd.read_feather(path.join(DATA_DIR, \"chen/embeddings/seqvec/seqvec_chen_embeddings.ftr\"))\n",
    "    x_chen_train = x_chen.merge(train_df[[\"Antibody_ID\", \"Y\"]].reset_index(), left_on=\"Ab_ID\", right_on=\"Antibody_ID\").set_index('index').drop(\"Antibody_ID\", axis=1)\n",
    "    x_chen_test = x_chen.merge(test_df[[\"Antibody_ID\", \"Y\"]].reset_index(), left_on=\"Ab_ID\", right_on=\"Antibody_ID\").set_index('index').drop(\"Antibody_ID\", axis=1)\n",
    "    x_tap = pd.read_csv(path.join(DATA_DIR, \"tap/embeddings/seqvec/seqvec_tap_embeddings.csv\"), index_col=0)\n",
    "    x_tap = x_tap.drop(\"Ab_ID\", axis=1)\n",
    "    return x_chen_train, x_chen_test, x_tap\n",
    "\n",
    "\n",
    "def sapiens(train_df, test_df):\n",
    "    x_chen = pd.read_csv(path.join(DATA_DIR, \"chen/embeddings/sapiens/sapiens_chen_embeddings.csv\"), index_col=0).drop(\"Y\", axis=1)\n",
    "    x_chen_train = x_chen.merge(train_df[[\"Antibody_ID\", \"Y\"]].reset_index(), left_on=\"Ab_ID\", right_on=\"Antibody_ID\").set_index('index').drop(\"Antibody_ID\", axis=1)\n",
    "    x_chen_test = x_chen.merge(test_df[[\"Antibody_ID\", \"Y\"]].reset_index(), left_on=\"Ab_ID\", right_on=\"Antibody_ID\").set_index('index').drop(\"Antibody_ID\", axis=1)\n",
    "    x_tap = pd.read_csv(path.join(DATA_DIR, \"tap/embeddings/sapiens/sapiens_tap_embeddings.csv\"), index_col=0)\n",
    "    x_tap = x_tap.drop([\"Ab_ID\", \"Y\"], axis=1)\n",
    "    return x_chen_train, x_chen_test, x_tap\n",
    "\n",
    "\n",
    "def onehot(train_df, test_df):\n",
    "    x_chen = pd.read_feather(path.join(DATA_DIR, \"chen/onehot/chen_onehot_short.ftr\"))\n",
    "    x_chen_train = x_chen.merge(train_df[[\"Antibody_ID\", \"Y\"]].reset_index(), left_on=\"Ab_ID\", right_on=\"Antibody_ID\").set_index('index').drop(\"Antibody_ID\", axis=1)\n",
    "    x_chen_test = x_chen.merge(test_df[[\"Antibody_ID\", \"Y\"]].reset_index(), left_on=\"Ab_ID\", right_on=\"Antibody_ID\").set_index('index').drop(\"Antibody_ID\", axis=1)\n",
    "    x_tap = pd.read_feather(path.join(DATA_DIR, \"tap/onehot/tap_onehot_short.ftr\"))\n",
    "    x_tap = x_tap.drop([\"Ab_ID\"], axis=1)\n",
    "    return x_chen_train, x_chen_test, x_tap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc9b0c76-38b5-4abb-828f-2b65ec70aac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(train_df, test_df, tap_df):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_df.drop([\"Ab_ID\", \"Y\"], axis=1))\n",
    "    x_train_tr = scaler.transform(train_df.drop([\"Ab_ID\", \"Y\"], axis=1))\n",
    "    x_train_df = pd.DataFrame(data=train_df,  index=train_df.index, columns=train_df.drop([\"Ab_ID\", \"Y\"], axis=1).columns)\n",
    "    x_train_df[\"Ab_ID\"] = train_df[\"Ab_ID\"]\n",
    "    \n",
    "    x_test_tr = scaler.transform(test_df.drop([\"Ab_ID\", \"Y\"], axis=1))\n",
    "    x_test_df = pd.DataFrame(data=test_df,  index=test_df.index, columns=test_df.drop([\"Ab_ID\", \"Y\"], axis=1).columns)\n",
    "    x_test_df[\"Y\"] = test_df[\"Y\"]\n",
    "    x_test_df[\"Ab_ID\"] = test_df[\"Ab_ID\"]\n",
    "    \n",
    "    x_tap_tr = scaler.transform(tap_df)\n",
    "    x_tap_df = pd.DataFrame(data=tap_df,  index=tap_df.index, columns=tap_df.columns)\n",
    "\n",
    "    return x_train_df, train_df[\"Y\"], x_test_df, x_tap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1d64930-3cce-46ad-af48-b7071d7f6707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_evaluation(model_type, metrics, data, outpath, preprocessing):\n",
    "    filename_sum = os.path.join(DATA_DIR, f\"evaluations/{outpath}/all.csv\")\n",
    "    line = [model_type, data, preprocessing, metrics[\"f1\"], metrics[\"mcc\"], metrics[\"acc\"],metrics[\"precision\"],metrics[\"recall\"],metrics[\"auc\"]]\n",
    "    with open(filename_sum, 'a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile, delimiter='\\t')\n",
    "        csvwriter.writerow(line)\n",
    "\n",
    "\n",
    "def train_and_eval(model_name, classifier, X_train, y_train, X_valid, y_valid,\n",
    "                   data_name, outpath, preprocessing):\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_valid)\n",
    "    filename = path.join(DATA_DIR, \"evaluations\", outpath, \"models\", f\"{model_name}_{data_name}_{preprocessing}.pkl\")\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(classifier, f)\n",
    "    filename = path.join(DATA_DIR, \"evaluations\", outpath, f\"{model_name}_{data_name}_{preprocessing}.csv\")\n",
    "    str_preds = [str(int(pred)) for pred in y_pred]\n",
    "    with open(filename, \"wt\") as f:\n",
    "        f.write(\",\".join(str_preds) + \"\\n\")\n",
    "    metric_dict = {\n",
    "        \"f1\": float(metrics.f1_score(y_valid, y_pred)),\n",
    "        \"acc\": float(metrics.accuracy_score(y_valid, y_pred)),\n",
    "        \"mcc\": float(metrics.matthews_corrcoef(y_valid, y_pred)),\n",
    "        \"auc\": float(metrics.roc_auc_score(y_valid, y_pred)),\n",
    "        \"precision\": float(metrics.precision_score(y_valid, y_pred)),\n",
    "        \"recall\": float(metrics.recall_score(y_valid, y_pred))\n",
    "    }\n",
    "    \n",
    "    output_evaluation(model_name, metric_dict, data_name, outpath, preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b42101f-1723-498d-9cf0-0ae9fb318bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_tap(model_name, x_test, y_test,\n",
    "                   data_name, outpath, preprocessing):\n",
    "    filename = path.join(DATA_DIR, \"evaluations\", outpath, \"models\", f\"{model_name}_{data_name}_{preprocessing}.pkl\")\n",
    "    with open(filename, 'rb') as f:\n",
    "        estimator = pickle.load(f)\n",
    "    y_pred = estimator.predict(x_test)\n",
    "    metric_dict = {\n",
    "        \"f1\": float(metrics.f1_score(y_test, y_pred)),\n",
    "        \"acc\": float(metrics.accuracy_score(y_test, y_pred)),\n",
    "        \"mcc\": float(metrics.matthews_corrcoef(y_test, y_pred)),\n",
    "        \"auc\": float(metrics.roc_auc_score(y_test, y_pred)),\n",
    "        \"precision\": float(metrics.precision_score(y_test, y_pred)),\n",
    "        \"recall\": float(metrics.recall_score(y_test, y_pred))\n",
    "    }\n",
    "    filename_sum = os.path.join(DATA_DIR, f\"evaluations/{outpath}/tap.csv\")\n",
    "    line = [model_name, data_name, preprocessing, metric_dict[\"f1\"], metric_dict[\"mcc\"], metric_dict[\"acc\"],metric_dict[\"precision\"],metric_dict[\"recall\"],metric_dict[\"auc\"], filename]\n",
    "    with open(filename_sum, 'a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile, delimiter='\\t')\n",
    "        csvwriter.writerow(line)\n",
    "    filename = path.join(DATA_DIR, \"evaluations\", outpath, f\"{model_name}_{data_name}_{preprocessing}_tap.csv\")\n",
    "    str_preds = [str(int(pred)) for pred in y_pred]\n",
    "    with open(filename, \"wt\") as f:\n",
    "        f.write(\",\".join(str_preds) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f711ced-dfac-410c-a597-1d0734696b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_train = [\n",
    "    (logistic_regression, bert),\n",
    "    (logistic_regression, seqvec),\n",
    "    (logistic_regression, pybiomed),\n",
    "    (svm, bert),\n",
    "    (svm, seqvec),\n",
    "    (knn, seqvec),\n",
    "    (gradient_boosting, protparam),\n",
    "    (multilayer_perceptron, bert),\n",
    "    (random_forest, seqvec),\n",
    "    (logistic_regression, onehot),\n",
    "    (logistic_regression, sapiens),\n",
    "    (random_forest, integer_encoded),\n",
    "    (random_forest, protparam)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fcd4089-708f-4775-8e7a-294009da7c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final(train_df, test_df, eval_dir, tap_data):\n",
    "    for model_creator, data_rep in to_train:\n",
    "        data_name = data_rep.__name__\n",
    "        x_train, x_test, x_tap = data_rep(train_df, test_df)\n",
    "        for prepro in [scaling]:\n",
    "            prepro_name = prepro.__name__\n",
    "            x_train_tr, y_train_tr, x_test_tr, tap_tr = prepro(x_train, x_test, x_tap)\n",
    "            \n",
    "            classifier, params, model_label = model_creator(prepro_name, data_name, path.join(DATA_DIR, \"evaluations/hyperparameters\"))\n",
    "            train_and_eval(\n",
    "                model_label, classifier, x_train_tr.drop([\"Ab_ID\"], axis=1), \n",
    "                y_train_tr, x_test_tr.drop([\"Ab_ID\", \"Y\"], axis=1), x_test_tr[\"Y\"], \n",
    "                data_name, eval_dir, prepro_name\n",
    "            )\n",
    "            test_on_tap(\n",
    "                model_label, tap_tr, tap_data[\"Y\"], data_name, eval_dir, prepro_name\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2029cb2-dd36-4845-b3fa-3f4f42009104",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dir = \"final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82a3bc6d-2fb9-4d58-8922-83d90ff5903b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "train_final(chen_train, chen_test, eval_dir, tap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2be08b-0793-420c-a337-a68de3b3c4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
