{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f37c38b7-7645-4a37-b0bd-1c3ed0c08489",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from os import path\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13b5db07-e604-4fdc-be52-8d4e963fe915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from proteinbert.finetuning import encode_train_and_valid_sets, encode_dataset\n",
    "from proteinbert import OutputType, OutputSpec, evaluate_by_len, load_pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58f06fc6-cc5b-4ba5-b76c-350428a49552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from proteinbert import OutputType, OutputSpec, FinetuningModelGenerator, load_pretrained_model, \\\n",
    "finetune, evaluate_by_len\n",
    "\n",
    "from proteinbert.conv_and_global_attention_model import get_model_with_hidden_layers_as_outputs\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb5cdb0f-aecd-4a46-bb86-605505e74c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05704d2b-73eb-4f0f-97f0-20a00cbfbbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17f00a4c-4f9d-4245-b312-03281caeaba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89fb9d1d-5ecb-45a1-8d55-26d6ee7f5010",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_TYPE = OutputType(False, 'binary')\n",
    "UNIQUE_LABELS = [0, 1]\n",
    "OUTPUT_SPEC = OutputSpec(OUTPUT_TYPE, UNIQUE_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b894730-5000-4539-95a2-9795b3a54513",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_generator, input_encoder = load_pretrained_model(\"../../data/protein_bert/\", \"epoch_92400_sample_23500000.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1246f6e-d5b3-4907-87fc-57c90f40d8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df18af30-11ad-421a-bb04-acbcb4da64b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_train.csv\"), index_col=0)\n",
    "test_set = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_test.csv\"), index_col=0)\n",
    "train_set[\"seq\"] = train_set[\"heavy\"] + train_set[\"light\"]\n",
    "test_set[\"seq\"] = test_set[\"heavy\"] + test_set[\"light\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c75228d6-d97a-4ee6-90d7-e56e5dab4631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1291"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "669e0ae8-04cc-437d-9876-b9b8c437336d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "010440f5-accb-4866-8d0f-a31b5be34976",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "patience = (6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "284f8c1b-8f09-4805-bd0f-7e618216a75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [0.5, 0.6, 0.7, 0.8, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89a129f-8f51-4cb8-8090-96f99ee7aaf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2198e715-c2f5-408d-a787-aa8d1451e324",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_save_model(train_data, valid_data, test_data, size):\n",
    "    wandb.init(project=f\"Dataset size exp\", entity=\"kvetab\")\n",
    "    model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "            get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)\n",
    "\n",
    "    training_callbacks = [\n",
    "        keras.callbacks.ReduceLROnPlateau(patience = patience[1], factor = 0.25, min_lr = 1e-07, verbose = 1),\n",
    "        keras.callbacks.EarlyStopping(patience = patience[0], restore_best_weights = True),\n",
    "        WandbCallback()\n",
    "    ]\n",
    "\n",
    "    epoch_num = 100\n",
    "    batch_size = 128\n",
    "    #learning_rate = 1e-5\n",
    "    wandb.config = {\n",
    "          \"learning_rate\": learning_rate,\n",
    "          \"epochs\": epoch_num * 2,\n",
    "          \"batch_size\": batch_size\n",
    "        }\n",
    "    finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data[\"seq\"], train_data[\"Y\"], valid_data['seq'], valid_data[\"Y\"], \\\n",
    "            seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = learning_rate, begin_with_frozen_pretrained_layers = True, \\\n",
    "            lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = learning_rate / 10, callbacks = training_callbacks)\n",
    "    mod = model_generator.create_model(seq_len = 512)\n",
    "    mod_name = f\"2022_04_22_size{size}\"\n",
    "    mod.save(path.join(DATA_DIR, f\"protein_bert/by_data_size/{mod_name}\"))\n",
    "\n",
    "    results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "            start_seq_len = 512, start_batch_size = 32)\n",
    "    fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "    f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "    return confusion_matrix, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "702b647d-8583-44d6-bf53-f983dbb7597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = {}\n",
    "f1s = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a954139-d02d-407e-ac45-57bdb4a1a890",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645 646 260\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_and_save_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/pbs.42087.lich-compute.vscht.cz/ipykernel_15666/1083666877.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#valid, test = train_test_split(test, test_size=0.5, random_state=333, stratify=test[\"Y\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_save_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mcms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mf1s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_and_save_model' is not defined"
     ]
    }
   ],
   "source": [
    "for size in sizes:\n",
    "    train, valid = train_test_split(train_set, test_size=1-size, random_state=42, stratify=train_set[\"Y\"])\n",
    "    #test = pd.concat([test, test_set])\n",
    "    test = test_set\n",
    "    #valid, test = train_test_split(test, test_size=0.5, random_state=333, stratify=test[\"Y\"])\n",
    "    print(len(train), len(valid), len(test))\n",
    "    cm, f1_score = train_and_save_model(train, valid, test, size)\n",
    "    cms[size] = cm\n",
    "    f1s[size] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a38702c-f746-4ec6-9911-b21bdaaa56aa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/pbs.42087.lich-compute.vscht.cz/ipykernel_15666/1399195940.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m333\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_save_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_set' is not defined"
     ]
    }
   ],
   "source": [
    "size = 1\n",
    "train = train_set\n",
    "valid, test = train_test_split(test_set, test_size=0.5, random_state=333, stratify=test_set[\"Y\"])\n",
    "print(len(train_set), len(valid), len(test))\n",
    "cm, f1_score = train_and_save_model(train, valid, test, size)\n",
    "cms[size] = cm\n",
    "f1s[size] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0a467ff-94ff-4c75-8685-0acfbb0097c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.5: 0.37894736842105264,\n",
       " 0.6: 0.4864864864864865,\n",
       " 0.7: 0.43010752688172044,\n",
       " 0.8: 0.4807692307692308,\n",
       " 0.9: 0.5094339622641509,\n",
       " 1: 0.5531914893617021}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4ca6bd-16f7-48d8-95b2-224c86cddf40",
   "metadata": {},
   "source": [
    "### Get other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9726d777-a1f9-4ddd-9ac9-504dcfd274db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, dir_name, x_test, y_test):\n",
    "    model_path = path.join(DATA_DIR, f\"protein_bert/{dir_name}/{model_name}\")\n",
    "    model = keras.models.load_model(model_path)\n",
    "    \n",
    "    y_pred = model.predict(x_test, batch_size=32)\n",
    "    y_pred_classes = (y_pred >= 0.5)\n",
    "    #print(f\"Model {model_name}\")\n",
    "    #print(f\"Test F1: {f1}\")\n",
    "    metric_dict = {\n",
    "        \"f1\": float(metrics.f1_score(y_test, y_pred_classes)),\n",
    "        \"acc\": float(metrics.accuracy_score(y_test, y_pred_classes)),\n",
    "        \"mcc\": float(metrics.matthews_corrcoef(y_test, y_pred_classes)),\n",
    "        \"auc\": float(metrics.roc_auc_score(y_test, y_pred_classes)),\n",
    "        \"precision\": float(metrics.precision_score(y_test, y_pred_classes)),\n",
    "        \"recall\": float(metrics.recall_score(y_test, y_pred_classes))\n",
    "    }\n",
    "    print(\"======================\")\n",
    "    print(model_name)\n",
    "    print(metric_dict)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "305bf8ed-3864-4869-9561-eb319171d7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c742e55-6ca8-478c-b902-24f1407b9f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_05_03-13:08:02] Test set: Filtered out 0 of 260 (0.0%) records of lengths exceeding 510.\n",
      "======================\n",
      "2022_04_22_size0.5\n",
      "{'f1': 0.37894736842105264, 'acc': 0.7730769230769231, 'mcc': 0.2467451337568806, 'auc': 0.61084142394822, 'precision': 0.43902439024390244, 'recall': 0.3333333333333333}\n",
      "\n",
      "\n",
      "[2022_05_03-13:08:10] Test set: Filtered out 0 of 260 (0.0%) records of lengths exceeding 510.\n",
      "======================\n",
      "2022_04_22_size0.6\n",
      "{'f1': 0.4864864864864865, 'acc': 0.7807692307692308, 'mcc': 0.3474566939734222, 'auc': 0.6771844660194175, 'precision': 0.47368421052631576, 'recall': 0.5}\n",
      "\n",
      "\n",
      "[2022_05_03-13:08:19] Test set: Filtered out 0 of 260 (0.0%) records of lengths exceeding 510.\n",
      "======================\n",
      "2022_04_22_size0.7\n",
      "{'f1': 0.43010752688172044, 'acc': 0.7961538461538461, 'mcc': 0.3159812981021656, 'auc': 0.6390686803308163, 'precision': 0.5128205128205128, 'recall': 0.37037037037037035}\n",
      "\n",
      "\n",
      "[2022_05_03-13:08:27] Test set: Filtered out 0 of 260 (0.0%) records of lengths exceeding 510.\n",
      "======================\n",
      "2022_04_22_size0.8\n",
      "{'f1': 0.4807692307692307, 'acc': 0.7923076923076923, 'mcc': 0.3516077645162995, 'auc': 0.6708018698309962, 'precision': 0.5, 'recall': 0.46296296296296297}\n",
      "\n",
      "\n",
      "[2022_05_03-13:08:35] Test set: Filtered out 0 of 260 (0.0%) records of lengths exceeding 510.\n",
      "======================\n",
      "2022_04_22_size0.9\n",
      "{'f1': 0.509433962264151, 'acc': 0.8, 'mcc': 0.38399408369228766, 'auc': 0.6893203883495146, 'precision': 0.5192307692307693, 'recall': 0.5}\n",
      "\n",
      "\n",
      "[2022_05_03-13:08:44] Test set: Filtered out 0 of 130 (0.0%) records of lengths exceeding 510.\n",
      "======================\n",
      "2022_04_22_size1\n",
      "{'f1': 0.553191489361702, 'acc': 0.8384615384615385, 'mcc': 0.464928324826858, 'auc': 0.7067601582164689, 'precision': 0.65, 'recall': 0.48148148148148145}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for size in sizes:\n",
    "    train, valid = train_test_split(train_set, test_size=1-size, random_state=42, stratify=train_set[\"Y\"])\n",
    "    test = test_set\n",
    "    encoded_test_set = encode_dataset(test[\"seq\"], test[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "                dataset_name = 'Test set')\n",
    "    test_X, test_Y, test_sample_weigths = encoded_test_set\n",
    "    model_name = f\"2022_04_22_size{size}\"\n",
    "    evaluate_model(model_name, \"by_data_size\", test_X, test_Y)\n",
    "train = train_set\n",
    "valid, test = train_test_split(test_set, test_size=0.5, random_state=333, stratify=test_set[\"Y\"])\n",
    "encoded_test_set = encode_dataset(test[\"seq\"], test[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'Test set')\n",
    "test_X, test_Y, test_sample_weigths = encoded_test_set\n",
    "model_name = f\"2022_04_22_size1\"\n",
    "evaluate_model(model_name, \"by_data_size\", test_X, test_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aea957b-aa33-4283-9043-cb9e59d68af9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129299b0-136c-4705-9585-8d8352209f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7e914b2-7036-40db-b5b3-65bc9431f18b",
   "metadata": {},
   "source": [
    "# CV on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aaf483c7-69e0-4449-954c-8cbf889455bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antibody_ID</th>\n",
       "      <th>heavy</th>\n",
       "      <th>light</th>\n",
       "      <th>Y</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12e8</td>\n",
       "      <td>EVQLQQSGAEVVRSGASVKLSCTASGFNIKDYYIHWVKQRPEKGLE...</td>\n",
       "      <td>DIVMTQSQKFMSTSVGDRVSITCKASQNVGTAVAWYQQKPGQSPKL...</td>\n",
       "      <td>0</td>\n",
       "      <td>677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15c8</td>\n",
       "      <td>EVQLQQSGAELVKPGASVKLSCTASGFNIKDTYMHWVKQKPEQGLE...</td>\n",
       "      <td>DIVLTQSPAIMSASLGERVTMTCTASSSVSSSNLHWYQQKPGSSPK...</td>\n",
       "      <td>0</td>\n",
       "      <td>685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1a0q</td>\n",
       "      <td>EVQLQESDAELVKPGASVKISCKASGYTFTDHVIHWVKQKPEQGLE...</td>\n",
       "      <td>DIELTQSPSSLSASLGGKVTITCKASQDIKKYIGWYQHKPGKQPRL...</td>\n",
       "      <td>1</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1a14</td>\n",
       "      <td>QVQLQQSGAELVKPGASVRMSCKASGYTFTNYNMYWVKQSPGQGLE...</td>\n",
       "      <td>DIELTQTTSSLSASLGDRVTISCRASQDISNYLNWYQQNPDGTVKL...</td>\n",
       "      <td>0</td>\n",
       "      <td>442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1a2y</td>\n",
       "      <td>QVQLQESGPGLVAPSQSLSITCTVSGFSLTGYGVNWVRQPPGKGLE...</td>\n",
       "      <td>DIVLTQSPASLSASVGETVTITCRASGNIHNYLAWYQQKQGKSPQL...</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Antibody_ID                                              heavy  \\\n",
       "0        12e8  EVQLQQSGAEVVRSGASVKLSCTASGFNIKDYYIHWVKQRPEKGLE...   \n",
       "1        15c8  EVQLQQSGAELVKPGASVKLSCTASGFNIKDTYMHWVKQKPEQGLE...   \n",
       "2        1a0q  EVQLQESDAELVKPGASVKISCKASGYTFTDHVIHWVKQKPEQGLE...   \n",
       "3        1a14  QVQLQQSGAELVKPGASVRMSCKASGYTFTNYNMYWVKQSPGQGLE...   \n",
       "4        1a2y  QVQLQESGPGLVAPSQSLSITCTVSGFSLTGYGVNWVRQPPGKGLE...   \n",
       "\n",
       "                                               light  Y  cluster  \n",
       "0  DIVMTQSQKFMSTSVGDRVSITCKASQNVGTAVAWYQQKPGQSPKL...  0      677  \n",
       "1  DIVLTQSPAIMSASLGERVTMTCTASSSVSSSNLHWYQQKPGSSPK...  0      685  \n",
       "2  DIELTQSPSSLSASLGGKVTITCKASQDIKKYIGWYQHKPGKQPRL...  1      102  \n",
       "3  DIELTQTTSSLSASLGDRVTISCRASQDISNYLNWYQQNPDGTVKL...  0      442  \n",
       "4  DIVLTQSPASLSASVGETVTITCRASGNIHNYLAWYQQKQGKSPQL...  0       59  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chen_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_data_w_clusters.csv\"), index_col=0)\n",
    "chen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "04e07964-4f10-4bdf-8155-2a6ca501d13a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18     59\n",
       "24     35\n",
       "28     28\n",
       "8      25\n",
       "7      21\n",
       "       ..\n",
       "588     1\n",
       "562     1\n",
       "786     1\n",
       "722     1\n",
       "329     1\n",
       "Name: cluster, Length: 932, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chen_data[\"cluster\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ed9606ba-ee16-428b-b36c-9be6342598ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_k_sets(k, data):\n",
    "    total = len(data)\n",
    "    size = total // k + 1\n",
    "    clusters_by_size = data[\"cluster\"].value_counts().index\n",
    "    cluster_sizes = data[\"cluster\"].value_counts()\n",
    "    groups = { i: [] for i in range(k) }\n",
    "    group_nums = { i: [] for i in range(k) }\n",
    "    group = 0\n",
    "    for clust in clusters_by_size:\n",
    "        start_group = group\n",
    "        if len(groups[group]) + cluster_sizes[clust] > size:\n",
    "            group += 1\n",
    "            group = group % k\n",
    "        while len(groups[group]) + cluster_sizes[clust] > size and group != start_group:\n",
    "            group += 1\n",
    "            group = group % k\n",
    "        if len(groups[group]) < size:\n",
    "            groups[group] += list(data[data[\"cluster\"] == clust].index)\n",
    "            group_nums[group].append(clust)\n",
    "        else:\n",
    "            print(\"error\")\n",
    "    return groups, group_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d79864e4-8da7-4a49-ad11-3b2a16248c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n",
      "154\n",
      "155\n",
      "154\n",
      "156\n",
      "156\n",
      "156\n",
      "156\n",
      "156\n",
      "156\n"
     ]
    }
   ],
   "source": [
    "indices, clusters = split_into_k_sets(10, chen_data)\n",
    "for key, gr in indices.items():\n",
    "    print(len(gr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d42cd957-18dc-4347-880c-62f97048a018",
   "metadata": {},
   "outputs": [],
   "source": [
    "chen_data[\"seq\"] = chen_data[\"heavy\"] + chen_data[\"light\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b0d461-e757-4fbd-a003-b4ed187c7b52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a454118a-9e79-4c92-beff-c3cb26e7f542",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_save_named_model(train_data, valid_data, test_data, name, project_name):\n",
    "    wandb.init(project=project_name, entity=\"kvetab\")\n",
    "    model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "            get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)\n",
    "\n",
    "    training_callbacks = [\n",
    "        keras.callbacks.ReduceLROnPlateau(patience = patience[1], factor = 0.25, min_lr = 1e-07, verbose = 1),\n",
    "        keras.callbacks.EarlyStopping(patience = patience[0], restore_best_weights = True),\n",
    "        WandbCallback()\n",
    "    ]\n",
    "\n",
    "    epoch_num = 100\n",
    "    batch_size = 128\n",
    "    #learning_rate = 1e-5\n",
    "    wandb.config = {\n",
    "          \"learning_rate\": learning_rate,\n",
    "          \"epochs\": epoch_num * 2,\n",
    "          \"batch_size\": batch_size\n",
    "        }\n",
    "    finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data[\"seq\"], train_data[\"Y\"], valid_data['seq'], valid_data[\"Y\"], \\\n",
    "            seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = learning_rate, begin_with_frozen_pretrained_layers = True, \\\n",
    "            lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = learning_rate / 10, callbacks = training_callbacks)\n",
    "    mod = model_generator.create_model(seq_len = 512)\n",
    "    mod.save(path.join(DATA_DIR, f\"protein_bert/{name}\"))\n",
    "\n",
    "    results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "            start_seq_len = 512, start_batch_size = 32)\n",
    "    fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "    f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "    return confusion_matrix, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "10abe629-851d-4f20-af43-ba822d8babdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = {}\n",
    "f1s = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "960bc93a-3bd3-42c1-a575-424936bbe0c3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1zr6vh19) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 1055... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▅▅▆▆▇▇█▁▂▂▃▃▄▅▅▆▁</td></tr><tr><td>loss</td><td>▃▇█▆▂▃▄▁▁▃▃▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████▃▃▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇█▆▂▂▃▁▂▃▃▃▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>2</td></tr><tr><td>best_val_loss</td><td>0.5482</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.47977</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.55076</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">still-aardvark-20</strong>: <a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/1zr6vh19\" target=\"_blank\">https://wandb.ai/kvetab/10_fold_cv/runs/1zr6vh19</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220424_193343-1zr6vh19/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1zr6vh19). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/1i98r18m\" target=\"_blank\">magic-dew-21</a></strong> to <a href=\"https://wandb.ai/kvetab/10_fold_cv\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_25-08:53:30] Training set: Filtered out 0 of 1259 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-08:53:30] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-08:53:30] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 10s 310ms/step - loss: 0.8610 - val_loss: 0.6941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.5770 - val_loss: 0.5120\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.4843 - val_loss: 0.5192\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.4796 - val_loss: 0.5333\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4511 - val_loss: 0.4876\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.4228 - val_loss: 0.4560\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.4083 - val_loss: 0.4341\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3963 - val_loss: 0.4334\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3870 - val_loss: 0.4646\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3892 - val_loss: 0.4552\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3833 - val_loss: 0.4250\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3841 - val_loss: 0.4255\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3888 - val_loss: 0.4601\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3744 - val_loss: 0.4247\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3750 - val_loss: 0.4511\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3691 - val_loss: 0.4184\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3779 - val_loss: 0.4165\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3785 - val_loss: 0.4494\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3551 - val_loss: 0.4145\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3354 - val_loss: 0.4243\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3472 - val_loss: 0.4353\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3529 - val_loss: 0.4152\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3644 - val_loss: 0.4298\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3422 - val_loss: 0.4154\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3304 - val_loss: 0.4170\n",
      "[2022_04_25-08:54:08] Training the entire fine-tuned model...\n",
      "[2022_04_25-08:54:28] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.3613WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1136s vs `on_train_batch_end` time: 0.1442s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1136s vs `on_train_batch_end` time: 0.1442s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 10s 469ms/step - loss: 0.3714 - val_loss: 0.4478\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3496 - val_loss: 0.4266\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3423 - val_loss: 0.4137\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 3s 265ms/step - loss: 0.3290 - val_loss: 0.4124\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3305 - val_loss: 0.4131\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3111 - val_loss: 0.4119\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3026 - val_loss: 0.4150\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2976 - val_loss: 0.4260\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2897 - val_loss: 0.4377\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2865 - val_loss: 0.4220\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2687 - val_loss: 0.4225\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2679 - val_loss: 0.4278\n",
      "[2022_04_25-08:55:08] Training on final epochs of sequence length 1024...\n",
      "[2022_04_25-08:55:08] Training set: Filtered out 0 of 1259 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_25-08:55:15] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 1022.\n",
      " 6/20 [========>.....................] - ETA: 3s - loss: 0.3189WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1072s vs `on_train_batch_end` time: 0.1445s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1072s vs `on_train_batch_end` time: 0.1445s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 13s 349ms/step - loss: 0.3239 - val_loss: 0.4070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_0/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1i98r18m) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 7695... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▄▄▄▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂</td></tr><tr><td>lr</td><td>███████████████████████▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▄▄▃▂▂▂▂▂▁▁▂▁▂▁▁▂▁▁▂▁▂▁▁▂▁▁▁▁▁▁▁▂▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.40695</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32394</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.40695</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">magic-dew-21</strong>: <a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/1i98r18m\" target=\"_blank\">https://wandb.ai/kvetab/10_fold_cv/runs/1i98r18m</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220425_085314-1i98r18m/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1i98r18m). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/1az6la9h\" target=\"_blank\">driven-frog-22</a></strong> to <a href=\"https://wandb.ai/kvetab/10_fold_cv\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_25-08:56:09] Training set: Filtered out 0 of 1257 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-08:56:09] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-08:56:09] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 10s 307ms/step - loss: 0.8050 - val_loss: 0.8118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.5750 - val_loss: 0.5913\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.4917 - val_loss: 0.5065\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.4633 - val_loss: 0.4744\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.4332 - val_loss: 0.4574\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4395 - val_loss: 0.4602\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4275 - val_loss: 0.4517\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.4151 - val_loss: 0.4393\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.4046 - val_loss: 0.4240\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.4015 - val_loss: 0.4253\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3901 - val_loss: 0.4113\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3966 - val_loss: 0.4052\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3811 - val_loss: 0.4063\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3881 - val_loss: 0.3968\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3990 - val_loss: 0.3962\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.3919 - val_loss: 0.3923\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3867 - val_loss: 0.4261\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3667 - val_loss: 0.3960\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3849 - val_loss: 0.4550\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.3944 - val_loss: 0.3953\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3837 - val_loss: 0.4103\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3734 - val_loss: 0.3869\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3582 - val_loss: 0.3857\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3592 - val_loss: 0.3828\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3494 - val_loss: 0.3840\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3615 - val_loss: 0.3855\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3550 - val_loss: 0.3834\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.3642 - val_loss: 0.3847\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3532 - val_loss: 0.3818\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3703 - val_loss: 0.3839\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3634 - val_loss: 0.3844\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3565 - val_loss: 0.3813\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3606 - val_loss: 0.3828\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3664 - val_loss: 0.3837\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3568 - val_loss: 0.3810\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3560 - val_loss: 0.3828\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3670 - val_loss: 0.3805\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3635 - val_loss: 0.3870\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3534 - val_loss: 0.3808\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3578 - val_loss: 0.3823\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3590 - val_loss: 0.3816\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 42/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3621 - val_loss: 0.3805\n",
      "Epoch 43/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3484 - val_loss: 0.3804\n",
      "Epoch 44/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3474 - val_loss: 0.3814\n",
      "Epoch 45/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3613 - val_loss: 0.3818\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 46/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3503 - val_loss: 0.3820\n",
      "Epoch 47/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3597 - val_loss: 0.3819\n",
      "Epoch 48/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3528 - val_loss: 0.3817\n",
      "Epoch 49/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3565 - val_loss: 0.3814\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "[2022_04_25-08:57:15] Training the entire fine-tuned model...\n",
      "[2022_04_25-08:57:24] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.3770WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1088s vs `on_train_batch_end` time: 0.1484s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1088s vs `on_train_batch_end` time: 0.1484s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 10s 452ms/step - loss: 0.3701 - val_loss: 0.3781\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 3s 271ms/step - loss: 0.3523 - val_loss: 0.3793\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3573 - val_loss: 0.3706\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3525 - val_loss: 0.3650\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3419 - val_loss: 0.3608\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3438 - val_loss: 0.3591\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3342 - val_loss: 0.3749\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 3s 271ms/step - loss: 0.3362 - val_loss: 0.3531\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.3207 - val_loss: 0.3515\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.3261 - val_loss: 0.3509\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3080 - val_loss: 0.3586\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 3s 272ms/step - loss: 0.2992 - val_loss: 0.3598\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 3s 270ms/step - loss: 0.2926 - val_loss: 0.3567\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 3s 273ms/step - loss: 0.2885 - val_loss: 0.3605\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 3s 270ms/step - loss: 0.2769 - val_loss: 0.3573\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 3s 270ms/step - loss: 0.2623 - val_loss: 0.3569\n",
      "[2022_04_25-08:58:15] Training on final epochs of sequence length 1024...\n",
      "[2022_04_25-08:58:15] Training set: Filtered out 0 of 1257 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_25-08:58:15] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 1022.\n",
      " 6/20 [========>.....................] - ETA: 3s - loss: 0.3066WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1122s vs `on_train_batch_end` time: 0.1427s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1122s vs `on_train_batch_end` time: 0.1427s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 13s 353ms/step - loss: 0.3109 - val_loss: 0.3575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_1/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1az6la9h) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 7984... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▃▃▂▂▂▂▂▂▃▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>9</td></tr><tr><td>best_val_loss</td><td>0.35093</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31087</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.35749</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">driven-frog-22</strong>: <a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/1az6la9h\" target=\"_blank\">https://wandb.ai/kvetab/10_fold_cv/runs/1az6la9h</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220425_085554-1az6la9h/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1az6la9h). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/140pjwfs\" target=\"_blank\">efficient-pond-23</a></strong> to <a href=\"https://wandb.ai/kvetab/10_fold_cv\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_25-08:59:11] Training set: Filtered out 0 of 1256 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-08:59:11] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-08:59:11] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 11s 335ms/step - loss: 0.7739 - val_loss: 0.4855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.5156 - val_loss: 0.4227\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.4792 - val_loss: 0.4273\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.4431 - val_loss: 0.4503\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.4515 - val_loss: 0.3991\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.4399 - val_loss: 0.4018\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.4258 - val_loss: 0.3785\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.4088 - val_loss: 0.3853\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.4079 - val_loss: 0.3714\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3959 - val_loss: 0.3658\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.4010 - val_loss: 0.3623\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3951 - val_loss: 0.3707\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3948 - val_loss: 0.3656\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3898 - val_loss: 0.3896\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.4123 - val_loss: 0.3767\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3781 - val_loss: 0.3547\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3822 - val_loss: 0.3525\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3757 - val_loss: 0.3561\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3719 - val_loss: 0.3491\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3655 - val_loss: 0.3503\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3601 - val_loss: 0.3482\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3616 - val_loss: 0.3490\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3801 - val_loss: 0.3635\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3683 - val_loss: 0.3488\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3654 - val_loss: 0.3475\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3683 - val_loss: 0.3483\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3667 - val_loss: 0.3469\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3692 - val_loss: 0.3535\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3783 - val_loss: 0.3609\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3648 - val_loss: 0.3454\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3680 - val_loss: 0.3587\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3633 - val_loss: 0.3461\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3585 - val_loss: 0.3449\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3729 - val_loss: 0.3484\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3726 - val_loss: 0.3734\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3818 - val_loss: 0.3443\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3511 - val_loss: 0.3424\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3600 - val_loss: 0.3424\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3533 - val_loss: 0.3427\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3610 - val_loss: 0.3457\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3607 - val_loss: 0.3443\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 42/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3499 - val_loss: 0.3409\n",
      "Epoch 43/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3553 - val_loss: 0.3411\n",
      "Epoch 44/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3447 - val_loss: 0.3412\n",
      "Epoch 45/100\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3616 - val_loss: 0.3422\n",
      "Epoch 46/100\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3526 - val_loss: 0.3403\n",
      "Epoch 47/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3565 - val_loss: 0.3403\n",
      "Epoch 48/100\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3544 - val_loss: 0.3403\n",
      "Epoch 49/100\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3515 - val_loss: 0.3403\n",
      "Epoch 50/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3487 - val_loss: 0.3402\n",
      "Epoch 51/100\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.3488 - val_loss: 0.3402\n",
      "Epoch 52/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3521 - val_loss: 0.3401\n",
      "Epoch 53/100\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3474 - val_loss: 0.3402\n",
      "Epoch 54/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3466 - val_loss: 0.3404\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 55/100\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3424 - val_loss: 0.3401\n",
      "Epoch 56/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3514 - val_loss: 0.3401\n",
      "Epoch 57/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3533 - val_loss: 0.3401\n",
      "Epoch 58/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3479 - val_loss: 0.3401\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 59/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3545 - val_loss: 0.3401\n",
      "Epoch 60/100\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.3460 - val_loss: 0.3401\n",
      "Epoch 61/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3538 - val_loss: 0.3401\n",
      "Epoch 62/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3429 - val_loss: 0.3401\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 63/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3480 - val_loss: 0.3401\n",
      "Epoch 64/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3481 - val_loss: 0.3401\n",
      "Epoch 65/100\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.3578 - val_loss: 0.3401\n",
      "Epoch 66/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3432 - val_loss: 0.3401\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "Epoch 67/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3554 - val_loss: 0.3401\n",
      "Epoch 68/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3434 - val_loss: 0.3401\n",
      "Epoch 69/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3569 - val_loss: 0.3401\n",
      "Epoch 70/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3556 - val_loss: 0.3401\n",
      "\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "[2022_04_25-09:00:42] Training the entire fine-tuned model...\n",
      "[2022_04_25-09:01:50] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.3470WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1129s vs `on_train_batch_end` time: 0.1454s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1129s vs `on_train_batch_end` time: 0.1454s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 10s 452ms/step - loss: 0.3772 - val_loss: 0.3399\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3864 - val_loss: 0.3710\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 3s 265ms/step - loss: 0.3561 - val_loss: 0.3381\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 3s 265ms/step - loss: 0.3599 - val_loss: 0.3372\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3453 - val_loss: 0.3385\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3210 - val_loss: 0.3327\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.3300 - val_loss: 0.3319\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3157 - val_loss: 0.3483\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3108 - val_loss: 0.3486\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3011 - val_loss: 0.3408\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2958 - val_loss: 0.3338\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.2733 - val_loss: 0.3279\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2770 - val_loss: 0.3305\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.2837 - val_loss: 0.3292\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2664 - val_loss: 0.3313\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2764 - val_loss: 0.3308\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.2816 - val_loss: 0.3307\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 3s 272ms/step - loss: 0.2616 - val_loss: 0.3321\n",
      "[2022_04_25-09:02:46] Training on final epochs of sequence length 1024...\n",
      "[2022_04_25-09:02:46] Training set: Filtered out 0 of 1256 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_25-09:03:17] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 1022.\n",
      " 6/20 [========>.....................] - ETA: 3s - loss: 0.3001WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1074s vs `on_train_batch_end` time: 0.1427s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1074s vs `on_train_batch_end` time: 0.1427s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 13s 352ms/step - loss: 0.3096 - val_loss: 0.3388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_2/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:140pjwfs) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 8427... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂</td></tr><tr><td>lr</td><td>███████▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▃▄▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>11</td></tr><tr><td>best_val_loss</td><td>0.3279</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3096</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.33878</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">efficient-pond-23</strong>: <a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/140pjwfs\" target=\"_blank\">https://wandb.ai/kvetab/10_fold_cv/runs/140pjwfs</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220425_085855-140pjwfs/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:140pjwfs). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/3jjb4y0g\" target=\"_blank\">dazzling-deluge-24</a></strong> to <a href=\"https://wandb.ai/kvetab/10_fold_cv\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_25-09:04:16] Training set: Filtered out 0 of 1257 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-09:04:16] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-09:04:16] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 10s 305ms/step - loss: 0.7361 - val_loss: 0.6745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.5401 - val_loss: 0.5419\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.5139 - val_loss: 0.4968\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4533 - val_loss: 0.4624\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4309 - val_loss: 0.4514\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.4210 - val_loss: 0.4470\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4214 - val_loss: 0.4446\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.4105 - val_loss: 0.4490\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3999 - val_loss: 0.4275\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3960 - val_loss: 0.4170\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3910 - val_loss: 0.4207\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3864 - val_loss: 0.4764\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.4071 - val_loss: 0.4096\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3910 - val_loss: 0.4107\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3710 - val_loss: 0.4062\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3746 - val_loss: 0.4134\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3715 - val_loss: 0.4189\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3833 - val_loss: 0.3963\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3782 - val_loss: 0.4338\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3715 - val_loss: 0.3907\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3684 - val_loss: 0.4156\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3745 - val_loss: 0.4495\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.4004 - val_loss: 0.4413\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3859 - val_loss: 0.4539\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3627 - val_loss: 0.3861\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3599 - val_loss: 0.3855\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3505 - val_loss: 0.4157\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3454 - val_loss: 0.3843\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3577 - val_loss: 0.3851\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3432 - val_loss: 0.4032\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3533 - val_loss: 0.3834\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3549 - val_loss: 0.3882\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3580 - val_loss: 0.3827\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3555 - val_loss: 0.3933\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3427 - val_loss: 0.3873\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3484 - val_loss: 0.3838\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3511 - val_loss: 0.3965\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3501 - val_loss: 0.3851\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3414 - val_loss: 0.3887\n",
      "[2022_04_25-09:05:11] Training the entire fine-tuned model...\n",
      "[2022_04_25-09:05:19] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.3777WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1127s vs `on_train_batch_end` time: 0.1470s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1127s vs `on_train_batch_end` time: 0.1470s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 10s 455ms/step - loss: 0.3671 - val_loss: 0.3825\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3464 - val_loss: 0.3771\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3357 - val_loss: 0.3734\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3267 - val_loss: 0.3699\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.3249 - val_loss: 0.3719\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 3s 272ms/step - loss: 0.3258 - val_loss: 0.3664\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.3272 - val_loss: 0.3636\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 3s 270ms/step - loss: 0.3020 - val_loss: 0.3670\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3066 - val_loss: 0.3799\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.2992 - val_loss: 0.4688\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.3021 - val_loss: 0.4132\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2796 - val_loss: 0.3796\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.2674 - val_loss: 0.4063\n",
      "[2022_04_25-09:06:02] Training on final epochs of sequence length 1024...\n",
      "[2022_04_25-09:06:02] Training set: Filtered out 0 of 1257 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_25-09:06:06] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 1022.\n",
      " 6/20 [========>.....................] - ETA: 3s - loss: 0.3318WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1061s vs `on_train_batch_end` time: 0.1435s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1061s vs `on_train_batch_end` time: 0.1435s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 13s 349ms/step - loss: 0.3259 - val_loss: 0.3732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_3/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3jjb4y0g) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9010... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▅▃▃▃▃▃▃▃▃▂▂▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂</td></tr><tr><td>lr</td><td>██████████████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▃▂▃▃▂▁▂▁▂▁▁▂▂▂▁▂▁▁▁▁▁▁▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>6</td></tr><tr><td>best_val_loss</td><td>0.36359</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32595</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.37324</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dazzling-deluge-24</strong>: <a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/3jjb4y0g\" target=\"_blank\">https://wandb.ai/kvetab/10_fold_cv/runs/3jjb4y0g</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220425_090400-3jjb4y0g/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3jjb4y0g). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/7ettjqe6\" target=\"_blank\">sparkling-violet-25</a></strong> to <a href=\"https://wandb.ai/kvetab/10_fold_cv\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_25-09:07:03] Training set: Filtered out 0 of 1255 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-09:07:03] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-09:07:03] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 10s 315ms/step - loss: 0.7822 - val_loss: 0.5628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.5269 - val_loss: 0.4314\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.4867 - val_loss: 0.4171\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.4573 - val_loss: 0.4122\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.4662 - val_loss: 0.3966\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.4584 - val_loss: 0.3954\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.4232 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.4297 - val_loss: 0.3895\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.4057 - val_loss: 0.3583\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.4138 - val_loss: 0.3791\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3991 - val_loss: 0.3884\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.4290 - val_loss: 0.3515\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.4034 - val_loss: 0.3437\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.4062 - val_loss: 0.4179\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.4346 - val_loss: 0.3412\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.4127 - val_loss: 0.3825\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3947 - val_loss: 0.3594\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3804 - val_loss: 0.3308\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.3731 - val_loss: 0.3303\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3885 - val_loss: 0.3403\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3790 - val_loss: 0.3283\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3654 - val_loss: 0.3291\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3681 - val_loss: 0.3443\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3621 - val_loss: 0.3268\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3593 - val_loss: 0.3251\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3704 - val_loss: 0.3253\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3627 - val_loss: 0.3349\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3513 - val_loss: 0.3630\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3626 - val_loss: 0.3256\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3565 - val_loss: 0.3263\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3468 - val_loss: 0.3225\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3564 - val_loss: 0.3216\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3464 - val_loss: 0.3209\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3502 - val_loss: 0.3204\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3482 - val_loss: 0.3205\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3460 - val_loss: 0.3201\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3503 - val_loss: 0.3287\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3563 - val_loss: 0.3341\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3574 - val_loss: 0.3202\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3506 - val_loss: 0.3204\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3414 - val_loss: 0.3202\n",
      "Epoch 42/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3559 - val_loss: 0.3207\n",
      "[2022_04_25-09:08:02] Training the entire fine-tuned model...\n",
      "[2022_04_25-09:08:42] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.3826WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1127s vs `on_train_batch_end` time: 0.1472s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1127s vs `on_train_batch_end` time: 0.1472s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 10s 461ms/step - loss: 0.3759 - val_loss: 0.3166\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3650 - val_loss: 0.3214\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3287 - val_loss: 0.3244\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3359 - val_loss: 0.3169\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 3s 270ms/step - loss: 0.3403 - val_loss: 0.3210\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 3s 265ms/step - loss: 0.3173 - val_loss: 0.3161\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3247 - val_loss: 0.3149\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3195 - val_loss: 0.3151\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3009 - val_loss: 0.3145\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3055 - val_loss: 0.3139\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3035 - val_loss: 0.3143\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2992 - val_loss: 0.3158\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 3s 265ms/step - loss: 0.2979 - val_loss: 0.3133\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.2819 - val_loss: 0.3118\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2999 - val_loss: 0.3107\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2864 - val_loss: 0.3102\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2904 - val_loss: 0.3123\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2852 - val_loss: 0.3138\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2845 - val_loss: 0.3163\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2922 - val_loss: 0.3134\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2743 - val_loss: 0.3143\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.2828 - val_loss: 0.3136\n",
      "[2022_04_25-09:09:50] Training on final epochs of sequence length 1024...\n",
      "[2022_04_25-09:09:50] Training set: Filtered out 0 of 1255 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_25-09:10:10] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 1022.\n",
      " 6/20 [========>.....................] - ETA: 3s - loss: 0.3525WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1076s vs `on_train_batch_end` time: 0.1425s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1076s vs `on_train_batch_end` time: 0.1425s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 13s 351ms/step - loss: 0.3286 - val_loss: 0.3127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_4/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:7ettjqe6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9444... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇██▁▁▂▂▂▂▃▃▃▄▄▄▄▁</td></tr><tr><td>loss</td><td>█▄▄▄▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂</td></tr><tr><td>lr</td><td>██████████████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▄▃▃▂▃▂▄▂▂▂▂▂▂▁▂▂▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>15</td></tr><tr><td>best_val_loss</td><td>0.31023</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32862</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.31268</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">sparkling-violet-25</strong>: <a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/7ettjqe6\" target=\"_blank\">https://wandb.ai/kvetab/10_fold_cv/runs/7ettjqe6</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220425_090646-7ettjqe6/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:7ettjqe6). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/36p6imek\" target=\"_blank\">dainty-wood-26</a></strong> to <a href=\"https://wandb.ai/kvetab/10_fold_cv\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_25-09:11:07] Training set: Filtered out 0 of 1255 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-09:11:07] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-09:11:07] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 10s 312ms/step - loss: 0.7287 - val_loss: 0.6076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.5179 - val_loss: 0.4699\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4529 - val_loss: 0.4603\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.4431 - val_loss: 0.4489\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4268 - val_loss: 0.4501\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4283 - val_loss: 0.4665\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4226 - val_loss: 0.4513\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4135 - val_loss: 0.4288\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.4003 - val_loss: 0.4266\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3889 - val_loss: 0.4172\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3992 - val_loss: 0.4146\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3765 - val_loss: 0.4107\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.4010 - val_loss: 0.4161\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3982 - val_loss: 0.4065\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3782 - val_loss: 0.4186\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.3722 - val_loss: 0.4021\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3768 - val_loss: 0.4967\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4098 - val_loss: 0.4007\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.4010 - val_loss: 0.4210\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.3859 - val_loss: 0.4574\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.3867 - val_loss: 0.4043\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.3723 - val_loss: 0.3986\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3722 - val_loss: 0.4202\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3888 - val_loss: 0.4248\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3676 - val_loss: 0.4647\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3819 - val_loss: 0.3893\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3784 - val_loss: 0.4002\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3496 - val_loss: 0.4144\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3480 - val_loss: 0.3857\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3604 - val_loss: 0.3835\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3617 - val_loss: 0.3895\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3622 - val_loss: 0.3804\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3616 - val_loss: 0.3813\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3445 - val_loss: 0.3855\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3386 - val_loss: 0.3817\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3442 - val_loss: 0.3840\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3316 - val_loss: 0.3851\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3419 - val_loss: 0.3832\n",
      "[2022_04_25-09:12:01] Training the entire fine-tuned model...\n",
      "[2022_04_25-09:12:09] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.3417WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1092s vs `on_train_batch_end` time: 0.1483s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1092s vs `on_train_batch_end` time: 0.1483s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 10s 449ms/step - loss: 0.3426 - val_loss: 0.3866\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3522 - val_loss: 0.3932\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3347 - val_loss: 0.3794\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3166 - val_loss: 0.3813\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3293 - val_loss: 0.3750\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 3s 265ms/step - loss: 0.3391 - val_loss: 0.3698\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3293 - val_loss: 0.3694\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3332 - val_loss: 0.4352\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3287 - val_loss: 0.3951\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.3057 - val_loss: 0.3666\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.3057 - val_loss: 0.3674\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2988 - val_loss: 0.3695\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2884 - val_loss: 0.3655\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2895 - val_loss: 0.3813\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2871 - val_loss: 0.3692\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2881 - val_loss: 0.3718\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2622 - val_loss: 0.3668\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 3s 272ms/step - loss: 0.2724 - val_loss: 0.3639\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.2606 - val_loss: 0.3665\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 3s 270ms/step - loss: 0.2539 - val_loss: 0.3682\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2537 - val_loss: 0.3708\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.2656 - val_loss: 0.3749\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2425 - val_loss: 0.3708\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2532 - val_loss: 0.3709\n",
      "[2022_04_25-09:13:21] Training on final epochs of sequence length 1024...\n",
      "[2022_04_25-09:13:21] Training set: Filtered out 0 of 1255 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_25-09:13:21] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 1022.\n",
      " 6/20 [========>.....................] - ETA: 3s - loss: 0.2975WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1097s vs `on_train_batch_end` time: 0.1429s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1097s vs `on_train_batch_end` time: 0.1429s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 13s 350ms/step - loss: 0.2875 - val_loss: 0.3577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_5/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:36p6imek) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9891... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅</td></tr><tr><td>loss</td><td>█▅▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████████▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▃▄▃▃▂▂▃▂▂▄▂▃▃▂▂▂▂▁▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.35775</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.28746</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.35775</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dainty-wood-26</strong>: <a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/36p6imek\" target=\"_blank\">https://wandb.ai/kvetab/10_fold_cv/runs/36p6imek</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220425_091050-36p6imek/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:36p6imek). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/1gke3ejq\" target=\"_blank\">earnest-bird-27</a></strong> to <a href=\"https://wandb.ai/kvetab/10_fold_cv\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_25-09:14:19] Training set: Filtered out 0 of 1255 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-09:14:19] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-09:14:19] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 10s 317ms/step - loss: 0.7653 - val_loss: 0.6297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.5217 - val_loss: 0.5100\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.4588 - val_loss: 0.4614\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.4360 - val_loss: 0.4555\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.4331 - val_loss: 0.4640\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.4194 - val_loss: 0.4377\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.4145 - val_loss: 0.4281\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4001 - val_loss: 0.4284\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.4077 - val_loss: 0.4547\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4039 - val_loss: 0.4171\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3847 - val_loss: 0.4202\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.4041 - val_loss: 0.4174\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.4176 - val_loss: 0.4439\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3868 - val_loss: 0.4159\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3744 - val_loss: 0.4165\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3848 - val_loss: 0.4215\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3618 - val_loss: 0.4127\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3717 - val_loss: 0.4076\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3601 - val_loss: 0.4053\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3468 - val_loss: 0.4061\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3519 - val_loss: 0.4152\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3675 - val_loss: 0.4108\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3511 - val_loss: 0.4033\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3632 - val_loss: 0.4031\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3542 - val_loss: 0.4091\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3396 - val_loss: 0.4027\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3491 - val_loss: 0.4111\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3318 - val_loss: 0.4060\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3355 - val_loss: 0.4111\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3483 - val_loss: 0.4289\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3388 - val_loss: 0.4031\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3397 - val_loss: 0.4036\n",
      "[2022_04_25-09:15:06] Training the entire fine-tuned model...\n",
      "[2022_04_25-09:15:17] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.3476WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1130s vs `on_train_batch_end` time: 0.1482s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1130s vs `on_train_batch_end` time: 0.1482s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 11s 531ms/step - loss: 0.3537 - val_loss: 0.4090\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3474 - val_loss: 0.4174\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 3s 265ms/step - loss: 0.3383 - val_loss: 0.4027\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3451 - val_loss: 0.4016\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.3285 - val_loss: 0.4159\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3305 - val_loss: 0.4007\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3265 - val_loss: 0.4019\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3253 - val_loss: 0.4122\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3271 - val_loss: 0.4293\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3057 - val_loss: 0.4111\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3004 - val_loss: 0.4034\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2997 - val_loss: 0.4072\n",
      "[2022_04_25-09:15:58] Training on final epochs of sequence length 1024...\n",
      "[2022_04_25-09:15:58] Training set: Filtered out 0 of 1255 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_25-09:15:59] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 1022.\n",
      " 6/20 [========>.....................] - ETA: 3s - loss: 0.2938WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1067s vs `on_train_batch_end` time: 0.1458s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1067s vs `on_train_batch_end` time: 0.1458s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 13s 350ms/step - loss: 0.3294 - val_loss: 0.4074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_6/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1gke3ejq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10306... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇███▁▁▁▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▃▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████████████▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▃▃▂▂▂▁▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.40072</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32937</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.40744</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">earnest-bird-27</strong>: <a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/1gke3ejq\" target=\"_blank\">https://wandb.ai/kvetab/10_fold_cv/runs/1gke3ejq</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220425_091404-1gke3ejq/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1gke3ejq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/zleuaf64\" target=\"_blank\">eager-armadillo-28</a></strong> to <a href=\"https://wandb.ai/kvetab/10_fold_cv\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_25-09:16:54] Training set: Filtered out 0 of 1255 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-09:16:54] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-09:16:54] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 10s 411ms/step - loss: 0.7098 - val_loss: 0.6714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.5580 - val_loss: 0.4641\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.4748 - val_loss: 0.4528\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.4416 - val_loss: 0.4293\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.4390 - val_loss: 0.4345\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4256 - val_loss: 0.4266\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.4148 - val_loss: 0.4156\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4086 - val_loss: 0.4114\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4082 - val_loss: 0.4307\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.4151 - val_loss: 0.4991\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.4117 - val_loss: 0.4079\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3727 - val_loss: 0.4237\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3855 - val_loss: 0.4105\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3797 - val_loss: 0.4024\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3721 - val_loss: 0.4747\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3784 - val_loss: 0.4006\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3786 - val_loss: 0.4030\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3642 - val_loss: 0.3982\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3622 - val_loss: 0.4103\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3668 - val_loss: 0.4218\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3833 - val_loss: 0.3965\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3558 - val_loss: 0.3947\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3652 - val_loss: 0.4204\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3517 - val_loss: 0.3970\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3690 - val_loss: 0.3959\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3567 - val_loss: 0.4271\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3531 - val_loss: 0.3941\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3444 - val_loss: 0.3991\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3627 - val_loss: 0.3939\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3449 - val_loss: 0.3964\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3512 - val_loss: 0.3975\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3463 - val_loss: 0.3938\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3485 - val_loss: 0.3974\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3420 - val_loss: 0.4086\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3387 - val_loss: 0.3983\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3380 - val_loss: 0.3937\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3330 - val_loss: 0.3957\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3448 - val_loss: 0.3994\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3422 - val_loss: 0.3956\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3356 - val_loss: 0.3959\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3403 - val_loss: 0.3970\n",
      "Epoch 42/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3523 - val_loss: 0.3973\n",
      "[2022_04_25-09:17:52] Training the entire fine-tuned model...\n",
      "[2022_04_25-09:18:31] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.3634WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1121s vs `on_train_batch_end` time: 0.1464s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1121s vs `on_train_batch_end` time: 0.1464s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 10s 453ms/step - loss: 0.3620 - val_loss: 0.4231\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3644 - val_loss: 0.3881\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3476 - val_loss: 0.4657\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3637 - val_loss: 0.4235\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3486 - val_loss: 0.3896\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3375 - val_loss: 0.4100\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3408 - val_loss: 0.3992\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.3180 - val_loss: 0.3921\n",
      "[2022_04_25-09:19:01] Training on final epochs of sequence length 1024...\n",
      "[2022_04_25-09:19:01] Training set: Filtered out 0 of 1255 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_25-09:19:01] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 1022.\n",
      " 6/20 [========>.....................] - ETA: 3s - loss: 0.3376WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1101s vs `on_train_batch_end` time: 0.1423s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1101s vs `on_train_batch_end` time: 0.1423s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 13s 354ms/step - loss: 0.3397 - val_loss: 0.3862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_7/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:zleuaf64) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10631... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇██▁▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████████▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▂▂▂▂▂▂▂▂▃▁▁▁▂▁▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▂▁▃▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.38623</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33967</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.38623</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">eager-armadillo-28</strong>: <a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/zleuaf64\" target=\"_blank\">https://wandb.ai/kvetab/10_fold_cv/runs/zleuaf64</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220425_091639-zleuaf64/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:zleuaf64). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/2mtvyzmm\" target=\"_blank\">olive-paper-29</a></strong> to <a href=\"https://wandb.ai/kvetab/10_fold_cv\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_25-09:19:57] Training set: Filtered out 0 of 1255 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-09:19:57] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-09:19:57] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 11s 309ms/step - loss: 0.7575 - val_loss: 0.8898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.5707 - val_loss: 0.6389\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.4822 - val_loss: 0.5535\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.4654 - val_loss: 0.4804\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4401 - val_loss: 0.4506\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4142 - val_loss: 0.4489\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4141 - val_loss: 0.4386\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4112 - val_loss: 0.4393\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4001 - val_loss: 0.4756\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3929 - val_loss: 0.4321\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.4033 - val_loss: 0.4201\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4023 - val_loss: 0.4163\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.4081 - val_loss: 0.5364\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3970 - val_loss: 0.4156\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3812 - val_loss: 0.4046\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3665 - val_loss: 0.4513\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3634 - val_loss: 0.4530\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3840 - val_loss: 0.4034\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3765 - val_loss: 0.4170\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.3629 - val_loss: 0.4035\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3569 - val_loss: 0.3953\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3576 - val_loss: 0.3998\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3599 - val_loss: 0.4788\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3727 - val_loss: 0.3905\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3560 - val_loss: 0.4084\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3532 - val_loss: 0.3991\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3447 - val_loss: 0.4362\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3595 - val_loss: 0.3977\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3382 - val_loss: 0.3937\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3442 - val_loss: 0.3862\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3333 - val_loss: 0.4066\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3447 - val_loss: 0.3846\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3422 - val_loss: 0.3816\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.3363 - val_loss: 0.3955\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3425 - val_loss: 0.3816\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3408 - val_loss: 0.3906\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3425 - val_loss: 0.4311\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3500 - val_loss: 0.3829\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3354 - val_loss: 0.3884\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3328 - val_loss: 0.3918\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3344 - val_loss: 0.3943\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_25-09:20:54] Training the entire fine-tuned model...\n",
      "[2022_04_25-09:21:37] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.3430WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1143s vs `on_train_batch_end` time: 0.1442s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1143s vs `on_train_batch_end` time: 0.1442s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 10s 454ms/step - loss: 0.3507 - val_loss: 0.3770\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 3s 264ms/step - loss: 0.3349 - val_loss: 0.3769\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 3s 265ms/step - loss: 0.3318 - val_loss: 0.3748\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 3s 265ms/step - loss: 0.3268 - val_loss: 0.3781\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3221 - val_loss: 0.4427\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3301 - val_loss: 0.4516\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3132 - val_loss: 0.4228\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 3s 272ms/step - loss: 0.3105 - val_loss: 0.3724\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3058 - val_loss: 0.4007\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3002 - val_loss: 0.3773\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2971 - val_loss: 0.3857\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3035 - val_loss: 0.3809\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2955 - val_loss: 0.3776\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3020 - val_loss: 0.3822\n",
      "[2022_04_25-09:22:22] Training on final epochs of sequence length 1024...\n",
      "[2022_04_25-09:22:22] Training set: Filtered out 0 of 1255 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_25-09:22:22] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 1022.\n",
      " 6/20 [========>.....................] - ETA: 3s - loss: 0.3527WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1085s vs `on_train_batch_end` time: 0.1438s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1085s vs `on_train_batch_end` time: 0.1438s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 12s 347ms/step - loss: 0.3258 - val_loss: 0.3823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_8/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2mtvyzmm) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10993... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▂▂▂▂▃▁▂▂▂▁▁▂▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.37241</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3258</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.38233</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">olive-paper-29</strong>: <a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/2mtvyzmm\" target=\"_blank\">https://wandb.ai/kvetab/10_fold_cv/runs/2mtvyzmm</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220425_091941-2mtvyzmm/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2mtvyzmm). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/3jwkfo4a\" target=\"_blank\">dutiful-totem-30</a></strong> to <a href=\"https://wandb.ai/kvetab/10_fold_cv\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_25-09:23:21] Training set: Filtered out 0 of 1255 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-09:23:21] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-09:23:21] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 11s 336ms/step - loss: 0.8156 - val_loss: 0.7028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.6446 - val_loss: 0.4759\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.5166 - val_loss: 0.4526\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.4499 - val_loss: 0.4427\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.4295 - val_loss: 0.4454\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.4271 - val_loss: 0.4376\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.4161 - val_loss: 0.4305\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.4028 - val_loss: 0.4279\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.4039 - val_loss: 0.4305\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3937 - val_loss: 0.4262\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3885 - val_loss: 0.4260\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3966 - val_loss: 0.4232\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3882 - val_loss: 0.4429\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3857 - val_loss: 0.4234\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3724 - val_loss: 0.4303\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3690 - val_loss: 0.4175\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3658 - val_loss: 0.4321\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3643 - val_loss: 0.4137\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3655 - val_loss: 0.4144\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3564 - val_loss: 0.4337\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3649 - val_loss: 0.4234\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3611 - val_loss: 0.4113\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3654 - val_loss: 0.4653\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3717 - val_loss: 0.4307\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3711 - val_loss: 0.4028\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3733 - val_loss: 0.4498\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3524 - val_loss: 0.4151\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3489 - val_loss: 0.4287\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3557 - val_loss: 0.4151\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3401 - val_loss: 0.4100\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3369 - val_loss: 0.4033\n",
      "[2022_04_25-09:24:08] Training the entire fine-tuned model...\n",
      "[2022_04_25-09:24:16] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.3569WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1129s vs `on_train_batch_end` time: 0.1462s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1129s vs `on_train_batch_end` time: 0.1462s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 11s 466ms/step - loss: 0.3588 - val_loss: 0.4069\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3572 - val_loss: 0.4109\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3392 - val_loss: 0.4149\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3302 - val_loss: 0.4168\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3467 - val_loss: 0.4003\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 3s 271ms/step - loss: 0.3347 - val_loss: 0.4144\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3256 - val_loss: 0.4085\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.3112 - val_loss: 0.3991\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3152 - val_loss: 0.4014\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 3s 271ms/step - loss: 0.3037 - val_loss: 0.3940\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.3026 - val_loss: 0.3957\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2770 - val_loss: 0.3984\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.2770 - val_loss: 0.3978\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2838 - val_loss: 0.4059\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2535 - val_loss: 0.4030\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2490 - val_loss: 0.4062\n",
      "[2022_04_25-09:25:08] Training on final epochs of sequence length 1024...\n",
      "[2022_04_25-09:25:08] Training set: Filtered out 0 of 1255 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_25-09:25:44] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 1022.\n",
      " 6/20 [========>.....................] - ETA: 3s - loss: 0.3346WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1078s vs `on_train_batch_end` time: 0.1434s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1078s vs `on_train_batch_end` time: 0.1434s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 12s 346ms/step - loss: 0.3001 - val_loss: 0.3876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_9/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    test = chen_data.loc[indices[i]]\n",
    "    remaining = [idx for idx in list(chen_data.index) if idx not in indices[i]]\n",
    "    train = chen_data.loc[remaining]\n",
    "    train, valid = train_test_split(train, test_size=0.1, random_state=333)\n",
    "    cm, f1 = train_and_save_named_model(train, valid, test, f\"10-fold-cv/2022_04_24_split_{i}\", \"10_fold_cv\")\n",
    "    cms[i] = cm\n",
    "    f1s[i] = f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f769ca8a-307b-46d6-81ae-a9af893fdf19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.44155844155844154,\n",
       " 1: 0.5806451612903226,\n",
       " 2: 0.5714285714285714,\n",
       " 3: 0.4897959183673469,\n",
       " 4: 0.41509433962264153,\n",
       " 5: 0.5714285714285714,\n",
       " 6: 0.5666666666666667,\n",
       " 7: 0.42857142857142855,\n",
       " 8: 0.38596491228070173,\n",
       " 9: 0.4642857142857143}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e51ada6b-4e15-46dd-8adc-8efaed286cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e6fb3768-e7bb-46b3-b272-2b7a0645a964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4915439725500407"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_scores = [value for key, value in f1s.items()]\n",
    "mean(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "756817f3-96f6-423c-963c-9e135d5f387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cd4549be-1792-4081-92e6-9dc9f1875bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07494233825267892"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "stdev(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7d7e6e32-9bc9-48ec-a265-e56230db0195",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "84baab7b-4cbd-4385-95d4-302908196294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_25-11:05:07] Tap set: Filtered out 0 of 152 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-11:05:15] Tap set: Filtered out 0 of 154 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-11:05:23] Tap set: Filtered out 0 of 155 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-11:05:32] Tap set: Filtered out 0 of 154 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-11:05:40] Tap set: Filtered out 0 of 156 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-11:05:49] Tap set: Filtered out 0 of 156 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-11:05:57] Tap set: Filtered out 0 of 156 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-11:06:06] Tap set: Filtered out 0 of 156 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-11:06:14] Tap set: Filtered out 0 of 156 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-11:06:23] Tap set: Filtered out 0 of 156 (0.0%) records of lengths exceeding 510.\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    test = chen_data.loc[indices[i]]\n",
    "    encoded_test_set = encode_dataset(test[\"seq\"], test[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "        dataset_name = 'Tap set')\n",
    "    test_X, test_Y, test_sample_weigths = encoded_test_set\n",
    "    filename = path.join(DATA_DIR, f\"evaluations/protein_bert/10-fold-cv/2022_04_24_split_{i}.csv\")\n",
    "    model_path = path.join(DATA_DIR, f\"protein_bert/10-fold-cv/2022_04_24_split_{i}\")\n",
    "    model = keras.models.load_model(model_path)\n",
    "    y_pred = model.predict(test_X, batch_size=32)\n",
    "    y_pred_classes = (y_pred >= 0.5)\n",
    "    str_preds = [str(int(pred)) for pred in y_pred_classes]\n",
    "    with open(filename, \"wt\") as f:\n",
    "        f.write(\",\".join(str_preds) + \"\\n\")\n",
    "    metric_dict = {\n",
    "        \"f1\": float(metrics.f1_score(test_Y, y_pred_classes)),\n",
    "        \"acc\": float(metrics.accuracy_score(test_Y, y_pred_classes)),\n",
    "        \"mcc\": float(metrics.matthews_corrcoef(test_Y, y_pred_classes)),\n",
    "        \"auc\": float(metrics.roc_auc_score(test_Y, y_pred_classes)),\n",
    "        \"precision\": float(metrics.precision_score(test_Y, y_pred_classes)),\n",
    "        \"recall\": float(metrics.recall_score(test_Y, y_pred_classes))\n",
    "    }\n",
    "    filename_sum = path.join(DATA_DIR, f\"evaluations/protein_bert/10-fold-cv/all.csv\")\n",
    "    line = [f\"10-fold-cv/2022_04_24_split_{i}\", metric_dict[\"f1\"], metric_dict[\"mcc\"], metric_dict[\"acc\"],metric_dict[\"precision\"],metric_dict[\"recall\"],metric_dict[\"auc\"]]\n",
    "    with open(filename_sum, 'a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile, delimiter='\\t')\n",
    "        csvwriter.writerow(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8231f87e-2b2c-4e9b-bf07-8e982df39f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = chen_data.loc[indices[0]]\n",
    "test.to_csv(path.join(DATA_DIR, \"evaluations/comparison/y_true_0.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09c5b8f-28ed-428f-9f97-6def301da6f8",
   "metadata": {},
   "source": [
    "# Comparison with best ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4bcc6a52-bf7e-484e-929d-a800bc728a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "481ec0cd-e930-4cab-93b4-edcf9f45ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(preprocessing, data_name, hp_dir):\n",
    "    filename = path.join(hp_dir, f\"logistic_regression_{data_name}_{preprocessing}.json\")\n",
    "    parameters = json.load(open(filename))\n",
    "    #C = float(parameters[\"C\"])\n",
    "    lr = LogisticRegression(\n",
    "        class_weight='balanced', max_iter=1000, random_state=42,\n",
    "        C=float(parameters[\"C\"]), penalty=parameters[\"penalty\"], solver=parameters[\"solver\"]\n",
    "    )\n",
    "    return lr, parameters, \"logistic_regression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4baeebc8-63e2-4f89-8889-be50b7125c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pybiomed(train_df, test_df, tap_df):\n",
    "    x_chen = pd.read_feather(path.join(DATA_DIR, \"chen/pybiomed/X_data.ftr\"))\n",
    "    x_chen_train = x_chen.merge(train_df[[\"Antibody_ID\", \"Y\"]].reset_index(), left_on=\"Ab_ID\", right_on=\"Antibody_ID\").set_index('index').drop(\"Antibody_ID\", axis=1)\n",
    "    x_chen_test = x_chen.merge(test_df[[\"Antibody_ID\", \"Y\"]].reset_index(), left_on=\"Ab_ID\", right_on=\"Antibody_ID\").set_index('index').drop(\"Antibody_ID\", axis=1)\n",
    "    x_tap = pd.read_feather(path.join(DATA_DIR, \"tap/pybiomed/X_TAP_data.ftr\"))\n",
    "    x_tap = x_tap.loc[tap_df.index]\n",
    "    return x_chen_train, x_chen_test, x_tap\n",
    "\n",
    "def bert(train_df, test_df, tap_df):\n",
    "    x_chen = pd.read_feather(path.join(DATA_DIR, \"chen/embeddings/bert/bert_chen_embeddings.ftr\"))\n",
    "    x_chen_train = x_chen.merge(train_df[[\"Antibody_ID\", \"Y\"]].reset_index(), left_on=\"Ab_ID\", right_on=\"Antibody_ID\").set_index('index').drop(\"Antibody_ID\", axis=1)\n",
    "    x_chen_test = x_chen.merge(test_df[[\"Antibody_ID\", \"Y\"]].reset_index(), left_on=\"Ab_ID\", right_on=\"Antibody_ID\").set_index('index').drop(\"Antibody_ID\", axis=1)\n",
    "    x_tap = pd.read_feather(path.join(DATA_DIR, \"tap/embeddings/bert/bert_tap_embeddings.ftr\"))\n",
    "    x_tap = x_tap.drop(\"Ab_ID\", axis=1)\n",
    "    x_tap = x_tap.loc[tap_df.index]\n",
    "    return x_chen_train, x_chen_test, x_tap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "28c4c948-cf15-4780-a108-3fa493430143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(train_df, test_df, tap_df):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_df.drop([\"Ab_ID\", \"Y\"], axis=1))\n",
    "    x_train_tr = scaler.transform(train_df.drop([\"Ab_ID\", \"Y\"], axis=1))\n",
    "    x_train_df = pd.DataFrame(data=train_df,  index=train_df.index, columns=train_df.drop([\"Ab_ID\", \"Y\"], axis=1).columns)\n",
    "    x_train_df[\"Ab_ID\"] = train_df[\"Ab_ID\"]\n",
    "    \n",
    "    x_test_tr = scaler.transform(test_df.drop([\"Ab_ID\", \"Y\"], axis=1))\n",
    "    x_test_df = pd.DataFrame(data=test_df,  index=test_df.index, columns=test_df.drop([\"Ab_ID\", \"Y\"], axis=1).columns)\n",
    "    x_test_df[\"Y\"] = test_df[\"Y\"]\n",
    "    x_test_df[\"Ab_ID\"] = test_df[\"Ab_ID\"]\n",
    "    \n",
    "    x_tap_tr = scaler.transform(tap_df)\n",
    "    x_tap_df = pd.DataFrame(data=tap_df,  index=tap_df.index, columns=tap_df.columns)\n",
    "\n",
    "    return x_train_df, train_df[\"Y\"], x_test_df, x_tap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f446f135-0f48-4ea7-bea7-9b5224111255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(model_name, classifier, X_train, y_train, X_valid, y_valid):\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_valid)\n",
    "    filename = path.join(DATA_DIR, \"evaluations/comparison\", \"models\", f\"{model_name}.pkl\")\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(classifier, f)\n",
    "    filename = path.join(DATA_DIR, \"evaluations/comparison\", f\"{model_name}.csv\")\n",
    "    str_preds = [str(int(pred)) for pred in y_pred]\n",
    "    with open(filename, \"wt\") as f:\n",
    "        f.write(\",\".join(str_preds) + \"\\n\")\n",
    "    metric_dict = {\n",
    "        \"f1\": float(metrics.f1_score(y_valid, y_pred)),\n",
    "        \"acc\": float(metrics.accuracy_score(y_valid, y_pred)),\n",
    "        \"mcc\": float(metrics.matthews_corrcoef(y_valid, y_pred)),\n",
    "        \"auc\": float(metrics.roc_auc_score(y_valid, y_pred)),\n",
    "        \"precision\": float(metrics.precision_score(y_valid, y_pred)),\n",
    "        \"recall\": float(metrics.recall_score(y_valid, y_pred))\n",
    "    }\n",
    "    filename_sum = os.path.join(DATA_DIR, f\"evaluations/comparison/all.csv\")\n",
    "    line = [model_name, metric_dict[\"f1\"], metric_dict[\"mcc\"], metric_dict[\"acc\"],metric_dict[\"precision\"],metric_dict[\"recall\"],metric_dict[\"auc\"]]\n",
    "    with open(filename_sum, 'a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile, delimiter='\\t')\n",
    "        csvwriter.writerow(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8f6d7568-6c2b-4d33-8625-839ae78d9c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "tap_data = pd.read_csv(path.join(DATA_DIR, \"tap/tap_not_in_chen.csv\"))\n",
    "for i in range(10):\n",
    "    test = chen_data.loc[indices[i]]\n",
    "    remaining = [idx for idx in list(chen_data.index) if idx not in indices[i]]\n",
    "    train = chen_data.loc[remaining]\n",
    "    x_train, x_test, x_tap = pybiomed(train, test, tap_data)\n",
    "    x_train_tr, y_train_tr, x_test_tr, tap_tr = scaling(x_train, x_test, x_tap)\n",
    "    classifier, params, model_label = logistic_regression(\"scaling\", \"pybiomed\", path.join(DATA_DIR, \"evaluations/hyperparameters\"))\n",
    "    train_and_eval(f\"logistic_regression_pybiomed_{i}\", classifier, x_train_tr.drop([\"Ab_ID\"], axis=1), \n",
    "                    y_train_tr, x_test_tr.drop([\"Ab_ID\", \"Y\"], axis=1), x_test_tr[\"Y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b1d9872d-47c5-4324-b2ec-21b0879a4219",
   "metadata": {},
   "outputs": [],
   "source": [
    "tap_data = pd.read_csv(path.join(DATA_DIR, \"tap/tap_not_in_chen.csv\"))\n",
    "for i in range(10):\n",
    "    test = chen_data.loc[indices[i]]\n",
    "    remaining = [idx for idx in list(chen_data.index) if idx not in indices[i]]\n",
    "    train = chen_data.loc[remaining]\n",
    "    x_train, x_test, x_tap = bert(train, test, tap_data)\n",
    "    x_train_tr, y_train_tr, x_test_tr, tap_tr = scaling(x_train, x_test, x_tap)\n",
    "    classifier, params, model_label = logistic_regression(\"scaling\", \"bert\", path.join(DATA_DIR, \"evaluations/hyperparameters\"))\n",
    "    train_and_eval(f\"logistic_regression_bert_{i}\", classifier, x_train_tr.drop([\"Ab_ID\"], axis=1), \n",
    "                    y_train_tr, x_test_tr.drop([\"Ab_ID\", \"Y\"], axis=1), x_test_tr[\"Y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaffcb3-78f0-4fe8-8247-e5b38ef00bdb",
   "metadata": {},
   "source": [
    "# Review results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4a697d4-7375-4d54-b6be-952b5de42690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>F1</th>\n",
       "      <th>MCC</th>\n",
       "      <th>Acc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10-fold-cv/2022_04_24_split_0</td>\n",
       "      <td>0.441558</td>\n",
       "      <td>0.254159</td>\n",
       "      <td>0.717105</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>0.632663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10-fold-cv/2022_04_24_split_1</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.477060</td>\n",
       "      <td>0.831169</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.750345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10-fold-cv/2022_04_24_split_2</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.453460</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.745968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10-fold-cv/2022_04_24_split_3</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.422231</td>\n",
       "      <td>0.837662</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.669158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-fold-cv/2022_04_24_split_4</td>\n",
       "      <td>0.415094</td>\n",
       "      <td>0.296554</td>\n",
       "      <td>0.801282</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.641741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10-fold-cv/2022_04_24_split_5</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.487377</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10-fold-cv/2022_04_24_split_6</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.490738</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>0.704520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10-fold-cv/2022_04_24_split_7</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.303571</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.651786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10-fold-cv/2022_04_24_split_8</td>\n",
       "      <td>0.385965</td>\n",
       "      <td>0.251447</td>\n",
       "      <td>0.775641</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.617419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10-fold-cv/2022_04_24_split_9</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.360133</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.656319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Model        F1       MCC       Acc  Precision  \\\n",
       "0  10-fold-cv/2022_04_24_split_0  0.441558  0.254159  0.717105   0.414634   \n",
       "1  10-fold-cv/2022_04_24_split_1  0.580645  0.477060  0.831169   0.545455   \n",
       "2  10-fold-cv/2022_04_24_split_2  0.571429  0.453460  0.806452   0.512821   \n",
       "3  10-fold-cv/2022_04_24_split_3  0.489796  0.422231  0.837662   0.666667   \n",
       "4  10-fold-cv/2022_04_24_split_4  0.415094  0.296554  0.801282   0.440000   \n",
       "5  10-fold-cv/2022_04_24_split_5  0.571429  0.487377  0.846154   0.666667   \n",
       "6  10-fold-cv/2022_04_24_split_6  0.566667  0.490738  0.833333   0.739130   \n",
       "7  10-fold-cv/2022_04_24_split_7  0.428571  0.303571  0.794872   0.428571   \n",
       "8  10-fold-cv/2022_04_24_split_8  0.385965  0.251447  0.775641   0.423077   \n",
       "9  10-fold-cv/2022_04_24_split_9  0.464286  0.360133  0.807692   0.565217   \n",
       "\n",
       "     Recall       AUC  \n",
       "0  0.472222  0.632663  \n",
       "1  0.620690  0.750345  \n",
       "2  0.645161  0.745968  \n",
       "3  0.387097  0.669158  \n",
       "4  0.392857  0.641741  \n",
       "5  0.500000  0.717742  \n",
       "6  0.459459  0.704520  \n",
       "7  0.428571  0.651786  \n",
       "8  0.354839  0.617419  \n",
       "9  0.393939  0.656319  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein_bert_cv = pd.read_csv(path.join(DATA_DIR, f\"evaluations/protein_bert/10-fold-cv/all.csv\"), sep=\"\\t\", header=None)\n",
    "protein_bert_cv.columns = [\"Model\", \"F1\", \"MCC\", \"Acc\", \"Precision\", \"Recall\", \"AUC\"]\n",
    "protein_bert_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff00b76d-b062-4d77-a0d8-1c3057b07343",
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_bert_cv.round(3).to_csv(path.join(DATA_DIR, \"evaluations/comparison/for_latex.csv\"), sep=\"&\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb1a61f1-2f2b-4082-ab31-8d8a9712ddb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "F1           0.491544\n",
       "MCC          0.379673\n",
       "Acc          0.805136\n",
       "Precision    0.540224\n",
       "Recall       0.465484\n",
       "AUC          0.678766\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein_bert_cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6be8bdc4-83b3-4620-93f1-c6b843a6ab53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "F1           0.074942\n",
       "MCC          0.097726\n",
       "Acc          0.037949\n",
       "Precision    0.117827\n",
       "Recall       0.098764\n",
       "AUC          0.047616\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein_bert_cv.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5716c1e2-bd6f-461f-af29-dc1ebebe4173",
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_bert_cv[\"label\"] = \"ProteinBERT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea24e691-c55c-4f6c-a2d7-2a4d40d4057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trad_ml_cv = pd.read_csv(path.join(DATA_DIR, f\"evaluations/comparison/all.csv\"), sep=\"\\t\", header=None)\n",
    "trad_ml_cv.columns = [\"Model\", \"F1\", \"MCC\", \"Acc\", \"Precision\", \"Recall\", \"AUC\"]\n",
    "#trad_ml_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93c7931f-5f26-488e-a573-1a5658f9805e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>F1</th>\n",
       "      <th>MCC</th>\n",
       "      <th>Acc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic_regression_pybiomed_0</td>\n",
       "      <td>0.407767</td>\n",
       "      <td>0.159944</td>\n",
       "      <td>0.598684</td>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.593391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistic_regression_pybiomed_1</td>\n",
       "      <td>0.478873</td>\n",
       "      <td>0.339032</td>\n",
       "      <td>0.759740</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.693103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logistic_regression_pybiomed_2</td>\n",
       "      <td>0.579710</td>\n",
       "      <td>0.464919</td>\n",
       "      <td>0.812903</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logistic_regression_pybiomed_3</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.357122</td>\n",
       "      <td>0.766234</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.696827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>logistic_regression_pybiomed_4</td>\n",
       "      <td>0.317460</td>\n",
       "      <td>0.148873</td>\n",
       "      <td>0.724359</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.580915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logistic_regression_pybiomed_5</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.567540</td>\n",
       "      <td>0.858974</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.783770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>logistic_regression_pybiomed_6</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.548158</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.641026</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.779014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>logistic_regression_pybiomed_7</td>\n",
       "      <td>0.507463</td>\n",
       "      <td>0.385758</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.717634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>logistic_regression_pybiomed_8</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.337849</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.392157</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.698581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logistic_regression_pybiomed_9</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.344074</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.693274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Model        F1       MCC       Acc  Precision  \\\n",
       "0  logistic_regression_pybiomed_0  0.407767  0.159944  0.598684   0.313433   \n",
       "1  logistic_regression_pybiomed_1  0.478873  0.339032  0.759740   0.404762   \n",
       "2  logistic_regression_pybiomed_2  0.579710  0.464919  0.812903   0.526316   \n",
       "3  logistic_regression_pybiomed_3  0.500000  0.357122  0.766234   0.439024   \n",
       "4  logistic_regression_pybiomed_4  0.317460  0.148873  0.724359   0.285714   \n",
       "5  logistic_regression_pybiomed_5  0.656250  0.567540  0.858974   0.656250   \n",
       "6  logistic_regression_pybiomed_6  0.657895  0.548158  0.833333   0.641026   \n",
       "7  logistic_regression_pybiomed_7  0.507463  0.385758  0.788462   0.435897   \n",
       "8  logistic_regression_pybiomed_8  0.487805  0.337849  0.730769   0.392157   \n",
       "9  logistic_regression_pybiomed_9  0.500000  0.344074  0.743590   0.425532   \n",
       "\n",
       "     Recall       AUC  \n",
       "0  0.583333  0.593391  \n",
       "1  0.586207  0.693103  \n",
       "2  0.645161  0.750000  \n",
       "3  0.580645  0.696827  \n",
       "4  0.357143  0.580915  \n",
       "5  0.656250  0.783770  \n",
       "6  0.675676  0.779014  \n",
       "7  0.607143  0.717634  \n",
       "8  0.645161  0.698581  \n",
       "9  0.606061  0.693274  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logi_reg_pybiomed = trad_ml_cv.iloc[:10]\n",
    "logi_reg_bert = trad_ml_cv.iloc[10:].reset_index()\n",
    "display(logi_reg_pybiomed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5d63c4b-59ce-4488-920e-cb05d68aea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "logi_reg_pybiomed.round(3).to_csv(path.join(DATA_DIR, \"evaluations/comparison/for_latex_lrp.csv\"), sep=\"&\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4911fa6d-2b6b-46de-bfc6-47882aaf85ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "F1           0.509322\n",
       "MCC          0.365327\n",
       "Acc          0.761705\n",
       "Precision    0.452011\n",
       "Recall       0.594278\n",
       "AUC          0.698651\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logi_reg_pybiomed.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "857b6498-3d65-4690-a847-f1f9cef38008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "F1           0.103981\n",
       "MCC          0.139703\n",
       "Acc          0.072426\n",
       "Precision    0.123272\n",
       "Recall       0.089785\n",
       "AUC          0.068073\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logi_reg_pybiomed.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a46f170-f3f5-425e-9aa1-30b0e9cdfb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "logi_reg_pybiomed[\"label\"] = \"Logistic regression \\n+ PyBioMed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67aa1533-1ac9-4f0b-9540-e4ffbba627cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Model</th>\n",
       "      <th>F1</th>\n",
       "      <th>MCC</th>\n",
       "      <th>Acc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>logistic_regression_bert_0</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.285190</td>\n",
       "      <td>0.664474</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.665230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>logistic_regression_bert_1</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.499073</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.446429</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.807034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>logistic_regression_bert_2</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.457892</td>\n",
       "      <td>0.780645</td>\n",
       "      <td>0.469388</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.766129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>logistic_regression_bert_3</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.423325</td>\n",
       "      <td>0.805195</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.721217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>logistic_regression_bert_4</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.228957</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.624442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>logistic_regression_bert_5</td>\n",
       "      <td>0.520548</td>\n",
       "      <td>0.381930</td>\n",
       "      <td>0.775641</td>\n",
       "      <td>0.463415</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.708165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16</td>\n",
       "      <td>logistic_regression_bert_6</td>\n",
       "      <td>0.582278</td>\n",
       "      <td>0.442996</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.730979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17</td>\n",
       "      <td>logistic_regression_bert_7</td>\n",
       "      <td>0.481013</td>\n",
       "      <td>0.350613</td>\n",
       "      <td>0.737179</td>\n",
       "      <td>0.372549</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>18</td>\n",
       "      <td>logistic_regression_bert_8</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.380861</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.677419</td>\n",
       "      <td>0.722710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19</td>\n",
       "      <td>logistic_regression_bert_9</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.473943</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.742055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                       Model        F1       MCC       Acc  Precision  \\\n",
       "0     10  logistic_regression_bert_0  0.484848  0.285190  0.664474   0.380952   \n",
       "1     11  logistic_regression_bert_1  0.588235  0.499073  0.772727   0.446429   \n",
       "2     12  logistic_regression_bert_2  0.575000  0.457892  0.780645   0.469388   \n",
       "3     13  logistic_regression_bert_3  0.545455  0.423325  0.805195   0.514286   \n",
       "4     14  logistic_regression_bert_4  0.380952  0.228957  0.750000   0.342857   \n",
       "5     15  logistic_regression_bert_5  0.520548  0.381930  0.775641   0.463415   \n",
       "6     16  logistic_regression_bert_6  0.582278  0.442996  0.788462   0.547619   \n",
       "7     17  logistic_regression_bert_7  0.481013  0.350613  0.737179   0.372549   \n",
       "8     18  logistic_regression_bert_8  0.518519  0.380861  0.750000   0.420000   \n",
       "9     19  logistic_regression_bert_9  0.588235  0.473943  0.820513   0.571429   \n",
       "\n",
       "     Recall       AUC  \n",
       "0  0.666667  0.665230  \n",
       "1  0.862069  0.807034  \n",
       "2  0.741935  0.766129  \n",
       "3  0.580645  0.721217  \n",
       "4  0.428571  0.624442  \n",
       "5  0.593750  0.708165  \n",
       "6  0.621622  0.730979  \n",
       "7  0.678571  0.714286  \n",
       "8  0.677419  0.722710  \n",
       "9  0.606061  0.742055  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logi_reg_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64fb757b-9197-4d7c-90e9-faffb757e32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logi_reg_bert.round(3).to_csv(path.join(DATA_DIR, \"evaluations/comparison/for_latex_lrb.csv\"), sep=\"&\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4d7b362-3b7c-4af6-9945-cffeaa2afbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "index        14.500000\n",
       "F1            0.526508\n",
       "MCC           0.392478\n",
       "Acc           0.764484\n",
       "Precision     0.452892\n",
       "Recall        0.645731\n",
       "AUC           0.720225\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logi_reg_bert.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "124a35f1-a474-4e8f-a6c5-c3c5a10619ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "index        3.027650\n",
       "F1           0.065432\n",
       "MCC          0.085655\n",
       "Acc          0.043452\n",
       "Precision    0.076039\n",
       "Recall       0.112955\n",
       "AUC          0.050135\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logi_reg_bert.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b2b44829-2c6e-4c2c-8882-f149858bdcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logi_reg_bert[\"label\"] = \"Logistic regression \\n + BERT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e945aba-64cc-44da-a891-a8fd23066eed",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4cc01786-1ed0-4c29-bc75-4a7d313a63bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to_plot = pd.DataFrame({\n",
    "#    \"split\": range(10),\n",
    "#    \"ProteinBERT\": protein_bert_cv[\"F1\"],\n",
    "#    \"Logistic regression + PyBioMed\": logi_reg_pybiomed[\"F1\"],\n",
    "#    \"Logistic regression + BERT\": logi_reg_bert[\"F1\"]\n",
    "#})\n",
    "\n",
    "to_plot = pd.concat([\n",
    "    protein_bert_cv[[\"F1\", \"label\"]],\n",
    "    logi_reg_pybiomed[[\"F1\", \"label\"]],\n",
    "    logi_reg_bert[[\"F1\", \"label\"]]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00cac63e-9ab4-49ff-bfc3-8dd4f197febf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.441558</td>\n",
       "      <td>ProteinBERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.580645</td>\n",
       "      <td>ProteinBERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>ProteinBERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.489796</td>\n",
       "      <td>ProteinBERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.415094</td>\n",
       "      <td>ProteinBERT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         F1        label\n",
       "0  0.441558  ProteinBERT\n",
       "1  0.580645  ProteinBERT\n",
       "2  0.571429  ProteinBERT\n",
       "3  0.489796  ProteinBERT\n",
       "4  0.415094  ProteinBERT"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_plot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21a920cd-3133-4a5a-b640-d016a1ffbf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "54a308e3-f867-41af-ae22-d43331516c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "ax = sns.boxplot(y=\"F1\", x=\"label\", data=to_plot, width=0.5) #, order=order)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation = 30, size=12)\n",
    "plt.yticks(fontsize=12)\n",
    "ax.set_xlabel(\"Model\", size=15)\n",
    "ax.set_ylabel(\"F1 score\", size=15)\n",
    "plt.savefig(path.join(DATA_DIR, \"images/box_final_comparison.png\"), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c914d3-9f48-4d59-8fcc-8f93ac9d531d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
