{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f37c38b7-7645-4a37-b0bd-1c3ed0c08489",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from os import path\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13b5db07-e604-4fdc-be52-8d4e963fe915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from proteinbert.finetuning import encode_train_and_valid_sets, encode_dataset\n",
    "from proteinbert import OutputType, OutputSpec, evaluate_by_len, load_pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58f06fc6-cc5b-4ba5-b76c-350428a49552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from proteinbert import OutputType, OutputSpec, FinetuningModelGenerator, load_pretrained_model, \\\n",
    "finetune, evaluate_by_len\n",
    "\n",
    "from proteinbert.conv_and_global_attention_model import get_model_with_hidden_layers_as_outputs\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb5cdb0f-aecd-4a46-bb86-605505e74c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05704d2b-73eb-4f0f-97f0-20a00cbfbbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17f00a4c-4f9d-4245-b312-03281caeaba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89fb9d1d-5ecb-45a1-8d55-26d6ee7f5010",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_TYPE = OutputType(False, 'binary')\n",
    "UNIQUE_LABELS = [0, 1]\n",
    "OUTPUT_SPEC = OutputSpec(OUTPUT_TYPE, UNIQUE_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b894730-5000-4539-95a2-9795b3a54513",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_generator, input_encoder = load_pretrained_model(\"../../data/protein_bert/\", \"epoch_92400_sample_23500000.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1246f6e-d5b3-4907-87fc-57c90f40d8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df18af30-11ad-421a-bb04-acbcb4da64b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_train.csv\"), index_col=0)\n",
    "test_set = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_test.csv\"), index_col=0)\n",
    "train_set[\"seq\"] = train_set[\"heavy\"] + train_set[\"light\"]\n",
    "test_set[\"seq\"] = test_set[\"heavy\"] + test_set[\"light\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c75228d6-d97a-4ee6-90d7-e56e5dab4631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1291"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "669e0ae8-04cc-437d-9876-b9b8c437336d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "010440f5-accb-4866-8d0f-a31b5be34976",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "patience = (6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "284f8c1b-8f09-4805-bd0f-7e618216a75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [0.5, 0.6, 0.7, 0.8, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89a129f-8f51-4cb8-8090-96f99ee7aaf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2198e715-c2f5-408d-a787-aa8d1451e324",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_save_model(train_data, valid_data, test_data, size):\n",
    "    wandb.init(project=f\"Dataset size exp\", entity=\"kvetab\")\n",
    "    model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "            get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)\n",
    "\n",
    "    training_callbacks = [\n",
    "        keras.callbacks.ReduceLROnPlateau(patience = patience[1], factor = 0.25, min_lr = 1e-07, verbose = 1),\n",
    "        keras.callbacks.EarlyStopping(patience = patience[0], restore_best_weights = True),\n",
    "        WandbCallback()\n",
    "    ]\n",
    "\n",
    "    epoch_num = 100\n",
    "    batch_size = 128\n",
    "    #learning_rate = 1e-5\n",
    "    wandb.config = {\n",
    "          \"learning_rate\": learning_rate,\n",
    "          \"epochs\": epoch_num * 2,\n",
    "          \"batch_size\": batch_size\n",
    "        }\n",
    "    finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data[\"seq\"], train_data[\"Y\"], valid_data['seq'], valid_data[\"Y\"], \\\n",
    "            seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = learning_rate, begin_with_frozen_pretrained_layers = True, \\\n",
    "            lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = learning_rate / 10, callbacks = training_callbacks)\n",
    "    mod = model_generator.create_model(seq_len = 512)\n",
    "    mod_name = f\"2022_04_22_size{size}\"\n",
    "    mod.save(path.join(DATA_DIR, f\"protein_bert/by_data_size/{mod_name}\"))\n",
    "\n",
    "    results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "            start_seq_len = 512, start_batch_size = 32)\n",
    "    fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "    f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "    return confusion_matrix, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "702b647d-8583-44d6-bf53-f983dbb7597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = {}\n",
    "f1s = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a954139-d02d-407e-ac45-57bdb4a1a890",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645 646 260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkvetab\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp/runs/3cz0r5gn\" target=\"_blank\">colorful-meadow-17</a></strong> to <a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_24-16:50:26] Training set: Filtered out 0 of 645 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_24-16:50:27] Validation set: Filtered out 0 of 646 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_24-16:50:27] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-24 16:50:27.159831: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-24 16:50:27.728399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9656 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:41:00.0, compute capability: 7.5\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "2022-04-24 16:50:29.565892: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-24 16:50:37.412848: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 7605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 609ms/step - loss: 0.9011 - val_loss: 1.3253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.8461 - val_loss: 0.9959\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.8655 - val_loss: 0.5400\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5879 - val_loss: 0.6815\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.7116 - val_loss: 0.4534\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 223ms/step - loss: 0.4763 - val_loss: 0.5230\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.5687 - val_loss: 0.4924\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.5178 - val_loss: 0.6362\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.6347 - val_loss: 0.5206\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5334 - val_loss: 0.4370\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4381 - val_loss: 0.5133\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4809 - val_loss: 0.4504\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4323 - val_loss: 0.4633\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4629 - val_loss: 0.4350\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4163 - val_loss: 0.4501\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4125 - val_loss: 0.4364\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4509 - val_loss: 0.4307\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4187 - val_loss: 0.4584\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4422 - val_loss: 0.4377\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4086 - val_loss: 0.4285\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4272 - val_loss: 0.4269\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4103 - val_loss: 0.4298\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4119 - val_loss: 0.4247\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4199 - val_loss: 0.4291\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4072 - val_loss: 0.4256\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4074 - val_loss: 0.4216\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4069 - val_loss: 0.4194\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3960 - val_loss: 0.4326\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4040 - val_loss: 0.4182\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3974 - val_loss: 0.4199\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4070 - val_loss: 0.4170\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4053 - val_loss: 0.4252\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3985 - val_loss: 0.4336\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.4138 - val_loss: 0.4203\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3988 - val_loss: 0.4182\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3938 - val_loss: 0.4136\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3959 - val_loss: 0.4164\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3868 - val_loss: 0.4139\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4003 - val_loss: 0.4133\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3929 - val_loss: 0.4133\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3954 - val_loss: 0.4129\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4043 - val_loss: 0.4127\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3878 - val_loss: 0.4156\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4037 - val_loss: 0.4203\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3992 - val_loss: 0.4173\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3917 - val_loss: 0.4138\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3918 - val_loss: 0.4133\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3932 - val_loss: 0.4129\n",
      "[2022_04_24-16:51:32] Training the entire fine-tuned model...\n",
      "[2022_04_24-16:51:40] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3972WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1104s vs `on_train_batch_end` time: 0.1177s). Check your callbacks.\n",
      "6/6 [==============================] - 10s 675ms/step - loss: 0.3972 - val_loss: 0.4124\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.4102 - val_loss: 0.4177\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3908 - val_loss: 0.4170\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3805 - val_loss: 0.4184\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3823 - val_loss: 0.4078\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3746 - val_loss: 0.4059\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3715 - val_loss: 0.4056\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3676 - val_loss: 0.4037\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3678 - val_loss: 0.4094\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3742 - val_loss: 0.4032\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3590 - val_loss: 0.4052\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3574 - val_loss: 0.4235\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3603 - val_loss: 0.4021\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3447 - val_loss: 0.4028\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3395 - val_loss: 0.4036\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3370 - val_loss: 0.3978\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3224 - val_loss: 0.4022\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3130 - val_loss: 0.4007\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3223 - val_loss: 0.4226\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3129 - val_loss: 0.4030\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.2944 - val_loss: 0.4150\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3046 - val_loss: 0.4075\n",
      "[2022_04_24-16:52:30] Training on final epochs of sequence length 1024...\n",
      "[2022_04_24-16:52:30] Training set: Filtered out 0 of 645 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_24-16:52:30] Validation set: Filtered out 0 of 646 (0.0%) records of lengths exceeding 1022.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3361WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1087s vs `on_train_batch_end` time: 0.1388s). Check your callbacks.\n",
      "11/11 [==============================] - 11s 512ms/step - loss: 0.3426 - val_loss: 0.4012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-24 16:53:02.290053: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/by_data_size/2022_04_22_size0.5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/by_data_size/2022_04_22_size0.5/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "774 517 260\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3cz0r5gn) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 43820... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇███▁▁▂▂▂▂▃▃▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▇▄▃▄▅▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂</td></tr><tr><td>lr</td><td>██████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▃▂▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>15</td></tr><tr><td>best_val_loss</td><td>0.39776</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34265</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.40121</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">colorful-meadow-17</strong>: <a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp/runs/3cz0r5gn\" target=\"_blank\">https://wandb.ai/kvetab/Dataset%20size%20exp/runs/3cz0r5gn</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220424_165014-3cz0r5gn/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3cz0r5gn). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp/runs/6q8rn4cn\" target=\"_blank\">polar-armadillo-18</a></strong> to <a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_24-16:53:31] Training set: Filtered out 0 of 774 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_24-16:53:31] Validation set: Filtered out 0 of 517 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_24-16:53:31] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 [==============================] - 10s 479ms/step - loss: 0.7844 - val_loss: 1.2625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.7420 - val_loss: 0.7453\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.5806 - val_loss: 0.4596\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 1s 159ms/step - loss: 0.5049 - val_loss: 0.4933\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.4567 - val_loss: 0.4445\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.4497 - val_loss: 0.4567\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 1s 159ms/step - loss: 0.4312 - val_loss: 0.4647\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.4609 - val_loss: 0.4299\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.4131 - val_loss: 0.5083\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 1s 163ms/step - loss: 0.4520 - val_loss: 0.4966\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.4220 - val_loss: 0.4186\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 1s 159ms/step - loss: 0.3983 - val_loss: 0.4186\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.3992 - val_loss: 0.4181\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.3977 - val_loss: 0.4069\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.4065 - val_loss: 0.6564\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.5069 - val_loss: 0.5494\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 1s 163ms/step - loss: 0.4672 - val_loss: 0.4541\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.4062 - val_loss: 0.4469\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 1s 159ms/step - loss: 0.4120 - val_loss: 0.4216\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.4000 - val_loss: 0.3978\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.3838 - val_loss: 0.3979\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.3841 - val_loss: 0.4177\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 1s 158ms/step - loss: 0.4025 - val_loss: 0.3947\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.3909 - val_loss: 0.3935\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.3884 - val_loss: 0.4005\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.3773 - val_loss: 0.3958\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.4029 - val_loss: 0.3968\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.4087 - val_loss: 0.4065\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.3868 - val_loss: 0.3901\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.3759 - val_loss: 0.3924\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 1s 158ms/step - loss: 0.3718 - val_loss: 0.3900\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.3814 - val_loss: 0.3938\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.3773 - val_loss: 0.3986\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.3778 - val_loss: 0.3969\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.3716 - val_loss: 0.3895\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 1s 159ms/step - loss: 0.3901 - val_loss: 0.3930\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.3805 - val_loss: 0.3894\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.3729 - val_loss: 0.3892\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.3693 - val_loss: 0.3896\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.3699 - val_loss: 0.3972\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 1s 159ms/step - loss: 0.3681 - val_loss: 0.3959\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.3666 - val_loss: 0.3890\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.3816 - val_loss: 0.3889\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.3666 - val_loss: 0.3899\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.3694 - val_loss: 0.3902\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.3731 - val_loss: 0.3908\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.3658 - val_loss: 0.3895\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 1s 163ms/step - loss: 0.3694 - val_loss: 0.3893\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.3614 - val_loss: 0.3885\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.3534 - val_loss: 0.3885\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.3696 - val_loss: 0.3885\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.3705 - val_loss: 0.3884\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.3733 - val_loss: 0.3884\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.3751 - val_loss: 0.3886\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.3592 - val_loss: 0.3900\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.3657 - val_loss: 0.3907\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.3657 - val_loss: 0.3919\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.3787 - val_loss: 0.3913\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.3509 - val_loss: 0.3905\n",
      "[2022_04_24-16:54:46] Training the entire fine-tuned model...\n",
      "[2022_04_24-16:55:10] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.3683WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1151s vs `on_train_batch_end` time: 0.1450s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1151s vs `on_train_batch_end` time: 0.1450s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 10s 613ms/step - loss: 0.3676 - val_loss: 0.3935\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 2s 290ms/step - loss: 0.3667 - val_loss: 0.3923\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 2s 290ms/step - loss: 0.3668 - val_loss: 0.3860\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 2s 296ms/step - loss: 0.3535 - val_loss: 0.3868\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 2s 291ms/step - loss: 0.3746 - val_loss: 0.3933\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 2s 289ms/step - loss: 0.3626 - val_loss: 0.4090\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 2s 290ms/step - loss: 0.3619 - val_loss: 0.3809\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 2s 293ms/step - loss: 0.3333 - val_loss: 0.3802\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 2s 293ms/step - loss: 0.3553 - val_loss: 0.3881\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 2s 291ms/step - loss: 0.3464 - val_loss: 0.3813\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 2s 290ms/step - loss: 0.3402 - val_loss: 0.3946\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 2s 290ms/step - loss: 0.3257 - val_loss: 0.4213\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 2s 290ms/step - loss: 0.3667 - val_loss: 0.3783\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 2s 292ms/step - loss: 0.3403 - val_loss: 0.4078\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 2s 290ms/step - loss: 0.3332 - val_loss: 0.3783\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 2s 292ms/step - loss: 0.3151 - val_loss: 0.3815\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 2s 292ms/step - loss: 0.3109 - val_loss: 0.3870\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 2s 291ms/step - loss: 0.3166 - val_loss: 0.3867\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 2s 290ms/step - loss: 0.3084 - val_loss: 0.3803\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 2s 291ms/step - loss: 0.3104 - val_loss: 0.3765\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 2s 292ms/step - loss: 0.3049 - val_loss: 0.3752\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 2s 296ms/step - loss: 0.3127 - val_loss: 0.3751\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 2s 293ms/step - loss: 0.3011 - val_loss: 0.3756\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 2s 296ms/step - loss: 0.3017 - val_loss: 0.3750\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 2s 291ms/step - loss: 0.3229 - val_loss: 0.3753\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 2s 296ms/step - loss: 0.3124 - val_loss: 0.3745\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 2s 291ms/step - loss: 0.3013 - val_loss: 0.3778\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 2s 291ms/step - loss: 0.3178 - val_loss: 0.3771\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 2s 291ms/step - loss: 0.3030 - val_loss: 0.3749\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 2s 292ms/step - loss: 0.3048 - val_loss: 0.3739\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 2s 294ms/step - loss: 0.2991 - val_loss: 0.3739\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 2s 291ms/step - loss: 0.3068 - val_loss: 0.3751\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 2s 294ms/step - loss: 0.3055 - val_loss: 0.3776\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 2s 292ms/step - loss: 0.3087 - val_loss: 0.3805\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 2s 296ms/step - loss: 0.3098 - val_loss: 0.3809\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 2s 293ms/step - loss: 0.3037 - val_loss: 0.3801\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 2s 292ms/step - loss: 0.3040 - val_loss: 0.3791\n",
      "[2022_04_24-16:56:34] Training on final epochs of sequence length 1024...\n",
      "[2022_04_24-16:56:34] Training set: Filtered out 0 of 774 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_24-16:56:34] Validation set: Filtered out 0 of 517 (0.0%) records of lengths exceeding 1022.\n",
      " 6/13 [============>.................] - ETA: 1s - loss: 0.3243WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1091s vs `on_train_batch_end` time: 0.1445s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1091s vs `on_train_batch_end` time: 0.1445s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 12s 493ms/step - loss: 0.3256 - val_loss: 0.3798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/by_data_size/2022_04_22_size0.6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/by_data_size/2022_04_22_size0.6/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "903 388 260\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:6q8rn4cn) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 44299... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▇▇▇▇██▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▂▃▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▁▂▁▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>30</td></tr><tr><td>best_val_loss</td><td>0.37393</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3256</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.37983</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">polar-armadillo-18</strong>: <a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp/runs/6q8rn4cn\" target=\"_blank\">https://wandb.ai/kvetab/Dataset%20size%20exp/runs/6q8rn4cn</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220424_165314-6q8rn4cn/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:6q8rn4cn). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp/runs/1e8v5bpp\" target=\"_blank\">hopeful-meadow-19</a></strong> to <a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_24-16:57:29] Training set: Filtered out 0 of 903 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_24-16:57:29] Validation set: Filtered out 0 of 388 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_24-16:57:29] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 10s 542ms/step - loss: 0.8713 - val_loss: 0.7257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "8/8 [==============================] - 1s 139ms/step - loss: 0.7373 - val_loss: 0.7516\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 1s 140ms/step - loss: 0.5679 - val_loss: 0.4721\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 1s 140ms/step - loss: 0.4953 - val_loss: 0.5011\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 1s 140ms/step - loss: 0.4437 - val_loss: 0.4825\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 1s 139ms/step - loss: 0.4500 - val_loss: 0.4701\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 1s 140ms/step - loss: 0.4634 - val_loss: 0.4459\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 1s 137ms/step - loss: 0.4439 - val_loss: 0.4407\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 1s 138ms/step - loss: 0.4150 - val_loss: 0.4357\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 1s 139ms/step - loss: 0.4053 - val_loss: 0.4368\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 1s 138ms/step - loss: 0.4187 - val_loss: 0.4266\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 1s 139ms/step - loss: 0.4288 - val_loss: 0.4231\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 1s 138ms/step - loss: 0.3904 - val_loss: 0.5334\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 1s 140ms/step - loss: 0.4118 - val_loss: 0.4562\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 1s 140ms/step - loss: 0.3825 - val_loss: 0.4171\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 1s 139ms/step - loss: 0.3713 - val_loss: 0.4458\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 1s 140ms/step - loss: 0.4105 - val_loss: 0.4184\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 1s 141ms/step - loss: 0.3901 - val_loss: 0.5625\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 1s 139ms/step - loss: 0.4561 - val_loss: 0.4306\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 1s 138ms/step - loss: 0.3845 - val_loss: 0.4176\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 1s 138ms/step - loss: 0.3632 - val_loss: 0.4174\n",
      "[2022_04_24-16:58:04] Training the entire fine-tuned model...\n",
      "[2022_04_24-16:58:12] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/8 [=====================>........] - ETA: 0s - loss: 0.3871WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1107s vs `on_train_batch_end` time: 0.1470s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1107s vs `on_train_batch_end` time: 0.1470s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 10s 528ms/step - loss: 0.3956 - val_loss: 0.4169\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 2s 274ms/step - loss: 0.3812 - val_loss: 0.4165\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 2s 272ms/step - loss: 0.3705 - val_loss: 0.4106\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 2s 273ms/step - loss: 0.3746 - val_loss: 0.4090\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 2s 272ms/step - loss: 0.3572 - val_loss: 0.4092\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 2s 273ms/step - loss: 0.3557 - val_loss: 0.4413\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 2s 273ms/step - loss: 0.4071 - val_loss: 0.4293\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 2s 272ms/step - loss: 0.3491 - val_loss: 0.4012\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 2s 273ms/step - loss: 0.3593 - val_loss: 0.4029\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 2s 273ms/step - loss: 0.3387 - val_loss: 0.3952\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 2s 274ms/step - loss: 0.3368 - val_loss: 0.4089\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 2s 273ms/step - loss: 0.3363 - val_loss: 0.3931\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 2s 274ms/step - loss: 0.3208 - val_loss: 0.3942\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 2s 274ms/step - loss: 0.3099 - val_loss: 0.3925\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 2s 274ms/step - loss: 0.3146 - val_loss: 0.4264\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 2s 277ms/step - loss: 0.3188 - val_loss: 0.4048\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 2s 274ms/step - loss: 0.3024 - val_loss: 0.3857\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 2s 276ms/step - loss: 0.2978 - val_loss: 0.4408\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 2s 274ms/step - loss: 0.3353 - val_loss: 0.3923\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 2s 273ms/step - loss: 0.3164 - val_loss: 0.3925\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 2s 274ms/step - loss: 0.3040 - val_loss: 0.4102\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 2s 273ms/step - loss: 0.2875 - val_loss: 0.3930\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 2s 274ms/step - loss: 0.2667 - val_loss: 0.3902\n",
      "[2022_04_24-16:59:11] Training on final epochs of sequence length 1024...\n",
      "[2022_04_24-16:59:11] Training set: Filtered out 0 of 903 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_24-16:59:11] Validation set: Filtered out 0 of 388 (0.0%) records of lengths exceeding 1022.\n",
      " 6/15 [===========>..................] - ETA: 2s - loss: 0.3185WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1100s vs `on_train_batch_end` time: 0.1422s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1100s vs `on_train_batch_end` time: 0.1422s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 12s 436ms/step - loss: 0.3069 - val_loss: 0.3876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/by_data_size/2022_04_22_size0.7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/by_data_size/2022_04_22_size0.7/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1032 259 260\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1e8v5bpp) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 44889... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇█▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██▁</td></tr><tr><td>loss</td><td>█▆▄▃▃▃▃▃▂▃▃▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>██▃▃▃▃▂▂▂▂▂▄▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>16</td></tr><tr><td>best_val_loss</td><td>0.38567</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30687</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.38759</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">hopeful-meadow-19</strong>: <a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp/runs/1e8v5bpp\" target=\"_blank\">https://wandb.ai/kvetab/Dataset%20size%20exp/runs/1e8v5bpp</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220424_165713-1e8v5bpp/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1e8v5bpp). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp/runs/1u4ibbsa\" target=\"_blank\">atomic-microwave-20</a></strong> to <a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_24-17:00:07] Training set: Filtered out 0 of 1032 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_24-17:00:07] Validation set: Filtered out 0 of 259 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_24-17:00:07] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 10s 474ms/step - loss: 0.7933 - val_loss: 0.7243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "9/9 [==============================] - 1s 121ms/step - loss: 0.6566 - val_loss: 0.5182\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 1s 123ms/step - loss: 0.5281 - val_loss: 0.5336\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 1s 120ms/step - loss: 0.4699 - val_loss: 0.4596\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 1s 123ms/step - loss: 0.4411 - val_loss: 0.4903\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 1s 119ms/step - loss: 0.4525 - val_loss: 0.4995\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 1s 122ms/step - loss: 0.4816 - val_loss: 0.4532\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 1s 120ms/step - loss: 0.4585 - val_loss: 0.5802\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 1s 123ms/step - loss: 0.4795 - val_loss: 0.4355\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 1s 121ms/step - loss: 0.4112 - val_loss: 0.4579\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.4324 - val_loss: 0.4288\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 1s 122ms/step - loss: 0.4505 - val_loss: 0.4452\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 1s 134ms/step - loss: 0.4573 - val_loss: 0.4195\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 1s 122ms/step - loss: 0.4983 - val_loss: 0.4230\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 1s 120ms/step - loss: 0.4542 - val_loss: 0.4942\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 1s 123ms/step - loss: 0.4573 - val_loss: 0.6945\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 1s 123ms/step - loss: 0.5255 - val_loss: 0.4038\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 1s 122ms/step - loss: 0.3945 - val_loss: 0.4090\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 1s 123ms/step - loss: 0.4120 - val_loss: 0.4032\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 1s 122ms/step - loss: 0.3910 - val_loss: 0.4030\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 1s 124ms/step - loss: 0.3567 - val_loss: 0.4020\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 1s 121ms/step - loss: 0.3756 - val_loss: 0.4070\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 1s 122ms/step - loss: 0.3813 - val_loss: 0.4288\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 1s 121ms/step - loss: 0.3756 - val_loss: 0.4633\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 1s 124ms/step - loss: 0.3967 - val_loss: 0.4246\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 1s 121ms/step - loss: 0.3683 - val_loss: 0.3993\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 1s 121ms/step - loss: 0.3554 - val_loss: 0.4049\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 1s 123ms/step - loss: 0.3568 - val_loss: 0.4029\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 1s 122ms/step - loss: 0.3701 - val_loss: 0.4341\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 1s 121ms/step - loss: 0.4077 - val_loss: 0.3997\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 1s 122ms/step - loss: 0.3648 - val_loss: 0.3998\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 1s 122ms/step - loss: 0.3480 - val_loss: 0.4044\n",
      "[2022_04_24-17:00:54] Training the entire fine-tuned model...\n",
      "[2022_04_24-17:01:02] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 0.3485WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1120s vs `on_train_batch_end` time: 0.1475s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1120s vs `on_train_batch_end` time: 0.1475s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 10s 488ms/step - loss: 0.3549 - val_loss: 0.3983\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 2s 259ms/step - loss: 0.3582 - val_loss: 0.4191\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 2s 258ms/step - loss: 0.3687 - val_loss: 0.3885\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 2s 259ms/step - loss: 0.3447 - val_loss: 0.3864\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 2s 260ms/step - loss: 0.3486 - val_loss: 0.4222\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 2s 260ms/step - loss: 0.3549 - val_loss: 0.3929\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 2s 259ms/step - loss: 0.3518 - val_loss: 0.3980\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 2s 260ms/step - loss: 0.3227 - val_loss: 0.3781\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 2s 261ms/step - loss: 0.3343 - val_loss: 0.3800\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 2s 261ms/step - loss: 0.3369 - val_loss: 0.3931\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 2s 259ms/step - loss: 0.3519 - val_loss: 0.3980\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 2s 259ms/step - loss: 0.3329 - val_loss: 0.4000\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 2s 261ms/step - loss: 0.3165 - val_loss: 0.3728\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 2s 261ms/step - loss: 0.3206 - val_loss: 0.3736\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 2s 272ms/step - loss: 0.3177 - val_loss: 0.3717\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 2s 259ms/step - loss: 0.3000 - val_loss: 0.3733\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 2s 261ms/step - loss: 0.3150 - val_loss: 0.3775\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 2s 260ms/step - loss: 0.3085 - val_loss: 0.3720\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 2s 259ms/step - loss: 0.3004 - val_loss: 0.3712\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 2s 261ms/step - loss: 0.2918 - val_loss: 0.3702\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 2s 259ms/step - loss: 0.2880 - val_loss: 0.3811\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 2s 260ms/step - loss: 0.3075 - val_loss: 0.3694\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 2s 262ms/step - loss: 0.3028 - val_loss: 0.3760\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 2s 260ms/step - loss: 0.2973 - val_loss: 0.3708\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 2s 274ms/step - loss: 0.2876 - val_loss: 0.3694\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 2s 261ms/step - loss: 0.2776 - val_loss: 0.3728\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 2s 260ms/step - loss: 0.2664 - val_loss: 0.3711\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 2s 259ms/step - loss: 0.2803 - val_loss: 0.3693\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 2s 260ms/step - loss: 0.2755 - val_loss: 0.3701\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 2s 260ms/step - loss: 0.2816 - val_loss: 0.3702\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 2s 262ms/step - loss: 0.2844 - val_loss: 0.3702\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 2s 271ms/step - loss: 0.2750 - val_loss: 0.3721\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "Epoch 33/100\n",
      "9/9 [==============================] - 2s 262ms/step - loss: 0.2673 - val_loss: 0.3722\n",
      "Epoch 34/100\n",
      "9/9 [==============================] - 2s 260ms/step - loss: 0.2727 - val_loss: 0.3708\n",
      "[2022_04_24-17:02:31] Training on final epochs of sequence length 1024...\n",
      "[2022_04_24-17:02:31] Training set: Filtered out 0 of 1032 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_24-17:02:31] Validation set: Filtered out 0 of 259 (0.0%) records of lengths exceeding 1022.\n",
      " 6/17 [=========>....................] - ETA: 2s - loss: 0.3292WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1092s vs `on_train_batch_end` time: 0.1430s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1092s vs `on_train_batch_end` time: 0.1430s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 12s 401ms/step - loss: 0.3080 - val_loss: 0.3827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/by_data_size/2022_04_22_size0.8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/by_data_size/2022_04_22_size0.8/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161 130 260\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1u4ibbsa) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 45226... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇█▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▆▇▇██▁</td></tr><tr><td>loss</td><td>█▆▄▃▄▄▃▃▄▄▄▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>lr</td><td>███████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▄▃▂▂▂▂▇▂▂▂▂▃▂▂▂▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>27</td></tr><tr><td>best_val_loss</td><td>0.36927</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30799</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.38269</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">atomic-microwave-20</strong>: <a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp/runs/1u4ibbsa\" target=\"_blank\">https://wandb.ai/kvetab/Dataset%20size%20exp/runs/1u4ibbsa</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220424_165950-1u4ibbsa/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1u4ibbsa). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp/runs/2ouippi6\" target=\"_blank\">earnest-wildflower-21</a></strong> to <a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_24-17:03:27] Training set: Filtered out 0 of 1161 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_24-17:03:27] Validation set: Filtered out 0 of 130 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_24-17:03:27] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 10s 309ms/step - loss: 0.8445 - val_loss: 0.6501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.5500 - val_loss: 0.5621\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.4897 - val_loss: 0.4898\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.4600 - val_loss: 0.4561\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4398 - val_loss: 0.4398\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.4347 - val_loss: 0.4205\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.4260 - val_loss: 0.4178\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.4149 - val_loss: 0.4127\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.4075 - val_loss: 0.4772\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.4315 - val_loss: 0.4717\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4353 - val_loss: 0.4696\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.4082 - val_loss: 0.4221\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3892 - val_loss: 0.3925\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3871 - val_loss: 0.3910\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3807 - val_loss: 0.3916\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3786 - val_loss: 0.3888\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3853 - val_loss: 0.3882\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3775 - val_loss: 0.3869\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3755 - val_loss: 0.3934\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3776 - val_loss: 0.3887\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3835 - val_loss: 0.3941\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3758 - val_loss: 0.3882\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3774 - val_loss: 0.3899\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3741 - val_loss: 0.3845\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3718 - val_loss: 0.3844\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3739 - val_loss: 0.3880\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3690 - val_loss: 0.3831\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3660 - val_loss: 0.3832\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3668 - val_loss: 0.3855\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3747 - val_loss: 0.3827\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3781 - val_loss: 0.3825\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3668 - val_loss: 0.3818\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3626 - val_loss: 0.3833\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.3797 - val_loss: 0.3868\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3645 - val_loss: 0.3817\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3706 - val_loss: 0.3840\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3693 - val_loss: 0.3820\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3724 - val_loss: 0.3818\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3654 - val_loss: 0.3824\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3755 - val_loss: 0.3825\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3686 - val_loss: 0.3825\n",
      "[2022_04_24-17:04:23] Training the entire fine-tuned model...\n",
      "[2022_04_24-17:04:32] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.3623WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1119s vs `on_train_batch_end` time: 0.1465s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1119s vs `on_train_batch_end` time: 0.1465s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 10s 465ms/step - loss: 0.3663 - val_loss: 0.3977\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 2s 248ms/step - loss: 0.3775 - val_loss: 0.3766\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 2s 248ms/step - loss: 0.3638 - val_loss: 0.3760\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 2s 248ms/step - loss: 0.3658 - val_loss: 0.3703\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 3s 249ms/step - loss: 0.3539 - val_loss: 0.4184\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 2s 248ms/step - loss: 0.3805 - val_loss: 0.3657\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 3s 253ms/step - loss: 0.3624 - val_loss: 0.3786\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 2s 248ms/step - loss: 0.3466 - val_loss: 0.3756\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 3s 252ms/step - loss: 0.3505 - val_loss: 0.3670\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 3s 253ms/step - loss: 0.3324 - val_loss: 0.3572\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 3s 251ms/step - loss: 0.3333 - val_loss: 0.3606\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 2s 249ms/step - loss: 0.3276 - val_loss: 0.3496\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 3s 249ms/step - loss: 0.3184 - val_loss: 0.3602\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 3s 249ms/step - loss: 0.3165 - val_loss: 0.3625\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 3s 250ms/step - loss: 0.3269 - val_loss: 0.3438\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 3s 249ms/step - loss: 0.2969 - val_loss: 0.3492\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 3s 250ms/step - loss: 0.2959 - val_loss: 0.3439\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 2s 248ms/step - loss: 0.2924 - val_loss: 0.3452\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 3s 250ms/step - loss: 0.2771 - val_loss: 0.3287\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 3s 251ms/step - loss: 0.2695 - val_loss: 0.3535\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 3s 249ms/step - loss: 0.2547 - val_loss: 0.3445\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 3s 250ms/step - loss: 0.2468 - val_loss: 0.3579\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 3s 249ms/step - loss: 0.2376 - val_loss: 0.3895\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 3s 249ms/step - loss: 0.2248 - val_loss: 0.3555\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 3s 250ms/step - loss: 0.2066 - val_loss: 0.3593\n",
      "[2022_04_24-17:05:44] Training on final epochs of sequence length 1024...\n",
      "[2022_04_24-17:05:44] Training set: Filtered out 0 of 1161 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_24-17:06:01] Validation set: Filtered out 0 of 130 (0.0%) records of lengths exceeding 1022.\n",
      " 6/19 [========>.....................] - ETA: 3s - loss: 0.3386WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1096s vs `on_train_batch_end` time: 0.1426s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1096s vs `on_train_batch_end` time: 0.1426s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 13s 377ms/step - loss: 0.2923 - val_loss: 0.3448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/by_data_size/2022_04_22_size0.9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/by_data_size/2022_04_22_size0.9/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    }
   ],
   "source": [
    "for size in sizes:\n",
    "    train, valid = train_test_split(train_set, test_size=1-size, random_state=42, stratify=train_set[\"Y\"])\n",
    "    #test = pd.concat([test, test_set])\n",
    "    test = test_set\n",
    "    #valid, test = train_test_split(test, test_size=0.5, random_state=333, stratify=test[\"Y\"])\n",
    "    print(len(train), len(valid), len(test))\n",
    "    cm, f1_score = train_and_save_model(train, valid, test, size)\n",
    "    cms[size] = cm\n",
    "    f1s[size] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a38702c-f746-4ec6-9911-b21bdaaa56aa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1291 130 130\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2ouippi6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 45680... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▂▂▂▂▃▃▃▃▄▄▄▅▅▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▁▁▂</td></tr><tr><td>lr</td><td>████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▄▃▃▄▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▁▂▁▁▁▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>18</td></tr><tr><td>best_val_loss</td><td>0.32869</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.2923</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.34477</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">earnest-wildflower-21</strong>: <a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp/runs/2ouippi6\" target=\"_blank\">https://wandb.ai/kvetab/Dataset%20size%20exp/runs/2ouippi6</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220424_170310-2ouippi6/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2ouippi6). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp/runs/3sz8kwxf\" target=\"_blank\">revived-capybara-22</a></strong> to <a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_24-17:06:56] Training set: Filtered out 0 of 1291 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_24-17:06:56] Validation set: Filtered out 0 of 130 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_24-17:06:56] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 306ms/step - loss: 0.7751 - val_loss: 0.6868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.5290 - val_loss: 0.4886\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.4669 - val_loss: 0.4886\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4450 - val_loss: 0.5069\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4493 - val_loss: 0.5267\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4278 - val_loss: 0.5224\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4439 - val_loss: 0.4984\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.4117 - val_loss: 0.4788\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.4132 - val_loss: 0.4930\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.4049 - val_loss: 0.4924\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4035 - val_loss: 0.4696\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3916 - val_loss: 0.4779\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3936 - val_loss: 0.4674\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3928 - val_loss: 0.4667\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3910 - val_loss: 0.4658\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3955 - val_loss: 0.4959\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4101 - val_loss: 0.4652\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4056 - val_loss: 0.4799\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.4048 - val_loss: 0.4665\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3940 - val_loss: 0.4956\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 112ms/step - loss: 0.3974 - val_loss: 0.4628\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3885 - val_loss: 0.4903\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3994 - val_loss: 0.4801\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3829 - val_loss: 0.4688\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3741 - val_loss: 0.4612\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3762 - val_loss: 0.4618\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 112ms/step - loss: 0.3871 - val_loss: 0.4677\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3780 - val_loss: 0.4683\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3772 - val_loss: 0.4737\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3723 - val_loss: 0.4640\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3659 - val_loss: 0.4601\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3764 - val_loss: 0.4620\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3637 - val_loss: 0.4634\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 112ms/step - loss: 0.3642 - val_loss: 0.4603\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3697 - val_loss: 0.4666\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3699 - val_loss: 0.4647\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3755 - val_loss: 0.4642\n",
      "[2022_04_24-17:07:52] Training the entire fine-tuned model...\n",
      "[2022_04_24-17:08:35] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.4034WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1119s vs `on_train_batch_end` time: 0.1467s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1119s vs `on_train_batch_end` time: 0.1467s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 422ms/step - loss: 0.3829 - val_loss: 0.4600\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3655 - val_loss: 0.4604\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3596 - val_loss: 0.4616\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 248ms/step - loss: 0.3575 - val_loss: 0.4787\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 248ms/step - loss: 0.3616 - val_loss: 0.4599\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 248ms/step - loss: 0.3555 - val_loss: 0.4684\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3428 - val_loss: 0.4660\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3364 - val_loss: 0.4654\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3249 - val_loss: 0.4736\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3215 - val_loss: 0.4638\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3209 - val_loss: 0.4657\n",
      "[2022_04_24-17:09:13] Training on final epochs of sequence length 1024...\n",
      "[2022_04_24-17:09:13] Training set: Filtered out 0 of 1291 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_24-17:09:14] Validation set: Filtered out 0 of 130 (0.0%) records of lengths exceeding 1022.\n",
      " 6/21 [=======>......................] - ETA: 3s - loss: 0.3424WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1101s vs `on_train_batch_end` time: 0.1423s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1101s vs `on_train_batch_end` time: 0.1423s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 13s 362ms/step - loss: 0.3510 - val_loss: 0.4543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/by_data_size/2022_04_22_size1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/by_data_size/2022_04_22_size1/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    }
   ],
   "source": [
    "size = 1\n",
    "train = train_set\n",
    "valid, test = train_test_split(test_set, test_size=0.5, random_state=333, stratify=test_set[\"Y\"])\n",
    "print(len(train_set), len(valid), len(test))\n",
    "cm, f1_score = train_and_save_model(train, valid, test, size)\n",
    "cms[size] = cm\n",
    "f1s[size] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0a467ff-94ff-4c75-8685-0acfbb0097c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.5: 0.37894736842105264,\n",
       " 0.6: 0.4864864864864865,\n",
       " 0.7: 0.43010752688172044,\n",
       " 0.8: 0.4807692307692308,\n",
       " 0.9: 0.5094339622641509,\n",
       " 1: 0.5531914893617021}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ae230f-e4d7-4045-81ec-22f88f817888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c742e55-6ca8-478c-b902-24f1407b9f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antibody_ID</th>\n",
       "      <th>heavy</th>\n",
       "      <th>light</th>\n",
       "      <th>Y</th>\n",
       "      <th>cluster</th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>2g60</td>\n",
       "      <td>EVQLQQSGGELAKPGASVKMSCKSSGYTFTAYAIHWAKQAAGAGLE...</td>\n",
       "      <td>DVLMTQAPLTLPVSLGDQASISCRSSQAIVHANGNTYLEWYLQKPG...</td>\n",
       "      <td>0</td>\n",
       "      <td>911</td>\n",
       "      <td>EVQLQQSGGELAKPGASVKMSCKSSGYTFTAYAIHWAKQAAGAGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>2a1w</td>\n",
       "      <td>DVKLVESGGGLVKPGGSLRLSCAASGFTFRNYGMSWVRQTPEKRLE...</td>\n",
       "      <td>DVLMTQSPLSLPVSLGDQASISCRCSQSIVKSNGHTYLEWYLQKPG...</td>\n",
       "      <td>0</td>\n",
       "      <td>723</td>\n",
       "      <td>DVKLVESGGGLVKPGGSLRLSCAASGFTFRNYGMSWVRQTPEKRLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>2a77</td>\n",
       "      <td>DVKLVESGGGLVKPGGSLRLSCAASGFTFRNYGMSWVRQTPEKRLE...</td>\n",
       "      <td>DVLMTQSPLSLPVSLGDQASISCRCSQSIVKSNGHTYLEWYLQKPG...</td>\n",
       "      <td>0</td>\n",
       "      <td>723</td>\n",
       "      <td>DVKLVESGGGLVKPGGSLRLSCAASGFTFRNYGMSWVRQTPEKRLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>4ffy</td>\n",
       "      <td>QVQLLQPGAELVKPGASMKLSCKASGYTFTNWWMHWVRLRPGRGLE...</td>\n",
       "      <td>NIVLTQSPASLAVSLGQRATISCRASESVDHYGNSFIYWYQQKPGQ...</td>\n",
       "      <td>0</td>\n",
       "      <td>478</td>\n",
       "      <td>QVQLLQPGAELVKPGASMKLSCKASGYTFTNWWMHWVRLRPGRGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>3l5x</td>\n",
       "      <td>EVTLKESGPVLVKPTETLTLTCTVSGFSLSTYGMGVGWIRQPPGKA...</td>\n",
       "      <td>EIVLTQSPATLSLSPGERATLSCRASKSISKYLAWYQQKPGQAPRL...</td>\n",
       "      <td>0</td>\n",
       "      <td>433</td>\n",
       "      <td>EVTLKESGPVLVKPTETLTLTCTVSGFSLSTYGMGVGWIRQPPGKA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>5f9w</td>\n",
       "      <td>QVQLVQSGAEVKKPGASVTVSCQASGYTFTNYYVHWVRQAPGQGLQ...</td>\n",
       "      <td>EIVLTQSPATLSVSPGERATLSCRASQSVRSNLAWYQQRPGQAPRL...</td>\n",
       "      <td>0</td>\n",
       "      <td>271</td>\n",
       "      <td>QVQLVQSGAEVKKPGASVTVSCQASGYTFTNYYVHWVRQAPGQGLQ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>5x5x</td>\n",
       "      <td>QVKLQQSGAEFVKAGASVKLSCKTSGYTFNNYWIHWVKQSPGQGLE...</td>\n",
       "      <td>DIELTQSPLSLPVSLGDQASISCTSSQSLLHSNGDTYLHWYLQKPG...</td>\n",
       "      <td>0</td>\n",
       "      <td>861</td>\n",
       "      <td>QVKLQQSGAEFVKAGASVKLSCKTSGYTFNNYWIHWVKQSPGQGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>4qww</td>\n",
       "      <td>EVQLVESGGGLVQPKGSLKLSCAASGFTFNTYAMHWVRQAPGKGLE...</td>\n",
       "      <td>QIVLTQSPAIMSASPGEKVTMTCSASSSVSYMYWYHQKPGSSPKPW...</td>\n",
       "      <td>0</td>\n",
       "      <td>436</td>\n",
       "      <td>EVQLVESGGGLVQPKGSLKLSCAASGFTFNTYAMHWVRQAPGKGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1cgs</td>\n",
       "      <td>RVQLLESGAELMKPGASVQISCKATGYTFSEYWIEWVKERPGHGLE...</td>\n",
       "      <td>ELVMTQSPLSLPVSLGDQASISCRPSQSLVHSNGNTYLHWYLQKPG...</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>RVQLLESGAELMKPGASVQISCKATGYTFSEYWIEWVKERPGHGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>1ynl</td>\n",
       "      <td>RVQLLESGAELMKPGASVQISCKATGYTFSFYWIEWVKERPGHGLE...</td>\n",
       "      <td>ELVMTQSPLSLPVSLGDQASISCRPSQSLVHSNGNTYLHWYLQKPG...</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>RVQLLESGAELMKPGASVQISCKATGYTFSFYWIEWVKERPGHGLE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1291 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Antibody_ID                                              heavy  \\\n",
       "535         2g60  EVQLQQSGGELAKPGASVKMSCKSSGYTFTAYAIHWAKQAAGAGLE...   \n",
       "455         2a1w  DVKLVESGGGLVKPGGSLRLSCAASGFTFRNYGMSWVRQTPEKRLE...   \n",
       "459         2a77  DVKLVESGGGLVKPGGSLRLSCAASGFTFRNYGMSWVRQTPEKRLE...   \n",
       "1120        4ffy  QVQLLQPGAELVKPGASMKLSCKASGYTFTNWWMHWVRLRPGRGLE...   \n",
       "851         3l5x  EVTLKESGPVLVKPTETLTLTCTVSGFSLSTYGMGVGWIRQPPGKA...   \n",
       "...          ...                                                ...   \n",
       "1664        5f9w  QVQLVQSGAEVKKPGASVTVSCQASGYTFTNYYVHWVRQAPGQGLQ...   \n",
       "2017        5x5x  QVKLQQSGAEFVKAGASVKLSCKTSGYTFNNYWIHWVKQSPGQGLE...   \n",
       "1400        4qww  EVQLVESGGGLVQPKGSLKLSCAASGFTFNTYAMHWVRQAPGKGLE...   \n",
       "59          1cgs  RVQLLESGAELMKPGASVQISCKATGYTFSEYWIEWVKERPGHGLE...   \n",
       "439         1ynl  RVQLLESGAELMKPGASVQISCKATGYTFSFYWIEWVKERPGHGLE...   \n",
       "\n",
       "                                                  light  Y  cluster  \\\n",
       "535   DVLMTQAPLTLPVSLGDQASISCRSSQAIVHANGNTYLEWYLQKPG...  0      911   \n",
       "455   DVLMTQSPLSLPVSLGDQASISCRCSQSIVKSNGHTYLEWYLQKPG...  0      723   \n",
       "459   DVLMTQSPLSLPVSLGDQASISCRCSQSIVKSNGHTYLEWYLQKPG...  0      723   \n",
       "1120  NIVLTQSPASLAVSLGQRATISCRASESVDHYGNSFIYWYQQKPGQ...  0      478   \n",
       "851   EIVLTQSPATLSLSPGERATLSCRASKSISKYLAWYQQKPGQAPRL...  0      433   \n",
       "...                                                 ... ..      ...   \n",
       "1664  EIVLTQSPATLSVSPGERATLSCRASQSVRSNLAWYQQRPGQAPRL...  0      271   \n",
       "2017  DIELTQSPLSLPVSLGDQASISCTSSQSLLHSNGDTYLHWYLQKPG...  0      861   \n",
       "1400  QIVLTQSPAIMSASPGEKVTMTCSASSSVSYMYWYHQKPGSSPKPW...  0      436   \n",
       "59    ELVMTQSPLSLPVSLGDQASISCRPSQSLVHSNGNTYLHWYLQKPG...  0      103   \n",
       "439   ELVMTQSPLSLPVSLGDQASISCRPSQSLVHSNGNTYLHWYLQKPG...  0      103   \n",
       "\n",
       "                                                    seq  \n",
       "535   EVQLQQSGGELAKPGASVKMSCKSSGYTFTAYAIHWAKQAAGAGLE...  \n",
       "455   DVKLVESGGGLVKPGGSLRLSCAASGFTFRNYGMSWVRQTPEKRLE...  \n",
       "459   DVKLVESGGGLVKPGGSLRLSCAASGFTFRNYGMSWVRQTPEKRLE...  \n",
       "1120  QVQLLQPGAELVKPGASMKLSCKASGYTFTNWWMHWVRLRPGRGLE...  \n",
       "851   EVTLKESGPVLVKPTETLTLTCTVSGFSLSTYGMGVGWIRQPPGKA...  \n",
       "...                                                 ...  \n",
       "1664  QVQLVQSGAEVKKPGASVTVSCQASGYTFTNYYVHWVRQAPGQGLQ...  \n",
       "2017  QVKLQQSGAEFVKAGASVKLSCKTSGYTFNNYWIHWVKQSPGQGLE...  \n",
       "1400  EVQLVESGGGLVQPKGSLKLSCAASGFTFNTYAMHWVRQAPGKGLE...  \n",
       "59    RVQLLESGAELMKPGASVQISCKATGYTFSEYWIEWVKERPGHGLE...  \n",
       "439   RVQLLESGAELMKPGASVQISCKATGYTFSFYWIEWVKERPGHGLE...  \n",
       "\n",
       "[1291 rows x 6 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e914b2-7036-40db-b5b3-65bc9431f18b",
   "metadata": {},
   "source": [
    "# CV on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aaf483c7-69e0-4449-954c-8cbf889455bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antibody_ID</th>\n",
       "      <th>heavy</th>\n",
       "      <th>light</th>\n",
       "      <th>Y</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12e8</td>\n",
       "      <td>EVQLQQSGAEVVRSGASVKLSCTASGFNIKDYYIHWVKQRPEKGLE...</td>\n",
       "      <td>DIVMTQSQKFMSTSVGDRVSITCKASQNVGTAVAWYQQKPGQSPKL...</td>\n",
       "      <td>0</td>\n",
       "      <td>677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15c8</td>\n",
       "      <td>EVQLQQSGAELVKPGASVKLSCTASGFNIKDTYMHWVKQKPEQGLE...</td>\n",
       "      <td>DIVLTQSPAIMSASLGERVTMTCTASSSVSSSNLHWYQQKPGSSPK...</td>\n",
       "      <td>0</td>\n",
       "      <td>685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1a0q</td>\n",
       "      <td>EVQLQESDAELVKPGASVKISCKASGYTFTDHVIHWVKQKPEQGLE...</td>\n",
       "      <td>DIELTQSPSSLSASLGGKVTITCKASQDIKKYIGWYQHKPGKQPRL...</td>\n",
       "      <td>1</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1a14</td>\n",
       "      <td>QVQLQQSGAELVKPGASVRMSCKASGYTFTNYNMYWVKQSPGQGLE...</td>\n",
       "      <td>DIELTQTTSSLSASLGDRVTISCRASQDISNYLNWYQQNPDGTVKL...</td>\n",
       "      <td>0</td>\n",
       "      <td>442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1a2y</td>\n",
       "      <td>QVQLQESGPGLVAPSQSLSITCTVSGFSLTGYGVNWVRQPPGKGLE...</td>\n",
       "      <td>DIVLTQSPASLSASVGETVTITCRASGNIHNYLAWYQQKQGKSPQL...</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Antibody_ID                                              heavy  \\\n",
       "0        12e8  EVQLQQSGAEVVRSGASVKLSCTASGFNIKDYYIHWVKQRPEKGLE...   \n",
       "1        15c8  EVQLQQSGAELVKPGASVKLSCTASGFNIKDTYMHWVKQKPEQGLE...   \n",
       "2        1a0q  EVQLQESDAELVKPGASVKISCKASGYTFTDHVIHWVKQKPEQGLE...   \n",
       "3        1a14  QVQLQQSGAELVKPGASVRMSCKASGYTFTNYNMYWVKQSPGQGLE...   \n",
       "4        1a2y  QVQLQESGPGLVAPSQSLSITCTVSGFSLTGYGVNWVRQPPGKGLE...   \n",
       "\n",
       "                                               light  Y  cluster  \n",
       "0  DIVMTQSQKFMSTSVGDRVSITCKASQNVGTAVAWYQQKPGQSPKL...  0      677  \n",
       "1  DIVLTQSPAIMSASLGERVTMTCTASSSVSSSNLHWYQQKPGSSPK...  0      685  \n",
       "2  DIELTQSPSSLSASLGGKVTITCKASQDIKKYIGWYQHKPGKQPRL...  1      102  \n",
       "3  DIELTQTTSSLSASLGDRVTISCRASQDISNYLNWYQQNPDGTVKL...  0      442  \n",
       "4  DIVLTQSPASLSASVGETVTITCRASGNIHNYLAWYQQKQGKSPQL...  0       59  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chen_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_data_w_clusters.csv\"), index_col=0)\n",
    "chen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "04e07964-4f10-4bdf-8155-2a6ca501d13a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18     59\n",
       "24     35\n",
       "28     28\n",
       "8      25\n",
       "7      21\n",
       "       ..\n",
       "588     1\n",
       "562     1\n",
       "786     1\n",
       "722     1\n",
       "329     1\n",
       "Name: cluster, Length: 932, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chen_data[\"cluster\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ed9606ba-ee16-428b-b36c-9be6342598ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_k_sets(k, data):\n",
    "    total = len(data)\n",
    "    size = total // k + 1\n",
    "    clusters_by_size = data[\"cluster\"].value_counts().index\n",
    "    cluster_sizes = data[\"cluster\"].value_counts()\n",
    "    groups = { i: [] for i in range(k) }\n",
    "    group = 0\n",
    "    for clust in clusters_by_size:\n",
    "        start_group = group\n",
    "        if len(groups[group]) + cluster_sizes[clust] > size:\n",
    "            group += 1\n",
    "            group = group % k\n",
    "        while len(groups[group]) + cluster_sizes[clust] > size and group != start_group:\n",
    "            group += 1\n",
    "            group = group % k\n",
    "        if len(groups[group]) < size:\n",
    "            groups[group] += list(data[data[\"cluster\"] == clust].index)\n",
    "        else:\n",
    "            print(\"error\")\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d79864e4-8da7-4a49-ad11-3b2a16248c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n",
      "154\n",
      "155\n",
      "154\n",
      "156\n",
      "156\n",
      "156\n",
      "156\n",
      "156\n",
      "156\n"
     ]
    }
   ],
   "source": [
    "g = split_into_k_sets(10, chen_data)\n",
    "for key, gr in g.items():\n",
    "    print(len(gr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c25adec1-38c0-469d-88ad-e0ad79cc1695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_clusters(df_in):\n",
    "    df = df_in.copy()\n",
    "    df[\"cluster_merged\"] = df[\"cluster\"]\n",
    "    df[\"cluster_merged\"][df[\"cluster\"] < 300] = df[\"cluster\"][df[\"cluster\"] < 300] // 30\n",
    "    df[\"cluster_merged\"][df[\"cluster\"] >= 300] = df[\"cluster\"][df[\"cluster\"] >= 300] // 100\n",
    "    print(f'Unique clusters after merge: {df[\"cluster_merged\"].nunique()}')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cdc96cde-6c53-4666-87ac-2cd796d6b252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique clusters after merge: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "merged = merge_clusters(chen_data[~chen_data[\"cluster\"].isin([18, 24, 28])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "25d9450d-0a66-4916-b4b2-6d5c5a253c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    214\n",
       "0    198\n",
       "4    164\n",
       "7    161\n",
       "6    159\n",
       "5    158\n",
       "8    135\n",
       "1    104\n",
       "2     71\n",
       "9     65\n",
       "Name: cluster_merged, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged[\"cluster_merged\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "290e5fc3-9458-4ac3-83ea-03e6950db9cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antibody_ID</th>\n",
       "      <th>heavy</th>\n",
       "      <th>light</th>\n",
       "      <th>Y</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1dqj</td>\n",
       "      <td>EVQLQESGPSLVKPSQTLSLTCSVTGDSVTSDYWSWIRKFPGNKLE...</td>\n",
       "      <td>DIVLTQSPATLSVTPGDSVSLSCRASQSISNNLHWYQQKSHESPRL...</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1fvd</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGFNIKDTYIHWVRQAPGKGLE...</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCRASQDVNTAVAWYQQKPGKAPKL...</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>1fve</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGFNIKDTYIHWVRQAPGKGLE...</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCRASQDVNTAVAWYQQKPGKAPKL...</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1gpo</td>\n",
       "      <td>EVKLQESGPSLVKPSQTLSLTCSVTGDSITSDFWSWIRQFPGNRLE...</td>\n",
       "      <td>DIELTQSPATLSVTPGNSVSISCRASQSLVNEDGNTYLFWYQQKSH...</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>1ic4</td>\n",
       "      <td>DVQLQESGPSLVKPSQTLSLTCSVTGDSITSAYWSWIRKFPGNRLE...</td>\n",
       "      <td>DIVLTQSPATLSVTPGNSVSLSCRASQSIGNNLHWYQQKSHESPRL...</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2347</th>\n",
       "      <td>6my5</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGFNIKDTWIHWVRQAPGKGLE...</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCRASQDIPRRISGYVAWYQQKPGK...</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2368</th>\n",
       "      <td>6o39</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGFNIHSSSIHWVRQAPGKGLE...</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCRASQSVSSAVAWYQQKPGKAPKL...</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2369</th>\n",
       "      <td>6o3a</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGFNFSSSSIHWVRQAPGKGLE...</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCRASQSVSSAVAWYQQKPGKAPKL...</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2370</th>\n",
       "      <td>6o3b</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGFNIYYYSMHWVRQAPGKGLE...</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCRASQSVSSAVAWYQQKPGKAPKL...</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2381</th>\n",
       "      <td>6otc</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGFNISYYYIHWVRQAPGKGLE...</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCRASQSVSSAVAWYQQKPGKAPKL...</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Antibody_ID                                              heavy  \\\n",
       "82          1dqj  EVQLQESGPSLVKPSQTLSLTCSVTGDSVTSDYWSWIRKFPGNKLE...   \n",
       "123         1fvd  EVQLVESGGGLVQPGGSLRLSCAASGFNIKDTYIHWVRQAPGKGLE...   \n",
       "124         1fve  EVQLVESGGGLVQPGGSLRLSCAASGFNIKDTYIHWVRQAPGKGLE...   \n",
       "139         1gpo  EVKLQESGPSLVKPSQTLSLTCSVTGDSITSDFWSWIRQFPGNRLE...   \n",
       "163         1ic4  DVQLQESGPSLVKPSQTLSLTCSVTGDSITSAYWSWIRKFPGNRLE...   \n",
       "...          ...                                                ...   \n",
       "2347        6my5  EVQLVESGGGLVQPGGSLRLSCAASGFNIKDTWIHWVRQAPGKGLE...   \n",
       "2368        6o39  EVQLVESGGGLVQPGGSLRLSCAASGFNIHSSSIHWVRQAPGKGLE...   \n",
       "2369        6o3a  EVQLVESGGGLVQPGGSLRLSCAASGFNFSSSSIHWVRQAPGKGLE...   \n",
       "2370        6o3b  EVQLVESGGGLVQPGGSLRLSCAASGFNIYYYSMHWVRQAPGKGLE...   \n",
       "2381        6otc  EVQLVESGGGLVQPGGSLRLSCAASGFNISYYYIHWVRQAPGKGLE...   \n",
       "\n",
       "                                                  light  Y  cluster  \n",
       "82    DIVLTQSPATLSVTPGDSVSLSCRASQSISNNLHWYQQKSHESPRL...  1       28  \n",
       "123   DIQMTQSPSSLSASVGDRVTITCRASQDVNTAVAWYQQKPGKAPKL...  1       24  \n",
       "124   DIQMTQSPSSLSASVGDRVTITCRASQDVNTAVAWYQQKPGKAPKL...  1       24  \n",
       "139   DIELTQSPATLSVTPGNSVSISCRASQSLVNEDGNTYLFWYQQKSH...  1       28  \n",
       "163   DIVLTQSPATLSVTPGNSVSLSCRASQSIGNNLHWYQQKSHESPRL...  0       28  \n",
       "...                                                 ... ..      ...  \n",
       "2347  DIQMTQSPSSLSASVGDRVTITCRASQDIPRRISGYVAWYQQKPGK...  0       18  \n",
       "2368  DIQMTQSPSSLSASVGDRVTITCRASQSVSSAVAWYQQKPGKAPKL...  0       24  \n",
       "2369  DIQMTQSPSSLSASVGDRVTITCRASQSVSSAVAWYQQKPGKAPKL...  0       24  \n",
       "2370  DIQMTQSPSSLSASVGDRVTITCRASQSVSSAVAWYQQKPGKAPKL...  0       24  \n",
       "2381  DIQMTQSPSSLSASVGDRVTITCRASQSVSSAVAWYQQKPGKAPKL...  0       24  \n",
       "\n",
       "[122 rows x 5 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest = chen_data[chen_data[\"cluster\"].isin([18, 24, 28])].copy()\n",
    "rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4da27981-420b-48d8-b9d5-e2941b40981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments = {\n",
    "    18: 9,\n",
    "    24: 2,\n",
    "    28: 1\n",
    "}\n",
    "\n",
    "def assign_rest(cluster):\n",
    "    return assignments[cluster]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "04de5cb8-984b-4e79-ad3b-a4f9b23d5863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antibody_ID</th>\n",
       "      <th>heavy</th>\n",
       "      <th>light</th>\n",
       "      <th>Y</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster_merged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1dqj</td>\n",
       "      <td>EVQLQESGPSLVKPSQTLSLTCSVTGDSVTSDYWSWIRKFPGNKLE...</td>\n",
       "      <td>DIVLTQSPATLSVTPGDSVSLSCRASQSISNNLHWYQQKSHESPRL...</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1fvd</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGFNIKDTYIHWVRQAPGKGLE...</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCRASQDVNTAVAWYQQKPGKAPKL...</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>1fve</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGFNIKDTYIHWVRQAPGKGLE...</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCRASQDVNTAVAWYQQKPGKAPKL...</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1gpo</td>\n",
       "      <td>EVKLQESGPSLVKPSQTLSLTCSVTGDSITSDFWSWIRQFPGNRLE...</td>\n",
       "      <td>DIELTQSPATLSVTPGNSVSISCRASQSLVNEDGNTYLFWYQQKSH...</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>1ic4</td>\n",
       "      <td>DVQLQESGPSLVKPSQTLSLTCSVTGDSITSAYWSWIRKFPGNRLE...</td>\n",
       "      <td>DIVLTQSPATLSVTPGNSVSLSCRASQSIGNNLHWYQQKSHESPRL...</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Antibody_ID                                              heavy  \\\n",
       "82         1dqj  EVQLQESGPSLVKPSQTLSLTCSVTGDSVTSDYWSWIRKFPGNKLE...   \n",
       "123        1fvd  EVQLVESGGGLVQPGGSLRLSCAASGFNIKDTYIHWVRQAPGKGLE...   \n",
       "124        1fve  EVQLVESGGGLVQPGGSLRLSCAASGFNIKDTYIHWVRQAPGKGLE...   \n",
       "139        1gpo  EVKLQESGPSLVKPSQTLSLTCSVTGDSITSDFWSWIRQFPGNRLE...   \n",
       "163        1ic4  DVQLQESGPSLVKPSQTLSLTCSVTGDSITSAYWSWIRKFPGNRLE...   \n",
       "\n",
       "                                                 light  Y  cluster  \\\n",
       "82   DIVLTQSPATLSVTPGDSVSLSCRASQSISNNLHWYQQKSHESPRL...  1       28   \n",
       "123  DIQMTQSPSSLSASVGDRVTITCRASQDVNTAVAWYQQKPGKAPKL...  1       24   \n",
       "124  DIQMTQSPSSLSASVGDRVTITCRASQDVNTAVAWYQQKPGKAPKL...  1       24   \n",
       "139  DIELTQSPATLSVTPGNSVSISCRASQSLVNEDGNTYLFWYQQKSH...  1       28   \n",
       "163  DIVLTQSPATLSVTPGNSVSLSCRASQSIGNNLHWYQQKSHESPRL...  0       28   \n",
       "\n",
       "     cluster_merged  \n",
       "82                1  \n",
       "123               2  \n",
       "124               2  \n",
       "139               1  \n",
       "163               1  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest[\"cluster_merged\"] = rest[\"cluster\"].apply(assign_rest)\n",
    "rest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cfd088c6-f733-49d5-8916-ff20072db33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    214\n",
       "0    198\n",
       "4    164\n",
       "7    161\n",
       "6    159\n",
       "5    158\n",
       "8    135\n",
       "1    132\n",
       "9    124\n",
       "2    106\n",
       "Name: cluster_merged, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = pd.concat([merged, rest])\n",
    "merged[\"cluster_merged\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6e4ed599-fb58-459e-8599-26f8348071a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[\"seq\"] = merged[\"heavy\"] + merged[\"light\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a454118a-9e79-4c92-beff-c3cb26e7f542",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_save_named_model(train_data, valid_data, test_data, name, project_name):\n",
    "    wandb.init(project=project_name, entity=\"kvetab\")\n",
    "    model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "            get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)\n",
    "\n",
    "    training_callbacks = [\n",
    "        keras.callbacks.ReduceLROnPlateau(patience = patience[1], factor = 0.25, min_lr = 1e-07, verbose = 1),\n",
    "        keras.callbacks.EarlyStopping(patience = patience[0], restore_best_weights = True),\n",
    "        WandbCallback()\n",
    "    ]\n",
    "\n",
    "    epoch_num = 100\n",
    "    batch_size = 128\n",
    "    #learning_rate = 1e-5\n",
    "    wandb.config = {\n",
    "          \"learning_rate\": learning_rate,\n",
    "          \"epochs\": epoch_num * 2,\n",
    "          \"batch_size\": batch_size\n",
    "        }\n",
    "    finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data[\"seq\"], train_data[\"Y\"], valid_data['seq'], valid_data[\"Y\"], \\\n",
    "            seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = learning_rate, begin_with_frozen_pretrained_layers = True, \\\n",
    "            lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = learning_rate / 10, callbacks = training_callbacks)\n",
    "    mod = model_generator.create_model(seq_len = 512)\n",
    "    mod.save(path.join(DATA_DIR, f\"protein_bert/{name}\"))\n",
    "\n",
    "    results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "            start_seq_len = 512, start_batch_size = 32)\n",
    "    fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "    f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "    return confusion_matrix, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10abe629-851d-4f20-af43-ba822d8babdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = {}\n",
    "f1s = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "960bc93a-3bd3-42c1-a575-424936bbe0c3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/2e5dt2sn\" target=\"_blank\">happy-surf-1</a></strong> to <a href=\"https://wandb.ai/kvetab/10_fold_cv\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_22-15:58:00] Training set: Filtered out 0 of 1082 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_22-15:58:00] Validation set: Filtered out 0 of 271 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_22-15:58:00] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 11s 367ms/step - loss: 0.8908 - val_loss: 0.7102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.7149 - val_loss: 0.6553\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.5343 - val_loss: 0.4920\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.4778 - val_loss: 0.4598\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 1s 127ms/step - loss: 0.4408 - val_loss: 0.4490\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.4222 - val_loss: 0.4594\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 1s 127ms/step - loss: 0.4148 - val_loss: 0.4293\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.4132 - val_loss: 0.4243\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.4165 - val_loss: 0.4202\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.4115 - val_loss: 0.4325\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 1s 124ms/step - loss: 0.4046 - val_loss: 0.4140\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.3867 - val_loss: 0.4111\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 1s 127ms/step - loss: 0.3812 - val_loss: 0.4205\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.3734 - val_loss: 0.4364\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 1s 127ms/step - loss: 0.3847 - val_loss: 0.4617\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 1s 128ms/step - loss: 0.3778 - val_loss: 0.4077\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.3739 - val_loss: 0.4124\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 1s 128ms/step - loss: 0.3650 - val_loss: 0.4542\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.3706 - val_loss: 0.4427\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.3893 - val_loss: 0.3989\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 1s 127ms/step - loss: 0.3837 - val_loss: 0.4020\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 1s 128ms/step - loss: 0.3889 - val_loss: 0.4045\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.3577 - val_loss: 0.4011\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.3588 - val_loss: 0.3934\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.3410 - val_loss: 0.4111\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 1s 127ms/step - loss: 0.3651 - val_loss: 0.3987\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.3575 - val_loss: 0.3954\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 1s 127ms/step - loss: 0.3539 - val_loss: 0.3963\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 1s 127ms/step - loss: 0.3473 - val_loss: 0.4137\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 1s 127ms/step - loss: 0.3351 - val_loss: 0.4000\n",
      "[2022_04_22-15:58:46] Training the entire fine-tuned model...\n",
      "[2022_04_22-15:58:54] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 0.3602WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1143s vs `on_train_batch_end` time: 0.1457s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1143s vs `on_train_batch_end` time: 0.1457s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 10s 500ms/step - loss: 0.3572 - val_loss: 0.4218\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 2s 267ms/step - loss: 0.3638 - val_loss: 0.4070\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 2s 269ms/step - loss: 0.3425 - val_loss: 0.4112\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 2s 272ms/step - loss: 0.3421 - val_loss: 0.4217\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 2s 269ms/step - loss: 0.3325 - val_loss: 0.3917\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 2s 268ms/step - loss: 0.3246 - val_loss: 0.3865\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 2s 270ms/step - loss: 0.3225 - val_loss: 0.4009\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 2s 270ms/step - loss: 0.3033 - val_loss: 0.4424\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 2s 270ms/step - loss: 0.3166 - val_loss: 0.3836\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 2s 271ms/step - loss: 0.2998 - val_loss: 0.3933\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 2s 270ms/step - loss: 0.3265 - val_loss: 0.3965\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 3s 282ms/step - loss: 0.2876 - val_loss: 0.4248\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 2s 270ms/step - loss: 0.2939 - val_loss: 0.3935\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 2s 272ms/step - loss: 0.2809 - val_loss: 0.3980\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 2s 271ms/step - loss: 0.2794 - val_loss: 0.3922\n",
      "[2022_04_22-15:59:39] Training on final epochs of sequence length 1024...\n",
      "[2022_04_22-15:59:39] Training set: Filtered out 0 of 1082 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_22-15:59:41] Validation set: Filtered out 0 of 271 (0.0%) records of lengths exceeding 1022.\n",
      " 6/17 [=========>....................] - ETA: 2s - loss: 0.3059WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1105s vs `on_train_batch_end` time: 0.1407s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1105s vs `on_train_batch_end` time: 0.1407s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 12s 387ms/step - loss: 0.3257 - val_loss: 0.3903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_22_split_0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_22_split_0/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2e5dt2sn) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9012... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▄▄▄▄</td></tr><tr><td>loss</td><td>█▆▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████████████▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▃▃▂▃▂▂▂▂▂▂▂▃▂▃▂▁▁▁▁▂▁▁▁▂▁▂▂▂▁▁▁▁▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_val_loss</td><td>0.38365</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32567</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.39026</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">happy-surf-1</strong>: <a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/2e5dt2sn\" target=\"_blank\">https://wandb.ai/kvetab/10_fold_cv/runs/2e5dt2sn</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220422_155750-2e5dt2sn/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2e5dt2sn). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/230r5cuy\" target=\"_blank\">polished-dew-2</a></strong> to <a href=\"https://wandb.ai/kvetab/10_fold_cv\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_22-16:00:38] Training set: Filtered out 0 of 1135 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_22-16:00:38] Validation set: Filtered out 0 of 284 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_22-16:00:38] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 11s 351ms/step - loss: 0.7254 - val_loss: 0.7251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.5702 - val_loss: 0.6182\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.4973 - val_loss: 0.4978\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.4328 - val_loss: 0.4712\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.4134 - val_loss: 0.4592\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.4079 - val_loss: 0.4609\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.4127 - val_loss: 0.4532\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3938 - val_loss: 0.4406\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.3884 - val_loss: 0.4484\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.3832 - val_loss: 0.4332\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.3838 - val_loss: 0.4294\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.3808 - val_loss: 0.4572\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.4036 - val_loss: 0.5001\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3865 - val_loss: 0.4497\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.3781 - val_loss: 0.4160\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.3580 - val_loss: 0.4168\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.3673 - val_loss: 0.4231\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3678 - val_loss: 0.4445\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3808 - val_loss: 0.4212\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.3834 - val_loss: 0.4368\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.3613 - val_loss: 0.4096\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3483 - val_loss: 0.4077\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 1s 134ms/step - loss: 0.3490 - val_loss: 0.4286\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3503 - val_loss: 0.4090\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 1s 142ms/step - loss: 0.3466 - val_loss: 0.4234\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3508 - val_loss: 0.4074\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.3404 - val_loss: 0.4123\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.3411 - val_loss: 0.4079\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3429 - val_loss: 0.4066\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3482 - val_loss: 0.4065\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3426 - val_loss: 0.4179\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.3427 - val_loss: 0.4064\n",
      "Epoch 33/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.3421 - val_loss: 0.4092\n",
      "Epoch 34/100\n",
      "9/9 [==============================] - 1s 143ms/step - loss: 0.3464 - val_loss: 0.4073\n",
      "Epoch 35/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.3463 - val_loss: 0.4084\n",
      "Epoch 36/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3505 - val_loss: 0.4127\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 37/100\n",
      "9/9 [==============================] - 1s 142ms/step - loss: 0.3416 - val_loss: 0.4076\n",
      "Epoch 38/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.3441 - val_loss: 0.4135\n",
      "[2022_04_22-16:01:34] Training the entire fine-tuned model...\n",
      "[2022_04_22-16:02:17] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 0.3662WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1179s vs `on_train_batch_end` time: 0.1462s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1179s vs `on_train_batch_end` time: 0.1462s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 10s 521ms/step - loss: 0.3661 - val_loss: 0.4106\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 3s 282ms/step - loss: 0.3489 - val_loss: 0.4106\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 3s 284ms/step - loss: 0.3349 - val_loss: 0.4063\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 3s 283ms/step - loss: 0.3398 - val_loss: 0.4251\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 3s 284ms/step - loss: 0.3296 - val_loss: 0.4063\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 3s 284ms/step - loss: 0.3225 - val_loss: 0.4576\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 3s 283ms/step - loss: 0.3358 - val_loss: 0.4218\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 3s 284ms/step - loss: 0.3184 - val_loss: 0.4065\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 3s 284ms/step - loss: 0.3135 - val_loss: 0.4038\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 3s 285ms/step - loss: 0.3246 - val_loss: 0.4204\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 3s 286ms/step - loss: 0.3060 - val_loss: 0.4042\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 3s 295ms/step - loss: 0.3120 - val_loss: 0.4080\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 3s 296ms/step - loss: 0.2973 - val_loss: 0.4058\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 3s 284ms/step - loss: 0.2951 - val_loss: 0.4080\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 3s 285ms/step - loss: 0.3015 - val_loss: 0.4077\n",
      "[2022_04_22-16:03:03] Training on final epochs of sequence length 1024...\n",
      "[2022_04_22-16:03:03] Training set: Filtered out 0 of 1135 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_22-16:03:04] Validation set: Filtered out 0 of 284 (0.0%) records of lengths exceeding 1022.\n",
      " 6/18 [=========>....................] - ETA: 2s - loss: 0.3442WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1114s vs `on_train_batch_end` time: 0.1409s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1114s vs `on_train_batch_end` time: 0.1409s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 12s 385ms/step - loss: 0.3226 - val_loss: 0.4008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_22_split_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_22_split_1/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:230r5cuy) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9406... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▁▂▂▂▃▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▃▂▂▂▂▂▂▃▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.40079</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32257</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.40079</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">polished-dew-2</strong>: <a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/230r5cuy\" target=\"_blank\">https://wandb.ai/kvetab/10_fold_cv/runs/230r5cuy</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220422_160020-230r5cuy/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:230r5cuy). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/2z9fgh9e\" target=\"_blank\">genial-totem-3</a></strong> to <a href=\"https://wandb.ai/kvetab/10_fold_cv\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_22-16:04:02] Training set: Filtered out 0 of 1156 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_22-16:04:02] Validation set: Filtered out 0 of 289 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_22-16:04:02] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 10s 311ms/step - loss: 0.7945 - val_loss: 0.9538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.6552 - val_loss: 0.7137\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.5754 - val_loss: 0.6708\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.6183 - val_loss: 0.4861\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.5020 - val_loss: 0.5103\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.4440 - val_loss: 0.4684\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.4213 - val_loss: 0.4700\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.4207 - val_loss: 0.5473\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.4783 - val_loss: 0.5825\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.5319 - val_loss: 0.6642\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.5188 - val_loss: 0.4553\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.4173 - val_loss: 0.4978\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.4093 - val_loss: 0.4542\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3969 - val_loss: 0.4620\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3906 - val_loss: 0.4569\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3887 - val_loss: 0.4490\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3937 - val_loss: 0.4554\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3956 - val_loss: 0.4547\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3837 - val_loss: 0.4442\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.4041 - val_loss: 0.4557\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3811 - val_loss: 0.4712\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.4197 - val_loss: 0.4402\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3868 - val_loss: 0.4497\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3887 - val_loss: 0.4404\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3770 - val_loss: 0.4368\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3827 - val_loss: 0.4780\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3929 - val_loss: 0.4339\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3843 - val_loss: 0.4680\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3904 - val_loss: 0.4363\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.4047 - val_loss: 0.4825\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3854 - val_loss: 0.4359\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3706 - val_loss: 0.4490\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3758 - val_loss: 0.4455\n",
      "[2022_04_22-16:04:52] Training the entire fine-tuned model...\n",
      "[2022_04_22-16:05:00] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.3831WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1119s vs `on_train_batch_end` time: 0.1462s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1119s vs `on_train_batch_end` time: 0.1462s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 10s 460ms/step - loss: 0.3838 - val_loss: 0.4623\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 3s 259ms/step - loss: 0.3778 - val_loss: 0.4512\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 3s 260ms/step - loss: 0.3781 - val_loss: 0.4314\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 3s 261ms/step - loss: 0.3703 - val_loss: 0.4337\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 3s 261ms/step - loss: 0.3783 - val_loss: 0.4778\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 3s 260ms/step - loss: 0.3746 - val_loss: 0.5134\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 3s 261ms/step - loss: 0.4000 - val_loss: 0.4490\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 3s 262ms/step - loss: 0.3749 - val_loss: 0.4366\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 3s 259ms/step - loss: 0.3597 - val_loss: 0.4406\n",
      "[2022_04_22-16:05:32] Training on final epochs of sequence length 1024...\n",
      "[2022_04_22-16:05:32] Training set: Filtered out 0 of 1156 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_22-16:05:47] Validation set: Filtered out 0 of 289 (0.0%) records of lengths exceeding 1022.\n",
      " 6/19 [========>.....................] - ETA: 3s - loss: 0.4100WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1105s vs `on_train_batch_end` time: 0.1406s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1105s vs `on_train_batch_end` time: 0.1406s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 13s 401ms/step - loss: 0.3782 - val_loss: 0.4343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_22_split_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_22_split_2/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2z9fgh9e) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9800... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇███▁▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▆▄▅▃▂▂▂▃▄▃▂▂▁▁▁▁▁▂▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▄▂▂▁▂▃▃▄▁▂▁▁▁▁▁▁▁▂▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>2</td></tr><tr><td>best_val_loss</td><td>0.43135</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37825</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.43428</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">genial-totem-3</strong>: <a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/2z9fgh9e\" target=\"_blank\">https://wandb.ai/kvetab/10_fold_cv/runs/2z9fgh9e</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220422_160347-2z9fgh9e/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2z9fgh9e). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/jmp9fncs\" target=\"_blank\">super-universe-4</a></strong> to <a href=\"https://wandb.ai/kvetab/10_fold_cv\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_22-16:06:45] Training set: Filtered out 0 of 1069 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_22-16:06:46] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_22-16:06:46] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 9s 344ms/step - loss: 0.8405 - val_loss: 0.5694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.5570 - val_loss: 0.4720\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.5214 - val_loss: 0.4596\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.4941 - val_loss: 0.4730\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.4600 - val_loss: 0.4635\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.4347 - val_loss: 0.4364\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.4118 - val_loss: 0.4342\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.4137 - val_loss: 0.4296\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.4048 - val_loss: 0.4353\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.4091 - val_loss: 0.4206\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.4009 - val_loss: 0.4186\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.3893 - val_loss: 0.4284\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.3904 - val_loss: 0.4119\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.3918 - val_loss: 0.4087\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.3807 - val_loss: 0.4077\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 1s 124ms/step - loss: 0.3842 - val_loss: 0.4064\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.3798 - val_loss: 0.4085\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.3928 - val_loss: 0.4014\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.3833 - val_loss: 0.4090\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.3841 - val_loss: 0.4254\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 1s 138ms/step - loss: 0.3799 - val_loss: 0.3963\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.3670 - val_loss: 0.3941\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.3740 - val_loss: 0.4001\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.3698 - val_loss: 0.4052\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.3546 - val_loss: 0.4106\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.3816 - val_loss: 0.5071\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.4037 - val_loss: 0.4331\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.3690 - val_loss: 0.3906\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.3743 - val_loss: 0.3917\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.3566 - val_loss: 0.3949\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.3572 - val_loss: 0.3926\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 1s 127ms/step - loss: 0.3603 - val_loss: 0.3905\n",
      "Epoch 33/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.3712 - val_loss: 0.3960\n",
      "Epoch 34/100\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.3540 - val_loss: 0.3959\n",
      "Epoch 35/100\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.3592 - val_loss: 0.3905\n",
      "Epoch 36/100\n",
      "9/9 [==============================] - 1s 124ms/step - loss: 0.3462 - val_loss: 0.3899\n",
      "Epoch 37/100\n",
      "9/9 [==============================] - 1s 124ms/step - loss: 0.3578 - val_loss: 0.3918\n",
      "Epoch 38/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.3526 - val_loss: 0.3995\n",
      "Epoch 39/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.3623 - val_loss: 0.3992\n",
      "Epoch 40/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.3576 - val_loss: 0.3937\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 41/100\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.3524 - val_loss: 0.3898\n",
      "Epoch 42/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.3509 - val_loss: 0.3903\n",
      "Epoch 43/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.3544 - val_loss: 0.3901\n",
      "Epoch 44/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.3427 - val_loss: 0.3910\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 45/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.3481 - val_loss: 0.3897\n",
      "Epoch 46/100\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.3517 - val_loss: 0.3897\n",
      "Epoch 47/100\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.3457 - val_loss: 0.3894\n",
      "Epoch 48/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.3470 - val_loss: 0.3893\n",
      "Epoch 49/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.3513 - val_loss: 0.3893\n",
      "Epoch 50/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.3401 - val_loss: 0.3894\n",
      "Epoch 51/100\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.3459 - val_loss: 0.3893\n",
      "Epoch 52/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.3543 - val_loss: 0.3894\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 53/100\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.3459 - val_loss: 0.3894\n",
      "Epoch 54/100\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.3589 - val_loss: 0.3894\n",
      "[2022_04_22-16:07:58] Training the entire fine-tuned model...\n",
      "[2022_04_22-16:08:06] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 0.3559WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1138s vs `on_train_batch_end` time: 0.1475s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1138s vs `on_train_batch_end` time: 0.1475s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 10s 492ms/step - loss: 0.3549 - val_loss: 0.4130\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 2s 267ms/step - loss: 0.3620 - val_loss: 0.3916\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 2s 268ms/step - loss: 0.3560 - val_loss: 0.3902\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 2s 279ms/step - loss: 0.3493 - val_loss: 0.3876\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 2s 268ms/step - loss: 0.3300 - val_loss: 0.3886\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 2s 268ms/step - loss: 0.3262 - val_loss: 0.4013\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 2s 268ms/step - loss: 0.3296 - val_loss: 0.4441\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 2s 268ms/step - loss: 0.3318 - val_loss: 0.3845\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 2s 268ms/step - loss: 0.3137 - val_loss: 0.3874\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 2s 269ms/step - loss: 0.2995 - val_loss: 0.3881\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 2s 269ms/step - loss: 0.3028 - val_loss: 0.4004\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 2s 269ms/step - loss: 0.2855 - val_loss: 0.4026\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 2s 270ms/step - loss: 0.2885 - val_loss: 0.3986\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 2s 271ms/step - loss: 0.2812 - val_loss: 0.3949\n",
      "[2022_04_22-16:08:48] Training on final epochs of sequence length 1024...\n",
      "[2022_04_22-16:08:48] Training set: Filtered out 0 of 1069 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_22-16:08:53] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 1022.\n",
      " 6/17 [=========>....................] - ETA: 2s - loss: 0.3479WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1101s vs `on_train_batch_end` time: 0.1416s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1101s vs `on_train_batch_end` time: 0.1416s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 12s 384ms/step - loss: 0.3277 - val_loss: 0.3841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_22_split_3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_22_split_3/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:jmp9fncs) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10115... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇██▁▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▄▃▃▃▂▂▂▂▂▃▁▂▂▃▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▂▁▁▂▁▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.38409</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32773</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.38409</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">super-universe-4</strong>: <a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/jmp9fncs\" target=\"_blank\">https://wandb.ai/kvetab/10_fold_cv/runs/jmp9fncs</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220422_160629-jmp9fncs/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:jmp9fncs). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/12pszpgz\" target=\"_blank\">driven-star-5</a></strong> to <a href=\"https://wandb.ai/kvetab/10_fold_cv\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_22-16:09:48] Training set: Filtered out 0 of 1109 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_22-16:09:48] Validation set: Filtered out 0 of 278 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_22-16:09:48] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 11s 361ms/step - loss: 0.7644 - val_loss: 0.5655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.5257 - val_loss: 0.4630\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.4816 - val_loss: 0.4068\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.4375 - val_loss: 0.4017\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.4371 - val_loss: 0.3890\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 1s 128ms/step - loss: 0.4181 - val_loss: 0.3791\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.4062 - val_loss: 0.3745\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.4004 - val_loss: 0.3777\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 1s 128ms/step - loss: 0.4024 - val_loss: 0.3658\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.4045 - val_loss: 0.3629\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 1s 141ms/step - loss: 0.3869 - val_loss: 0.4005\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.4196 - val_loss: 0.3615\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 1s 128ms/step - loss: 0.3950 - val_loss: 0.3531\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.4090 - val_loss: 0.3974\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.4109 - val_loss: 0.3542\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 1s 128ms/step - loss: 0.3738 - val_loss: 0.4293\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.3937 - val_loss: 0.3835\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.3698 - val_loss: 0.3459\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.3675 - val_loss: 0.3446\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 1s 128ms/step - loss: 0.3694 - val_loss: 0.3547\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.3701 - val_loss: 0.3499\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.3678 - val_loss: 0.3483\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.3601 - val_loss: 0.3459\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.3487 - val_loss: 0.3445\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.3624 - val_loss: 0.3425\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 1s 128ms/step - loss: 0.3490 - val_loss: 0.3433\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.3561 - val_loss: 0.3438\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.3600 - val_loss: 0.3432\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.3538 - val_loss: 0.3427\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.3573 - val_loss: 0.3426\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.3541 - val_loss: 0.3429\n",
      "[2022_04_22-16:10:36] Training the entire fine-tuned model...\n",
      "[2022_04_22-16:10:46] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 0.3744WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1117s vs `on_train_batch_end` time: 0.1465s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1117s vs `on_train_batch_end` time: 0.1465s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 10s 500ms/step - loss: 0.3659 - val_loss: 0.3433\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 2s 277ms/step - loss: 0.3663 - val_loss: 0.3384\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 2s 278ms/step - loss: 0.3519 - val_loss: 0.3372\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 2s 278ms/step - loss: 0.3487 - val_loss: 0.3964\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 2s 277ms/step - loss: 0.3568 - val_loss: 0.3361\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 3s 290ms/step - loss: 0.3493 - val_loss: 0.3380\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 2s 278ms/step - loss: 0.3312 - val_loss: 0.3435\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 2s 278ms/step - loss: 0.3267 - val_loss: 0.3314\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 2s 278ms/step - loss: 0.3203 - val_loss: 0.3301\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 2s 278ms/step - loss: 0.3208 - val_loss: 0.3468\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 2s 279ms/step - loss: 0.3107 - val_loss: 0.3305\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 2s 279ms/step - loss: 0.3042 - val_loss: 0.3314\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 3s 280ms/step - loss: 0.3038 - val_loss: 0.3581\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 2s 279ms/step - loss: 0.2947 - val_loss: 0.3362\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 2s 278ms/step - loss: 0.2944 - val_loss: 0.3325\n",
      "[2022_04_22-16:11:32] Training on final epochs of sequence length 1024...\n",
      "[2022_04_22-16:11:32] Training set: Filtered out 0 of 1109 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_22-16:11:33] Validation set: Filtered out 0 of 278 (0.0%) records of lengths exceeding 1022.\n",
      " 6/18 [=========>....................] - ETA: 3s - loss: 0.3368WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1090s vs `on_train_batch_end` time: 0.1448s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1090s vs `on_train_batch_end` time: 0.1448s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 12s 375ms/step - loss: 0.3413 - val_loss: 0.3320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_22_split_4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_22_split_4/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:12pszpgz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10558... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇███▁▁▂▂▂▂▃▃▃▄▄▄▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▃▃▃▃▂▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂</td></tr><tr><td>lr</td><td>███████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▃▃▂▂▂▂▃▂▂▂▄▃▁▁▂▂▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_val_loss</td><td>0.33011</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34131</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.33201</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">driven-star-5</strong>: <a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/12pszpgz\" target=\"_blank\">https://wandb.ai/kvetab/10_fold_cv/runs/12pszpgz</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220422_160932-12pszpgz/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:12pszpgz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/24eaf3rx\" target=\"_blank\">gentle-snow-6</a></strong> to <a href=\"https://wandb.ai/kvetab/10_fold_cv\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_22-16:12:32] Training set: Filtered out 0 of 1114 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_22-16:12:32] Validation set: Filtered out 0 of 279 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_22-16:12:32] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 11s 413ms/step - loss: 0.7948 - val_loss: 0.5993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.5518 - val_loss: 0.4527\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.4861 - val_loss: 0.4542\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.4477 - val_loss: 0.4347\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.4494 - val_loss: 0.4781\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.4390 - val_loss: 0.4333\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.4263 - val_loss: 0.4642\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.4518 - val_loss: 0.4229\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 1s 128ms/step - loss: 0.4303 - val_loss: 0.4701\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.4081 - val_loss: 0.4257\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.4035 - val_loss: 0.4044\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.4167 - val_loss: 0.4132\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 1s 141ms/step - loss: 0.4019 - val_loss: 0.4058\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.3751 - val_loss: 0.4143\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.3814 - val_loss: 0.4027\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.3773 - val_loss: 0.4372\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.4010 - val_loss: 0.4185\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.3796 - val_loss: 0.4491\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.3733 - val_loss: 0.3935\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.3643 - val_loss: 0.4132\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.3561 - val_loss: 0.3962\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.3533 - val_loss: 0.4053\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.3552 - val_loss: 0.3929\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.3675 - val_loss: 0.3932\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.3406 - val_loss: 0.4266\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.3621 - val_loss: 0.3946\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.3781 - val_loss: 0.4400\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.3633 - val_loss: 0.4042\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.3629 - val_loss: 0.4308\n",
      "[2022_04_22-16:13:18] Training the entire fine-tuned model...\n",
      "[2022_04_22-16:13:30] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 0.3777WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1116s vs `on_train_batch_end` time: 0.1472s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1116s vs `on_train_batch_end` time: 0.1472s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 10s 546ms/step - loss: 0.3727 - val_loss: 0.4184\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 2s 278ms/step - loss: 0.3635 - val_loss: 0.4046\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 2s 278ms/step - loss: 0.3386 - val_loss: 0.3881\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 2s 280ms/step - loss: 0.3410 - val_loss: 0.3852\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 2s 279ms/step - loss: 0.3251 - val_loss: 0.3857\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 2s 279ms/step - loss: 0.3172 - val_loss: 0.3860\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 2s 279ms/step - loss: 0.3041 - val_loss: 0.3899\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 2s 279ms/step - loss: 0.3279 - val_loss: 0.3905\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 2s 279ms/step - loss: 0.3002 - val_loss: 0.3975\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 2s 279ms/step - loss: 0.2926 - val_loss: 0.3940\n",
      "[2022_04_22-16:14:04] Training on final epochs of sequence length 1024...\n",
      "[2022_04_22-16:14:04] Training set: Filtered out 0 of 1114 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_22-16:14:17] Validation set: Filtered out 0 of 279 (0.0%) records of lengths exceeding 1022.\n",
      " 6/18 [=========>....................] - ETA: 3s - loss: 0.3448WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1119s vs `on_train_batch_end` time: 0.1413s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1119s vs `on_train_batch_end` time: 0.1413s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 13s 373ms/step - loss: 0.3438 - val_loss: 0.3860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_22_split_5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_22_split_5/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:24eaf3rx) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10912... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██▁▁▁▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▃▃▃▃▃▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂</td></tr><tr><td>lr</td><td>███████████████████████████▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▃▄▃▄▂▄▂▂▂▂▂▂▃▂▃▁▂▁▂▁▁▂▁▃▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>3</td></tr><tr><td>best_val_loss</td><td>0.38524</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34379</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.38604</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">gentle-snow-6</strong>: <a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/24eaf3rx\" target=\"_blank\">https://wandb.ai/kvetab/10_fold_cv/runs/24eaf3rx</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220422_161217-24eaf3rx/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:24eaf3rx). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/n68frun7\" target=\"_blank\">dulcet-field-7</a></strong> to <a href=\"https://wandb.ai/kvetab/10_fold_cv\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_22-16:15:12] Training set: Filtered out 0 of 1113 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_22-16:15:12] Validation set: Filtered out 0 of 279 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_22-16:15:12] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 10s 373ms/step - loss: 0.7671 - val_loss: 0.5531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.5322 - val_loss: 0.5062\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.4927 - val_loss: 0.4474\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.4765 - val_loss: 0.5117\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.5026 - val_loss: 0.5295\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.4675 - val_loss: 0.4570\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.4399 - val_loss: 0.4211\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.4170 - val_loss: 0.4030\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.4147 - val_loss: 0.4141\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 1s 128ms/step - loss: 0.4082 - val_loss: 0.4082\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.4117 - val_loss: 0.4193\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 1s 128ms/step - loss: 0.4142 - val_loss: 0.3888\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 1s 140ms/step - loss: 0.4010 - val_loss: 0.3892\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.3827 - val_loss: 0.3863\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.3887 - val_loss: 0.3999\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.3963 - val_loss: 0.3860\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.3882 - val_loss: 0.3836\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 1s 128ms/step - loss: 0.3726 - val_loss: 0.4023\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.3794 - val_loss: 0.3906\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 1s 128ms/step - loss: 0.3633 - val_loss: 0.3783\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.3749 - val_loss: 0.3878\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.3675 - val_loss: 0.3711\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.3769 - val_loss: 0.3723\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 1s 141ms/step - loss: 0.3869 - val_loss: 0.3808\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.4066 - val_loss: 0.3979\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.3775 - val_loss: 0.3636\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.3632 - val_loss: 0.3726\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.3551 - val_loss: 0.4327\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.3716 - val_loss: 0.4058\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.3598 - val_loss: 0.3636\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.3561 - val_loss: 0.3625\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.3438 - val_loss: 0.3639\n",
      "Epoch 33/100\n",
      "9/9 [==============================] - 1s 141ms/step - loss: 0.3490 - val_loss: 0.3629\n",
      "Epoch 34/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.3548 - val_loss: 0.3619\n",
      "Epoch 35/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.3574 - val_loss: 0.3717\n",
      "Epoch 36/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.3474 - val_loss: 0.3715\n",
      "Epoch 37/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.3494 - val_loss: 0.3775\n",
      "Epoch 38/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.3521 - val_loss: 0.3623\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 39/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.3433 - val_loss: 0.3633\n",
      "Epoch 40/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.3517 - val_loss: 0.3633\n",
      "[2022_04_22-16:16:10] Training the entire fine-tuned model...\n",
      "[2022_04_22-16:16:51] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 0.3514WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1134s vs `on_train_batch_end` time: 0.1473s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1134s vs `on_train_batch_end` time: 0.1473s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 10s 497ms/step - loss: 0.3636 - val_loss: 0.3624\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 2s 277ms/step - loss: 0.3589 - val_loss: 0.3617\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 2s 276ms/step - loss: 0.3485 - val_loss: 0.3616\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 3s 289ms/step - loss: 0.3395 - val_loss: 0.3733\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 3s 282ms/step - loss: 0.3301 - val_loss: 0.3746\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 2s 278ms/step - loss: 0.3298 - val_loss: 0.3636\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 3s 290ms/step - loss: 0.3118 - val_loss: 0.3531\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 2s 280ms/step - loss: 0.3093 - val_loss: 0.3531\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 2s 278ms/step - loss: 0.3090 - val_loss: 0.3520\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 2s 278ms/step - loss: 0.3195 - val_loss: 0.3523\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 3s 280ms/step - loss: 0.3113 - val_loss: 0.3565\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 3s 281ms/step - loss: 0.3030 - val_loss: 0.3541\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 3s 280ms/step - loss: 0.3038 - val_loss: 0.3547\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 2s 280ms/step - loss: 0.3036 - val_loss: 0.3529\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 3s 291ms/step - loss: 0.3159 - val_loss: 0.3541\n",
      "[2022_04_22-16:17:38] Training on final epochs of sequence length 1024...\n",
      "[2022_04_22-16:17:38] Training set: Filtered out 0 of 1113 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_22-16:17:38] Validation set: Filtered out 0 of 279 (0.0%) records of lengths exceeding 1022.\n",
      " 6/18 [=========>....................] - ETA: 3s - loss: 0.3244WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1094s vs `on_train_batch_end` time: 0.1425s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1094s vs `on_train_batch_end` time: 0.1425s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 12s 372ms/step - loss: 0.3252 - val_loss: 0.3541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_22_split_6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_22_split_6/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:n68frun7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 11211... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇██▁▁▂▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▄▇▅▃▃▃▂▂▃▂▂▂▂▂▂▂▁▂▃▁▁▁▁▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_val_loss</td><td>0.35197</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32524</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.35409</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dulcet-field-7</strong>: <a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/n68frun7\" target=\"_blank\">https://wandb.ai/kvetab/10_fold_cv/runs/n68frun7</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220422_161456-n68frun7/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:n68frun7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/3eo92ik5\" target=\"_blank\">celestial-sun-8</a></strong> to <a href=\"https://wandb.ai/kvetab/10_fold_cv\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_22-16:18:34] Training set: Filtered out 0 of 1112 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_22-16:18:34] Validation set: Filtered out 0 of 278 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_22-16:18:34] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 10s 356ms/step - loss: 0.7790 - val_loss: 0.5692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.5781 - val_loss: 0.5111\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.5279 - val_loss: 0.4413\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.4703 - val_loss: 0.4079\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.4504 - val_loss: 0.3918\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 1s 128ms/step - loss: 0.4370 - val_loss: 0.4290\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 1s 142ms/step - loss: 0.4698 - val_loss: 0.3923\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.4352 - val_loss: 0.3772\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 1s 139ms/step - loss: 0.4219 - val_loss: 0.3784\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 1s 127ms/step - loss: 0.4097 - val_loss: 0.3732\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.4064 - val_loss: 0.3606\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.4027 - val_loss: 0.3666\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.3958 - val_loss: 0.3533\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.3954 - val_loss: 0.3522\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.3983 - val_loss: 0.3554\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 1s 128ms/step - loss: 0.3925 - val_loss: 0.4019\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.4087 - val_loss: 0.3472\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 1s 128ms/step - loss: 0.3774 - val_loss: 0.3460\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.3710 - val_loss: 0.3468\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.3724 - val_loss: 0.3609\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.3819 - val_loss: 0.3463\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.3993 - val_loss: 0.4279\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.4037 - val_loss: 0.4028\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.3942 - val_loss: 0.3572\n",
      "[2022_04_22-16:19:12] Training the entire fine-tuned model...\n",
      "[2022_04_22-16:19:21] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 0.4209WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1116s vs `on_train_batch_end` time: 0.1474s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1116s vs `on_train_batch_end` time: 0.1474s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 10s 513ms/step - loss: 0.4086 - val_loss: 0.3471\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 2s 277ms/step - loss: 0.3856 - val_loss: 0.3630\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 3s 280ms/step - loss: 0.3800 - val_loss: 0.3568\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 2s 278ms/step - loss: 0.3620 - val_loss: 0.3406\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 2s 280ms/step - loss: 0.3592 - val_loss: 0.3379\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 3s 290ms/step - loss: 0.3685 - val_loss: 0.3525\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 2s 279ms/step - loss: 0.3552 - val_loss: 0.3298\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 2s 279ms/step - loss: 0.3475 - val_loss: 0.3365\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 2s 279ms/step - loss: 0.3458 - val_loss: 0.3262\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 3s 282ms/step - loss: 0.3328 - val_loss: 0.3332\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 2s 279ms/step - loss: 0.3300 - val_loss: 0.3394\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 2s 279ms/step - loss: 0.3215 - val_loss: 0.3228\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 2s 278ms/step - loss: 0.3198 - val_loss: 0.3235\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 2s 279ms/step - loss: 0.2998 - val_loss: 0.3194\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 2s 278ms/step - loss: 0.2988 - val_loss: 0.3219\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 3s 281ms/step - loss: 0.2867 - val_loss: 0.3269\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 2s 278ms/step - loss: 0.2771 - val_loss: 0.3202\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 3s 280ms/step - loss: 0.2625 - val_loss: 0.3235\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 2s 279ms/step - loss: 0.2442 - val_loss: 0.3239\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 3s 280ms/step - loss: 0.2403 - val_loss: 0.3269\n",
      "[2022_04_22-16:20:20] Training on final epochs of sequence length 1024...\n",
      "[2022_04_22-16:20:20] Training set: Filtered out 0 of 1112 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_22-16:20:49] Validation set: Filtered out 0 of 278 (0.0%) records of lengths exceeding 1022.\n",
      " 6/18 [=========>....................] - ETA: 3s - loss: 0.3041WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1085s vs `on_train_batch_end` time: 0.1439s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1085s vs `on_train_batch_end` time: 0.1439s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 13s 371ms/step - loss: 0.3087 - val_loss: 0.3206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_22_split_7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_22_split_7/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3eo92ik5) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 11604... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▃▃▃▃▄▄▅▅▅▆▆▆▆▁</td></tr><tr><td>loss</td><td>█▅▅▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▂</td></tr><tr><td>lr</td><td>████████████████████▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▄▃▃▄▃▃▃▂▂▂▂▂▃▂▂▂▂▄▃▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>13</td></tr><tr><td>best_val_loss</td><td>0.31945</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30867</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.32061</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">celestial-sun-8</strong>: <a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/3eo92ik5\" target=\"_blank\">https://wandb.ai/kvetab/10_fold_cv/runs/3eo92ik5</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220422_161818-3eo92ik5/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3eo92ik5). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/3dncsgz1\" target=\"_blank\">atomic-plasma-9</a></strong> to <a href=\"https://wandb.ai/kvetab/10_fold_cv\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_22-16:21:44] Training set: Filtered out 0 of 1132 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_22-16:21:44] Validation set: Filtered out 0 of 284 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_22-16:21:44] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 11s 507ms/step - loss: 0.7813 - val_loss: 0.5286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "9/9 [==============================] - 1s 134ms/step - loss: 0.5650 - val_loss: 0.4625\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.4657 - val_loss: 0.4600\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.4490 - val_loss: 0.4296\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.4299 - val_loss: 0.4222\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.4216 - val_loss: 0.4111\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.4158 - val_loss: 0.4074\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.4128 - val_loss: 0.4116\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.4016 - val_loss: 0.4028\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3993 - val_loss: 0.4214\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.3918 - val_loss: 0.3974\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.3814 - val_loss: 0.3929\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.3764 - val_loss: 0.4217\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 1s 134ms/step - loss: 0.3883 - val_loss: 0.3962\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3746 - val_loss: 0.3901\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 1s 135ms/step - loss: 0.3652 - val_loss: 0.3873\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3644 - val_loss: 0.3916\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.3628 - val_loss: 0.3879\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.3583 - val_loss: 0.3937\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 1s 135ms/step - loss: 0.3541 - val_loss: 0.4001\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3510 - val_loss: 0.3872\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.3397 - val_loss: 0.3867\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3342 - val_loss: 0.3858\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3461 - val_loss: 0.3844\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.3474 - val_loss: 0.3850\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.3458 - val_loss: 0.3834\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3348 - val_loss: 0.3848\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3459 - val_loss: 0.3949\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3374 - val_loss: 0.3873\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.3413 - val_loss: 0.3913\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3397 - val_loss: 0.3833\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.3375 - val_loss: 0.3830\n",
      "Epoch 33/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3408 - val_loss: 0.3861\n",
      "Epoch 34/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3378 - val_loss: 0.3832\n",
      "Epoch 35/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3402 - val_loss: 0.3829\n",
      "Epoch 36/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.3437 - val_loss: 0.3851\n",
      "Epoch 37/100\n",
      "9/9 [==============================] - 1s 134ms/step - loss: 0.3387 - val_loss: 0.3831\n",
      "Epoch 38/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3389 - val_loss: 0.3841\n",
      "Epoch 39/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3354 - val_loss: 0.3828\n",
      "Epoch 40/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.3381 - val_loss: 0.3830\n",
      "Epoch 41/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.3347 - val_loss: 0.3828\n",
      "Epoch 42/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.3361 - val_loss: 0.3830\n",
      "Epoch 43/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.3386 - val_loss: 0.3831\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 44/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.3362 - val_loss: 0.3827\n",
      "Epoch 45/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3315 - val_loss: 0.3826\n",
      "Epoch 46/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3395 - val_loss: 0.3827\n",
      "Epoch 47/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.3433 - val_loss: 0.3828\n",
      "Epoch 48/100\n",
      "9/9 [==============================] - 1s 143ms/step - loss: 0.3322 - val_loss: 0.3831\n",
      "Epoch 49/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.3325 - val_loss: 0.3834\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 50/100\n",
      "9/9 [==============================] - 1s 134ms/step - loss: 0.3349 - val_loss: 0.3833\n",
      "Epoch 51/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.3380 - val_loss: 0.3830\n",
      "[2022_04_22-16:22:57] Training the entire fine-tuned model...\n",
      "[2022_04_22-16:23:23] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 0.3567WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1133s vs `on_train_batch_end` time: 0.1459s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1133s vs `on_train_batch_end` time: 0.1459s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 10s 505ms/step - loss: 0.3498 - val_loss: 0.4012\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 3s 283ms/step - loss: 0.3489 - val_loss: 0.3878\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 3s 284ms/step - loss: 0.3213 - val_loss: 0.4041\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 3s 283ms/step - loss: 0.3356 - val_loss: 0.3844\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 3s 283ms/step - loss: 0.3367 - val_loss: 0.3841\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 3s 283ms/step - loss: 0.3175 - val_loss: 0.3842\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 3s 283ms/step - loss: 0.3040 - val_loss: 0.3901\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 3s 283ms/step - loss: 0.3093 - val_loss: 0.3856\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 3s 294ms/step - loss: 0.2967 - val_loss: 0.4072\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 3s 284ms/step - loss: 0.3029 - val_loss: 0.3907\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 3s 287ms/step - loss: 0.2934 - val_loss: 0.3918\n",
      "[2022_04_22-16:23:59] Training on final epochs of sequence length 1024...\n",
      "[2022_04_22-16:23:59] Training set: Filtered out 0 of 1132 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_22-16:23:59] Validation set: Filtered out 0 of 284 (0.0%) records of lengths exceeding 1022.\n",
      " 6/18 [=========>....................] - ETA: 3s - loss: 0.3268WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1125s vs `on_train_batch_end` time: 0.1421s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1125s vs `on_train_batch_end` time: 0.1421s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 12s 380ms/step - loss: 0.3176 - val_loss: 0.3851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_22_split_8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_22_split_8/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3dncsgz1) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 11945... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▂</td></tr><tr><td>loss</td><td>█▅▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▃▂▂▃▁▃▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>44</td></tr><tr><td>best_val_loss</td><td>0.38264</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31755</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.38513</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">atomic-plasma-9</strong>: <a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/3dncsgz1\" target=\"_blank\">https://wandb.ai/kvetab/10_fold_cv/runs/3dncsgz1</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220422_162129-3dncsgz1/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3dncsgz1). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/ivz3udr4\" target=\"_blank\">dainty-lion-10</a></strong> to <a href=\"https://wandb.ai/kvetab/10_fold_cv\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_22-16:24:54] Training set: Filtered out 0 of 1141 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_22-16:24:55] Validation set: Filtered out 0 of 286 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_22-16:24:55] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 10s 369ms/step - loss: 0.7579 - val_loss: 0.6757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.5977 - val_loss: 0.5570\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.5389 - val_loss: 0.4483\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.5144 - val_loss: 0.4356\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 1s 134ms/step - loss: 0.4716 - val_loss: 0.4283\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.4546 - val_loss: 0.4255\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.4285 - val_loss: 0.4103\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.4180 - val_loss: 0.4104\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.4059 - val_loss: 0.4189\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.4178 - val_loss: 0.4018\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.3989 - val_loss: 0.3980\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.4014 - val_loss: 0.3950\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3917 - val_loss: 0.4594\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.3975 - val_loss: 0.4241\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3821 - val_loss: 0.3885\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3806 - val_loss: 0.3871\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.3786 - val_loss: 0.4305\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.4089 - val_loss: 0.3958\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3756 - val_loss: 0.3792\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.3661 - val_loss: 0.3775\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3614 - val_loss: 0.3872\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.3696 - val_loss: 0.3809\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3693 - val_loss: 0.3835\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3598 - val_loss: 0.3919\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3603 - val_loss: 0.3735\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 1s 135ms/step - loss: 0.3625 - val_loss: 0.3734\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 1s 134ms/step - loss: 0.3597 - val_loss: 0.3790\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 0.3517 - val_loss: 0.3737\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.3592 - val_loss: 0.3765\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.3483 - val_loss: 0.3737\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.3495 - val_loss: 0.3737\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 1s 134ms/step - loss: 0.3532 - val_loss: 0.3759\n",
      "[2022_04_22-16:25:43] Training the entire fine-tuned model...\n",
      "[2022_04_22-16:25:52] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 0.3869WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1156s vs `on_train_batch_end` time: 0.1451s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1156s vs `on_train_batch_end` time: 0.1451s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 11s 511ms/step - loss: 0.3761 - val_loss: 0.3794\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 3s 286ms/step - loss: 0.3649 - val_loss: 0.3875\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 3s 288ms/step - loss: 0.3512 - val_loss: 0.3911\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 3s 287ms/step - loss: 0.3385 - val_loss: 0.3672\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 3s 285ms/step - loss: 0.3356 - val_loss: 0.3860\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 3s 286ms/step - loss: 0.3247 - val_loss: 0.3755\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 3s 286ms/step - loss: 0.3310 - val_loss: 0.3649\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 3s 287ms/step - loss: 0.3289 - val_loss: 0.3650\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 3s 287ms/step - loss: 0.3120 - val_loss: 0.3750\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 3s 297ms/step - loss: 0.3168 - val_loss: 0.3612\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 3s 286ms/step - loss: 0.3037 - val_loss: 0.3791\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 3s 287ms/step - loss: 0.2806 - val_loss: 0.3677\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 3s 286ms/step - loss: 0.2718 - val_loss: 0.3786\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 3s 286ms/step - loss: 0.2827 - val_loss: 0.3635\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 3s 287ms/step - loss: 0.2613 - val_loss: 0.3797\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 3s 287ms/step - loss: 0.2690 - val_loss: 0.3762\n",
      "[2022_04_22-16:26:42] Training on final epochs of sequence length 1024...\n",
      "[2022_04_22-16:26:42] Training set: Filtered out 0 of 1141 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_22-16:27:21] Validation set: Filtered out 0 of 286 (0.0%) records of lengths exceeding 1022.\n",
      " 6/18 [=========>....................] - ETA: 3s - loss: 0.3647WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1107s vs `on_train_batch_end` time: 0.1414s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1107s vs `on_train_batch_end` time: 0.1414s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 13s 382ms/step - loss: 0.3279 - val_loss: 0.3513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_22_split_9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_22_split_9/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    train = merged[merged[\"cluster_merged\"] != i]\n",
    "    test = merged[merged[\"cluster_merged\"] == i]\n",
    "    train, valid = train_test_split(train, test_size=0.2, random_state=333)\n",
    "    cm, f1 = train_and_save_named_model(train, valid, test, f\"10-fold-cv/2022_04_22_split_{i}\", \"10_fold_cv\")\n",
    "    cms[i] = cm\n",
    "    f1s[i] = f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f769ca8a-307b-46d6-81ae-a9af893fdf19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.3870967741935484,\n",
       " 1: 0.4375,\n",
       " 2: 0.4,\n",
       " 3: 0.5789473684210527,\n",
       " 4: 0.19230769230769232,\n",
       " 5: 0.2978723404255319,\n",
       " 6: 0.4827586206896552,\n",
       " 7: 0.5245901639344263,\n",
       " 8: 0.5277777777777778,\n",
       " 9: 0.5957446808510638}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e51ada6b-4e15-46dd-8adc-8efaed286cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8c712b9-a8fb-41b7-b500-5097698e89f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_dict = {0: 0.3870967741935484,\n",
    " 1: 0.4375,\n",
    " 2: 0.4,\n",
    " 3: 0.5789473684210527,\n",
    " 4: 0.19230769230769232,\n",
    " 5: 0.2978723404255319,\n",
    " 6: 0.4827586206896552,\n",
    " 7: 0.5245901639344263,\n",
    " 8: 0.5277777777777778,\n",
    " 9: 0.5957446808510638}\n",
    "\n",
    "f1_scores = [value for key, value in f1_dict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6fb3768-e7bb-46b3-b272-2b7a0645a964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44245954186007486"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756817f3-96f6-423c-963c-9e135d5f387e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
