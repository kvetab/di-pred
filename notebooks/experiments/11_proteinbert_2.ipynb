{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f37c38b7-7645-4a37-b0bd-1c3ed0c08489",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from os import path\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13b5db07-e604-4fdc-be52-8d4e963fe915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from proteinbert.finetuning import encode_train_and_valid_sets, encode_dataset\n",
    "from proteinbert import OutputType, OutputSpec, evaluate_by_len, load_pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58f06fc6-cc5b-4ba5-b76c-350428a49552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from proteinbert import OutputType, OutputSpec, FinetuningModelGenerator, load_pretrained_model, \\\n",
    "finetune, evaluate_by_len\n",
    "\n",
    "from proteinbert.conv_and_global_attention_model import get_model_with_hidden_layers_as_outputs\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb5cdb0f-aecd-4a46-bb86-605505e74c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05704d2b-73eb-4f0f-97f0-20a00cbfbbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17f00a4c-4f9d-4245-b312-03281caeaba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89fb9d1d-5ecb-45a1-8d55-26d6ee7f5010",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_TYPE = OutputType(False, 'binary')\n",
    "UNIQUE_LABELS = [0, 1]\n",
    "OUTPUT_SPEC = OutputSpec(OUTPUT_TYPE, UNIQUE_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b894730-5000-4539-95a2-9795b3a54513",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_generator, input_encoder = load_pretrained_model(\"../../data/protein_bert/\", \"epoch_92400_sample_23500000.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1246f6e-d5b3-4907-87fc-57c90f40d8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df18af30-11ad-421a-bb04-acbcb4da64b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_train.csv\"), index_col=0)\n",
    "test_set = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_test.csv\"), index_col=0)\n",
    "train_set[\"seq\"] = train_set[\"heavy\"] + train_set[\"light\"]\n",
    "test_set[\"seq\"] = test_set[\"heavy\"] + test_set[\"light\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c75228d6-d97a-4ee6-90d7-e56e5dab4631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1291"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "669e0ae8-04cc-437d-9876-b9b8c437336d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "010440f5-accb-4866-8d0f-a31b5be34976",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "patience = (6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "284f8c1b-8f09-4805-bd0f-7e618216a75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [0.5, 0.6, 0.7, 0.8, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89a129f-8f51-4cb8-8090-96f99ee7aaf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2198e715-c2f5-408d-a787-aa8d1451e324",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_save_model(train_data, valid_data, test_data, size):\n",
    "    wandb.init(project=f\"Dataset size exp\", entity=\"kvetab\")\n",
    "    model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "            get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)\n",
    "\n",
    "    training_callbacks = [\n",
    "        keras.callbacks.ReduceLROnPlateau(patience = patience[1], factor = 0.25, min_lr = 1e-07, verbose = 1),\n",
    "        keras.callbacks.EarlyStopping(patience = patience[0], restore_best_weights = True),\n",
    "        WandbCallback()\n",
    "    ]\n",
    "\n",
    "    epoch_num = 100\n",
    "    batch_size = 128\n",
    "    #learning_rate = 1e-5\n",
    "    wandb.config = {\n",
    "          \"learning_rate\": learning_rate,\n",
    "          \"epochs\": epoch_num * 2,\n",
    "          \"batch_size\": batch_size\n",
    "        }\n",
    "    finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data[\"seq\"], train_data[\"Y\"], valid_data['seq'], valid_data[\"Y\"], \\\n",
    "            seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = learning_rate, begin_with_frozen_pretrained_layers = True, \\\n",
    "            lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = learning_rate / 10, callbacks = training_callbacks)\n",
    "    mod = model_generator.create_model(seq_len = 512)\n",
    "    mod_name = f\"2022_04_22_size{size}\"\n",
    "    mod.save(path.join(DATA_DIR, f\"protein_bert/by_data_size/{mod_name}\"))\n",
    "\n",
    "    results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "            start_seq_len = 512, start_batch_size = 32)\n",
    "    fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "    f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "    return confusion_matrix, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "702b647d-8583-44d6-bf53-f983dbb7597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = {}\n",
    "f1s = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a954139-d02d-407e-ac45-57bdb4a1a890",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645 646 260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkvetab\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp/runs/3cz0r5gn\" target=\"_blank\">colorful-meadow-17</a></strong> to <a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_24-16:50:26] Training set: Filtered out 0 of 645 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_24-16:50:27] Validation set: Filtered out 0 of 646 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_24-16:50:27] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-24 16:50:27.159831: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-24 16:50:27.728399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9656 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:41:00.0, compute capability: 7.5\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "2022-04-24 16:50:29.565892: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-24 16:50:37.412848: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 7605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 609ms/step - loss: 0.9011 - val_loss: 1.3253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.8461 - val_loss: 0.9959\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.8655 - val_loss: 0.5400\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5879 - val_loss: 0.6815\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.7116 - val_loss: 0.4534\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 223ms/step - loss: 0.4763 - val_loss: 0.5230\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.5687 - val_loss: 0.4924\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.5178 - val_loss: 0.6362\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.6347 - val_loss: 0.5206\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5334 - val_loss: 0.4370\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4381 - val_loss: 0.5133\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4809 - val_loss: 0.4504\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4323 - val_loss: 0.4633\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4629 - val_loss: 0.4350\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4163 - val_loss: 0.4501\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4125 - val_loss: 0.4364\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4509 - val_loss: 0.4307\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4187 - val_loss: 0.4584\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4422 - val_loss: 0.4377\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4086 - val_loss: 0.4285\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4272 - val_loss: 0.4269\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4103 - val_loss: 0.4298\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4119 - val_loss: 0.4247\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4199 - val_loss: 0.4291\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4072 - val_loss: 0.4256\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4074 - val_loss: 0.4216\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4069 - val_loss: 0.4194\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3960 - val_loss: 0.4326\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4040 - val_loss: 0.4182\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3974 - val_loss: 0.4199\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4070 - val_loss: 0.4170\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4053 - val_loss: 0.4252\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3985 - val_loss: 0.4336\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.4138 - val_loss: 0.4203\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3988 - val_loss: 0.4182\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3938 - val_loss: 0.4136\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3959 - val_loss: 0.4164\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3868 - val_loss: 0.4139\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4003 - val_loss: 0.4133\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3929 - val_loss: 0.4133\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3954 - val_loss: 0.4129\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4043 - val_loss: 0.4127\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3878 - val_loss: 0.4156\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4037 - val_loss: 0.4203\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3992 - val_loss: 0.4173\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3917 - val_loss: 0.4138\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3918 - val_loss: 0.4133\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3932 - val_loss: 0.4129\n",
      "[2022_04_24-16:51:32] Training the entire fine-tuned model...\n",
      "[2022_04_24-16:51:40] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3972WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1104s vs `on_train_batch_end` time: 0.1177s). Check your callbacks.\n",
      "6/6 [==============================] - 10s 675ms/step - loss: 0.3972 - val_loss: 0.4124\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.4102 - val_loss: 0.4177\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3908 - val_loss: 0.4170\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3805 - val_loss: 0.4184\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3823 - val_loss: 0.4078\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3746 - val_loss: 0.4059\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3715 - val_loss: 0.4056\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3676 - val_loss: 0.4037\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3678 - val_loss: 0.4094\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3742 - val_loss: 0.4032\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3590 - val_loss: 0.4052\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3574 - val_loss: 0.4235\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3603 - val_loss: 0.4021\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3447 - val_loss: 0.4028\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3395 - val_loss: 0.4036\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3370 - val_loss: 0.3978\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3224 - val_loss: 0.4022\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3130 - val_loss: 0.4007\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3223 - val_loss: 0.4226\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3129 - val_loss: 0.4030\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.2944 - val_loss: 0.4150\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3046 - val_loss: 0.4075\n",
      "[2022_04_24-16:52:30] Training on final epochs of sequence length 1024...\n",
      "[2022_04_24-16:52:30] Training set: Filtered out 0 of 645 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_24-16:52:30] Validation set: Filtered out 0 of 646 (0.0%) records of lengths exceeding 1022.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3361WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1087s vs `on_train_batch_end` time: 0.1388s). Check your callbacks.\n",
      "11/11 [==============================] - 11s 512ms/step - loss: 0.3426 - val_loss: 0.4012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-24 16:53:02.290053: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/by_data_size/2022_04_22_size0.5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/by_data_size/2022_04_22_size0.5/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "774 517 260\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3cz0r5gn) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 43820... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇███▁▁▂▂▂▂▃▃▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▇▄▃▄▅▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂</td></tr><tr><td>lr</td><td>██████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▃▂▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>15</td></tr><tr><td>best_val_loss</td><td>0.39776</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34265</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.40121</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">colorful-meadow-17</strong>: <a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp/runs/3cz0r5gn\" target=\"_blank\">https://wandb.ai/kvetab/Dataset%20size%20exp/runs/3cz0r5gn</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220424_165014-3cz0r5gn/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3cz0r5gn). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp/runs/6q8rn4cn\" target=\"_blank\">polar-armadillo-18</a></strong> to <a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_24-16:53:31] Training set: Filtered out 0 of 774 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_24-16:53:31] Validation set: Filtered out 0 of 517 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_24-16:53:31] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 [==============================] - 10s 479ms/step - loss: 0.7844 - val_loss: 1.2625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.7420 - val_loss: 0.7453\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.5806 - val_loss: 0.4596\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 1s 159ms/step - loss: 0.5049 - val_loss: 0.4933\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.4567 - val_loss: 0.4445\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.4497 - val_loss: 0.4567\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 1s 159ms/step - loss: 0.4312 - val_loss: 0.4647\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.4609 - val_loss: 0.4299\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.4131 - val_loss: 0.5083\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 1s 163ms/step - loss: 0.4520 - val_loss: 0.4966\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.4220 - val_loss: 0.4186\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 1s 159ms/step - loss: 0.3983 - val_loss: 0.4186\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.3992 - val_loss: 0.4181\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.3977 - val_loss: 0.4069\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.4065 - val_loss: 0.6564\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.5069 - val_loss: 0.5494\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 1s 163ms/step - loss: 0.4672 - val_loss: 0.4541\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.4062 - val_loss: 0.4469\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 1s 159ms/step - loss: 0.4120 - val_loss: 0.4216\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.4000 - val_loss: 0.3978\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.3838 - val_loss: 0.3979\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.3841 - val_loss: 0.4177\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 1s 158ms/step - loss: 0.4025 - val_loss: 0.3947\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.3909 - val_loss: 0.3935\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.3884 - val_loss: 0.4005\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.3773 - val_loss: 0.3958\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.4029 - val_loss: 0.3968\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.4087 - val_loss: 0.4065\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.3868 - val_loss: 0.3901\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.3759 - val_loss: 0.3924\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 1s 158ms/step - loss: 0.3718 - val_loss: 0.3900\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.3814 - val_loss: 0.3938\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.3773 - val_loss: 0.3986\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.3778 - val_loss: 0.3969\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.3716 - val_loss: 0.3895\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 1s 159ms/step - loss: 0.3901 - val_loss: 0.3930\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.3805 - val_loss: 0.3894\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.3729 - val_loss: 0.3892\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.3693 - val_loss: 0.3896\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.3699 - val_loss: 0.3972\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 1s 159ms/step - loss: 0.3681 - val_loss: 0.3959\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.3666 - val_loss: 0.3890\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.3816 - val_loss: 0.3889\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.3666 - val_loss: 0.3899\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.3694 - val_loss: 0.3902\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.3731 - val_loss: 0.3908\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.3658 - val_loss: 0.3895\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 1s 163ms/step - loss: 0.3694 - val_loss: 0.3893\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.3614 - val_loss: 0.3885\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.3534 - val_loss: 0.3885\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.3696 - val_loss: 0.3885\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.3705 - val_loss: 0.3884\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.3733 - val_loss: 0.3884\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.3751 - val_loss: 0.3886\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.3592 - val_loss: 0.3900\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.3657 - val_loss: 0.3907\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.3657 - val_loss: 0.3919\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.3787 - val_loss: 0.3913\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.3509 - val_loss: 0.3905\n",
      "[2022_04_24-16:54:46] Training the entire fine-tuned model...\n",
      "[2022_04_24-16:55:10] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.3683WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1151s vs `on_train_batch_end` time: 0.1450s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1151s vs `on_train_batch_end` time: 0.1450s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 10s 613ms/step - loss: 0.3676 - val_loss: 0.3935\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 2s 290ms/step - loss: 0.3667 - val_loss: 0.3923\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 2s 290ms/step - loss: 0.3668 - val_loss: 0.3860\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 2s 296ms/step - loss: 0.3535 - val_loss: 0.3868\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 2s 291ms/step - loss: 0.3746 - val_loss: 0.3933\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 2s 289ms/step - loss: 0.3626 - val_loss: 0.4090\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 2s 290ms/step - loss: 0.3619 - val_loss: 0.3809\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 2s 293ms/step - loss: 0.3333 - val_loss: 0.3802\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 2s 293ms/step - loss: 0.3553 - val_loss: 0.3881\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 2s 291ms/step - loss: 0.3464 - val_loss: 0.3813\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 2s 290ms/step - loss: 0.3402 - val_loss: 0.3946\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 2s 290ms/step - loss: 0.3257 - val_loss: 0.4213\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 2s 290ms/step - loss: 0.3667 - val_loss: 0.3783\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 2s 292ms/step - loss: 0.3403 - val_loss: 0.4078\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 2s 290ms/step - loss: 0.3332 - val_loss: 0.3783\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 2s 292ms/step - loss: 0.3151 - val_loss: 0.3815\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 2s 292ms/step - loss: 0.3109 - val_loss: 0.3870\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 2s 291ms/step - loss: 0.3166 - val_loss: 0.3867\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 2s 290ms/step - loss: 0.3084 - val_loss: 0.3803\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 2s 291ms/step - loss: 0.3104 - val_loss: 0.3765\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 2s 292ms/step - loss: 0.3049 - val_loss: 0.3752\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 2s 296ms/step - loss: 0.3127 - val_loss: 0.3751\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 2s 293ms/step - loss: 0.3011 - val_loss: 0.3756\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 2s 296ms/step - loss: 0.3017 - val_loss: 0.3750\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 2s 291ms/step - loss: 0.3229 - val_loss: 0.3753\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 2s 296ms/step - loss: 0.3124 - val_loss: 0.3745\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 2s 291ms/step - loss: 0.3013 - val_loss: 0.3778\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 2s 291ms/step - loss: 0.3178 - val_loss: 0.3771\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 2s 291ms/step - loss: 0.3030 - val_loss: 0.3749\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 2s 292ms/step - loss: 0.3048 - val_loss: 0.3739\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 2s 294ms/step - loss: 0.2991 - val_loss: 0.3739\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 2s 291ms/step - loss: 0.3068 - val_loss: 0.3751\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 2s 294ms/step - loss: 0.3055 - val_loss: 0.3776\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 2s 292ms/step - loss: 0.3087 - val_loss: 0.3805\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 2s 296ms/step - loss: 0.3098 - val_loss: 0.3809\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 2s 293ms/step - loss: 0.3037 - val_loss: 0.3801\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 2s 292ms/step - loss: 0.3040 - val_loss: 0.3791\n",
      "[2022_04_24-16:56:34] Training on final epochs of sequence length 1024...\n",
      "[2022_04_24-16:56:34] Training set: Filtered out 0 of 774 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_24-16:56:34] Validation set: Filtered out 0 of 517 (0.0%) records of lengths exceeding 1022.\n",
      " 6/13 [============>.................] - ETA: 1s - loss: 0.3243WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1091s vs `on_train_batch_end` time: 0.1445s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1091s vs `on_train_batch_end` time: 0.1445s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 12s 493ms/step - loss: 0.3256 - val_loss: 0.3798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/by_data_size/2022_04_22_size0.6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/by_data_size/2022_04_22_size0.6/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "903 388 260\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:6q8rn4cn) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 44299... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▇▇▇▇██▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▂▃▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▁▂▁▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>30</td></tr><tr><td>best_val_loss</td><td>0.37393</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3256</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.37983</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">polar-armadillo-18</strong>: <a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp/runs/6q8rn4cn\" target=\"_blank\">https://wandb.ai/kvetab/Dataset%20size%20exp/runs/6q8rn4cn</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220424_165314-6q8rn4cn/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:6q8rn4cn). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp/runs/1e8v5bpp\" target=\"_blank\">hopeful-meadow-19</a></strong> to <a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_24-16:57:29] Training set: Filtered out 0 of 903 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_24-16:57:29] Validation set: Filtered out 0 of 388 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_24-16:57:29] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 10s 542ms/step - loss: 0.8713 - val_loss: 0.7257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "8/8 [==============================] - 1s 139ms/step - loss: 0.7373 - val_loss: 0.7516\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 1s 140ms/step - loss: 0.5679 - val_loss: 0.4721\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 1s 140ms/step - loss: 0.4953 - val_loss: 0.5011\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 1s 140ms/step - loss: 0.4437 - val_loss: 0.4825\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 1s 139ms/step - loss: 0.4500 - val_loss: 0.4701\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 1s 140ms/step - loss: 0.4634 - val_loss: 0.4459\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 1s 137ms/step - loss: 0.4439 - val_loss: 0.4407\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 1s 138ms/step - loss: 0.4150 - val_loss: 0.4357\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 1s 139ms/step - loss: 0.4053 - val_loss: 0.4368\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 1s 138ms/step - loss: 0.4187 - val_loss: 0.4266\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 1s 139ms/step - loss: 0.4288 - val_loss: 0.4231\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 1s 138ms/step - loss: 0.3904 - val_loss: 0.5334\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 1s 140ms/step - loss: 0.4118 - val_loss: 0.4562\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 1s 140ms/step - loss: 0.3825 - val_loss: 0.4171\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 1s 139ms/step - loss: 0.3713 - val_loss: 0.4458\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 1s 140ms/step - loss: 0.4105 - val_loss: 0.4184\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 1s 141ms/step - loss: 0.3901 - val_loss: 0.5625\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 1s 139ms/step - loss: 0.4561 - val_loss: 0.4306\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 1s 138ms/step - loss: 0.3845 - val_loss: 0.4176\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 1s 138ms/step - loss: 0.3632 - val_loss: 0.4174\n",
      "[2022_04_24-16:58:04] Training the entire fine-tuned model...\n",
      "[2022_04_24-16:58:12] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/8 [=====================>........] - ETA: 0s - loss: 0.3871WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1107s vs `on_train_batch_end` time: 0.1470s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1107s vs `on_train_batch_end` time: 0.1470s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 10s 528ms/step - loss: 0.3956 - val_loss: 0.4169\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 2s 274ms/step - loss: 0.3812 - val_loss: 0.4165\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 2s 272ms/step - loss: 0.3705 - val_loss: 0.4106\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 2s 273ms/step - loss: 0.3746 - val_loss: 0.4090\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 2s 272ms/step - loss: 0.3572 - val_loss: 0.4092\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 2s 273ms/step - loss: 0.3557 - val_loss: 0.4413\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 2s 273ms/step - loss: 0.4071 - val_loss: 0.4293\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 2s 272ms/step - loss: 0.3491 - val_loss: 0.4012\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 2s 273ms/step - loss: 0.3593 - val_loss: 0.4029\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 2s 273ms/step - loss: 0.3387 - val_loss: 0.3952\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 2s 274ms/step - loss: 0.3368 - val_loss: 0.4089\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 2s 273ms/step - loss: 0.3363 - val_loss: 0.3931\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 2s 274ms/step - loss: 0.3208 - val_loss: 0.3942\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 2s 274ms/step - loss: 0.3099 - val_loss: 0.3925\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 2s 274ms/step - loss: 0.3146 - val_loss: 0.4264\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 2s 277ms/step - loss: 0.3188 - val_loss: 0.4048\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 2s 274ms/step - loss: 0.3024 - val_loss: 0.3857\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 2s 276ms/step - loss: 0.2978 - val_loss: 0.4408\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 2s 274ms/step - loss: 0.3353 - val_loss: 0.3923\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 2s 273ms/step - loss: 0.3164 - val_loss: 0.3925\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 2s 274ms/step - loss: 0.3040 - val_loss: 0.4102\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 2s 273ms/step - loss: 0.2875 - val_loss: 0.3930\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 2s 274ms/step - loss: 0.2667 - val_loss: 0.3902\n",
      "[2022_04_24-16:59:11] Training on final epochs of sequence length 1024...\n",
      "[2022_04_24-16:59:11] Training set: Filtered out 0 of 903 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_24-16:59:11] Validation set: Filtered out 0 of 388 (0.0%) records of lengths exceeding 1022.\n",
      " 6/15 [===========>..................] - ETA: 2s - loss: 0.3185WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1100s vs `on_train_batch_end` time: 0.1422s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1100s vs `on_train_batch_end` time: 0.1422s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 12s 436ms/step - loss: 0.3069 - val_loss: 0.3876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/by_data_size/2022_04_22_size0.7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/by_data_size/2022_04_22_size0.7/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1032 259 260\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1e8v5bpp) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 44889... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇█▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██▁</td></tr><tr><td>loss</td><td>█▆▄▃▃▃▃▃▂▃▃▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>██▃▃▃▃▂▂▂▂▂▄▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>16</td></tr><tr><td>best_val_loss</td><td>0.38567</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30687</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.38759</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">hopeful-meadow-19</strong>: <a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp/runs/1e8v5bpp\" target=\"_blank\">https://wandb.ai/kvetab/Dataset%20size%20exp/runs/1e8v5bpp</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220424_165713-1e8v5bpp/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1e8v5bpp). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp/runs/1u4ibbsa\" target=\"_blank\">atomic-microwave-20</a></strong> to <a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_24-17:00:07] Training set: Filtered out 0 of 1032 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_24-17:00:07] Validation set: Filtered out 0 of 259 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_24-17:00:07] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 10s 474ms/step - loss: 0.7933 - val_loss: 0.7243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "9/9 [==============================] - 1s 121ms/step - loss: 0.6566 - val_loss: 0.5182\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 1s 123ms/step - loss: 0.5281 - val_loss: 0.5336\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 1s 120ms/step - loss: 0.4699 - val_loss: 0.4596\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 1s 123ms/step - loss: 0.4411 - val_loss: 0.4903\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 1s 119ms/step - loss: 0.4525 - val_loss: 0.4995\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 1s 122ms/step - loss: 0.4816 - val_loss: 0.4532\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 1s 120ms/step - loss: 0.4585 - val_loss: 0.5802\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 1s 123ms/step - loss: 0.4795 - val_loss: 0.4355\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 1s 121ms/step - loss: 0.4112 - val_loss: 0.4579\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.4324 - val_loss: 0.4288\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 1s 122ms/step - loss: 0.4505 - val_loss: 0.4452\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 1s 134ms/step - loss: 0.4573 - val_loss: 0.4195\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 1s 122ms/step - loss: 0.4983 - val_loss: 0.4230\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 1s 120ms/step - loss: 0.4542 - val_loss: 0.4942\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 1s 123ms/step - loss: 0.4573 - val_loss: 0.6945\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 1s 123ms/step - loss: 0.5255 - val_loss: 0.4038\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 1s 122ms/step - loss: 0.3945 - val_loss: 0.4090\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 1s 123ms/step - loss: 0.4120 - val_loss: 0.4032\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 1s 122ms/step - loss: 0.3910 - val_loss: 0.4030\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 1s 124ms/step - loss: 0.3567 - val_loss: 0.4020\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 1s 121ms/step - loss: 0.3756 - val_loss: 0.4070\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 1s 122ms/step - loss: 0.3813 - val_loss: 0.4288\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 1s 121ms/step - loss: 0.3756 - val_loss: 0.4633\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 1s 124ms/step - loss: 0.3967 - val_loss: 0.4246\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 1s 121ms/step - loss: 0.3683 - val_loss: 0.3993\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 1s 121ms/step - loss: 0.3554 - val_loss: 0.4049\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 1s 123ms/step - loss: 0.3568 - val_loss: 0.4029\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 1s 122ms/step - loss: 0.3701 - val_loss: 0.4341\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 1s 121ms/step - loss: 0.4077 - val_loss: 0.3997\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 1s 122ms/step - loss: 0.3648 - val_loss: 0.3998\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 1s 122ms/step - loss: 0.3480 - val_loss: 0.4044\n",
      "[2022_04_24-17:00:54] Training the entire fine-tuned model...\n",
      "[2022_04_24-17:01:02] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 0.3485WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1120s vs `on_train_batch_end` time: 0.1475s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1120s vs `on_train_batch_end` time: 0.1475s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 10s 488ms/step - loss: 0.3549 - val_loss: 0.3983\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 2s 259ms/step - loss: 0.3582 - val_loss: 0.4191\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 2s 258ms/step - loss: 0.3687 - val_loss: 0.3885\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 2s 259ms/step - loss: 0.3447 - val_loss: 0.3864\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 2s 260ms/step - loss: 0.3486 - val_loss: 0.4222\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 2s 260ms/step - loss: 0.3549 - val_loss: 0.3929\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 2s 259ms/step - loss: 0.3518 - val_loss: 0.3980\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 2s 260ms/step - loss: 0.3227 - val_loss: 0.3781\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 2s 261ms/step - loss: 0.3343 - val_loss: 0.3800\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 2s 261ms/step - loss: 0.3369 - val_loss: 0.3931\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 2s 259ms/step - loss: 0.3519 - val_loss: 0.3980\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 2s 259ms/step - loss: 0.3329 - val_loss: 0.4000\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 2s 261ms/step - loss: 0.3165 - val_loss: 0.3728\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 2s 261ms/step - loss: 0.3206 - val_loss: 0.3736\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 2s 272ms/step - loss: 0.3177 - val_loss: 0.3717\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 2s 259ms/step - loss: 0.3000 - val_loss: 0.3733\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 2s 261ms/step - loss: 0.3150 - val_loss: 0.3775\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 2s 260ms/step - loss: 0.3085 - val_loss: 0.3720\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 2s 259ms/step - loss: 0.3004 - val_loss: 0.3712\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 2s 261ms/step - loss: 0.2918 - val_loss: 0.3702\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 2s 259ms/step - loss: 0.2880 - val_loss: 0.3811\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 2s 260ms/step - loss: 0.3075 - val_loss: 0.3694\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 2s 262ms/step - loss: 0.3028 - val_loss: 0.3760\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 2s 260ms/step - loss: 0.2973 - val_loss: 0.3708\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 2s 274ms/step - loss: 0.2876 - val_loss: 0.3694\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 2s 261ms/step - loss: 0.2776 - val_loss: 0.3728\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 2s 260ms/step - loss: 0.2664 - val_loss: 0.3711\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 2s 259ms/step - loss: 0.2803 - val_loss: 0.3693\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 2s 260ms/step - loss: 0.2755 - val_loss: 0.3701\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 2s 260ms/step - loss: 0.2816 - val_loss: 0.3702\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 2s 262ms/step - loss: 0.2844 - val_loss: 0.3702\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 2s 271ms/step - loss: 0.2750 - val_loss: 0.3721\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "Epoch 33/100\n",
      "9/9 [==============================] - 2s 262ms/step - loss: 0.2673 - val_loss: 0.3722\n",
      "Epoch 34/100\n",
      "9/9 [==============================] - 2s 260ms/step - loss: 0.2727 - val_loss: 0.3708\n",
      "[2022_04_24-17:02:31] Training on final epochs of sequence length 1024...\n",
      "[2022_04_24-17:02:31] Training set: Filtered out 0 of 1032 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_24-17:02:31] Validation set: Filtered out 0 of 259 (0.0%) records of lengths exceeding 1022.\n",
      " 6/17 [=========>....................] - ETA: 2s - loss: 0.3292WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1092s vs `on_train_batch_end` time: 0.1430s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1092s vs `on_train_batch_end` time: 0.1430s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 12s 401ms/step - loss: 0.3080 - val_loss: 0.3827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/by_data_size/2022_04_22_size0.8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/by_data_size/2022_04_22_size0.8/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161 130 260\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1u4ibbsa) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 45226... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇█▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▆▇▇██▁</td></tr><tr><td>loss</td><td>█▆▄▃▄▄▃▃▄▄▄▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>lr</td><td>███████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▄▃▂▂▂▂▇▂▂▂▂▃▂▂▂▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>27</td></tr><tr><td>best_val_loss</td><td>0.36927</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30799</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.38269</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">atomic-microwave-20</strong>: <a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp/runs/1u4ibbsa\" target=\"_blank\">https://wandb.ai/kvetab/Dataset%20size%20exp/runs/1u4ibbsa</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220424_165950-1u4ibbsa/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1u4ibbsa). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp/runs/2ouippi6\" target=\"_blank\">earnest-wildflower-21</a></strong> to <a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_24-17:03:27] Training set: Filtered out 0 of 1161 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_24-17:03:27] Validation set: Filtered out 0 of 130 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_24-17:03:27] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 10s 309ms/step - loss: 0.8445 - val_loss: 0.6501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.5500 - val_loss: 0.5621\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.4897 - val_loss: 0.4898\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.4600 - val_loss: 0.4561\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4398 - val_loss: 0.4398\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.4347 - val_loss: 0.4205\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.4260 - val_loss: 0.4178\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.4149 - val_loss: 0.4127\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.4075 - val_loss: 0.4772\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.4315 - val_loss: 0.4717\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4353 - val_loss: 0.4696\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.4082 - val_loss: 0.4221\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3892 - val_loss: 0.3925\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3871 - val_loss: 0.3910\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3807 - val_loss: 0.3916\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3786 - val_loss: 0.3888\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3853 - val_loss: 0.3882\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3775 - val_loss: 0.3869\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3755 - val_loss: 0.3934\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3776 - val_loss: 0.3887\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3835 - val_loss: 0.3941\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3758 - val_loss: 0.3882\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3774 - val_loss: 0.3899\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3741 - val_loss: 0.3845\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3718 - val_loss: 0.3844\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3739 - val_loss: 0.3880\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3690 - val_loss: 0.3831\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3660 - val_loss: 0.3832\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3668 - val_loss: 0.3855\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3747 - val_loss: 0.3827\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3781 - val_loss: 0.3825\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3668 - val_loss: 0.3818\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3626 - val_loss: 0.3833\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.3797 - val_loss: 0.3868\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3645 - val_loss: 0.3817\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3706 - val_loss: 0.3840\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3693 - val_loss: 0.3820\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3724 - val_loss: 0.3818\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3654 - val_loss: 0.3824\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3755 - val_loss: 0.3825\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3686 - val_loss: 0.3825\n",
      "[2022_04_24-17:04:23] Training the entire fine-tuned model...\n",
      "[2022_04_24-17:04:32] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.3623WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1119s vs `on_train_batch_end` time: 0.1465s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1119s vs `on_train_batch_end` time: 0.1465s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 10s 465ms/step - loss: 0.3663 - val_loss: 0.3977\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 2s 248ms/step - loss: 0.3775 - val_loss: 0.3766\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 2s 248ms/step - loss: 0.3638 - val_loss: 0.3760\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 2s 248ms/step - loss: 0.3658 - val_loss: 0.3703\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 3s 249ms/step - loss: 0.3539 - val_loss: 0.4184\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 2s 248ms/step - loss: 0.3805 - val_loss: 0.3657\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 3s 253ms/step - loss: 0.3624 - val_loss: 0.3786\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 2s 248ms/step - loss: 0.3466 - val_loss: 0.3756\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 3s 252ms/step - loss: 0.3505 - val_loss: 0.3670\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 3s 253ms/step - loss: 0.3324 - val_loss: 0.3572\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 3s 251ms/step - loss: 0.3333 - val_loss: 0.3606\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 2s 249ms/step - loss: 0.3276 - val_loss: 0.3496\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 3s 249ms/step - loss: 0.3184 - val_loss: 0.3602\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 3s 249ms/step - loss: 0.3165 - val_loss: 0.3625\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 3s 250ms/step - loss: 0.3269 - val_loss: 0.3438\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 3s 249ms/step - loss: 0.2969 - val_loss: 0.3492\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 3s 250ms/step - loss: 0.2959 - val_loss: 0.3439\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 2s 248ms/step - loss: 0.2924 - val_loss: 0.3452\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 3s 250ms/step - loss: 0.2771 - val_loss: 0.3287\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 3s 251ms/step - loss: 0.2695 - val_loss: 0.3535\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 3s 249ms/step - loss: 0.2547 - val_loss: 0.3445\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 3s 250ms/step - loss: 0.2468 - val_loss: 0.3579\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 3s 249ms/step - loss: 0.2376 - val_loss: 0.3895\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 3s 249ms/step - loss: 0.2248 - val_loss: 0.3555\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 3s 250ms/step - loss: 0.2066 - val_loss: 0.3593\n",
      "[2022_04_24-17:05:44] Training on final epochs of sequence length 1024...\n",
      "[2022_04_24-17:05:44] Training set: Filtered out 0 of 1161 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_24-17:06:01] Validation set: Filtered out 0 of 130 (0.0%) records of lengths exceeding 1022.\n",
      " 6/19 [========>.....................] - ETA: 3s - loss: 0.3386WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1096s vs `on_train_batch_end` time: 0.1426s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1096s vs `on_train_batch_end` time: 0.1426s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 13s 377ms/step - loss: 0.2923 - val_loss: 0.3448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/by_data_size/2022_04_22_size0.9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/by_data_size/2022_04_22_size0.9/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    }
   ],
   "source": [
    "for size in sizes:\n",
    "    train, valid = train_test_split(train_set, test_size=1-size, random_state=42, stratify=train_set[\"Y\"])\n",
    "    #test = pd.concat([test, test_set])\n",
    "    test = test_set\n",
    "    #valid, test = train_test_split(test, test_size=0.5, random_state=333, stratify=test[\"Y\"])\n",
    "    print(len(train), len(valid), len(test))\n",
    "    cm, f1_score = train_and_save_model(train, valid, test, size)\n",
    "    cms[size] = cm\n",
    "    f1s[size] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a38702c-f746-4ec6-9911-b21bdaaa56aa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1291 130 130\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2ouippi6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 45680... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▂▂▂▂▃▃▃▃▄▄▄▅▅▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▁▁▂</td></tr><tr><td>lr</td><td>████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▄▃▃▄▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▁▂▁▁▁▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>18</td></tr><tr><td>best_val_loss</td><td>0.32869</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.2923</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.34477</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">earnest-wildflower-21</strong>: <a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp/runs/2ouippi6\" target=\"_blank\">https://wandb.ai/kvetab/Dataset%20size%20exp/runs/2ouippi6</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220424_170310-2ouippi6/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2ouippi6). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp/runs/3sz8kwxf\" target=\"_blank\">revived-capybara-22</a></strong> to <a href=\"https://wandb.ai/kvetab/Dataset%20size%20exp\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_24-17:06:56] Training set: Filtered out 0 of 1291 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_24-17:06:56] Validation set: Filtered out 0 of 130 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_24-17:06:56] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 306ms/step - loss: 0.7751 - val_loss: 0.6868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.5290 - val_loss: 0.4886\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.4669 - val_loss: 0.4886\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4450 - val_loss: 0.5069\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4493 - val_loss: 0.5267\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4278 - val_loss: 0.5224\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4439 - val_loss: 0.4984\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.4117 - val_loss: 0.4788\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.4132 - val_loss: 0.4930\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.4049 - val_loss: 0.4924\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4035 - val_loss: 0.4696\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3916 - val_loss: 0.4779\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3936 - val_loss: 0.4674\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3928 - val_loss: 0.4667\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3910 - val_loss: 0.4658\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3955 - val_loss: 0.4959\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4101 - val_loss: 0.4652\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4056 - val_loss: 0.4799\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.4048 - val_loss: 0.4665\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3940 - val_loss: 0.4956\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 112ms/step - loss: 0.3974 - val_loss: 0.4628\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3885 - val_loss: 0.4903\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3994 - val_loss: 0.4801\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3829 - val_loss: 0.4688\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3741 - val_loss: 0.4612\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3762 - val_loss: 0.4618\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 112ms/step - loss: 0.3871 - val_loss: 0.4677\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3780 - val_loss: 0.4683\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3772 - val_loss: 0.4737\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3723 - val_loss: 0.4640\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3659 - val_loss: 0.4601\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3764 - val_loss: 0.4620\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3637 - val_loss: 0.4634\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 112ms/step - loss: 0.3642 - val_loss: 0.4603\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3697 - val_loss: 0.4666\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3699 - val_loss: 0.4647\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3755 - val_loss: 0.4642\n",
      "[2022_04_24-17:07:52] Training the entire fine-tuned model...\n",
      "[2022_04_24-17:08:35] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.4034WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1119s vs `on_train_batch_end` time: 0.1467s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1119s vs `on_train_batch_end` time: 0.1467s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 422ms/step - loss: 0.3829 - val_loss: 0.4600\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3655 - val_loss: 0.4604\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3596 - val_loss: 0.4616\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 248ms/step - loss: 0.3575 - val_loss: 0.4787\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 248ms/step - loss: 0.3616 - val_loss: 0.4599\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 248ms/step - loss: 0.3555 - val_loss: 0.4684\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3428 - val_loss: 0.4660\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3364 - val_loss: 0.4654\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3249 - val_loss: 0.4736\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3215 - val_loss: 0.4638\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3209 - val_loss: 0.4657\n",
      "[2022_04_24-17:09:13] Training on final epochs of sequence length 1024...\n",
      "[2022_04_24-17:09:13] Training set: Filtered out 0 of 1291 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_24-17:09:14] Validation set: Filtered out 0 of 130 (0.0%) records of lengths exceeding 1022.\n",
      " 6/21 [=======>......................] - ETA: 3s - loss: 0.3424WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1101s vs `on_train_batch_end` time: 0.1423s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1101s vs `on_train_batch_end` time: 0.1423s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 13s 362ms/step - loss: 0.3510 - val_loss: 0.4543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/by_data_size/2022_04_22_size1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/by_data_size/2022_04_22_size1/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    }
   ],
   "source": [
    "size = 1\n",
    "train = train_set\n",
    "valid, test = train_test_split(test_set, test_size=0.5, random_state=333, stratify=test_set[\"Y\"])\n",
    "print(len(train_set), len(valid), len(test))\n",
    "cm, f1_score = train_and_save_model(train, valid, test, size)\n",
    "cms[size] = cm\n",
    "f1s[size] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0a467ff-94ff-4c75-8685-0acfbb0097c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.5: 0.37894736842105264,\n",
       " 0.6: 0.4864864864864865,\n",
       " 0.7: 0.43010752688172044,\n",
       " 0.8: 0.4807692307692308,\n",
       " 0.9: 0.5094339622641509,\n",
       " 1: 0.5531914893617021}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ae230f-e4d7-4045-81ec-22f88f817888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c742e55-6ca8-478c-b902-24f1407b9f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antibody_ID</th>\n",
       "      <th>heavy</th>\n",
       "      <th>light</th>\n",
       "      <th>Y</th>\n",
       "      <th>cluster</th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>2g60</td>\n",
       "      <td>EVQLQQSGGELAKPGASVKMSCKSSGYTFTAYAIHWAKQAAGAGLE...</td>\n",
       "      <td>DVLMTQAPLTLPVSLGDQASISCRSSQAIVHANGNTYLEWYLQKPG...</td>\n",
       "      <td>0</td>\n",
       "      <td>911</td>\n",
       "      <td>EVQLQQSGGELAKPGASVKMSCKSSGYTFTAYAIHWAKQAAGAGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>2a1w</td>\n",
       "      <td>DVKLVESGGGLVKPGGSLRLSCAASGFTFRNYGMSWVRQTPEKRLE...</td>\n",
       "      <td>DVLMTQSPLSLPVSLGDQASISCRCSQSIVKSNGHTYLEWYLQKPG...</td>\n",
       "      <td>0</td>\n",
       "      <td>723</td>\n",
       "      <td>DVKLVESGGGLVKPGGSLRLSCAASGFTFRNYGMSWVRQTPEKRLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>2a77</td>\n",
       "      <td>DVKLVESGGGLVKPGGSLRLSCAASGFTFRNYGMSWVRQTPEKRLE...</td>\n",
       "      <td>DVLMTQSPLSLPVSLGDQASISCRCSQSIVKSNGHTYLEWYLQKPG...</td>\n",
       "      <td>0</td>\n",
       "      <td>723</td>\n",
       "      <td>DVKLVESGGGLVKPGGSLRLSCAASGFTFRNYGMSWVRQTPEKRLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>4ffy</td>\n",
       "      <td>QVQLLQPGAELVKPGASMKLSCKASGYTFTNWWMHWVRLRPGRGLE...</td>\n",
       "      <td>NIVLTQSPASLAVSLGQRATISCRASESVDHYGNSFIYWYQQKPGQ...</td>\n",
       "      <td>0</td>\n",
       "      <td>478</td>\n",
       "      <td>QVQLLQPGAELVKPGASMKLSCKASGYTFTNWWMHWVRLRPGRGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>3l5x</td>\n",
       "      <td>EVTLKESGPVLVKPTETLTLTCTVSGFSLSTYGMGVGWIRQPPGKA...</td>\n",
       "      <td>EIVLTQSPATLSLSPGERATLSCRASKSISKYLAWYQQKPGQAPRL...</td>\n",
       "      <td>0</td>\n",
       "      <td>433</td>\n",
       "      <td>EVTLKESGPVLVKPTETLTLTCTVSGFSLSTYGMGVGWIRQPPGKA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>5f9w</td>\n",
       "      <td>QVQLVQSGAEVKKPGASVTVSCQASGYTFTNYYVHWVRQAPGQGLQ...</td>\n",
       "      <td>EIVLTQSPATLSVSPGERATLSCRASQSVRSNLAWYQQRPGQAPRL...</td>\n",
       "      <td>0</td>\n",
       "      <td>271</td>\n",
       "      <td>QVQLVQSGAEVKKPGASVTVSCQASGYTFTNYYVHWVRQAPGQGLQ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>5x5x</td>\n",
       "      <td>QVKLQQSGAEFVKAGASVKLSCKTSGYTFNNYWIHWVKQSPGQGLE...</td>\n",
       "      <td>DIELTQSPLSLPVSLGDQASISCTSSQSLLHSNGDTYLHWYLQKPG...</td>\n",
       "      <td>0</td>\n",
       "      <td>861</td>\n",
       "      <td>QVKLQQSGAEFVKAGASVKLSCKTSGYTFNNYWIHWVKQSPGQGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>4qww</td>\n",
       "      <td>EVQLVESGGGLVQPKGSLKLSCAASGFTFNTYAMHWVRQAPGKGLE...</td>\n",
       "      <td>QIVLTQSPAIMSASPGEKVTMTCSASSSVSYMYWYHQKPGSSPKPW...</td>\n",
       "      <td>0</td>\n",
       "      <td>436</td>\n",
       "      <td>EVQLVESGGGLVQPKGSLKLSCAASGFTFNTYAMHWVRQAPGKGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1cgs</td>\n",
       "      <td>RVQLLESGAELMKPGASVQISCKATGYTFSEYWIEWVKERPGHGLE...</td>\n",
       "      <td>ELVMTQSPLSLPVSLGDQASISCRPSQSLVHSNGNTYLHWYLQKPG...</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>RVQLLESGAELMKPGASVQISCKATGYTFSEYWIEWVKERPGHGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>1ynl</td>\n",
       "      <td>RVQLLESGAELMKPGASVQISCKATGYTFSFYWIEWVKERPGHGLE...</td>\n",
       "      <td>ELVMTQSPLSLPVSLGDQASISCRPSQSLVHSNGNTYLHWYLQKPG...</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>RVQLLESGAELMKPGASVQISCKATGYTFSFYWIEWVKERPGHGLE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1291 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Antibody_ID                                              heavy  \\\n",
       "535         2g60  EVQLQQSGGELAKPGASVKMSCKSSGYTFTAYAIHWAKQAAGAGLE...   \n",
       "455         2a1w  DVKLVESGGGLVKPGGSLRLSCAASGFTFRNYGMSWVRQTPEKRLE...   \n",
       "459         2a77  DVKLVESGGGLVKPGGSLRLSCAASGFTFRNYGMSWVRQTPEKRLE...   \n",
       "1120        4ffy  QVQLLQPGAELVKPGASMKLSCKASGYTFTNWWMHWVRLRPGRGLE...   \n",
       "851         3l5x  EVTLKESGPVLVKPTETLTLTCTVSGFSLSTYGMGVGWIRQPPGKA...   \n",
       "...          ...                                                ...   \n",
       "1664        5f9w  QVQLVQSGAEVKKPGASVTVSCQASGYTFTNYYVHWVRQAPGQGLQ...   \n",
       "2017        5x5x  QVKLQQSGAEFVKAGASVKLSCKTSGYTFNNYWIHWVKQSPGQGLE...   \n",
       "1400        4qww  EVQLVESGGGLVQPKGSLKLSCAASGFTFNTYAMHWVRQAPGKGLE...   \n",
       "59          1cgs  RVQLLESGAELMKPGASVQISCKATGYTFSEYWIEWVKERPGHGLE...   \n",
       "439         1ynl  RVQLLESGAELMKPGASVQISCKATGYTFSFYWIEWVKERPGHGLE...   \n",
       "\n",
       "                                                  light  Y  cluster  \\\n",
       "535   DVLMTQAPLTLPVSLGDQASISCRSSQAIVHANGNTYLEWYLQKPG...  0      911   \n",
       "455   DVLMTQSPLSLPVSLGDQASISCRCSQSIVKSNGHTYLEWYLQKPG...  0      723   \n",
       "459   DVLMTQSPLSLPVSLGDQASISCRCSQSIVKSNGHTYLEWYLQKPG...  0      723   \n",
       "1120  NIVLTQSPASLAVSLGQRATISCRASESVDHYGNSFIYWYQQKPGQ...  0      478   \n",
       "851   EIVLTQSPATLSLSPGERATLSCRASKSISKYLAWYQQKPGQAPRL...  0      433   \n",
       "...                                                 ... ..      ...   \n",
       "1664  EIVLTQSPATLSVSPGERATLSCRASQSVRSNLAWYQQRPGQAPRL...  0      271   \n",
       "2017  DIELTQSPLSLPVSLGDQASISCTSSQSLLHSNGDTYLHWYLQKPG...  0      861   \n",
       "1400  QIVLTQSPAIMSASPGEKVTMTCSASSSVSYMYWYHQKPGSSPKPW...  0      436   \n",
       "59    ELVMTQSPLSLPVSLGDQASISCRPSQSLVHSNGNTYLHWYLQKPG...  0      103   \n",
       "439   ELVMTQSPLSLPVSLGDQASISCRPSQSLVHSNGNTYLHWYLQKPG...  0      103   \n",
       "\n",
       "                                                    seq  \n",
       "535   EVQLQQSGGELAKPGASVKMSCKSSGYTFTAYAIHWAKQAAGAGLE...  \n",
       "455   DVKLVESGGGLVKPGGSLRLSCAASGFTFRNYGMSWVRQTPEKRLE...  \n",
       "459   DVKLVESGGGLVKPGGSLRLSCAASGFTFRNYGMSWVRQTPEKRLE...  \n",
       "1120  QVQLLQPGAELVKPGASMKLSCKASGYTFTNWWMHWVRLRPGRGLE...  \n",
       "851   EVTLKESGPVLVKPTETLTLTCTVSGFSLSTYGMGVGWIRQPPGKA...  \n",
       "...                                                 ...  \n",
       "1664  QVQLVQSGAEVKKPGASVTVSCQASGYTFTNYYVHWVRQAPGQGLQ...  \n",
       "2017  QVKLQQSGAEFVKAGASVKLSCKTSGYTFNNYWIHWVKQSPGQGLE...  \n",
       "1400  EVQLVESGGGLVQPKGSLKLSCAASGFTFNTYAMHWVRQAPGKGLE...  \n",
       "59    RVQLLESGAELMKPGASVQISCKATGYTFSEYWIEWVKERPGHGLE...  \n",
       "439   RVQLLESGAELMKPGASVQISCKATGYTFSFYWIEWVKERPGHGLE...  \n",
       "\n",
       "[1291 rows x 6 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e914b2-7036-40db-b5b3-65bc9431f18b",
   "metadata": {},
   "source": [
    "# CV on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aaf483c7-69e0-4449-954c-8cbf889455bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antibody_ID</th>\n",
       "      <th>heavy</th>\n",
       "      <th>light</th>\n",
       "      <th>Y</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12e8</td>\n",
       "      <td>EVQLQQSGAEVVRSGASVKLSCTASGFNIKDYYIHWVKQRPEKGLE...</td>\n",
       "      <td>DIVMTQSQKFMSTSVGDRVSITCKASQNVGTAVAWYQQKPGQSPKL...</td>\n",
       "      <td>0</td>\n",
       "      <td>677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15c8</td>\n",
       "      <td>EVQLQQSGAELVKPGASVKLSCTASGFNIKDTYMHWVKQKPEQGLE...</td>\n",
       "      <td>DIVLTQSPAIMSASLGERVTMTCTASSSVSSSNLHWYQQKPGSSPK...</td>\n",
       "      <td>0</td>\n",
       "      <td>685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1a0q</td>\n",
       "      <td>EVQLQESDAELVKPGASVKISCKASGYTFTDHVIHWVKQKPEQGLE...</td>\n",
       "      <td>DIELTQSPSSLSASLGGKVTITCKASQDIKKYIGWYQHKPGKQPRL...</td>\n",
       "      <td>1</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1a14</td>\n",
       "      <td>QVQLQQSGAELVKPGASVRMSCKASGYTFTNYNMYWVKQSPGQGLE...</td>\n",
       "      <td>DIELTQTTSSLSASLGDRVTISCRASQDISNYLNWYQQNPDGTVKL...</td>\n",
       "      <td>0</td>\n",
       "      <td>442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1a2y</td>\n",
       "      <td>QVQLQESGPGLVAPSQSLSITCTVSGFSLTGYGVNWVRQPPGKGLE...</td>\n",
       "      <td>DIVLTQSPASLSASVGETVTITCRASGNIHNYLAWYQQKQGKSPQL...</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Antibody_ID                                              heavy  \\\n",
       "0        12e8  EVQLQQSGAEVVRSGASVKLSCTASGFNIKDYYIHWVKQRPEKGLE...   \n",
       "1        15c8  EVQLQQSGAELVKPGASVKLSCTASGFNIKDTYMHWVKQKPEQGLE...   \n",
       "2        1a0q  EVQLQESDAELVKPGASVKISCKASGYTFTDHVIHWVKQKPEQGLE...   \n",
       "3        1a14  QVQLQQSGAELVKPGASVRMSCKASGYTFTNYNMYWVKQSPGQGLE...   \n",
       "4        1a2y  QVQLQESGPGLVAPSQSLSITCTVSGFSLTGYGVNWVRQPPGKGLE...   \n",
       "\n",
       "                                               light  Y  cluster  \n",
       "0  DIVMTQSQKFMSTSVGDRVSITCKASQNVGTAVAWYQQKPGQSPKL...  0      677  \n",
       "1  DIVLTQSPAIMSASLGERVTMTCTASSSVSSSNLHWYQQKPGSSPK...  0      685  \n",
       "2  DIELTQSPSSLSASLGGKVTITCKASQDIKKYIGWYQHKPGKQPRL...  1      102  \n",
       "3  DIELTQTTSSLSASLGDRVTISCRASQDISNYLNWYQQNPDGTVKL...  0      442  \n",
       "4  DIVLTQSPASLSASVGETVTITCRASGNIHNYLAWYQQKQGKSPQL...  0       59  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chen_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_data_w_clusters.csv\"), index_col=0)\n",
    "chen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "04e07964-4f10-4bdf-8155-2a6ca501d13a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18     59\n",
       "24     35\n",
       "28     28\n",
       "8      25\n",
       "7      21\n",
       "       ..\n",
       "588     1\n",
       "562     1\n",
       "786     1\n",
       "722     1\n",
       "329     1\n",
       "Name: cluster, Length: 932, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chen_data[\"cluster\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ed9606ba-ee16-428b-b36c-9be6342598ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_k_sets(k, data):\n",
    "    total = len(data)\n",
    "    size = total // k + 1\n",
    "    clusters_by_size = data[\"cluster\"].value_counts().index\n",
    "    cluster_sizes = data[\"cluster\"].value_counts()\n",
    "    groups = { i: [] for i in range(k) }\n",
    "    group_nums = { i: [] for i in range(k) }\n",
    "    group = 0\n",
    "    for clust in clusters_by_size:\n",
    "        start_group = group\n",
    "        if len(groups[group]) + cluster_sizes[clust] > size:\n",
    "            group += 1\n",
    "            group = group % k\n",
    "        while len(groups[group]) + cluster_sizes[clust] > size and group != start_group:\n",
    "            group += 1\n",
    "            group = group % k\n",
    "        if len(groups[group]) < size:\n",
    "            groups[group] += list(data[data[\"cluster\"] == clust].index)\n",
    "            group_nums[group].append(clust)\n",
    "        else:\n",
    "            print(\"error\")\n",
    "    return groups, group_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d79864e4-8da7-4a49-ad11-3b2a16248c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n",
      "154\n",
      "155\n",
      "154\n",
      "156\n",
      "156\n",
      "156\n",
      "156\n",
      "156\n",
      "156\n"
     ]
    }
   ],
   "source": [
    "indices, clusters = split_into_k_sets(10, chen_data)\n",
    "for key, gr in indices.items():\n",
    "    print(len(gr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d42cd957-18dc-4347-880c-62f97048a018",
   "metadata": {},
   "outputs": [],
   "source": [
    "chen_data[\"seq\"] = chen_data[\"heavy\"] + chen_data[\"light\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b0d461-e757-4fbd-a003-b4ed187c7b52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a454118a-9e79-4c92-beff-c3cb26e7f542",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_save_named_model(train_data, valid_data, test_data, name, project_name):\n",
    "    wandb.init(project=project_name, entity=\"kvetab\")\n",
    "    model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "            get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)\n",
    "\n",
    "    training_callbacks = [\n",
    "        keras.callbacks.ReduceLROnPlateau(patience = patience[1], factor = 0.25, min_lr = 1e-07, verbose = 1),\n",
    "        keras.callbacks.EarlyStopping(patience = patience[0], restore_best_weights = True),\n",
    "        WandbCallback()\n",
    "    ]\n",
    "\n",
    "    epoch_num = 100\n",
    "    batch_size = 128\n",
    "    #learning_rate = 1e-5\n",
    "    wandb.config = {\n",
    "          \"learning_rate\": learning_rate,\n",
    "          \"epochs\": epoch_num * 2,\n",
    "          \"batch_size\": batch_size\n",
    "        }\n",
    "    finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data[\"seq\"], train_data[\"Y\"], valid_data['seq'], valid_data[\"Y\"], \\\n",
    "            seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = learning_rate, begin_with_frozen_pretrained_layers = True, \\\n",
    "            lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = learning_rate / 10, callbacks = training_callbacks)\n",
    "    mod = model_generator.create_model(seq_len = 512)\n",
    "    mod.save(path.join(DATA_DIR, f\"protein_bert/{name}\"))\n",
    "\n",
    "    results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "            start_seq_len = 512, start_batch_size = 32)\n",
    "    fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "    f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "    return confusion_matrix, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "10abe629-851d-4f20-af43-ba822d8babdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = {}\n",
    "f1s = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "960bc93a-3bd3-42c1-a575-424936bbe0c3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1zr6vh19) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 1055... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▅▅▆▆▇▇█▁▂▂▃▃▄▅▅▆▁</td></tr><tr><td>loss</td><td>▃▇█▆▂▃▄▁▁▃▃▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████▃▃▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇█▆▂▂▃▁▂▃▃▃▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>2</td></tr><tr><td>best_val_loss</td><td>0.5482</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.47977</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.55076</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">still-aardvark-20</strong>: <a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/1zr6vh19\" target=\"_blank\">https://wandb.ai/kvetab/10_fold_cv/runs/1zr6vh19</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220424_193343-1zr6vh19/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1zr6vh19). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/1i98r18m\" target=\"_blank\">magic-dew-21</a></strong> to <a href=\"https://wandb.ai/kvetab/10_fold_cv\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_25-08:53:30] Training set: Filtered out 0 of 1259 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-08:53:30] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-08:53:30] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 10s 310ms/step - loss: 0.8610 - val_loss: 0.6941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.5770 - val_loss: 0.5120\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.4843 - val_loss: 0.5192\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.4796 - val_loss: 0.5333\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4511 - val_loss: 0.4876\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.4228 - val_loss: 0.4560\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.4083 - val_loss: 0.4341\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3963 - val_loss: 0.4334\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3870 - val_loss: 0.4646\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3892 - val_loss: 0.4552\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3833 - val_loss: 0.4250\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3841 - val_loss: 0.4255\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3888 - val_loss: 0.4601\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3744 - val_loss: 0.4247\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3750 - val_loss: 0.4511\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3691 - val_loss: 0.4184\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3779 - val_loss: 0.4165\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3785 - val_loss: 0.4494\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3551 - val_loss: 0.4145\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3354 - val_loss: 0.4243\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3472 - val_loss: 0.4353\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3529 - val_loss: 0.4152\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3644 - val_loss: 0.4298\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3422 - val_loss: 0.4154\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3304 - val_loss: 0.4170\n",
      "[2022_04_25-08:54:08] Training the entire fine-tuned model...\n",
      "[2022_04_25-08:54:28] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.3613WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1136s vs `on_train_batch_end` time: 0.1442s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1136s vs `on_train_batch_end` time: 0.1442s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 10s 469ms/step - loss: 0.3714 - val_loss: 0.4478\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3496 - val_loss: 0.4266\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3423 - val_loss: 0.4137\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 3s 265ms/step - loss: 0.3290 - val_loss: 0.4124\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3305 - val_loss: 0.4131\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3111 - val_loss: 0.4119\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3026 - val_loss: 0.4150\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2976 - val_loss: 0.4260\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2897 - val_loss: 0.4377\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2865 - val_loss: 0.4220\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2687 - val_loss: 0.4225\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2679 - val_loss: 0.4278\n",
      "[2022_04_25-08:55:08] Training on final epochs of sequence length 1024...\n",
      "[2022_04_25-08:55:08] Training set: Filtered out 0 of 1259 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_25-08:55:15] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 1022.\n",
      " 6/20 [========>.....................] - ETA: 3s - loss: 0.3189WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1072s vs `on_train_batch_end` time: 0.1445s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1072s vs `on_train_batch_end` time: 0.1445s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 13s 349ms/step - loss: 0.3239 - val_loss: 0.4070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_0/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1i98r18m) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 7695... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▄▄▄▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂</td></tr><tr><td>lr</td><td>███████████████████████▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▄▄▃▂▂▂▂▂▁▁▂▁▂▁▁▂▁▁▂▁▂▁▁▂▁▁▁▁▁▁▁▂▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.40695</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32394</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.40695</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">magic-dew-21</strong>: <a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/1i98r18m\" target=\"_blank\">https://wandb.ai/kvetab/10_fold_cv/runs/1i98r18m</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220425_085314-1i98r18m/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1i98r18m). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/1az6la9h\" target=\"_blank\">driven-frog-22</a></strong> to <a href=\"https://wandb.ai/kvetab/10_fold_cv\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_25-08:56:09] Training set: Filtered out 0 of 1257 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-08:56:09] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-08:56:09] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 10s 307ms/step - loss: 0.8050 - val_loss: 0.8118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.5750 - val_loss: 0.5913\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.4917 - val_loss: 0.5065\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.4633 - val_loss: 0.4744\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.4332 - val_loss: 0.4574\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4395 - val_loss: 0.4602\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4275 - val_loss: 0.4517\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.4151 - val_loss: 0.4393\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.4046 - val_loss: 0.4240\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.4015 - val_loss: 0.4253\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3901 - val_loss: 0.4113\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3966 - val_loss: 0.4052\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3811 - val_loss: 0.4063\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3881 - val_loss: 0.3968\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3990 - val_loss: 0.3962\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.3919 - val_loss: 0.3923\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3867 - val_loss: 0.4261\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3667 - val_loss: 0.3960\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3849 - val_loss: 0.4550\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.3944 - val_loss: 0.3953\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3837 - val_loss: 0.4103\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3734 - val_loss: 0.3869\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3582 - val_loss: 0.3857\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3592 - val_loss: 0.3828\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3494 - val_loss: 0.3840\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3615 - val_loss: 0.3855\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3550 - val_loss: 0.3834\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.3642 - val_loss: 0.3847\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3532 - val_loss: 0.3818\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3703 - val_loss: 0.3839\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3634 - val_loss: 0.3844\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3565 - val_loss: 0.3813\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3606 - val_loss: 0.3828\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3664 - val_loss: 0.3837\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3568 - val_loss: 0.3810\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3560 - val_loss: 0.3828\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3670 - val_loss: 0.3805\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3635 - val_loss: 0.3870\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3534 - val_loss: 0.3808\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3578 - val_loss: 0.3823\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3590 - val_loss: 0.3816\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 42/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3621 - val_loss: 0.3805\n",
      "Epoch 43/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3484 - val_loss: 0.3804\n",
      "Epoch 44/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3474 - val_loss: 0.3814\n",
      "Epoch 45/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3613 - val_loss: 0.3818\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 46/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3503 - val_loss: 0.3820\n",
      "Epoch 47/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3597 - val_loss: 0.3819\n",
      "Epoch 48/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3528 - val_loss: 0.3817\n",
      "Epoch 49/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3565 - val_loss: 0.3814\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "[2022_04_25-08:57:15] Training the entire fine-tuned model...\n",
      "[2022_04_25-08:57:24] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.3770WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1088s vs `on_train_batch_end` time: 0.1484s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1088s vs `on_train_batch_end` time: 0.1484s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 10s 452ms/step - loss: 0.3701 - val_loss: 0.3781\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 3s 271ms/step - loss: 0.3523 - val_loss: 0.3793\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3573 - val_loss: 0.3706\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3525 - val_loss: 0.3650\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3419 - val_loss: 0.3608\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3438 - val_loss: 0.3591\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3342 - val_loss: 0.3749\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 3s 271ms/step - loss: 0.3362 - val_loss: 0.3531\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.3207 - val_loss: 0.3515\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.3261 - val_loss: 0.3509\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3080 - val_loss: 0.3586\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 3s 272ms/step - loss: 0.2992 - val_loss: 0.3598\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 3s 270ms/step - loss: 0.2926 - val_loss: 0.3567\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 3s 273ms/step - loss: 0.2885 - val_loss: 0.3605\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 3s 270ms/step - loss: 0.2769 - val_loss: 0.3573\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 3s 270ms/step - loss: 0.2623 - val_loss: 0.3569\n",
      "[2022_04_25-08:58:15] Training on final epochs of sequence length 1024...\n",
      "[2022_04_25-08:58:15] Training set: Filtered out 0 of 1257 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_25-08:58:15] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 1022.\n",
      " 6/20 [========>.....................] - ETA: 3s - loss: 0.3066WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1122s vs `on_train_batch_end` time: 0.1427s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1122s vs `on_train_batch_end` time: 0.1427s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 13s 353ms/step - loss: 0.3109 - val_loss: 0.3575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_1/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1az6la9h) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 7984... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▃▃▂▂▂▂▂▂▃▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>9</td></tr><tr><td>best_val_loss</td><td>0.35093</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31087</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.35749</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">driven-frog-22</strong>: <a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/1az6la9h\" target=\"_blank\">https://wandb.ai/kvetab/10_fold_cv/runs/1az6la9h</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220425_085554-1az6la9h/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1az6la9h). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/140pjwfs\" target=\"_blank\">efficient-pond-23</a></strong> to <a href=\"https://wandb.ai/kvetab/10_fold_cv\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_25-08:59:11] Training set: Filtered out 0 of 1256 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-08:59:11] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-08:59:11] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 11s 335ms/step - loss: 0.7739 - val_loss: 0.4855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.5156 - val_loss: 0.4227\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.4792 - val_loss: 0.4273\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.4431 - val_loss: 0.4503\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.4515 - val_loss: 0.3991\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.4399 - val_loss: 0.4018\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.4258 - val_loss: 0.3785\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.4088 - val_loss: 0.3853\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.4079 - val_loss: 0.3714\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3959 - val_loss: 0.3658\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.4010 - val_loss: 0.3623\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3951 - val_loss: 0.3707\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3948 - val_loss: 0.3656\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3898 - val_loss: 0.3896\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.4123 - val_loss: 0.3767\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3781 - val_loss: 0.3547\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3822 - val_loss: 0.3525\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3757 - val_loss: 0.3561\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3719 - val_loss: 0.3491\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3655 - val_loss: 0.3503\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3601 - val_loss: 0.3482\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3616 - val_loss: 0.3490\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3801 - val_loss: 0.3635\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3683 - val_loss: 0.3488\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3654 - val_loss: 0.3475\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3683 - val_loss: 0.3483\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3667 - val_loss: 0.3469\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3692 - val_loss: 0.3535\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3783 - val_loss: 0.3609\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3648 - val_loss: 0.3454\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3680 - val_loss: 0.3587\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3633 - val_loss: 0.3461\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3585 - val_loss: 0.3449\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3729 - val_loss: 0.3484\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3726 - val_loss: 0.3734\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3818 - val_loss: 0.3443\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3511 - val_loss: 0.3424\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3600 - val_loss: 0.3424\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3533 - val_loss: 0.3427\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3610 - val_loss: 0.3457\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3607 - val_loss: 0.3443\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 42/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3499 - val_loss: 0.3409\n",
      "Epoch 43/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3553 - val_loss: 0.3411\n",
      "Epoch 44/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3447 - val_loss: 0.3412\n",
      "Epoch 45/100\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3616 - val_loss: 0.3422\n",
      "Epoch 46/100\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3526 - val_loss: 0.3403\n",
      "Epoch 47/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3565 - val_loss: 0.3403\n",
      "Epoch 48/100\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3544 - val_loss: 0.3403\n",
      "Epoch 49/100\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3515 - val_loss: 0.3403\n",
      "Epoch 50/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3487 - val_loss: 0.3402\n",
      "Epoch 51/100\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.3488 - val_loss: 0.3402\n",
      "Epoch 52/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3521 - val_loss: 0.3401\n",
      "Epoch 53/100\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3474 - val_loss: 0.3402\n",
      "Epoch 54/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3466 - val_loss: 0.3404\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 55/100\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3424 - val_loss: 0.3401\n",
      "Epoch 56/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3514 - val_loss: 0.3401\n",
      "Epoch 57/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3533 - val_loss: 0.3401\n",
      "Epoch 58/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3479 - val_loss: 0.3401\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 59/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3545 - val_loss: 0.3401\n",
      "Epoch 60/100\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.3460 - val_loss: 0.3401\n",
      "Epoch 61/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3538 - val_loss: 0.3401\n",
      "Epoch 62/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3429 - val_loss: 0.3401\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 63/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3480 - val_loss: 0.3401\n",
      "Epoch 64/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3481 - val_loss: 0.3401\n",
      "Epoch 65/100\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.3578 - val_loss: 0.3401\n",
      "Epoch 66/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3432 - val_loss: 0.3401\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "Epoch 67/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3554 - val_loss: 0.3401\n",
      "Epoch 68/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3434 - val_loss: 0.3401\n",
      "Epoch 69/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3569 - val_loss: 0.3401\n",
      "Epoch 70/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3556 - val_loss: 0.3401\n",
      "\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "[2022_04_25-09:00:42] Training the entire fine-tuned model...\n",
      "[2022_04_25-09:01:50] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.3470WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1129s vs `on_train_batch_end` time: 0.1454s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1129s vs `on_train_batch_end` time: 0.1454s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 10s 452ms/step - loss: 0.3772 - val_loss: 0.3399\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3864 - val_loss: 0.3710\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 3s 265ms/step - loss: 0.3561 - val_loss: 0.3381\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 3s 265ms/step - loss: 0.3599 - val_loss: 0.3372\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3453 - val_loss: 0.3385\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3210 - val_loss: 0.3327\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.3300 - val_loss: 0.3319\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3157 - val_loss: 0.3483\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3108 - val_loss: 0.3486\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3011 - val_loss: 0.3408\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2958 - val_loss: 0.3338\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.2733 - val_loss: 0.3279\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2770 - val_loss: 0.3305\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.2837 - val_loss: 0.3292\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2664 - val_loss: 0.3313\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2764 - val_loss: 0.3308\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.2816 - val_loss: 0.3307\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 3s 272ms/step - loss: 0.2616 - val_loss: 0.3321\n",
      "[2022_04_25-09:02:46] Training on final epochs of sequence length 1024...\n",
      "[2022_04_25-09:02:46] Training set: Filtered out 0 of 1256 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_25-09:03:17] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 1022.\n",
      " 6/20 [========>.....................] - ETA: 3s - loss: 0.3001WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1074s vs `on_train_batch_end` time: 0.1427s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1074s vs `on_train_batch_end` time: 0.1427s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 13s 352ms/step - loss: 0.3096 - val_loss: 0.3388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_2/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:140pjwfs) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 8427... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂</td></tr><tr><td>lr</td><td>███████▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▃▄▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>11</td></tr><tr><td>best_val_loss</td><td>0.3279</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3096</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.33878</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">efficient-pond-23</strong>: <a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/140pjwfs\" target=\"_blank\">https://wandb.ai/kvetab/10_fold_cv/runs/140pjwfs</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220425_085855-140pjwfs/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:140pjwfs). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/3jjb4y0g\" target=\"_blank\">dazzling-deluge-24</a></strong> to <a href=\"https://wandb.ai/kvetab/10_fold_cv\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_25-09:04:16] Training set: Filtered out 0 of 1257 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-09:04:16] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-09:04:16] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 10s 305ms/step - loss: 0.7361 - val_loss: 0.6745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.5401 - val_loss: 0.5419\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.5139 - val_loss: 0.4968\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4533 - val_loss: 0.4624\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4309 - val_loss: 0.4514\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.4210 - val_loss: 0.4470\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4214 - val_loss: 0.4446\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.4105 - val_loss: 0.4490\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3999 - val_loss: 0.4275\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3960 - val_loss: 0.4170\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3910 - val_loss: 0.4207\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3864 - val_loss: 0.4764\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.4071 - val_loss: 0.4096\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3910 - val_loss: 0.4107\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3710 - val_loss: 0.4062\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3746 - val_loss: 0.4134\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3715 - val_loss: 0.4189\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3833 - val_loss: 0.3963\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3782 - val_loss: 0.4338\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3715 - val_loss: 0.3907\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3684 - val_loss: 0.4156\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3745 - val_loss: 0.4495\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.4004 - val_loss: 0.4413\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3859 - val_loss: 0.4539\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3627 - val_loss: 0.3861\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3599 - val_loss: 0.3855\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3505 - val_loss: 0.4157\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3454 - val_loss: 0.3843\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3577 - val_loss: 0.3851\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3432 - val_loss: 0.4032\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3533 - val_loss: 0.3834\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3549 - val_loss: 0.3882\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3580 - val_loss: 0.3827\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3555 - val_loss: 0.3933\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3427 - val_loss: 0.3873\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3484 - val_loss: 0.3838\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3511 - val_loss: 0.3965\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3501 - val_loss: 0.3851\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3414 - val_loss: 0.3887\n",
      "[2022_04_25-09:05:11] Training the entire fine-tuned model...\n",
      "[2022_04_25-09:05:19] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.3777WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1127s vs `on_train_batch_end` time: 0.1470s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1127s vs `on_train_batch_end` time: 0.1470s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 10s 455ms/step - loss: 0.3671 - val_loss: 0.3825\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3464 - val_loss: 0.3771\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3357 - val_loss: 0.3734\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3267 - val_loss: 0.3699\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.3249 - val_loss: 0.3719\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 3s 272ms/step - loss: 0.3258 - val_loss: 0.3664\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.3272 - val_loss: 0.3636\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 3s 270ms/step - loss: 0.3020 - val_loss: 0.3670\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3066 - val_loss: 0.3799\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.2992 - val_loss: 0.4688\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.3021 - val_loss: 0.4132\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2796 - val_loss: 0.3796\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.2674 - val_loss: 0.4063\n",
      "[2022_04_25-09:06:02] Training on final epochs of sequence length 1024...\n",
      "[2022_04_25-09:06:02] Training set: Filtered out 0 of 1257 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_25-09:06:06] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 1022.\n",
      " 6/20 [========>.....................] - ETA: 3s - loss: 0.3318WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1061s vs `on_train_batch_end` time: 0.1435s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1061s vs `on_train_batch_end` time: 0.1435s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 13s 349ms/step - loss: 0.3259 - val_loss: 0.3732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_3/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3jjb4y0g) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9010... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▅▃▃▃▃▃▃▃▃▂▂▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂</td></tr><tr><td>lr</td><td>██████████████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▃▂▃▃▂▁▂▁▂▁▁▂▂▂▁▂▁▁▁▁▁▁▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>6</td></tr><tr><td>best_val_loss</td><td>0.36359</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32595</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.37324</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dazzling-deluge-24</strong>: <a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/3jjb4y0g\" target=\"_blank\">https://wandb.ai/kvetab/10_fold_cv/runs/3jjb4y0g</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220425_090400-3jjb4y0g/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3jjb4y0g). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/7ettjqe6\" target=\"_blank\">sparkling-violet-25</a></strong> to <a href=\"https://wandb.ai/kvetab/10_fold_cv\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_25-09:07:03] Training set: Filtered out 0 of 1255 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-09:07:03] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-09:07:03] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 10s 315ms/step - loss: 0.7822 - val_loss: 0.5628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.5269 - val_loss: 0.4314\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.4867 - val_loss: 0.4171\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.4573 - val_loss: 0.4122\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.4662 - val_loss: 0.3966\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.4584 - val_loss: 0.3954\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.4232 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.4297 - val_loss: 0.3895\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.4057 - val_loss: 0.3583\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.4138 - val_loss: 0.3791\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3991 - val_loss: 0.3884\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.4290 - val_loss: 0.3515\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.4034 - val_loss: 0.3437\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.4062 - val_loss: 0.4179\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.4346 - val_loss: 0.3412\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.4127 - val_loss: 0.3825\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3947 - val_loss: 0.3594\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3804 - val_loss: 0.3308\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.3731 - val_loss: 0.3303\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3885 - val_loss: 0.3403\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3790 - val_loss: 0.3283\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3654 - val_loss: 0.3291\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3681 - val_loss: 0.3443\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3621 - val_loss: 0.3268\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3593 - val_loss: 0.3251\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3704 - val_loss: 0.3253\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3627 - val_loss: 0.3349\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3513 - val_loss: 0.3630\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3626 - val_loss: 0.3256\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3565 - val_loss: 0.3263\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3468 - val_loss: 0.3225\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3564 - val_loss: 0.3216\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3464 - val_loss: 0.3209\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3502 - val_loss: 0.3204\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3482 - val_loss: 0.3205\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3460 - val_loss: 0.3201\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3503 - val_loss: 0.3287\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3563 - val_loss: 0.3341\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3574 - val_loss: 0.3202\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3506 - val_loss: 0.3204\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3414 - val_loss: 0.3202\n",
      "Epoch 42/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3559 - val_loss: 0.3207\n",
      "[2022_04_25-09:08:02] Training the entire fine-tuned model...\n",
      "[2022_04_25-09:08:42] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.3826WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1127s vs `on_train_batch_end` time: 0.1472s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1127s vs `on_train_batch_end` time: 0.1472s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 10s 461ms/step - loss: 0.3759 - val_loss: 0.3166\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3650 - val_loss: 0.3214\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3287 - val_loss: 0.3244\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3359 - val_loss: 0.3169\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 3s 270ms/step - loss: 0.3403 - val_loss: 0.3210\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 3s 265ms/step - loss: 0.3173 - val_loss: 0.3161\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3247 - val_loss: 0.3149\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3195 - val_loss: 0.3151\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3009 - val_loss: 0.3145\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3055 - val_loss: 0.3139\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3035 - val_loss: 0.3143\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2992 - val_loss: 0.3158\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 3s 265ms/step - loss: 0.2979 - val_loss: 0.3133\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.2819 - val_loss: 0.3118\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2999 - val_loss: 0.3107\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2864 - val_loss: 0.3102\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2904 - val_loss: 0.3123\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2852 - val_loss: 0.3138\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2845 - val_loss: 0.3163\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2922 - val_loss: 0.3134\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2743 - val_loss: 0.3143\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.2828 - val_loss: 0.3136\n",
      "[2022_04_25-09:09:50] Training on final epochs of sequence length 1024...\n",
      "[2022_04_25-09:09:50] Training set: Filtered out 0 of 1255 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_25-09:10:10] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 1022.\n",
      " 6/20 [========>.....................] - ETA: 3s - loss: 0.3525WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1076s vs `on_train_batch_end` time: 0.1425s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1076s vs `on_train_batch_end` time: 0.1425s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 13s 351ms/step - loss: 0.3286 - val_loss: 0.3127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_4/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:7ettjqe6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9444... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇██▁▁▂▂▂▂▃▃▃▄▄▄▄▁</td></tr><tr><td>loss</td><td>█▄▄▄▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂</td></tr><tr><td>lr</td><td>██████████████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▄▃▃▂▃▂▄▂▂▂▂▂▂▁▂▂▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>15</td></tr><tr><td>best_val_loss</td><td>0.31023</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32862</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.31268</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">sparkling-violet-25</strong>: <a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/7ettjqe6\" target=\"_blank\">https://wandb.ai/kvetab/10_fold_cv/runs/7ettjqe6</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220425_090646-7ettjqe6/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:7ettjqe6). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/36p6imek\" target=\"_blank\">dainty-wood-26</a></strong> to <a href=\"https://wandb.ai/kvetab/10_fold_cv\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_25-09:11:07] Training set: Filtered out 0 of 1255 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-09:11:07] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-09:11:07] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 10s 312ms/step - loss: 0.7287 - val_loss: 0.6076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.5179 - val_loss: 0.4699\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4529 - val_loss: 0.4603\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.4431 - val_loss: 0.4489\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4268 - val_loss: 0.4501\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4283 - val_loss: 0.4665\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4226 - val_loss: 0.4513\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4135 - val_loss: 0.4288\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.4003 - val_loss: 0.4266\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3889 - val_loss: 0.4172\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3992 - val_loss: 0.4146\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3765 - val_loss: 0.4107\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.4010 - val_loss: 0.4161\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3982 - val_loss: 0.4065\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3782 - val_loss: 0.4186\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.3722 - val_loss: 0.4021\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3768 - val_loss: 0.4967\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4098 - val_loss: 0.4007\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.4010 - val_loss: 0.4210\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.3859 - val_loss: 0.4574\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.3867 - val_loss: 0.4043\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.3723 - val_loss: 0.3986\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3722 - val_loss: 0.4202\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3888 - val_loss: 0.4248\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3676 - val_loss: 0.4647\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3819 - val_loss: 0.3893\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3784 - val_loss: 0.4002\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3496 - val_loss: 0.4144\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3480 - val_loss: 0.3857\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3604 - val_loss: 0.3835\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3617 - val_loss: 0.3895\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3622 - val_loss: 0.3804\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3616 - val_loss: 0.3813\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3445 - val_loss: 0.3855\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3386 - val_loss: 0.3817\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3442 - val_loss: 0.3840\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3316 - val_loss: 0.3851\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3419 - val_loss: 0.3832\n",
      "[2022_04_25-09:12:01] Training the entire fine-tuned model...\n",
      "[2022_04_25-09:12:09] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.3417WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1092s vs `on_train_batch_end` time: 0.1483s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1092s vs `on_train_batch_end` time: 0.1483s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 10s 449ms/step - loss: 0.3426 - val_loss: 0.3866\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3522 - val_loss: 0.3932\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3347 - val_loss: 0.3794\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3166 - val_loss: 0.3813\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3293 - val_loss: 0.3750\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 3s 265ms/step - loss: 0.3391 - val_loss: 0.3698\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3293 - val_loss: 0.3694\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3332 - val_loss: 0.4352\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3287 - val_loss: 0.3951\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.3057 - val_loss: 0.3666\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.3057 - val_loss: 0.3674\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2988 - val_loss: 0.3695\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2884 - val_loss: 0.3655\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2895 - val_loss: 0.3813\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2871 - val_loss: 0.3692\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2881 - val_loss: 0.3718\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2622 - val_loss: 0.3668\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 3s 272ms/step - loss: 0.2724 - val_loss: 0.3639\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.2606 - val_loss: 0.3665\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 3s 270ms/step - loss: 0.2539 - val_loss: 0.3682\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2537 - val_loss: 0.3708\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.2656 - val_loss: 0.3749\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2425 - val_loss: 0.3708\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2532 - val_loss: 0.3709\n",
      "[2022_04_25-09:13:21] Training on final epochs of sequence length 1024...\n",
      "[2022_04_25-09:13:21] Training set: Filtered out 0 of 1255 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_25-09:13:21] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 1022.\n",
      " 6/20 [========>.....................] - ETA: 3s - loss: 0.2975WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1097s vs `on_train_batch_end` time: 0.1429s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1097s vs `on_train_batch_end` time: 0.1429s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 13s 350ms/step - loss: 0.2875 - val_loss: 0.3577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_5/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:36p6imek) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9891... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅</td></tr><tr><td>loss</td><td>█▅▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████████▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▃▄▃▃▂▂▃▂▂▄▂▃▃▂▂▂▂▁▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.35775</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.28746</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.35775</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dainty-wood-26</strong>: <a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/36p6imek\" target=\"_blank\">https://wandb.ai/kvetab/10_fold_cv/runs/36p6imek</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220425_091050-36p6imek/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:36p6imek). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/1gke3ejq\" target=\"_blank\">earnest-bird-27</a></strong> to <a href=\"https://wandb.ai/kvetab/10_fold_cv\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_25-09:14:19] Training set: Filtered out 0 of 1255 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-09:14:19] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-09:14:19] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 10s 317ms/step - loss: 0.7653 - val_loss: 0.6297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.5217 - val_loss: 0.5100\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.4588 - val_loss: 0.4614\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.4360 - val_loss: 0.4555\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.4331 - val_loss: 0.4640\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.4194 - val_loss: 0.4377\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.4145 - val_loss: 0.4281\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4001 - val_loss: 0.4284\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.4077 - val_loss: 0.4547\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4039 - val_loss: 0.4171\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3847 - val_loss: 0.4202\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.4041 - val_loss: 0.4174\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.4176 - val_loss: 0.4439\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3868 - val_loss: 0.4159\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3744 - val_loss: 0.4165\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3848 - val_loss: 0.4215\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3618 - val_loss: 0.4127\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3717 - val_loss: 0.4076\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3601 - val_loss: 0.4053\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3468 - val_loss: 0.4061\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3519 - val_loss: 0.4152\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3675 - val_loss: 0.4108\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3511 - val_loss: 0.4033\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3632 - val_loss: 0.4031\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3542 - val_loss: 0.4091\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3396 - val_loss: 0.4027\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3491 - val_loss: 0.4111\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3318 - val_loss: 0.4060\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3355 - val_loss: 0.4111\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3483 - val_loss: 0.4289\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3388 - val_loss: 0.4031\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3397 - val_loss: 0.4036\n",
      "[2022_04_25-09:15:06] Training the entire fine-tuned model...\n",
      "[2022_04_25-09:15:17] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.3476WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1130s vs `on_train_batch_end` time: 0.1482s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1130s vs `on_train_batch_end` time: 0.1482s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 11s 531ms/step - loss: 0.3537 - val_loss: 0.4090\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3474 - val_loss: 0.4174\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 3s 265ms/step - loss: 0.3383 - val_loss: 0.4027\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3451 - val_loss: 0.4016\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.3285 - val_loss: 0.4159\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3305 - val_loss: 0.4007\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3265 - val_loss: 0.4019\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3253 - val_loss: 0.4122\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3271 - val_loss: 0.4293\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3057 - val_loss: 0.4111\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3004 - val_loss: 0.4034\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2997 - val_loss: 0.4072\n",
      "[2022_04_25-09:15:58] Training on final epochs of sequence length 1024...\n",
      "[2022_04_25-09:15:58] Training set: Filtered out 0 of 1255 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_25-09:15:59] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 1022.\n",
      " 6/20 [========>.....................] - ETA: 3s - loss: 0.2938WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1067s vs `on_train_batch_end` time: 0.1458s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1067s vs `on_train_batch_end` time: 0.1458s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 13s 350ms/step - loss: 0.3294 - val_loss: 0.4074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_6/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1gke3ejq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10306... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇███▁▁▁▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▃▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████████████▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▃▃▂▂▂▁▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.40072</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32937</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.40744</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">earnest-bird-27</strong>: <a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/1gke3ejq\" target=\"_blank\">https://wandb.ai/kvetab/10_fold_cv/runs/1gke3ejq</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220425_091404-1gke3ejq/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1gke3ejq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/zleuaf64\" target=\"_blank\">eager-armadillo-28</a></strong> to <a href=\"https://wandb.ai/kvetab/10_fold_cv\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_25-09:16:54] Training set: Filtered out 0 of 1255 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-09:16:54] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-09:16:54] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 10s 411ms/step - loss: 0.7098 - val_loss: 0.6714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.5580 - val_loss: 0.4641\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.4748 - val_loss: 0.4528\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.4416 - val_loss: 0.4293\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.4390 - val_loss: 0.4345\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4256 - val_loss: 0.4266\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.4148 - val_loss: 0.4156\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4086 - val_loss: 0.4114\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4082 - val_loss: 0.4307\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.4151 - val_loss: 0.4991\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.4117 - val_loss: 0.4079\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3727 - val_loss: 0.4237\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3855 - val_loss: 0.4105\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3797 - val_loss: 0.4024\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3721 - val_loss: 0.4747\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3784 - val_loss: 0.4006\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3786 - val_loss: 0.4030\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3642 - val_loss: 0.3982\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3622 - val_loss: 0.4103\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3668 - val_loss: 0.4218\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3833 - val_loss: 0.3965\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3558 - val_loss: 0.3947\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3652 - val_loss: 0.4204\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3517 - val_loss: 0.3970\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3690 - val_loss: 0.3959\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3567 - val_loss: 0.4271\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3531 - val_loss: 0.3941\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3444 - val_loss: 0.3991\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3627 - val_loss: 0.3939\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3449 - val_loss: 0.3964\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3512 - val_loss: 0.3975\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3463 - val_loss: 0.3938\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3485 - val_loss: 0.3974\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3420 - val_loss: 0.4086\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3387 - val_loss: 0.3983\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3380 - val_loss: 0.3937\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3330 - val_loss: 0.3957\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3448 - val_loss: 0.3994\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3422 - val_loss: 0.3956\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3356 - val_loss: 0.3959\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3403 - val_loss: 0.3970\n",
      "Epoch 42/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3523 - val_loss: 0.3973\n",
      "[2022_04_25-09:17:52] Training the entire fine-tuned model...\n",
      "[2022_04_25-09:18:31] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.3634WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1121s vs `on_train_batch_end` time: 0.1464s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1121s vs `on_train_batch_end` time: 0.1464s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 10s 453ms/step - loss: 0.3620 - val_loss: 0.4231\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3644 - val_loss: 0.3881\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3476 - val_loss: 0.4657\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3637 - val_loss: 0.4235\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3486 - val_loss: 0.3896\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3375 - val_loss: 0.4100\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3408 - val_loss: 0.3992\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.3180 - val_loss: 0.3921\n",
      "[2022_04_25-09:19:01] Training on final epochs of sequence length 1024...\n",
      "[2022_04_25-09:19:01] Training set: Filtered out 0 of 1255 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_25-09:19:01] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 1022.\n",
      " 6/20 [========>.....................] - ETA: 3s - loss: 0.3376WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1101s vs `on_train_batch_end` time: 0.1423s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1101s vs `on_train_batch_end` time: 0.1423s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 13s 354ms/step - loss: 0.3397 - val_loss: 0.3862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_7/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:zleuaf64) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10631... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇██▁▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████████▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▂▂▂▂▂▂▂▂▃▁▁▁▂▁▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▂▁▃▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.38623</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33967</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.38623</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">eager-armadillo-28</strong>: <a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/zleuaf64\" target=\"_blank\">https://wandb.ai/kvetab/10_fold_cv/runs/zleuaf64</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220425_091639-zleuaf64/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:zleuaf64). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/2mtvyzmm\" target=\"_blank\">olive-paper-29</a></strong> to <a href=\"https://wandb.ai/kvetab/10_fold_cv\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_25-09:19:57] Training set: Filtered out 0 of 1255 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-09:19:57] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-09:19:57] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 11s 309ms/step - loss: 0.7575 - val_loss: 0.8898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.5707 - val_loss: 0.6389\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.4822 - val_loss: 0.5535\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.4654 - val_loss: 0.4804\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4401 - val_loss: 0.4506\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4142 - val_loss: 0.4489\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4141 - val_loss: 0.4386\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4112 - val_loss: 0.4393\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4001 - val_loss: 0.4756\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3929 - val_loss: 0.4321\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.4033 - val_loss: 0.4201\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4023 - val_loss: 0.4163\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.4081 - val_loss: 0.5364\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3970 - val_loss: 0.4156\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3812 - val_loss: 0.4046\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3665 - val_loss: 0.4513\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3634 - val_loss: 0.4530\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3840 - val_loss: 0.4034\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3765 - val_loss: 0.4170\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.3629 - val_loss: 0.4035\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3569 - val_loss: 0.3953\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3576 - val_loss: 0.3998\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3599 - val_loss: 0.4788\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3727 - val_loss: 0.3905\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3560 - val_loss: 0.4084\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3532 - val_loss: 0.3991\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3447 - val_loss: 0.4362\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3595 - val_loss: 0.3977\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3382 - val_loss: 0.3937\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3442 - val_loss: 0.3862\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3333 - val_loss: 0.4066\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3447 - val_loss: 0.3846\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.3422 - val_loss: 0.3816\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.3363 - val_loss: 0.3955\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.3425 - val_loss: 0.3816\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3408 - val_loss: 0.3906\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3425 - val_loss: 0.4311\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3500 - val_loss: 0.3829\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.3354 - val_loss: 0.3884\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3328 - val_loss: 0.3918\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3344 - val_loss: 0.3943\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_25-09:20:54] Training the entire fine-tuned model...\n",
      "[2022_04_25-09:21:37] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.3430WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1143s vs `on_train_batch_end` time: 0.1442s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1143s vs `on_train_batch_end` time: 0.1442s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 10s 454ms/step - loss: 0.3507 - val_loss: 0.3770\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 3s 264ms/step - loss: 0.3349 - val_loss: 0.3769\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 3s 265ms/step - loss: 0.3318 - val_loss: 0.3748\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 3s 265ms/step - loss: 0.3268 - val_loss: 0.3781\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3221 - val_loss: 0.4427\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.3301 - val_loss: 0.4516\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3132 - val_loss: 0.4228\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 3s 272ms/step - loss: 0.3105 - val_loss: 0.3724\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3058 - val_loss: 0.4007\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3002 - val_loss: 0.3773\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2971 - val_loss: 0.3857\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3035 - val_loss: 0.3809\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2955 - val_loss: 0.3776\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3020 - val_loss: 0.3822\n",
      "[2022_04_25-09:22:22] Training on final epochs of sequence length 1024...\n",
      "[2022_04_25-09:22:22] Training set: Filtered out 0 of 1255 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_25-09:22:22] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 1022.\n",
      " 6/20 [========>.....................] - ETA: 3s - loss: 0.3527WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1085s vs `on_train_batch_end` time: 0.1438s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1085s vs `on_train_batch_end` time: 0.1438s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 12s 347ms/step - loss: 0.3258 - val_loss: 0.3823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_8/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2mtvyzmm) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10993... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▂▂▂▂▃▁▂▂▂▁▁▂▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.37241</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3258</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.38233</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">olive-paper-29</strong>: <a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/2mtvyzmm\" target=\"_blank\">https://wandb.ai/kvetab/10_fold_cv/runs/2mtvyzmm</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220425_091941-2mtvyzmm/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2mtvyzmm). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/10_fold_cv/runs/3jwkfo4a\" target=\"_blank\">dutiful-totem-30</a></strong> to <a href=\"https://wandb.ai/kvetab/10_fold_cv\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_25-09:23:21] Training set: Filtered out 0 of 1255 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-09:23:21] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-09:23:21] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 11s 336ms/step - loss: 0.8156 - val_loss: 0.7028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.6446 - val_loss: 0.4759\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.5166 - val_loss: 0.4526\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.4499 - val_loss: 0.4427\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.4295 - val_loss: 0.4454\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.4271 - val_loss: 0.4376\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.4161 - val_loss: 0.4305\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.4028 - val_loss: 0.4279\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.4039 - val_loss: 0.4305\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3937 - val_loss: 0.4262\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3885 - val_loss: 0.4260\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3966 - val_loss: 0.4232\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3882 - val_loss: 0.4429\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.3857 - val_loss: 0.4234\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3724 - val_loss: 0.4303\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3690 - val_loss: 0.4175\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3658 - val_loss: 0.4321\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3643 - val_loss: 0.4137\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3655 - val_loss: 0.4144\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3564 - val_loss: 0.4337\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.3649 - val_loss: 0.4234\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.3611 - val_loss: 0.4113\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3654 - val_loss: 0.4653\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3717 - val_loss: 0.4307\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3711 - val_loss: 0.4028\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3733 - val_loss: 0.4498\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3524 - val_loss: 0.4151\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3489 - val_loss: 0.4287\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3557 - val_loss: 0.4151\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.3401 - val_loss: 0.4100\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3369 - val_loss: 0.4033\n",
      "[2022_04_25-09:24:08] Training the entire fine-tuned model...\n",
      "[2022_04_25-09:24:16] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.3569WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1129s vs `on_train_batch_end` time: 0.1462s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1129s vs `on_train_batch_end` time: 0.1462s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 11s 466ms/step - loss: 0.3588 - val_loss: 0.4069\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3572 - val_loss: 0.4109\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3392 - val_loss: 0.4149\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3302 - val_loss: 0.4168\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3467 - val_loss: 0.4003\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 3s 271ms/step - loss: 0.3347 - val_loss: 0.4144\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3256 - val_loss: 0.4085\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.3112 - val_loss: 0.3991\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.3152 - val_loss: 0.4014\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 3s 271ms/step - loss: 0.3037 - val_loss: 0.3940\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.3026 - val_loss: 0.3957\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2770 - val_loss: 0.3984\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.2770 - val_loss: 0.3978\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2838 - val_loss: 0.4059\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.2535 - val_loss: 0.4030\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.2490 - val_loss: 0.4062\n",
      "[2022_04_25-09:25:08] Training on final epochs of sequence length 1024...\n",
      "[2022_04_25-09:25:08] Training set: Filtered out 0 of 1255 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_25-09:25:44] Validation set: Filtered out 0 of 140 (0.0%) records of lengths exceeding 1022.\n",
      " 6/20 [========>.....................] - ETA: 3s - loss: 0.3346WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1078s vs `on_train_batch_end` time: 0.1434s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1078s vs `on_train_batch_end` time: 0.1434s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 12s 346ms/step - loss: 0.3001 - val_loss: 0.3876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/10-fold-cv/2022_04_24_split_9/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    test = chen_data.loc[indices[i]]\n",
    "    remaining = [idx for idx in list(chen_data.index) if idx not in indices[i]]\n",
    "    train = chen_data.loc[remaining]\n",
    "    train, valid = train_test_split(train, test_size=0.1, random_state=333)\n",
    "    cm, f1 = train_and_save_named_model(train, valid, test, f\"10-fold-cv/2022_04_24_split_{i}\", \"10_fold_cv\")\n",
    "    cms[i] = cm\n",
    "    f1s[i] = f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f769ca8a-307b-46d6-81ae-a9af893fdf19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.44155844155844154,\n",
       " 1: 0.5806451612903226,\n",
       " 2: 0.5714285714285714,\n",
       " 3: 0.4897959183673469,\n",
       " 4: 0.41509433962264153,\n",
       " 5: 0.5714285714285714,\n",
       " 6: 0.5666666666666667,\n",
       " 7: 0.42857142857142855,\n",
       " 8: 0.38596491228070173,\n",
       " 9: 0.4642857142857143}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e51ada6b-4e15-46dd-8adc-8efaed286cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e6fb3768-e7bb-46b3-b272-2b7a0645a964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4915439725500407"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_scores = [value for key, value in f1s.items()]\n",
    "mean(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "756817f3-96f6-423c-963c-9e135d5f387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cd4549be-1792-4081-92e6-9dc9f1875bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07494233825267892"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "stdev(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7d7e6e32-9bc9-48ec-a265-e56230db0195",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "84baab7b-4cbd-4385-95d4-302908196294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_25-11:05:07] Tap set: Filtered out 0 of 152 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-11:05:15] Tap set: Filtered out 0 of 154 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-11:05:23] Tap set: Filtered out 0 of 155 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-11:05:32] Tap set: Filtered out 0 of 154 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-11:05:40] Tap set: Filtered out 0 of 156 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-11:05:49] Tap set: Filtered out 0 of 156 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-11:05:57] Tap set: Filtered out 0 of 156 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-11:06:06] Tap set: Filtered out 0 of 156 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-11:06:14] Tap set: Filtered out 0 of 156 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_25-11:06:23] Tap set: Filtered out 0 of 156 (0.0%) records of lengths exceeding 510.\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    test = chen_data.loc[indices[i]]\n",
    "    encoded_test_set = encode_dataset(test[\"seq\"], test[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "        dataset_name = 'Tap set')\n",
    "    test_X, test_Y, test_sample_weigths = encoded_test_set\n",
    "    filename = path.join(DATA_DIR, f\"evaluations/protein_bert/10-fold-cv/2022_04_24_split_{i}.csv\")\n",
    "    model_path = path.join(DATA_DIR, f\"protein_bert/10-fold-cv/2022_04_24_split_{i}\")\n",
    "    model = keras.models.load_model(model_path)\n",
    "    y_pred = model.predict(test_X, batch_size=32)\n",
    "    y_pred_classes = (y_pred >= 0.5)\n",
    "    str_preds = [str(int(pred)) for pred in y_pred_classes]\n",
    "    with open(filename, \"wt\") as f:\n",
    "        f.write(\",\".join(str_preds) + \"\\n\")\n",
    "    metric_dict = {\n",
    "        \"f1\": float(metrics.f1_score(test_Y, y_pred_classes)),\n",
    "        \"acc\": float(metrics.accuracy_score(test_Y, y_pred_classes)),\n",
    "        \"mcc\": float(metrics.matthews_corrcoef(test_Y, y_pred_classes)),\n",
    "        \"auc\": float(metrics.roc_auc_score(test_Y, y_pred_classes)),\n",
    "        \"precision\": float(metrics.precision_score(test_Y, y_pred_classes)),\n",
    "        \"recall\": float(metrics.recall_score(test_Y, y_pred_classes))\n",
    "    }\n",
    "    filename_sum = path.join(DATA_DIR, f\"evaluations/protein_bert/10-fold-cv/all.csv\")\n",
    "    line = [f\"10-fold-cv/2022_04_24_split_{i}\", metric_dict[\"f1\"], metric_dict[\"mcc\"], metric_dict[\"acc\"],metric_dict[\"precision\"],metric_dict[\"recall\"],metric_dict[\"auc\"]]\n",
    "    with open(filename_sum, 'a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile, delimiter='\\t')\n",
    "        csvwriter.writerow(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8231f87e-2b2c-4e9b-bf07-8e982df39f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = chen_data.loc[indices[0]]\n",
    "test.to_csv(path.join(DATA_DIR, \"evaluations/comparison/y_true_0.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09c5b8f-28ed-428f-9f97-6def301da6f8",
   "metadata": {},
   "source": [
    "# Comparison with best ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4bcc6a52-bf7e-484e-929d-a800bc728a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "481ec0cd-e930-4cab-93b4-edcf9f45ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(preprocessing, data_name, hp_dir):\n",
    "    filename = path.join(hp_dir, f\"logistic_regression_{data_name}_{preprocessing}.json\")\n",
    "    parameters = json.load(open(filename))\n",
    "    #C = float(parameters[\"C\"])\n",
    "    lr = LogisticRegression(\n",
    "        class_weight='balanced', max_iter=1000, random_state=42,\n",
    "        C=float(parameters[\"C\"]), penalty=parameters[\"penalty\"], solver=parameters[\"solver\"]\n",
    "    )\n",
    "    return lr, parameters, \"logistic_regression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4baeebc8-63e2-4f89-8889-be50b7125c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pybiomed(train_df, test_df, tap_df):\n",
    "    x_chen = pd.read_feather(path.join(DATA_DIR, \"chen/pybiomed/X_data.ftr\"))\n",
    "    x_chen_train = x_chen.merge(train_df[[\"Antibody_ID\", \"Y\"]].reset_index(), left_on=\"Ab_ID\", right_on=\"Antibody_ID\").set_index('index').drop(\"Antibody_ID\", axis=1)\n",
    "    x_chen_test = x_chen.merge(test_df[[\"Antibody_ID\", \"Y\"]].reset_index(), left_on=\"Ab_ID\", right_on=\"Antibody_ID\").set_index('index').drop(\"Antibody_ID\", axis=1)\n",
    "    x_tap = pd.read_feather(path.join(DATA_DIR, \"tap/pybiomed/X_TAP_data.ftr\"))\n",
    "    x_tap = x_tap.loc[tap_df.index]\n",
    "    return x_chen_train, x_chen_test, x_tap\n",
    "\n",
    "def bert(train_df, test_df, tap_df):\n",
    "    x_chen = pd.read_feather(path.join(DATA_DIR, \"chen/embeddings/bert/bert_chen_embeddings.ftr\"))\n",
    "    x_chen_train = x_chen.merge(train_df[[\"Antibody_ID\", \"Y\"]].reset_index(), left_on=\"Ab_ID\", right_on=\"Antibody_ID\").set_index('index').drop(\"Antibody_ID\", axis=1)\n",
    "    x_chen_test = x_chen.merge(test_df[[\"Antibody_ID\", \"Y\"]].reset_index(), left_on=\"Ab_ID\", right_on=\"Antibody_ID\").set_index('index').drop(\"Antibody_ID\", axis=1)\n",
    "    x_tap = pd.read_feather(path.join(DATA_DIR, \"tap/embeddings/bert/bert_tap_embeddings.ftr\"))\n",
    "    x_tap = x_tap.drop(\"Ab_ID\", axis=1)\n",
    "    x_tap = x_tap.loc[tap_df.index]\n",
    "    return x_chen_train, x_chen_test, x_tap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "28c4c948-cf15-4780-a108-3fa493430143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(train_df, test_df, tap_df):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_df.drop([\"Ab_ID\", \"Y\"], axis=1))\n",
    "    x_train_tr = scaler.transform(train_df.drop([\"Ab_ID\", \"Y\"], axis=1))\n",
    "    x_train_df = pd.DataFrame(data=train_df,  index=train_df.index, columns=train_df.drop([\"Ab_ID\", \"Y\"], axis=1).columns)\n",
    "    x_train_df[\"Ab_ID\"] = train_df[\"Ab_ID\"]\n",
    "    \n",
    "    x_test_tr = scaler.transform(test_df.drop([\"Ab_ID\", \"Y\"], axis=1))\n",
    "    x_test_df = pd.DataFrame(data=test_df,  index=test_df.index, columns=test_df.drop([\"Ab_ID\", \"Y\"], axis=1).columns)\n",
    "    x_test_df[\"Y\"] = test_df[\"Y\"]\n",
    "    x_test_df[\"Ab_ID\"] = test_df[\"Ab_ID\"]\n",
    "    \n",
    "    x_tap_tr = scaler.transform(tap_df)\n",
    "    x_tap_df = pd.DataFrame(data=tap_df,  index=tap_df.index, columns=tap_df.columns)\n",
    "\n",
    "    return x_train_df, train_df[\"Y\"], x_test_df, x_tap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f446f135-0f48-4ea7-bea7-9b5224111255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(model_name, classifier, X_train, y_train, X_valid, y_valid):\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_valid)\n",
    "    filename = path.join(DATA_DIR, \"evaluations/comparison\", \"models\", f\"{model_name}.pkl\")\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(classifier, f)\n",
    "    filename = path.join(DATA_DIR, \"evaluations/comparison\", f\"{model_name}.csv\")\n",
    "    str_preds = [str(int(pred)) for pred in y_pred]\n",
    "    with open(filename, \"wt\") as f:\n",
    "        f.write(\",\".join(str_preds) + \"\\n\")\n",
    "    metric_dict = {\n",
    "        \"f1\": float(metrics.f1_score(y_valid, y_pred)),\n",
    "        \"acc\": float(metrics.accuracy_score(y_valid, y_pred)),\n",
    "        \"mcc\": float(metrics.matthews_corrcoef(y_valid, y_pred)),\n",
    "        \"auc\": float(metrics.roc_auc_score(y_valid, y_pred)),\n",
    "        \"precision\": float(metrics.precision_score(y_valid, y_pred)),\n",
    "        \"recall\": float(metrics.recall_score(y_valid, y_pred))\n",
    "    }\n",
    "    filename_sum = os.path.join(DATA_DIR, f\"evaluations/comparison/all.csv\")\n",
    "    line = [model_name, metric_dict[\"f1\"], metric_dict[\"mcc\"], metric_dict[\"acc\"],metric_dict[\"precision\"],metric_dict[\"recall\"],metric_dict[\"auc\"]]\n",
    "    with open(filename_sum, 'a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile, delimiter='\\t')\n",
    "        csvwriter.writerow(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8f6d7568-6c2b-4d33-8625-839ae78d9c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "tap_data = pd.read_csv(path.join(DATA_DIR, \"tap/tap_not_in_chen.csv\"))\n",
    "for i in range(10):\n",
    "    test = chen_data.loc[indices[i]]\n",
    "    remaining = [idx for idx in list(chen_data.index) if idx not in indices[i]]\n",
    "    train = chen_data.loc[remaining]\n",
    "    x_train, x_test, x_tap = pybiomed(train, test, tap_data)\n",
    "    x_train_tr, y_train_tr, x_test_tr, tap_tr = scaling(x_train, x_test, x_tap)\n",
    "    classifier, params, model_label = logistic_regression(\"scaling\", \"pybiomed\", path.join(DATA_DIR, \"evaluations/hyperparameters\"))\n",
    "    train_and_eval(f\"logistic_regression_pybiomed_{i}\", classifier, x_train_tr.drop([\"Ab_ID\"], axis=1), \n",
    "                    y_train_tr, x_test_tr.drop([\"Ab_ID\", \"Y\"], axis=1), x_test_tr[\"Y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b1d9872d-47c5-4324-b2ec-21b0879a4219",
   "metadata": {},
   "outputs": [],
   "source": [
    "tap_data = pd.read_csv(path.join(DATA_DIR, \"tap/tap_not_in_chen.csv\"))\n",
    "for i in range(10):\n",
    "    test = chen_data.loc[indices[i]]\n",
    "    remaining = [idx for idx in list(chen_data.index) if idx not in indices[i]]\n",
    "    train = chen_data.loc[remaining]\n",
    "    x_train, x_test, x_tap = bert(train, test, tap_data)\n",
    "    x_train_tr, y_train_tr, x_test_tr, tap_tr = scaling(x_train, x_test, x_tap)\n",
    "    classifier, params, model_label = logistic_regression(\"scaling\", \"bert\", path.join(DATA_DIR, \"evaluations/hyperparameters\"))\n",
    "    train_and_eval(f\"logistic_regression_bert_{i}\", classifier, x_train_tr.drop([\"Ab_ID\"], axis=1), \n",
    "                    y_train_tr, x_test_tr.drop([\"Ab_ID\", \"Y\"], axis=1), x_test_tr[\"Y\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
