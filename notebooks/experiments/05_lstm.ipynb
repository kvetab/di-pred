{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91890b4d-226c-45b5-839e-0c83b3b2feba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from os import path\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from numpy import hstack\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa5f595a-dadf-40e3-825e-565ffa374f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b269f8b-4c91-46dc-ae7d-89561f42817f",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eef47abb-218a-4b06-8615-2d35dcb1afe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antibody_ID</th>\n",
       "      <th>heavy</th>\n",
       "      <th>light</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2073</th>\n",
       "      <td>6aod</td>\n",
       "      <td>EVQLVQSGAEVKKPGASVKVSCKASGYTFTGYYMHWVRQAPGQGLE...</td>\n",
       "      <td>DIVMTKSPSSLSASVGDRVTITCRASQGIRNDLGWYQQKPGKAPKR...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>4yny</td>\n",
       "      <td>EVQLVESGGGLVQPGRSLKLSCAASGFTFSNYGMAWVRQTPTKGLE...</td>\n",
       "      <td>EFVLTQPNSVSTNLGSTVKLSCKRSTGNIGSNYVNWYQQHEGRSPT...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>5xcv</td>\n",
       "      <td>EVQLVESGGGLVQPGRSLKLSCAASGFTFSNYGMAWVRQTPTKGLE...</td>\n",
       "      <td>QFVLTQPNSVSTNLGSTVKLSCKRSTGNIGSNYVNWYQQHEGRSPT...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2070</th>\n",
       "      <td>6and</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGYEFSRSWMNWVRQAPGKGLE...</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCRSSQSIVHSVGNTFLEWYQQKPG...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>2xqy</td>\n",
       "      <td>QVQLQQPGAELVKPGASVKMSCKASGYSFTSYWMNWVKQRPGRGLE...</td>\n",
       "      <td>DIVLTQSPASLALSLGQRATISCRASKSVSTSGYSYMYWYQQKPGQ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Antibody_ID                                              heavy  \\\n",
       "2073        6aod  EVQLVQSGAEVKKPGASVKVSCKASGYTFTGYYMHWVRQAPGQGLE...   \n",
       "1517        4yny  EVQLVESGGGLVQPGRSLKLSCAASGFTFSNYGMAWVRQTPTKGLE...   \n",
       "2025        5xcv  EVQLVESGGGLVQPGRSLKLSCAASGFTFSNYGMAWVRQTPTKGLE...   \n",
       "2070        6and  EVQLVESGGGLVQPGGSLRLSCAASGYEFSRSWMNWVRQAPGKGLE...   \n",
       "666         2xqy  QVQLQQPGAELVKPGASVKMSCKASGYSFTSYWMNWVKQRPGRGLE...   \n",
       "\n",
       "                                                  light  Y  \n",
       "2073  DIVMTKSPSSLSASVGDRVTITCRASQGIRNDLGWYQQKPGKAPKR...  0  \n",
       "1517  EFVLTQPNSVSTNLGSTVKLSCKRSTGNIGSNYVNWYQQHEGRSPT...  1  \n",
       "2025  QFVLTQPNSVSTNLGSTVKLSCKRSTGNIGSNYVNWYQQHEGRSPT...  1  \n",
       "2070  DIQMTQSPSSLSASVGDRVTITCRSSQSIVHSVGNTFLEWYQQKPG...  1  \n",
       "666   DIVLTQSPASLALSLGQRATISCRASKSVSTSGYSYMYWYQQKPGQ...  0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chen_train = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_train_data.csv\"), index_col=0)\n",
    "chen_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be3c2a46-9a70-4fcd-ad0f-175155f3a723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antibody_ID</th>\n",
       "      <th>heavy</th>\n",
       "      <th>light</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2169</th>\n",
       "      <td>6ct7</td>\n",
       "      <td>EVQLVESGGGLVEPGGSLRLSCAVSGFDFEKAWMSWVRQAPGQGLQ...</td>\n",
       "      <td>SYELTQPPSVSVSPGQTARITCSGEALPMQFAHWYQQRPGKAPVIV...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>4nzu</td>\n",
       "      <td>AVSLVESGGGTVEPGSTLRLSCAASGFTFGSYAFHWVRQAPGDGLE...</td>\n",
       "      <td>DIEMTQSPSSLSASTGDKVTITCQASQDIAKFLDWYQQRPGKTPKL...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>5i8c</td>\n",
       "      <td>QEVLVQSGAEVKKPGASVKVSCRAFGYTFTGNALHWVRQAPGQGLE...</td>\n",
       "      <td>DIQLTQSPSFLSASVGDKVTITCRASQGVRNELAWYQQKPGKAPNL...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>5i8e</td>\n",
       "      <td>QEVLVQSGAEVKKPGASVKVSCRAFGYTFTGNALHWVRQAPGQGLE...</td>\n",
       "      <td>IQLTQSPSFLSASVGDKVTITCRASQGVRNELAWYQQKPGKAPNLL...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2114</th>\n",
       "      <td>6bb4</td>\n",
       "      <td>QVQLQQSDAELVKPGASVKISCKASGYTFTDRTIHWVKQRPEQGLE...</td>\n",
       "      <td>DVQMIQSPSSLSASLGDIVTMTCQASQDTSINLNWFQQKPGKAPKL...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Antibody_ID                                              heavy  \\\n",
       "2169        6ct7  EVQLVESGGGLVEPGGSLRLSCAVSGFDFEKAWMSWVRQAPGQGLQ...   \n",
       "1342        4nzu  AVSLVESGGGTVEPGSTLRLSCAASGFTFGSYAFHWVRQAPGDGLE...   \n",
       "1728        5i8c  QEVLVQSGAEVKKPGASVKVSCRAFGYTFTGNALHWVRQAPGQGLE...   \n",
       "1729        5i8e  QEVLVQSGAEVKKPGASVKVSCRAFGYTFTGNALHWVRQAPGQGLE...   \n",
       "2114        6bb4  QVQLQQSDAELVKPGASVKISCKASGYTFTDRTIHWVKQRPEQGLE...   \n",
       "\n",
       "                                                  light  Y  \n",
       "2169  SYELTQPPSVSVSPGQTARITCSGEALPMQFAHWYQQRPGKAPVIV...  0  \n",
       "1342  DIEMTQSPSSLSASTGDKVTITCQASQDIAKFLDWYQQRPGKTPKL...  0  \n",
       "1728  DIQLTQSPSFLSASVGDKVTITCRASQGVRNELAWYQQKPGKAPNL...  1  \n",
       "1729  IQLTQSPSFLSASVGDKVTITCRASQGVRNELAWYQQKPGKAPNLL...  0  \n",
       "2114  DVQMIQSPSSLSASLGDIVTMTCQASQDTSINLNWFQQKPGKAPKL...  0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chen_valid = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_valid_data.csv\"), index_col=0)\n",
    "chen_test = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_test_data.csv\"), index_col=0)\n",
    "chen_valid = pd.concat([chen_valid, chen_test])\n",
    "chen_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4318d85e-94bc-413f-a8d4-b18996b63f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ab_ID</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2073</th>\n",
       "      <td>6aod</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>4yny</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>5xcv</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2070</th>\n",
       "      <td>6and</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>2xqy</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 282 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Ab_ID   0   1   2   3   4   5   6  7  8  ...  271  272  273  274  275  \\\n",
       "2073  6aod   4  18  14  10  18  14  16  6  1  ...    0    0    0    0    0   \n",
       "1517  4yny   4  18  14  10  18   4  16  6  6  ...    0    0    0    0    0   \n",
       "2025  5xcv   4  18  14  10  18   4  16  6  6  ...    0    0    0    0    0   \n",
       "2070  6and   4  18  14  10  18   4  16  6  6  ...    0    0    0    0    0   \n",
       "666   2xqy  14  18  14  10  14  14  13  6  1  ...    0    0    0    0    0   \n",
       "\n",
       "      276  277  278  279  280  \n",
       "2073    0    0    0    0    0  \n",
       "1517    0    0    0    0    0  \n",
       "2025    0    0    0    0    0  \n",
       "2070    0    0    0    0    0  \n",
       "666     0    0    0    0    0  \n",
       "\n",
       "[5 rows x 282 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_chen = pd.read_csv(path.join(DATA_DIR, \"chen/integer_encoding/chen_integer_encoded.csv\"), index_col=0)\n",
    "x_chen_train = x_chen.loc[chen_train.index]\n",
    "x_chen_test = x_chen.loc[chen_valid.index]\n",
    "x_chen_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc938cdf-ef94-47af-8b96-14d1dff94661",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = x_chen_train.drop(\"Ab_ID\", axis=1).to_numpy()\n",
    "X_test = x_chen_test.drop(\"Ab_ID\", axis=1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fba5116c-1ba9-405b-b961-12660916f4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = chen_train[\"Y\"].to_numpy()\n",
    "y_test = chen_valid[\"Y\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d53dde92-9a07-4338-9c2a-4768fb811ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281\n"
     ]
    }
   ],
   "source": [
    "max_length = len(X_train[0])\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5049197a-41c5-4113-ae92-f6f8a8abf083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(prediction, thresh=0.5):\n",
    "    res = []\n",
    "    for pred in prediction:\n",
    "        if pred < thresh:\n",
    "            res.append(0)\n",
    "        else:\n",
    "            res.append(1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96597863-f3bd-48ab-a0ae-6b3f671963f0",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84fccc85-e7c9-4b37-a59f-884cd22b169b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 09:58:35.298624: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(\n",
    "    50, activation='sigmoid', return_sequences=True, input_shape=(max_length, 1), kernel_regularizer=\"l2\",\n",
    "    recurrent_regularizer=\"l2\", bias_regularizer=\"l2\"\n",
    "))\n",
    "model.add(LSTM(50, activation='sigmoid', kernel_regularizer=\"l2\", recurrent_regularizer=\"l2\", bias_regularizer=\"l2\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer='adam', loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9da5ed78-c3cf-4379-af8f-a305a6802839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 281, 50)           10400     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 50)                20200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,651\n",
      "Trainable params: 30,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dfc5358c-9cad-4885-b179-990f1eb3cb9a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "11/11 [==============================] - 6s 317ms/step - loss: 3.2986\n",
      "Epoch 2/200\n",
      "11/11 [==============================] - 3s 317ms/step - loss: 2.8671\n",
      "Epoch 3/200\n",
      "11/11 [==============================] - 4s 319ms/step - loss: 2.5602\n",
      "Epoch 4/200\n",
      "11/11 [==============================] - 3s 308ms/step - loss: 2.3076\n",
      "Epoch 5/200\n",
      "11/11 [==============================] - 3s 316ms/step - loss: 2.1045\n",
      "Epoch 6/200\n",
      "11/11 [==============================] - 3s 316ms/step - loss: 1.9381\n",
      "Epoch 7/200\n",
      "11/11 [==============================] - 4s 320ms/step - loss: 1.8027\n",
      "Epoch 8/200\n",
      "11/11 [==============================] - 4s 317ms/step - loss: 1.6919\n",
      "Epoch 9/200\n",
      "11/11 [==============================] - 3s 318ms/step - loss: 1.6005\n",
      "Epoch 10/200\n",
      "11/11 [==============================] - 3s 311ms/step - loss: 1.5245\n",
      "Epoch 11/200\n",
      "11/11 [==============================] - 3s 313ms/step - loss: 1.4613\n",
      "Epoch 12/200\n",
      "11/11 [==============================] - 3s 311ms/step - loss: 1.4079\n",
      "Epoch 13/200\n",
      "11/11 [==============================] - 3s 313ms/step - loss: 1.3623\n",
      "Epoch 14/200\n",
      "11/11 [==============================] - 3s 312ms/step - loss: 1.3228\n",
      "Epoch 15/200\n",
      "11/11 [==============================] - 3s 317ms/step - loss: 1.2889\n",
      "Epoch 16/200\n",
      "11/11 [==============================] - 3s 310ms/step - loss: 1.2585\n",
      "Epoch 17/200\n",
      "11/11 [==============================] - 3s 310ms/step - loss: 1.2316\n",
      "Epoch 18/200\n",
      "11/11 [==============================] - 3s 312ms/step - loss: 1.2071\n",
      "Epoch 19/200\n",
      "11/11 [==============================] - 3s 310ms/step - loss: 1.1849\n",
      "Epoch 20/200\n",
      "11/11 [==============================] - 4s 329ms/step - loss: 1.1644\n",
      "Epoch 21/200\n",
      "11/11 [==============================] - 4s 325ms/step - loss: 1.1452\n",
      "Epoch 22/200\n",
      "11/11 [==============================] - 4s 326ms/step - loss: 1.1270\n",
      "Epoch 23/200\n",
      "11/11 [==============================] - 4s 331ms/step - loss: 1.1097\n",
      "Epoch 24/200\n",
      "11/11 [==============================] - 4s 344ms/step - loss: 1.0935\n",
      "Epoch 25/200\n",
      "11/11 [==============================] - 4s 327ms/step - loss: 1.0776\n",
      "Epoch 26/200\n",
      "11/11 [==============================] - 4s 324ms/step - loss: 1.0628\n",
      "Epoch 27/200\n",
      "11/11 [==============================] - 4s 322ms/step - loss: 1.0476\n",
      "Epoch 28/200\n",
      "11/11 [==============================] - 4s 340ms/step - loss: 1.0336\n",
      "Epoch 29/200\n",
      "11/11 [==============================] - 4s 378ms/step - loss: 1.0198\n",
      "Epoch 30/200\n",
      "11/11 [==============================] - 4s 328ms/step - loss: 1.0058\n",
      "Epoch 31/200\n",
      "11/11 [==============================] - 4s 333ms/step - loss: 0.9932\n",
      "Epoch 32/200\n",
      "11/11 [==============================] - 4s 324ms/step - loss: 0.9810\n",
      "Epoch 33/200\n",
      "11/11 [==============================] - 4s 327ms/step - loss: 0.9681\n",
      "Epoch 34/200\n",
      "11/11 [==============================] - 4s 335ms/step - loss: 0.9576\n",
      "Epoch 35/200\n",
      "11/11 [==============================] - 4s 329ms/step - loss: 0.9438\n",
      "Epoch 36/200\n",
      "11/11 [==============================] - 4s 338ms/step - loss: 0.9320\n",
      "Epoch 37/200\n",
      "11/11 [==============================] - 4s 343ms/step - loss: 0.9207\n",
      "Epoch 38/200\n",
      "11/11 [==============================] - 4s 339ms/step - loss: 0.9091\n",
      "Epoch 39/200\n",
      "11/11 [==============================] - 4s 334ms/step - loss: 0.8982\n",
      "Epoch 40/200\n",
      "11/11 [==============================] - 4s 327ms/step - loss: 0.8875\n",
      "Epoch 41/200\n",
      "11/11 [==============================] - 4s 330ms/step - loss: 0.8770\n",
      "Epoch 42/200\n",
      "11/11 [==============================] - 4s 334ms/step - loss: 0.8668\n",
      "Epoch 43/200\n",
      "11/11 [==============================] - 4s 327ms/step - loss: 0.8569\n",
      "Epoch 44/200\n",
      "11/11 [==============================] - 4s 328ms/step - loss: 0.8470\n",
      "Epoch 45/200\n",
      "11/11 [==============================] - 4s 324ms/step - loss: 0.8379\n",
      "Epoch 46/200\n",
      "11/11 [==============================] - 4s 330ms/step - loss: 0.8285\n",
      "Epoch 47/200\n",
      "11/11 [==============================] - 4s 327ms/step - loss: 0.8192\n",
      "Epoch 48/200\n",
      "11/11 [==============================] - 4s 329ms/step - loss: 0.8102\n",
      "Epoch 49/200\n",
      "11/11 [==============================] - 4s 324ms/step - loss: 0.8017\n",
      "Epoch 50/200\n",
      "11/11 [==============================] - 4s 329ms/step - loss: 0.7933\n",
      "Epoch 51/200\n",
      "11/11 [==============================] - 4s 326ms/step - loss: 0.7848\n",
      "Epoch 52/200\n",
      "11/11 [==============================] - 4s 327ms/step - loss: 0.7792\n",
      "Epoch 53/200\n",
      "11/11 [==============================] - 4s 324ms/step - loss: 0.7723\n",
      "Epoch 54/200\n",
      "11/11 [==============================] - 4s 333ms/step - loss: 0.7617\n",
      "Epoch 55/200\n",
      "11/11 [==============================] - 4s 331ms/step - loss: 0.7540\n",
      "Epoch 56/200\n",
      "11/11 [==============================] - 4s 324ms/step - loss: 0.7468\n",
      "Epoch 57/200\n",
      "11/11 [==============================] - 4s 326ms/step - loss: 0.7410\n",
      "Epoch 58/200\n",
      "11/11 [==============================] - 4s 326ms/step - loss: 0.7324\n",
      "Epoch 59/200\n",
      "11/11 [==============================] - 4s 325ms/step - loss: 0.7253\n",
      "Epoch 60/200\n",
      "11/11 [==============================] - 4s 331ms/step - loss: 0.7186\n",
      "Epoch 61/200\n",
      "11/11 [==============================] - 4s 353ms/step - loss: 0.7120\n",
      "Epoch 62/200\n",
      "11/11 [==============================] - 4s 339ms/step - loss: 0.7058\n",
      "Epoch 63/200\n",
      "11/11 [==============================] - 4s 325ms/step - loss: 0.7001\n",
      "Epoch 64/200\n",
      "11/11 [==============================] - 4s 326ms/step - loss: 0.6937\n",
      "Epoch 65/200\n",
      "11/11 [==============================] - 4s 328ms/step - loss: 0.6876\n",
      "Epoch 66/200\n",
      "11/11 [==============================] - 4s 340ms/step - loss: 0.6819\n",
      "Epoch 67/200\n",
      "11/11 [==============================] - 4s 327ms/step - loss: 0.6775\n",
      "Epoch 68/200\n",
      "11/11 [==============================] - 4s 323ms/step - loss: 0.6721\n",
      "Epoch 69/200\n",
      "11/11 [==============================] - 4s 323ms/step - loss: 0.6658\n",
      "Epoch 70/200\n",
      "11/11 [==============================] - 4s 333ms/step - loss: 0.6608\n",
      "Epoch 71/200\n",
      "11/11 [==============================] - 4s 326ms/step - loss: 0.6558\n",
      "Epoch 72/200\n",
      "11/11 [==============================] - 3s 315ms/step - loss: 0.6518\n",
      "Epoch 73/200\n",
      "11/11 [==============================] - 4s 327ms/step - loss: 0.6462\n",
      "Epoch 74/200\n",
      "11/11 [==============================] - 4s 324ms/step - loss: 0.6439\n",
      "Epoch 75/200\n",
      "11/11 [==============================] - 4s 326ms/step - loss: 0.6372\n",
      "Epoch 76/200\n",
      "11/11 [==============================] - 4s 327ms/step - loss: 0.6338\n",
      "Epoch 77/200\n",
      "11/11 [==============================] - 4s 325ms/step - loss: 0.6287\n",
      "Epoch 78/200\n",
      "11/11 [==============================] - 4s 324ms/step - loss: 0.6248\n",
      "Epoch 79/200\n",
      "11/11 [==============================] - 4s 336ms/step - loss: 0.6216\n",
      "Epoch 80/200\n",
      "11/11 [==============================] - 4s 327ms/step - loss: 0.6167\n",
      "Epoch 81/200\n",
      "11/11 [==============================] - 4s 328ms/step - loss: 0.6133\n",
      "Epoch 82/200\n",
      "11/11 [==============================] - 4s 334ms/step - loss: 0.6095\n",
      "Epoch 83/200\n",
      "11/11 [==============================] - 4s 325ms/step - loss: 0.6058\n",
      "Epoch 84/200\n",
      "11/11 [==============================] - 4s 341ms/step - loss: 0.6028\n",
      "Epoch 85/200\n",
      "11/11 [==============================] - 4s 331ms/step - loss: 0.5993\n",
      "Epoch 86/200\n",
      "11/11 [==============================] - 4s 329ms/step - loss: 0.5964\n",
      "Epoch 87/200\n",
      "11/11 [==============================] - 4s 321ms/step - loss: 0.5929\n",
      "Epoch 88/200\n",
      "11/11 [==============================] - 4s 327ms/step - loss: 0.5905\n",
      "Epoch 89/200\n",
      "11/11 [==============================] - 4s 325ms/step - loss: 0.5883\n",
      "Epoch 90/200\n",
      "11/11 [==============================] - 4s 323ms/step - loss: 0.5849\n",
      "Epoch 91/200\n",
      "11/11 [==============================] - 4s 328ms/step - loss: 0.5816\n",
      "Epoch 92/200\n",
      "11/11 [==============================] - 4s 331ms/step - loss: 0.5791\n",
      "Epoch 93/200\n",
      "11/11 [==============================] - 4s 338ms/step - loss: 0.5766\n",
      "Epoch 94/200\n",
      "11/11 [==============================] - 4s 327ms/step - loss: 0.5741\n",
      "Epoch 95/200\n",
      "11/11 [==============================] - 4s 329ms/step - loss: 0.5717\n",
      "Epoch 96/200\n",
      "11/11 [==============================] - 4s 356ms/step - loss: 0.5692\n",
      "Epoch 97/200\n",
      "11/11 [==============================] - 4s 324ms/step - loss: 0.5675\n",
      "Epoch 98/200\n",
      "11/11 [==============================] - 4s 330ms/step - loss: 0.5647\n",
      "Epoch 99/200\n",
      "11/11 [==============================] - 4s 333ms/step - loss: 0.5628\n",
      "Epoch 100/200\n",
      "11/11 [==============================] - 4s 325ms/step - loss: 0.5606\n",
      "Epoch 101/200\n",
      "11/11 [==============================] - 4s 333ms/step - loss: 0.5588\n",
      "Epoch 102/200\n",
      "11/11 [==============================] - 4s 336ms/step - loss: 0.5571\n",
      "Epoch 103/200\n",
      "11/11 [==============================] - 4s 351ms/step - loss: 0.5551\n",
      "Epoch 104/200\n",
      "11/11 [==============================] - 4s 356ms/step - loss: 0.5534\n",
      "Epoch 105/200\n",
      "11/11 [==============================] - 4s 346ms/step - loss: 0.5524\n",
      "Epoch 106/200\n",
      "11/11 [==============================] - 4s 322ms/step - loss: 0.5504\n",
      "Epoch 107/200\n",
      "11/11 [==============================] - 4s 327ms/step - loss: 0.5485\n",
      "Epoch 108/200\n",
      "11/11 [==============================] - 4s 339ms/step - loss: 0.5470\n",
      "Epoch 109/200\n",
      "11/11 [==============================] - 4s 336ms/step - loss: 0.5458\n",
      "Epoch 110/200\n",
      "11/11 [==============================] - 4s 335ms/step - loss: 0.5445\n",
      "Epoch 111/200\n",
      "11/11 [==============================] - 4s 331ms/step - loss: 0.5441\n",
      "Epoch 112/200\n",
      "11/11 [==============================] - 4s 324ms/step - loss: 0.5424\n",
      "Epoch 113/200\n",
      "11/11 [==============================] - 4s 325ms/step - loss: 0.5402\n",
      "Epoch 114/200\n",
      "11/11 [==============================] - 4s 333ms/step - loss: 0.5395\n",
      "Epoch 115/200\n",
      "11/11 [==============================] - 4s 325ms/step - loss: 0.5384\n",
      "Epoch 116/200\n",
      "11/11 [==============================] - 4s 326ms/step - loss: 0.5368\n",
      "Epoch 117/200\n",
      "11/11 [==============================] - 4s 326ms/step - loss: 0.5358\n",
      "Epoch 118/200\n",
      "11/11 [==============================] - 4s 329ms/step - loss: 0.5349\n",
      "Epoch 119/200\n",
      "11/11 [==============================] - 4s 326ms/step - loss: 0.5340\n",
      "Epoch 120/200\n",
      "11/11 [==============================] - 4s 329ms/step - loss: 0.5329\n",
      "Epoch 121/200\n",
      "11/11 [==============================] - 4s 334ms/step - loss: 0.5320\n",
      "Epoch 122/200\n",
      "11/11 [==============================] - 4s 326ms/step - loss: 0.5311\n",
      "Epoch 123/200\n",
      "11/11 [==============================] - 4s 331ms/step - loss: 0.5312\n",
      "Epoch 124/200\n",
      "11/11 [==============================] - 4s 327ms/step - loss: 0.5295\n",
      "Epoch 125/200\n",
      "11/11 [==============================] - 4s 329ms/step - loss: 0.5290\n",
      "Epoch 126/200\n",
      "11/11 [==============================] - 4s 339ms/step - loss: 0.5280\n",
      "Epoch 127/200\n",
      "11/11 [==============================] - 4s 327ms/step - loss: 0.5275\n",
      "Epoch 128/200\n",
      "11/11 [==============================] - 4s 325ms/step - loss: 0.5266\n",
      "Epoch 129/200\n",
      "11/11 [==============================] - 4s 340ms/step - loss: 0.5262\n",
      "Epoch 130/200\n",
      "11/11 [==============================] - 4s 329ms/step - loss: 0.5257\n",
      "Epoch 131/200\n",
      "11/11 [==============================] - 4s 328ms/step - loss: 0.5248\n",
      "Epoch 132/200\n",
      "11/11 [==============================] - 4s 332ms/step - loss: 0.5244\n",
      "Epoch 133/200\n",
      "11/11 [==============================] - 4s 333ms/step - loss: 0.5238\n",
      "Epoch 134/200\n",
      "11/11 [==============================] - 4s 329ms/step - loss: 0.5235\n",
      "Epoch 135/200\n",
      "11/11 [==============================] - 4s 321ms/step - loss: 0.5232\n",
      "Epoch 136/200\n",
      "11/11 [==============================] - 4s 329ms/step - loss: 0.5225\n",
      "Epoch 137/200\n",
      "11/11 [==============================] - 4s 341ms/step - loss: 0.5220\n",
      "Epoch 138/200\n",
      "11/11 [==============================] - 4s 334ms/step - loss: 0.5224\n",
      "Epoch 139/200\n",
      "11/11 [==============================] - 4s 333ms/step - loss: 0.5213\n",
      "Epoch 140/200\n",
      "11/11 [==============================] - 4s 333ms/step - loss: 0.5212\n",
      "Epoch 141/200\n",
      "11/11 [==============================] - 4s 335ms/step - loss: 0.5204\n",
      "Epoch 142/200\n",
      "11/11 [==============================] - 4s 326ms/step - loss: 0.5200\n",
      "Epoch 143/200\n",
      "11/11 [==============================] - 4s 334ms/step - loss: 0.5197\n",
      "Epoch 144/200\n",
      "11/11 [==============================] - 4s 321ms/step - loss: 0.5212\n",
      "Epoch 145/200\n",
      "11/11 [==============================] - 4s 332ms/step - loss: 0.5192\n",
      "Epoch 146/200\n",
      "11/11 [==============================] - 4s 329ms/step - loss: 0.5190\n",
      "Epoch 147/200\n",
      "11/11 [==============================] - 4s 335ms/step - loss: 0.5185\n",
      "Epoch 148/200\n",
      "11/11 [==============================] - 4s 328ms/step - loss: 0.5183\n",
      "Epoch 149/200\n",
      "11/11 [==============================] - 4s 328ms/step - loss: 0.5181\n",
      "Epoch 150/200\n",
      "11/11 [==============================] - 4s 325ms/step - loss: 0.5178\n",
      "Epoch 151/200\n",
      "11/11 [==============================] - 4s 323ms/step - loss: 0.5179\n",
      "Epoch 152/200\n",
      "11/11 [==============================] - 4s 328ms/step - loss: 0.5178\n",
      "Epoch 153/200\n",
      "11/11 [==============================] - 4s 326ms/step - loss: 0.5172\n",
      "Epoch 154/200\n",
      "11/11 [==============================] - 4s 330ms/step - loss: 0.5179\n",
      "Epoch 155/200\n",
      "11/11 [==============================] - 4s 328ms/step - loss: 0.5180\n",
      "Epoch 156/200\n",
      "11/11 [==============================] - 4s 329ms/step - loss: 0.5166\n",
      "Epoch 157/200\n",
      "11/11 [==============================] - 4s 333ms/step - loss: 0.5173\n",
      "Epoch 158/200\n",
      "11/11 [==============================] - 4s 325ms/step - loss: 0.5161\n",
      "Epoch 159/200\n",
      "11/11 [==============================] - 4s 330ms/step - loss: 0.5168\n",
      "Epoch 160/200\n",
      "11/11 [==============================] - 4s 329ms/step - loss: 0.5165\n",
      "Epoch 161/200\n",
      "11/11 [==============================] - 4s 331ms/step - loss: 0.5160\n",
      "Epoch 162/200\n",
      "11/11 [==============================] - 4s 329ms/step - loss: 0.5160\n",
      "Epoch 163/200\n",
      "11/11 [==============================] - 4s 326ms/step - loss: 0.5157\n",
      "Epoch 164/200\n",
      "11/11 [==============================] - 4s 324ms/step - loss: 0.5159\n",
      "Epoch 165/200\n",
      "11/11 [==============================] - 4s 331ms/step - loss: 0.5158\n",
      "Epoch 166/200\n",
      "11/11 [==============================] - 4s 331ms/step - loss: 0.5155\n",
      "Epoch 167/200\n",
      "11/11 [==============================] - 4s 331ms/step - loss: 0.5158\n",
      "Epoch 168/200\n",
      "11/11 [==============================] - 4s 329ms/step - loss: 0.5158\n",
      "Epoch 169/200\n",
      "11/11 [==============================] - 4s 333ms/step - loss: 0.5159\n",
      "Epoch 170/200\n",
      "11/11 [==============================] - 4s 331ms/step - loss: 0.5156\n",
      "Epoch 171/200\n",
      "11/11 [==============================] - 4s 322ms/step - loss: 0.5151\n",
      "Epoch 172/200\n",
      "11/11 [==============================] - 4s 336ms/step - loss: 0.5151\n",
      "Epoch 173/200\n",
      "11/11 [==============================] - 4s 334ms/step - loss: 0.5149\n",
      "Epoch 174/200\n",
      "11/11 [==============================] - 4s 320ms/step - loss: 0.5151\n",
      "Epoch 175/200\n",
      "11/11 [==============================] - 4s 323ms/step - loss: 0.5149\n",
      "Epoch 176/200\n",
      "11/11 [==============================] - 4s 329ms/step - loss: 0.5152\n",
      "Epoch 177/200\n",
      "11/11 [==============================] - 4s 330ms/step - loss: 0.5150\n",
      "Epoch 178/200\n",
      "11/11 [==============================] - 4s 336ms/step - loss: 0.5147\n",
      "Epoch 179/200\n",
      "11/11 [==============================] - 4s 323ms/step - loss: 0.5149\n",
      "Epoch 180/200\n",
      "11/11 [==============================] - 4s 331ms/step - loss: 0.5156\n",
      "Epoch 181/200\n",
      "11/11 [==============================] - 4s 334ms/step - loss: 0.5144\n",
      "Epoch 182/200\n",
      "11/11 [==============================] - 4s 327ms/step - loss: 0.5154\n",
      "Epoch 183/200\n",
      "11/11 [==============================] - 4s 327ms/step - loss: 0.5145\n",
      "Epoch 184/200\n",
      "11/11 [==============================] - 4s 323ms/step - loss: 0.5146\n",
      "Epoch 185/200\n",
      "11/11 [==============================] - 4s 327ms/step - loss: 0.5146\n",
      "Epoch 186/200\n",
      "11/11 [==============================] - 4s 327ms/step - loss: 0.5146\n",
      "Epoch 187/200\n",
      "11/11 [==============================] - 4s 334ms/step - loss: 0.5144\n",
      "Epoch 188/200\n",
      "11/11 [==============================] - 4s 334ms/step - loss: 0.5148\n",
      "Epoch 189/200\n",
      "11/11 [==============================] - 4s 334ms/step - loss: 0.5146\n",
      "Epoch 190/200\n",
      "11/11 [==============================] - 4s 329ms/step - loss: 0.5146\n",
      "Epoch 191/200\n",
      "11/11 [==============================] - 4s 330ms/step - loss: 0.5141\n",
      "Epoch 192/200\n",
      "11/11 [==============================] - 4s 328ms/step - loss: 0.5157\n",
      "Epoch 193/200\n",
      "11/11 [==============================] - 4s 325ms/step - loss: 0.5147\n",
      "Epoch 194/200\n",
      "11/11 [==============================] - 4s 327ms/step - loss: 0.5148\n",
      "Epoch 195/200\n",
      "11/11 [==============================] - 4s 331ms/step - loss: 0.5142\n",
      "Epoch 196/200\n",
      "11/11 [==============================] - 4s 346ms/step - loss: 0.5149\n",
      "Epoch 197/200\n",
      "11/11 [==============================] - 4s 337ms/step - loss: 0.5143\n",
      "Epoch 198/200\n",
      "11/11 [==============================] - 4s 329ms/step - loss: 0.5148\n",
      "Epoch 199/200\n",
      "11/11 [==============================] - 4s 329ms/step - loss: 0.5156\n",
      "Epoch 200/200\n",
      "11/11 [==============================] - 4s 335ms/step - loss: 0.5141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fecb7595650>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=200, verbose=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5bc67358-36e0-4877-b5a2-6f684fdc6261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239/239 [==============================] - 7s 29ms/step - loss: 0.5075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5074639916419983"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920e77e6-e7de-4089-ba87-2e6f2dd7a430",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c043f9ee-c144-4dbe-83d9-4e0295e64dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(\n",
    "    50, activation='relu', return_sequences=True, input_shape=(max_length, 1), kernel_regularizer=\"l2\",\n",
    "    recurrent_regularizer=\"l2\", bias_regularizer=\"l2\"\n",
    "))\n",
    "model.add(LSTM(50, activation='relu', kernel_regularizer=\"l2\", recurrent_regularizer=\"l2\", bias_regularizer=\"l2\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer='adam', loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "515ce017-9ea5-4efd-a967-96b5e5a7d39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 281, 50)           10400     \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 50)                20200     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,651\n",
      "Trainable params: 30,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71645c09-d1f2-4c05-80b5-fd755cde9519",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "21/21 [==============================] - 7s 244ms/step - loss: 612956672.0000\n",
      "Epoch 2/200\n",
      "21/21 [==============================] - 5s 236ms/step - loss: 3.4039\n",
      "Epoch 3/200\n",
      "21/21 [==============================] - 5s 238ms/step - loss: 3.3552\n",
      "Epoch 4/200\n",
      "21/21 [==============================] - 5s 245ms/step - loss: 3.3202\n",
      "Epoch 5/200\n",
      "21/21 [==============================] - 5s 237ms/step - loss: 3.2948\n",
      "Epoch 6/200\n",
      "21/21 [==============================] - 5s 241ms/step - loss: 3.2754\n",
      "Epoch 7/200\n",
      "21/21 [==============================] - 5s 247ms/step - loss: 3.2599\n",
      "Epoch 8/200\n",
      "21/21 [==============================] - 5s 236ms/step - loss: 3.2471\n",
      "Epoch 9/200\n",
      "21/21 [==============================] - 5s 240ms/step - loss: 3.2353\n",
      "Epoch 10/200\n",
      "21/21 [==============================] - 5s 241ms/step - loss: 3.2236\n",
      "Epoch 11/200\n",
      "21/21 [==============================] - 5s 237ms/step - loss: 3.2103\n",
      "Epoch 12/200\n",
      "21/21 [==============================] - 5s 239ms/step - loss: 3.1900\n",
      "Epoch 13/200\n",
      "21/21 [==============================] - 5s 236ms/step - loss: 3.1368\n",
      "Epoch 14/200\n",
      "21/21 [==============================] - 5s 238ms/step - loss: 3.1114\n",
      "Epoch 15/200\n",
      "21/21 [==============================] - 5s 242ms/step - loss: 3.1047\n",
      "Epoch 16/200\n",
      "21/21 [==============================] - 5s 234ms/step - loss: 3.1004\n",
      "Epoch 17/200\n",
      "21/21 [==============================] - 5s 238ms/step - loss: 3.0975\n",
      "Epoch 18/200\n",
      "21/21 [==============================] - 5s 236ms/step - loss: 3.0943\n",
      "Epoch 19/200\n",
      "21/21 [==============================] - 5s 241ms/step - loss: 3.0922\n",
      "Epoch 20/200\n",
      "21/21 [==============================] - 5s 241ms/step - loss: 3.0918\n",
      "Epoch 21/200\n",
      "21/21 [==============================] - 5s 238ms/step - loss: 3.0885\n",
      "Epoch 22/200\n",
      "21/21 [==============================] - 5s 233ms/step - loss: 3.0863\n",
      "Epoch 23/200\n",
      "21/21 [==============================] - 5s 242ms/step - loss: 3.0850\n",
      "Epoch 24/200\n",
      "21/21 [==============================] - 5s 238ms/step - loss: 3.0825\n",
      "Epoch 25/200\n",
      "21/21 [==============================] - 5s 241ms/step - loss: 3.0804\n",
      "Epoch 26/200\n",
      "21/21 [==============================] - 5s 247ms/step - loss: 3.0784\n",
      "Epoch 27/200\n",
      "21/21 [==============================] - 5s 242ms/step - loss: 3.0771\n",
      "Epoch 28/200\n",
      "21/21 [==============================] - 5s 241ms/step - loss: 3.0754\n",
      "Epoch 29/200\n",
      "21/21 [==============================] - 5s 239ms/step - loss: 3.0736\n",
      "Epoch 30/200\n",
      "21/21 [==============================] - 5s 238ms/step - loss: 3.0732\n",
      "Epoch 31/200\n",
      "21/21 [==============================] - 5s 244ms/step - loss: 3.0738\n",
      "Epoch 32/200\n",
      "21/21 [==============================] - 5s 241ms/step - loss: 3.0712\n",
      "Epoch 33/200\n",
      "21/21 [==============================] - 5s 235ms/step - loss: 3.0681\n",
      "Epoch 34/200\n",
      "21/21 [==============================] - 5s 236ms/step - loss: 3.0736\n",
      "Epoch 35/200\n",
      "21/21 [==============================] - 5s 235ms/step - loss: 3.0667\n",
      "Epoch 36/200\n",
      "21/21 [==============================] - 5s 240ms/step - loss: 3.0642\n",
      "Epoch 37/200\n",
      "21/21 [==============================] - 5s 244ms/step - loss: 3.0645\n",
      "Epoch 38/200\n",
      "21/21 [==============================] - 5s 250ms/step - loss: 3.0632\n",
      "Epoch 39/200\n",
      "21/21 [==============================] - 5s 255ms/step - loss: 3.0632\n",
      "Epoch 40/200\n",
      "21/21 [==============================] - 5s 251ms/step - loss: 3.0738\n",
      "Epoch 41/200\n",
      "21/21 [==============================] - 5s 252ms/step - loss: 3.0651\n",
      "Epoch 42/200\n",
      "21/21 [==============================] - 5s 243ms/step - loss: 3.0625\n",
      "Epoch 43/200\n",
      "21/21 [==============================] - 5s 239ms/step - loss: 3.0611\n",
      "Epoch 44/200\n",
      "21/21 [==============================] - 5s 239ms/step - loss: 3.0605\n",
      "Epoch 45/200\n",
      "21/21 [==============================] - 5s 239ms/step - loss: 3.0602\n",
      "Epoch 46/200\n",
      "21/21 [==============================] - 5s 238ms/step - loss: 3.0576\n",
      "Epoch 47/200\n",
      "21/21 [==============================] - 5s 239ms/step - loss: 3.0560\n",
      "Epoch 48/200\n",
      "21/21 [==============================] - 5s 241ms/step - loss: 3.0537\n",
      "Epoch 49/200\n",
      "21/21 [==============================] - 5s 239ms/step - loss: 3.0530\n",
      "Epoch 50/200\n",
      "21/21 [==============================] - 5s 240ms/step - loss: 3.0547\n",
      "Epoch 51/200\n",
      "21/21 [==============================] - 5s 240ms/step - loss: 3.0513\n",
      "Epoch 52/200\n",
      "21/21 [==============================] - 5s 239ms/step - loss: 3.1732\n",
      "Epoch 53/200\n",
      "21/21 [==============================] - 5s 242ms/step - loss: 3.0965\n",
      "Epoch 54/200\n",
      "21/21 [==============================] - 5s 243ms/step - loss: 3.0998\n",
      "Epoch 55/200\n",
      "21/21 [==============================] - 5s 243ms/step - loss: 3.0886\n",
      "Epoch 56/200\n",
      "21/21 [==============================] - 5s 239ms/step - loss: 3.0738\n",
      "Epoch 57/200\n",
      "21/21 [==============================] - 5s 240ms/step - loss: 3.0654\n",
      "Epoch 58/200\n",
      "21/21 [==============================] - 5s 242ms/step - loss: 3.0645\n",
      "Epoch 59/200\n",
      "21/21 [==============================] - 5s 237ms/step - loss: 3.0634\n",
      "Epoch 60/200\n",
      "21/21 [==============================] - 5s 238ms/step - loss: 3.0619\n",
      "Epoch 61/200\n",
      "21/21 [==============================] - 5s 238ms/step - loss: 3.0613\n",
      "Epoch 62/200\n",
      " 7/21 [=========>....................] - ETA: 3s - loss: 3.0385"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/pbs.32262.lich-compute.vscht.cz/ipykernel_26906/2886491785.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=200, verbose=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdb5ad67-e4e8-4ade-ad3d-e8304e9e5fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239/239 [==============================] - 7s 29ms/step - loss: 3.0515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.0515201091766357"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2963b620-cedc-4c9e-9dc8-fe39f6626fc5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20992106],\n",
       "       [0.20791924],\n",
       "       [0.20984468],\n",
       "       [0.21016249],\n",
       "       [0.21216676],\n",
       "       [0.21036375],\n",
       "       [0.21095589],\n",
       "       [0.21002048],\n",
       "       [0.20772734],\n",
       "       [0.20795539],\n",
       "       [0.21061528],\n",
       "       [0.21096697],\n",
       "       [0.19717345],\n",
       "       [0.19761881],\n",
       "       [0.21084765],\n",
       "       [0.211494  ],\n",
       "       [0.21031821],\n",
       "       [0.21067479],\n",
       "       [0.20984936],\n",
       "       [0.21006978],\n",
       "       [0.20541656],\n",
       "       [0.20973724],\n",
       "       [0.20973724],\n",
       "       [0.21127683],\n",
       "       [0.21076229],\n",
       "       [0.21076572],\n",
       "       [0.20804015],\n",
       "       [0.18784446],\n",
       "       [0.21132302],\n",
       "       [0.21070066],\n",
       "       [0.2109569 ],\n",
       "       [0.21123934],\n",
       "       [0.20929185],\n",
       "       [0.21117595],\n",
       "       [0.21117586],\n",
       "       [0.21108863],\n",
       "       [0.21083713],\n",
       "       [0.21036819],\n",
       "       [0.2104238 ],\n",
       "       [0.21092072],\n",
       "       [0.21110901],\n",
       "       [0.21186692],\n",
       "       [0.20862168],\n",
       "       [0.21096504],\n",
       "       [0.20865539],\n",
       "       [0.20658135],\n",
       "       [0.1993657 ],\n",
       "       [0.21072653],\n",
       "       [0.21072647],\n",
       "       [0.20941553],\n",
       "       [0.20909801],\n",
       "       [0.21243897],\n",
       "       [0.21125904],\n",
       "       [0.20456883],\n",
       "       [0.20229614],\n",
       "       [0.20965597],\n",
       "       [0.21218029],\n",
       "       [0.2121802 ],\n",
       "       [0.21213487],\n",
       "       [0.21220547],\n",
       "       [0.21215892],\n",
       "       [0.20927688],\n",
       "       [0.21142489],\n",
       "       [0.21100864],\n",
       "       [0.21092954],\n",
       "       [0.21000877],\n",
       "       [0.21126801],\n",
       "       [0.21035644],\n",
       "       [0.21206713],\n",
       "       [0.20799977],\n",
       "       [0.20283198],\n",
       "       [0.20100191],\n",
       "       [0.21025383],\n",
       "       [0.21032953],\n",
       "       [0.21189848],\n",
       "       [0.2035    ],\n",
       "       [0.1992738 ],\n",
       "       [0.2098195 ],\n",
       "       [0.21135646],\n",
       "       [0.21143621],\n",
       "       [0.21194929],\n",
       "       [0.2105366 ],\n",
       "       [0.21158385],\n",
       "       [0.2082245 ],\n",
       "       [0.20846575],\n",
       "       [0.20738298],\n",
       "       [0.18499148],\n",
       "       [0.18499199],\n",
       "       [0.21119213],\n",
       "       [0.20892072],\n",
       "       [0.20552701],\n",
       "       [0.20742819],\n",
       "       [0.21232933],\n",
       "       [0.21247068],\n",
       "       [0.20032793],\n",
       "       [0.20301688],\n",
       "       [0.20032793],\n",
       "       [0.20966506],\n",
       "       [0.21068957],\n",
       "       [0.21072984],\n",
       "       [0.2117554 ],\n",
       "       [0.2117554 ],\n",
       "       [0.21150541],\n",
       "       [0.20590511],\n",
       "       [0.21093622],\n",
       "       [0.20055899],\n",
       "       [0.19750252],\n",
       "       [0.19758302],\n",
       "       [0.21074253],\n",
       "       [0.21172214],\n",
       "       [0.20971131],\n",
       "       [0.20921773],\n",
       "       [0.21143314],\n",
       "       [0.19916943],\n",
       "       [0.39586365],\n",
       "       [0.2108056 ],\n",
       "       [0.20917192],\n",
       "       [0.20986801],\n",
       "       [0.21108627],\n",
       "       [0.20667061],\n",
       "       [0.21171212],\n",
       "       [0.20815578],\n",
       "       [0.20815584],\n",
       "       [0.21079448],\n",
       "       [0.19609529],\n",
       "       [0.21096352],\n",
       "       [0.21188691],\n",
       "       [0.21070424],\n",
       "       [0.21158409],\n",
       "       [0.21036676],\n",
       "       [0.21036893],\n",
       "       [0.21064967],\n",
       "       [0.21036643],\n",
       "       [0.20676911],\n",
       "       [0.20666447],\n",
       "       [0.20923874],\n",
       "       [0.20116991],\n",
       "       [0.21035713],\n",
       "       [0.21054092],\n",
       "       [0.21128854],\n",
       "       [0.21226174],\n",
       "       [0.20947096],\n",
       "       [0.21156839],\n",
       "       [0.20925394],\n",
       "       [0.21150365],\n",
       "       [0.20149788],\n",
       "       [0.20149654],\n",
       "       [0.19318333],\n",
       "       [0.19318333],\n",
       "       [0.19318333],\n",
       "       [0.21042088],\n",
       "       [0.21078983],\n",
       "       [0.21126312],\n",
       "       [0.21240887],\n",
       "       [0.20459154],\n",
       "       [0.20994574],\n",
       "       [0.21020734],\n",
       "       [0.21120554],\n",
       "       [0.20805463],\n",
       "       [0.20822197],\n",
       "       [0.2113826 ],\n",
       "       [0.20959648],\n",
       "       [0.20603591],\n",
       "       [0.20605004],\n",
       "       [0.19753999],\n",
       "       [0.21174997],\n",
       "       [0.21056303],\n",
       "       [0.20680463],\n",
       "       [0.2114844 ],\n",
       "       [0.20824012],\n",
       "       [0.20603624],\n",
       "       [0.21020752],\n",
       "       [0.21081403],\n",
       "       [0.20919648],\n",
       "       [0.20995271],\n",
       "       [0.21128193],\n",
       "       [0.21104455],\n",
       "       [0.21144241],\n",
       "       [0.2110995 ],\n",
       "       [0.21102124],\n",
       "       [0.2110995 ],\n",
       "       [0.20965895],\n",
       "       [0.21016705],\n",
       "       [0.2095361 ],\n",
       "       [0.20841745],\n",
       "       [0.21016523],\n",
       "       [0.20951512],\n",
       "       [0.19475445],\n",
       "       [0.19418675],\n",
       "       [0.21112469],\n",
       "       [0.21189734],\n",
       "       [0.21057594],\n",
       "       [0.20410535],\n",
       "       [0.20876685],\n",
       "       [0.2087956 ],\n",
       "       [0.2016645 ],\n",
       "       [0.19253877],\n",
       "       [0.20950359],\n",
       "       [0.2112824 ],\n",
       "       [0.20642298],\n",
       "       [0.20643428],\n",
       "       [0.20751172],\n",
       "       [0.20894045],\n",
       "       [0.21035415],\n",
       "       [0.20664507],\n",
       "       [0.20664507],\n",
       "       [0.20967755],\n",
       "       [0.21178004],\n",
       "       [0.20845637],\n",
       "       [0.20870447],\n",
       "       [0.20747995],\n",
       "       [0.20876521],\n",
       "       [0.2113412 ],\n",
       "       [0.20518297],\n",
       "       [0.20865539],\n",
       "       [0.2090643 ],\n",
       "       [0.21042922],\n",
       "       [0.21108982],\n",
       "       [0.2121706 ],\n",
       "       [0.21070468],\n",
       "       [0.20916855],\n",
       "       [0.20916855],\n",
       "       [0.2108843 ],\n",
       "       [0.21085069],\n",
       "       [0.21023962],\n",
       "       [0.21135303],\n",
       "       [0.21021762],\n",
       "       [0.2018021 ],\n",
       "       [0.21131614],\n",
       "       [0.20418787],\n",
       "       [0.20418686],\n",
       "       [0.21118161],\n",
       "       [0.21193668],\n",
       "       [0.20657796],\n",
       "       [0.20974097],\n",
       "       [0.20975563],\n",
       "       [0.20717663],\n",
       "       [0.20686454],\n",
       "       [0.21038017]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750c4b01-0370-4da9-a89f-6299e620e50e",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ced16c71-7883-4daa-871c-a82c6b0cfe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(200, activation=\"relu\", input_shape=(max_length, 1), return_sequences=True))  \n",
    "model.add(Dropout(0.2))  \n",
    "model.add(LSTM(200, activation=\"relu\", return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fdacb2a0-eebf-44b1-9d98-135ddbf2cc62",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 9.4642\n",
      "Epoch 2/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 11.9568\n",
      "Epoch 3/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 12.0353\n",
      "Epoch 4/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 12.0467\n",
      "Epoch 5/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 12.0467\n",
      "Epoch 6/200\n",
      "11/11 [==============================] - 32s 3s/step - loss: 12.0353\n",
      "Epoch 7/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 12.0353\n",
      "Epoch 8/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 12.0354\n",
      "Epoch 9/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 12.0467\n",
      "Epoch 10/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 12.0467\n",
      "Epoch 11/200\n",
      "11/11 [==============================] - 32s 3s/step - loss: 12.0353\n",
      "Epoch 12/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 12.0353\n",
      "Epoch 13/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 12.0239\n",
      "Epoch 14/200\n",
      "11/11 [==============================] - 31s 3s/step - loss: 12.0467\n",
      "Epoch 15/200\n",
      "11/11 [==============================] - 32s 3s/step - loss: 12.0353\n",
      "Epoch 16/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 12.0239\n",
      "Epoch 17/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 12.0468\n",
      "Epoch 18/200\n",
      "11/11 [==============================] - 32s 3s/step - loss: 12.0582\n",
      "Epoch 19/200\n",
      "11/11 [==============================] - 32s 3s/step - loss: 12.0467\n",
      "Epoch 20/200\n",
      "11/11 [==============================] - 32s 3s/step - loss: 12.0467\n",
      "Epoch 21/200\n",
      "11/11 [==============================] - 32s 3s/step - loss: 12.0353\n",
      "Epoch 22/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 12.0467\n",
      "Epoch 23/200\n",
      "11/11 [==============================] - 32s 3s/step - loss: 12.0467\n",
      "Epoch 24/200\n",
      "11/11 [==============================] - 32s 3s/step - loss: 12.0353\n",
      "Epoch 25/200\n",
      "11/11 [==============================] - 32s 3s/step - loss: 12.0467\n",
      "Epoch 26/200\n",
      "11/11 [==============================] - 32s 3s/step - loss: 12.0467\n",
      "Epoch 27/200\n",
      "11/11 [==============================] - 32s 3s/step - loss: 12.0467\n",
      "Epoch 28/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 12.0467\n",
      "Epoch 29/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 12.0467\n",
      "Epoch 30/200\n",
      "11/11 [==============================] - 32s 3s/step - loss: 12.0467\n",
      "Epoch 31/200\n",
      "11/11 [==============================] - 32s 3s/step - loss: 12.0467\n",
      "Epoch 32/200\n",
      "11/11 [==============================] - 32s 3s/step - loss: 12.0467\n",
      "Epoch 33/200\n",
      "11/11 [==============================] - 32s 3s/step - loss: 12.0353\n",
      "Epoch 34/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 12.0467\n",
      "Epoch 35/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 12.0467\n",
      "Epoch 36/200\n",
      "11/11 [==============================] - 32s 3s/step - loss: 12.0353\n",
      "Epoch 37/200\n",
      "11/11 [==============================] - 32s 3s/step - loss: 12.0467\n",
      "Epoch 38/200\n",
      "11/11 [==============================] - 32s 3s/step - loss: 12.0353\n",
      "Epoch 39/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 12.0353\n",
      "Epoch 40/200\n",
      "11/11 [==============================] - 32s 3s/step - loss: 12.0467\n",
      "Epoch 41/200\n",
      "11/11 [==============================] - 32s 3s/step - loss: 12.0467\n",
      "Epoch 42/200\n",
      "11/11 [==============================] - 32s 3s/step - loss: 12.0353\n",
      "Epoch 43/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 12.0353\n",
      "Epoch 44/200\n",
      "11/11 [==============================] - 32s 3s/step - loss: 12.0353\n",
      "Epoch 45/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 12.0467\n",
      "Epoch 46/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 12.0467\n",
      "Epoch 47/200\n",
      "11/11 [==============================] - 32s 3s/step - loss: 12.0353\n",
      "Epoch 48/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 12.0353\n",
      "Epoch 49/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 12.0354\n",
      "Epoch 50/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 12.0353\n",
      "Epoch 51/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 12.0353\n",
      "Epoch 52/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 12.0353\n",
      "Epoch 53/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 12.0353\n",
      "Epoch 54/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 12.0467\n",
      "Epoch 55/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 12.0353\n",
      "Epoch 56/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 12.0353\n",
      "Epoch 57/200\n",
      "11/11 [==============================] - 33s 3s/step - loss: 12.0011\n",
      "Epoch 58/200\n",
      " 1/11 [=>............................] - ETA: 33s - loss: 11.9135"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/pbs.32262.lich-compute.vscht.cz/ipykernel_23219/3061335457.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=200, verbose=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a01fd0-3888-4ae2-959b-1e02cb68dc60",
   "metadata": {},
   "source": [
    "### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2e7dfa6c-11ec-4854-b7cc-625bb8c6ec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(200, activation=\"relu\", input_shape=(max_length, 1), return_sequences=True,\n",
    "              kernel_regularizer=\"l2\", recurrent_regularizer=\"l2\", bias_regularizer=\"l2\"))  \n",
    "model.add(Dropout(0.1))  \n",
    "model.add(LSTM(200, activation=\"relu\", return_sequences=False,\n",
    "              kernel_regularizer=\"l2\", recurrent_regularizer=\"l2\", bias_regularizer=\"l2\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f999f194-c736-4ad7-8ace-1b877192e95c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 33s 3s/step - loss: 11.3796\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 8.6393\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 7.1998\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 30s 3s/step - loss: 6.2318\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 5.5878\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 5.1504\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 4.8413\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 4.6171\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 4.4382\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 4.2898\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 4.1615\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 32s 3s/step - loss: 4.0487\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 3.9419\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 3.8440\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 3.7528\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 3.6620\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 3.5759\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 3.4940\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 32s 3s/step - loss: 3.4156\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 3.3397\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 3.2663\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 3.1937\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 3.1244\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 3.0572\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.9949\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.9313\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.8657\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.8043\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.7447\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.6870\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.6292\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.5746\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.5192\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 32s 3s/step - loss: 2.4670\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.4148\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.3654\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.3158\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 30s 3s/step - loss: 2.2686\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.2216\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.1771\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.1329\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.0886\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.0459\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.0043\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.9628\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.9231\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.8855\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.8419\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.8971\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.8064\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.7460\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 32s 3s/step - loss: 1.6998\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.6580\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.6208\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.5831\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.5539\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.5200\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.4887\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.4585\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.4281\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.4003\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.3730\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 30s 3s/step - loss: 1.3466\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.3208\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.2939\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.2702\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.2453\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.2223\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.1984\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 30s 3s/step - loss: 1.1789\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.1553\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.1363\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.1141\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.0956\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.0743\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.0568\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.0397\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.0205\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.0040\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.9892\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.9710\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.9554\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.9386\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.9277\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.9124\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.9008\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.8851\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.8726\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.8581\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.8448\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.8314\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.8221\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.8081\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.7985\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.7892\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.7802\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.7672\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.7595\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.7497\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.7405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fecb71d2d90>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, verbose=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181226ff-7c38-48a0-af60-056f6ab84954",
   "metadata": {},
   "source": [
    "## Over-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d95f240-f012-4a7a-bcb0-81b6253aeca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape Counter({0: 1057, 1: 1057})\n"
     ]
    }
   ],
   "source": [
    "sampler = RandomOverSampler(random_state=42)\n",
    "\n",
    "X_train_os, y_train_os = sampler.fit_resample(X_train, y_train) \n",
    "print('Resampled dataset shape %s' % Counter(y_train_os)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2007cb18-0218-47cd-a8e6-74fb3c7d57d9",
   "metadata": {},
   "source": [
    "### Model 1 + OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6410156-4b7a-4dc3-9b8d-e7b8ee78b71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(\n",
    "    50, activation='sigmoid', return_sequences=True, input_shape=(max_length, 1), kernel_regularizer=\"l2\",\n",
    "    recurrent_regularizer=\"l2\", bias_regularizer=\"l2\"\n",
    "))\n",
    "model.add(LSTM(50, activation='sigmoid', kernel_regularizer=\"l2\", recurrent_regularizer=\"l2\", bias_regularizer=\"l2\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer='adam', loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b3b732a-ca50-4b09-a936-81758174f9fe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 6s 329ms/step - loss: 3.1067\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 6s 337ms/step - loss: 2.6443\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 6s 328ms/step - loss: 2.3104\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 5s 323ms/step - loss: 2.0626\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 6s 338ms/step - loss: 1.8793\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 6s 329ms/step - loss: 1.7428\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 6s 325ms/step - loss: 1.6398\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 6s 326ms/step - loss: 1.5603\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 6s 347ms/step - loss: 1.4975\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 6s 327ms/step - loss: 1.4471\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 6s 327ms/step - loss: 1.4052\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 6s 365ms/step - loss: 1.3683\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 6s 366ms/step - loss: 1.3368\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 6s 363ms/step - loss: 1.3088\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 6s 335ms/step - loss: 1.2813\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 6s 328ms/step - loss: 1.2565\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 6s 352ms/step - loss: 1.2336\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 6s 340ms/step - loss: 1.2119\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - 6s 329ms/step - loss: 1.1927\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - 6s 332ms/step - loss: 1.1709\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - 6s 333ms/step - loss: 1.1492\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - 6s 337ms/step - loss: 1.1314\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - 6s 326ms/step - loss: 1.1134\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - 6s 331ms/step - loss: 1.0955\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - 6s 335ms/step - loss: 1.0783\n",
      "Epoch 26/100\n",
      "17/17 [==============================] - 6s 339ms/step - loss: 1.0627\n",
      "Epoch 27/100\n",
      "17/17 [==============================] - 6s 328ms/step - loss: 1.0468\n",
      "Epoch 28/100\n",
      "17/17 [==============================] - 6s 333ms/step - loss: 1.0333\n",
      "Epoch 29/100\n",
      "17/17 [==============================] - 6s 335ms/step - loss: 1.0169\n",
      "Epoch 30/100\n",
      "17/17 [==============================] - 6s 335ms/step - loss: 1.0019\n",
      "Epoch 31/100\n",
      "17/17 [==============================] - 6s 332ms/step - loss: 0.9883\n",
      "Epoch 32/100\n",
      "17/17 [==============================] - 6s 332ms/step - loss: 0.9753\n",
      "Epoch 33/100\n",
      "17/17 [==============================] - 6s 324ms/step - loss: 0.9630\n",
      "Epoch 34/100\n",
      "17/17 [==============================] - 6s 327ms/step - loss: 0.9504\n",
      "Epoch 35/100\n",
      "17/17 [==============================] - 6s 335ms/step - loss: 0.9385\n",
      "Epoch 36/100\n",
      "17/17 [==============================] - 6s 342ms/step - loss: 0.9269\n",
      "Epoch 37/100\n",
      "17/17 [==============================] - 6s 328ms/step - loss: 0.9155\n",
      "Epoch 38/100\n",
      "17/17 [==============================] - 6s 328ms/step - loss: 0.9046\n",
      "Epoch 39/100\n",
      "17/17 [==============================] - 6s 346ms/step - loss: 0.8948\n",
      "Epoch 40/100\n",
      "17/17 [==============================] - 6s 336ms/step - loss: 0.8853\n",
      "Epoch 41/100\n",
      "17/17 [==============================] - 6s 334ms/step - loss: 0.8756\n",
      "Epoch 42/100\n",
      "17/17 [==============================] - 6s 334ms/step - loss: 0.8663\n",
      "Epoch 43/100\n",
      "17/17 [==============================] - 5s 319ms/step - loss: 0.8576\n",
      "Epoch 44/100\n",
      "17/17 [==============================] - 6s 326ms/step - loss: 0.8508\n",
      "Epoch 45/100\n",
      "17/17 [==============================] - 6s 342ms/step - loss: 0.8424\n",
      "Epoch 46/100\n",
      "17/17 [==============================] - 6s 334ms/step - loss: 0.8356\n",
      "Epoch 47/100\n",
      "17/17 [==============================] - 6s 326ms/step - loss: 0.8271\n",
      "Epoch 48/100\n",
      "17/17 [==============================] - 6s 327ms/step - loss: 0.8203\n",
      "Epoch 49/100\n",
      "17/17 [==============================] - 6s 334ms/step - loss: 0.8124\n",
      "Epoch 50/100\n",
      "17/17 [==============================] - 6s 353ms/step - loss: 0.8061\n",
      "Epoch 51/100\n",
      "17/17 [==============================] - 6s 329ms/step - loss: 0.8003\n",
      "Epoch 52/100\n",
      "17/17 [==============================] - 6s 329ms/step - loss: 0.7949\n",
      "Epoch 53/100\n",
      "17/17 [==============================] - 6s 330ms/step - loss: 0.7886\n",
      "Epoch 54/100\n",
      "17/17 [==============================] - 6s 330ms/step - loss: 0.7831\n",
      "Epoch 55/100\n",
      "17/17 [==============================] - 6s 327ms/step - loss: 0.7787\n",
      "Epoch 56/100\n",
      "17/17 [==============================] - 6s 330ms/step - loss: 0.7742\n",
      "Epoch 57/100\n",
      "17/17 [==============================] - 6s 334ms/step - loss: 0.7694\n",
      "Epoch 58/100\n",
      "17/17 [==============================] - 6s 333ms/step - loss: 0.7647\n",
      "Epoch 59/100\n",
      "17/17 [==============================] - 6s 327ms/step - loss: 0.7604\n",
      "Epoch 60/100\n",
      "17/17 [==============================] - 6s 334ms/step - loss: 0.7568\n",
      "Epoch 61/100\n",
      "17/17 [==============================] - 6s 331ms/step - loss: 0.7531\n",
      "Epoch 62/100\n",
      "17/17 [==============================] - 6s 323ms/step - loss: 0.7491\n",
      "Epoch 63/100\n",
      "17/17 [==============================] - 6s 325ms/step - loss: 0.7457\n",
      "Epoch 64/100\n",
      "17/17 [==============================] - 5s 323ms/step - loss: 0.7421\n",
      "Epoch 65/100\n",
      "17/17 [==============================] - 6s 330ms/step - loss: 0.7389\n",
      "Epoch 66/100\n",
      "17/17 [==============================] - 6s 345ms/step - loss: 0.7361\n",
      "Epoch 67/100\n",
      "17/17 [==============================] - 6s 331ms/step - loss: 0.7331\n",
      "Epoch 68/100\n",
      "17/17 [==============================] - 6s 338ms/step - loss: 0.7306\n",
      "Epoch 69/100\n",
      "17/17 [==============================] - 6s 330ms/step - loss: 0.7283\n",
      "Epoch 70/100\n",
      "17/17 [==============================] - 6s 330ms/step - loss: 0.7261\n",
      "Epoch 71/100\n",
      "17/17 [==============================] - 6s 343ms/step - loss: 0.7237\n",
      "Epoch 72/100\n",
      "17/17 [==============================] - 6s 330ms/step - loss: 0.7215\n",
      "Epoch 73/100\n",
      "17/17 [==============================] - 6s 326ms/step - loss: 0.7196\n",
      "Epoch 74/100\n",
      "17/17 [==============================] - 6s 327ms/step - loss: 0.7181\n",
      "Epoch 75/100\n",
      "17/17 [==============================] - 6s 338ms/step - loss: 0.7172\n",
      "Epoch 76/100\n",
      "17/17 [==============================] - 6s 328ms/step - loss: 0.7148\n",
      "Epoch 77/100\n",
      "17/17 [==============================] - 5s 323ms/step - loss: 0.7132\n",
      "Epoch 78/100\n",
      "17/17 [==============================] - 6s 339ms/step - loss: 0.7116\n",
      "Epoch 79/100\n",
      "17/17 [==============================] - 6s 329ms/step - loss: 0.7107\n",
      "Epoch 80/100\n",
      "17/17 [==============================] - 6s 332ms/step - loss: 0.7098\n",
      "Epoch 81/100\n",
      "17/17 [==============================] - 6s 334ms/step - loss: 0.7079\n",
      "Epoch 82/100\n",
      "17/17 [==============================] - 6s 330ms/step - loss: 0.7069\n",
      "Epoch 83/100\n",
      "17/17 [==============================] - 6s 336ms/step - loss: 0.7061\n",
      "Epoch 84/100\n",
      "17/17 [==============================] - 6s 340ms/step - loss: 0.7049\n",
      "Epoch 85/100\n",
      "17/17 [==============================] - 6s 327ms/step - loss: 0.7043\n",
      "Epoch 86/100\n",
      "17/17 [==============================] - 6s 341ms/step - loss: 0.7033\n",
      "Epoch 87/100\n",
      "17/17 [==============================] - 6s 327ms/step - loss: 0.7020\n",
      "Epoch 88/100\n",
      "17/17 [==============================] - 6s 331ms/step - loss: 0.7018\n",
      "Epoch 89/100\n",
      "17/17 [==============================] - 6s 327ms/step - loss: 0.7016\n",
      "Epoch 90/100\n",
      "17/17 [==============================] - 6s 342ms/step - loss: 0.7004\n",
      "Epoch 91/100\n",
      "17/17 [==============================] - 6s 329ms/step - loss: 0.7003\n",
      "Epoch 92/100\n",
      "17/17 [==============================] - 6s 335ms/step - loss: 0.6994\n",
      "Epoch 93/100\n",
      "17/17 [==============================] - 6s 335ms/step - loss: 0.6998\n",
      "Epoch 94/100\n",
      "17/17 [==============================] - 6s 330ms/step - loss: 0.6988\n",
      "Epoch 95/100\n",
      "17/17 [==============================] - 6s 331ms/step - loss: 0.6979\n",
      "Epoch 96/100\n",
      "17/17 [==============================] - 6s 334ms/step - loss: 0.6976\n",
      "Epoch 97/100\n",
      "17/17 [==============================] - 6s 326ms/step - loss: 0.6970\n",
      "Epoch 98/100\n",
      "17/17 [==============================] - 6s 327ms/step - loss: 0.6967\n",
      "Epoch 99/100\n",
      "17/17 [==============================] - 6s 329ms/step - loss: 0.6969\n",
      "Epoch 100/100\n",
      "17/17 [==============================] - 6s 335ms/step - loss: 0.6963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f04a80e7810>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_os, y_train_os, epochs=100, verbose=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee248914-8f4e-4f51-9811-cf68e84a24e9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 52ms/step - loss: 0.7067\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901204],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216],\n",
       "       [0.50901216]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80618471-2587-4b02-9f4f-cd5c0815d559",
   "metadata": {},
   "source": [
    "### Model 4 + OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d00c8a0-cfb5-4a5e-8bf1-35a61cd6d10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(200, activation=\"relu\", input_shape=(max_length, 1), return_sequences=True,\n",
    "              kernel_regularizer=\"l2\", recurrent_regularizer=\"l2\", bias_regularizer=\"l2\"))  \n",
    "model.add(Dropout(0.1))  \n",
    "model.add(LSTM(200, activation=\"relu\", return_sequences=False,\n",
    "              kernel_regularizer=\"l2\", recurrent_regularizer=\"l2\", bias_regularizer=\"l2\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96080868-11c9-431f-9453-3b64e4e41779",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 134s 3s/step - loss: 16.8643\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 49s 3s/step - loss: 13.8616\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 49s 3s/step - loss: 12.3750\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 49s 3s/step - loss: 11.6591\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 49s 3s/step - loss: 11.2934\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 49s 3s/step - loss: 11.0774\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 49s 3s/step - loss: 10.9232\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 49s 3s/step - loss: 10.7946\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 49s 3s/step - loss: 10.6777\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 49s 3s/step - loss: 10.5672\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 49s 3s/step - loss: 10.4611\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 49s 3s/step - loss: 10.3585\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 50s 3s/step - loss: 10.2591\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 48s 3s/step - loss: 10.1627\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 49s 3s/step - loss: 10.0691\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 48s 3s/step - loss: 9.9784\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 48s 3s/step - loss: 9.8903\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 49s 3s/step - loss: 9.8048\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - 48s 3s/step - loss: 9.7219\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - 49s 3s/step - loss: 9.6416\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - 49s 3s/step - loss: 9.5638\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - 49s 3s/step - loss: 9.4884\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - 49s 3s/step - loss: 9.4153\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - 48s 3s/step - loss: 9.3446\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - 49s 3s/step - loss: 9.2762\n",
      "Epoch 26/100\n",
      "10/17 [================>.............] - ETA: 20s - loss: 9.4153"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/pbs.32262.lich-compute.vscht.cz/ipykernel_26906/610107383.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_os\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_os\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train_os, y_train_os, epochs=100, verbose=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93bfddb9-2230-4ff3-95d7-4553d8705f72",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 3s 346ms/step - loss: 4.6476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877],\n",
       "       [-0.00703877]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d80965b-c468-4089-9902-78f5ff4299f0",
   "metadata": {},
   "source": [
    "So it looks like over-sampling didn't help :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e6d5d7-67f1-4053-8265-3e995e48b6eb",
   "metadata": {},
   "source": [
    "### Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54ccea96-a6d7-4a96-a678-3c93c0e7143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, activation=\"relu\", input_shape=(max_length, 1), return_sequences=True,\n",
    "              kernel_regularizer=\"l2\", recurrent_regularizer=\"l2\", bias_regularizer=\"l2\"))  \n",
    "model.add(LSTM(128, activation=\"relu\", return_sequences=False,\n",
    "              kernel_regularizer=\"l2\", recurrent_regularizer=\"l2\", bias_regularizer=\"l2\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer='adam', loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "762f79a6-d1e5-458e-a6f3-2f92faebfd9f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 17s 1s/step - loss: 7.1326\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 5.6832\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 4.7034\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 4.0404\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 3.5787\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 3.2596\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 3.0345\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 16s 1s/step - loss: 2.8718\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 2.7510\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 2.6568\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 2.5803\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 2.5151\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 16s 1s/step - loss: 2.4576\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 16s 1s/step - loss: 2.4050\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 16s 1s/step - loss: 2.3575\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 16s 1s/step - loss: 2.3109\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 2.2661\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 16s 1s/step - loss: 2.2207\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 2.1788\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 2.1374\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 2.0969\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 14s 1s/step - loss: 2.0579\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 2.0196\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 14s 1s/step - loss: 1.9809\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.9438\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.9076\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.8727\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.8372\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 16s 1s/step - loss: 1.8028\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.7696\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.7364\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.7062\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.6731\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.6418\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 16s 1s/step - loss: 1.6126\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.5827\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.5540\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.5247\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.4973\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.4696\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.4447\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.4192\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.3920\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 16s 1s/step - loss: 1.3668\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 16s 1s/step - loss: 1.3424\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.3188\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.2973\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.2725\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.2514\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.2285\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.2094\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.1871\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.1666\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.1466\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.1275\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.1086\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 16s 1s/step - loss: 1.0914\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 16s 1s/step - loss: 1.0722\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 16s 1s/step - loss: 1.0556\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.0380\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.0213\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.0063\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.9910\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.9737\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 17s 2s/step - loss: 0.9600\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 16s 1s/step - loss: 0.9446\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.9300\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.9177\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.9041\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.8898\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.8775\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.8650\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.8539\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.8416\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.8298\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.8183\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.8083\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.7993\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.7879\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.7779\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.7677\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.7597\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.7500\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.7409\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.7324\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 16s 1s/step - loss: 0.7243\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.7159\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.7087\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.7014\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.6938\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.6868\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.6801\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.6734\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.6686\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.6620\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.6549\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.6500\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.6442\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.6387\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.6346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f04b415cfd0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, verbose=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a47e1d94-a953-4a13-b807-367a9f03fa60",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 2s 207ms/step - loss: 0.6243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.28220424],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855009],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006],\n",
       "       [0.21855006]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb269c69-c7ae-40e4-b668-f6dd937dc725",
   "metadata": {},
   "source": [
    "### Model 5 + OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c27a226-7174-4997-a292-277b3cef6116",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, activation=\"relu\", input_shape=(max_length, 1), return_sequences=True,\n",
    "              kernel_regularizer=\"l2\", recurrent_regularizer=\"l2\", bias_regularizer=\"l2\"))  \n",
    "model.add(LSTM(128, activation=\"relu\", return_sequences=False,\n",
    "              kernel_regularizer=\"l2\", recurrent_regularizer=\"l2\", bias_regularizer=\"l2\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer='adam', loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d59fc91-867b-4394-a9aa-bbe5575324c7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 25s 1s/step - loss: 6.7897\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 5.0410\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 4.0339\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 3.4642\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 3.1340\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 2.9322\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 2.7932\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 2.6893\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 2.6033\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 2.5267\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 2.4558\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 2.3885\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 2.3242\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 2.2637\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 2.2022\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 2.1435\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 2.0870\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 2.0327\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 1.9797\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 1.9277\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 1.8778\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 1.8296\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 1.7833\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 1.7385\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 1.6938\n",
      "Epoch 26/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 1.6514\n",
      "Epoch 27/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 1.6106\n",
      "Epoch 28/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 1.5712\n",
      "Epoch 29/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 1.5331\n",
      "Epoch 30/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 1.4956\n",
      "Epoch 31/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 1.4602\n",
      "Epoch 32/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 1.4257\n",
      "Epoch 33/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 1.3924\n",
      "Epoch 34/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 1.3603\n",
      "Epoch 35/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 1.3294\n",
      "Epoch 36/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 1.2997\n",
      "Epoch 37/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 1.2710\n",
      "Epoch 38/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 1.2436\n",
      "Epoch 39/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 1.2171\n",
      "Epoch 40/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 1.1915\n",
      "Epoch 41/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 1.1672\n",
      "Epoch 42/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 1.1437\n",
      "Epoch 43/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 1.1210\n",
      "Epoch 44/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 1.0993\n",
      "Epoch 45/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 1.0786\n",
      "Epoch 46/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 1.0585\n",
      "Epoch 47/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 1.0395\n",
      "Epoch 48/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 1.0211\n",
      "Epoch 49/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 1.0039\n",
      "Epoch 50/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.9871\n",
      "Epoch 51/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.9713\n",
      "Epoch 52/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 0.9559\n",
      "Epoch 53/100\n",
      "17/17 [==============================] - 25s 1s/step - loss: 0.9414\n",
      "Epoch 54/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.9271\n",
      "Epoch 55/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.9138\n",
      "Epoch 56/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 0.9013\n",
      "Epoch 57/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.8890\n",
      "Epoch 58/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.8777\n",
      "Epoch 59/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.8666\n",
      "Epoch 60/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.8565\n",
      "Epoch 61/100\n",
      "17/17 [==============================] - 25s 1s/step - loss: 0.8462\n",
      "Epoch 62/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 0.8370\n",
      "Epoch 63/100\n",
      "17/17 [==============================] - 25s 1s/step - loss: 0.8284\n",
      "Epoch 64/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 0.8196\n",
      "Epoch 65/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.8116\n",
      "Epoch 66/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.8040\n",
      "Epoch 67/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 0.7970\n",
      "Epoch 68/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.7904\n",
      "Epoch 69/100\n",
      "17/17 [==============================] - 25s 1s/step - loss: 0.7836\n",
      "Epoch 70/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.7778\n",
      "Epoch 71/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.7720\n",
      "Epoch 72/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 0.7670\n",
      "Epoch 73/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.7619\n",
      "Epoch 74/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.7573\n",
      "Epoch 75/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 0.7523\n",
      "Epoch 76/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.7483\n",
      "Epoch 77/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.7446\n",
      "Epoch 78/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.7403\n",
      "Epoch 79/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 0.7371\n",
      "Epoch 80/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.7336\n",
      "Epoch 81/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 0.7306\n",
      "Epoch 82/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.7279\n",
      "Epoch 83/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.7250\n",
      "Epoch 84/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 0.7226\n",
      "Epoch 85/100\n",
      "17/17 [==============================] - 25s 1s/step - loss: 0.7205\n",
      "Epoch 86/100\n",
      "17/17 [==============================] - 25s 1s/step - loss: 0.7182\n",
      "Epoch 87/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 0.7160\n",
      "Epoch 88/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 0.7144\n",
      "Epoch 89/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 0.7125\n",
      "Epoch 90/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 0.7113\n",
      "Epoch 91/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.7094\n",
      "Epoch 92/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 0.7082\n",
      "Epoch 93/100\n",
      "17/17 [==============================] - 25s 1s/step - loss: 0.7068\n",
      "Epoch 94/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 0.7054\n",
      "Epoch 95/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.7046\n",
      "Epoch 96/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.7035\n",
      "Epoch 97/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 0.7024\n",
      "Epoch 98/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.7015\n",
      "Epoch 99/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.7009\n",
      "Epoch 100/100\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.7003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f04b4eab110>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_os, y_train_os, epochs=100, verbose=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ca54408-0691-4a71-a02b-a35fb539d6e7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 2s 182ms/step - loss: 0.6943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.5447197],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502],\n",
       "       [0.4949502]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8861b7-1db9-40e3-b52b-dd90ebb73f20",
   "metadata": {},
   "source": [
    "### Model 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb70024c-32da-4547-afa4-9f920e1834bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(200, activation=\"relu\", input_shape=(max_length, 1), return_sequences=True,\n",
    "              kernel_regularizer=\"l2\", recurrent_regularizer=\"l2\", bias_regularizer=\"l2\"))  \n",
    "model.add(Dropout(0.1))  \n",
    "model.add(LSTM(200, activation=\"relu\", return_sequences=False,\n",
    "              kernel_regularizer=\"l2\", recurrent_regularizer=\"l2\", bias_regularizer=\"l2\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer='adam', loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "452ef0d8-e8e5-4ce8-a630-9009930988d6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 33s 3s/step - loss: 10.5423\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 30s 3s/step - loss: 7.9961\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 6.4224\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 5.4357\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 30s 3s/step - loss: 4.8218\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 30s 3s/step - loss: 4.4360\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 4.1834\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 4.0097\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 3.8825\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 3.7773\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 3.6868\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 3.6036\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 30s 3s/step - loss: 3.5269\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 3.4539\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 30s 3s/step - loss: 3.3855\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 30s 3s/step - loss: 3.3125\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 3.2451\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 3.1787\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 30s 3s/step - loss: 3.1122\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 3.0497\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.9885\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.9257\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.8652\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 30s 3s/step - loss: 2.8064\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.7482\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 30s 3s/step - loss: 2.6926\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 30s 3s/step - loss: 2.6382\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.5859\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.5283\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.4766\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 30s 3s/step - loss: 2.4252\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 30s 3s/step - loss: 2.3753\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 30s 3s/step - loss: 2.3256\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 30s 3s/step - loss: 2.2771\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.2294\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.1843\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.1383\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.0932\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.0499\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 2.0078\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.9664\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.9261\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.8865\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 30s 3s/step - loss: 1.8467\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.8090\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.7717\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.7355\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.6996\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.6650\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.6318\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 30s 3s/step - loss: 1.5986\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.5661\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.5351\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.5033\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.4729\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 30s 3s/step - loss: 1.4441\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.4144\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.3869\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.3599\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.3326\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.3077\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.2824\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.2563\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.2327\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.2094\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.1872\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.1658\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.1429\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.1220\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.1016\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.0820\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.0624\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.0434\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.0246\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 1.0073\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.9903\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.9732\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.9571\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 32s 3s/step - loss: 0.9412\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.9261\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.9111\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.8988\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.8826\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.8689\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.8567\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.8427\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.8312\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.8185\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.8066\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.7965\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.7840\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.7739\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.7639\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.7530\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.7439\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.7349\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.7257\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.7177\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 31s 3s/step - loss: 0.7087\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.7007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f04788cea50>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, verbose=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51bed85d-f73d-480b-91fb-9d3dda270bac",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 3s 323ms/step - loss: 0.6891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466086],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.29249224],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466089],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083],\n",
       "       [0.20466083]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cecc335-2083-449c-b570-a79df300402b",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "\n",
    "### Model 5 + ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6085c-b76c-4d05-b0b6-f12eda9826cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a16a929a-1b5c-4da3-ab84-5ba4ca0fe87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, activation=\"relu\", input_shape=(max_length, 1), return_sequences=True,\n",
    "              kernel_regularizer=\"l2\", recurrent_regularizer=\"l2\", bias_regularizer=\"l2\"))  \n",
    "model.add(LSTM(128, activation=\"relu\", return_sequences=False,\n",
    "              kernel_regularizer=\"l2\", recurrent_regularizer=\"l2\", bias_regularizer=\"l2\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer='adam', loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a256575f-b9db-41b4-bc19-845791024360",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 7.3776 - val_loss: 6.6472\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 12s 1s/step - loss: 6.2244 - val_loss: 5.6264\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 5.3372 - val_loss: 4.9049\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 4.6828 - val_loss: 4.3465\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 12s 1s/step - loss: 4.1796 - val_loss: 3.9246\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 3.7991 - val_loss: 3.5998\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 3.5061 - val_loss: 3.3495\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 3.2825 - val_loss: 3.1576\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 3.1080 - val_loss: 3.0124\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 15s 2s/step - loss: 2.9726 - val_loss: 2.8928\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 15s 2s/step - loss: 2.8611 - val_loss: 2.7932\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 15s 2s/step - loss: 2.7738 - val_loss: 2.7142\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 2.6994 - val_loss: 2.6486\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 2.6373 - val_loss: 2.5909\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 2.5831 - val_loss: 2.5407\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 2.5345 - val_loss: 2.4953\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 2.4907 - val_loss: 2.4538\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 12s 1s/step - loss: 2.4509 - val_loss: 2.4168\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 2.4129 - val_loss: 2.3781\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 12s 1s/step - loss: 2.3766 - val_loss: 2.3423\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 12s 1s/step - loss: 2.3422 - val_loss: 2.3094\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 12s 1s/step - loss: 2.3073 - val_loss: 2.2756\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 12s 1s/step - loss: 2.2752 - val_loss: 2.2422\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 12s 1s/step - loss: 2.2432 - val_loss: 2.2107\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 12s 1s/step - loss: 2.2115 - val_loss: 2.1808\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 2.1808 - val_loss: 2.1512\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 2.1508 - val_loss: 2.1209\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 2.1214 - val_loss: 2.0910\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 2.0923 - val_loss: 2.0623\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 2.0650 - val_loss: 2.0351\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 12s 1s/step - loss: 2.0353 - val_loss: 2.0057\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 2.0086 - val_loss: 1.9781\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 12s 1s/step - loss: 1.9804 - val_loss: 1.9511\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 1.9535 - val_loss: 1.9252\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 1.9268 - val_loss: 1.8985\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 12s 1s/step - loss: 1.9008 - val_loss: 1.8738\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 1.8756 - val_loss: 1.8469\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 1.8498 - val_loss: 1.8227\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 1.8244 - val_loss: 1.7976\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 1.7995 - val_loss: 1.7725\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 12s 1s/step - loss: 1.7754 - val_loss: 1.7483\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 1.7512 - val_loss: 1.7240\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 1.7284 - val_loss: 1.7006\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 12s 1s/step - loss: 1.7044 - val_loss: 1.6776\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 1.6809 - val_loss: 1.6564\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 1.6584 - val_loss: 1.6338\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 13052.7412 - val_loss: 1.6130\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 1.6332 - val_loss: 1.6312\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 12s 1s/step - loss: 1.6488 - val_loss: 1.6365\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 12s 1s/step - loss: 1.6493 - val_loss: 1.6293\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - ETA: 0s - loss: 1.6400Restoring model weights from the end of the best epoch: 47.\n",
      "8/8 [==============================] - 12s 1s/step - loss: 1.6400 - val_loss: 1.6166\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f04b42a5150>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience=4, verbose=1, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, epochs=100, verbose=1, batch_size=128, validation_split=0.3, callbacks=[es])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5d030392-6072-407b-953a-dc2832c47718",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 2s 190ms/step - loss: 1.6144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.21184444],\n",
       "       [0.21184433],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.21184525],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.21184427],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.21184433],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.21185115],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.21184751],\n",
       "       [0.21184751],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118443 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118443 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.21184441],\n",
       "       [0.21184441],\n",
       "       [0.21184441],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118443 ],\n",
       "       [0.3752784 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.21184427],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.21184471],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.21184427],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.21184465],\n",
       "       [0.21184465],\n",
       "       [0.21184465],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.21184444],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118445 ],\n",
       "       [0.21184453],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.21184427],\n",
       "       [0.21186331],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.21184427],\n",
       "       [0.2118442 ],\n",
       "       [0.21184444],\n",
       "       [0.21184444],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ],\n",
       "       [0.2118442 ]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ad8c76-3687-405e-8e6e-92dc876e8ec3",
   "metadata": {},
   "source": [
    "## Model 5 + ES + OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d7943236-ef2d-4e50-b30e-78be76319587",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, activation=\"relu\", input_shape=(max_length, 1), return_sequences=True,\n",
    "              kernel_regularizer=\"l2\", recurrent_regularizer=\"l2\", bias_regularizer=\"l2\"))  \n",
    "model.add(LSTM(128, activation=\"relu\", return_sequences=False,\n",
    "              kernel_regularizer=\"l2\", recurrent_regularizer=\"l2\", bias_regularizer=\"l2\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer='adam', loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e3b3ed3e-fab9-4b9f-a72b-9c6e58571c1c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - 23s 2s/step - loss: 7.0680 - val_loss: 6.5489\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 5.5926 - val_loss: 5.7582\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 4.6087 - val_loss: 4.7859\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 3.9531 - val_loss: 4.3299\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 18s 2s/step - loss: 3.5155 - val_loss: 3.9886\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 18s 2s/step - loss: 3.2196 - val_loss: 3.7645\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 3.0155 - val_loss: 3.5850\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 2.8688 - val_loss: 3.4322\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 2.7595 - val_loss: 3.3866\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 2.6731 - val_loss: 3.2940\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 20s 2s/step - loss: 2.6020 - val_loss: 3.2263\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 20s 2s/step - loss: 2.5384 - val_loss: 3.1387\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 20s 2s/step - loss: 2.4817 - val_loss: 3.1190\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 21s 2s/step - loss: 2.4281 - val_loss: 3.0448\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 20s 2s/step - loss: 2.3782 - val_loss: 2.9794\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 2.3299 - val_loss: 2.9761\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 2.2830 - val_loss: 2.9001\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 18s 2s/step - loss: 2.2384 - val_loss: 2.8981\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 2.1942 - val_loss: 2.8174\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 20s 2s/step - loss: 2.1499 - val_loss: 2.7892\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 2.1075 - val_loss: 2.7116\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 18s 2s/step - loss: 2.0674 - val_loss: 2.7281\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 18s 2s/step - loss: 2.0259 - val_loss: 2.6288\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 1.9863 - val_loss: 2.6322\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 18s 2s/step - loss: 1.9476 - val_loss: 2.5613\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 1.9095 - val_loss: 2.5586\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 20s 2s/step - loss: 1.8725 - val_loss: 2.5038\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 20s 2s/step - loss: 1.8379 - val_loss: 2.4842\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 1.8018 - val_loss: 2.4127\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 1.7659 - val_loss: 2.4449\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 20s 2s/step - loss: 1.7326 - val_loss: 2.3483\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 20s 2s/step - loss: 1.6996 - val_loss: 2.3500\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 1.6666 - val_loss: 2.2736\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 21s 2s/step - loss: 1.6362 - val_loss: 2.3074\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 21s 2s/step - loss: 1.6040 - val_loss: 2.2146\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 21s 2s/step - loss: 1.5750 - val_loss: 2.2188\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 20s 2s/step - loss: 1.5457 - val_loss: 2.2156\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 21s 2s/step - loss: 1.5154 - val_loss: 2.1101\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 18s 2s/step - loss: 1.4890 - val_loss: 2.1081\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 1.4604 - val_loss: 2.1292\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 1.4338 - val_loss: 2.0874\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 18s 2s/step - loss: 1.4071 - val_loss: 2.0388\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 1.3820 - val_loss: 2.0092\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 20s 2s/step - loss: 1.3575 - val_loss: 1.9830\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 1.3329 - val_loss: 2.0002\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 1.3093 - val_loss: 1.9738\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 1.2862 - val_loss: 1.8918\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 20s 2s/step - loss: 1.2638 - val_loss: 1.9375\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 1.2418 - val_loss: 1.8554\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 21s 2s/step - loss: 1.2214 - val_loss: 1.8775\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 1.1995 - val_loss: 1.8137\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 18s 2s/step - loss: 1.1799 - val_loss: 1.8104\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 1.1607 - val_loss: 1.8478\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 1.1410 - val_loss: 1.7681\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 18s 2s/step - loss: 1.1222 - val_loss: 1.7692\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 1.1044 - val_loss: 1.7571\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 18s 2s/step - loss: 1.0865 - val_loss: 1.7358\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 20s 2s/step - loss: 1.0695 - val_loss: 1.7337\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 24s 2s/step - loss: 1.0529 - val_loss: 1.6483\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 21s 2s/step - loss: 1.0377 - val_loss: 1.7396\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 22s 2s/step - loss: 1.0208 - val_loss: 1.6587\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 23s 2s/step - loss: 1.0062 - val_loss: 1.6294\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 24s 2s/step - loss: 0.9904 - val_loss: 1.6668\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 23s 2s/step - loss: 0.9766 - val_loss: 1.6371\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 24s 2s/step - loss: 0.9639 - val_loss: 1.5920\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 23s 2s/step - loss: 0.9487 - val_loss: 1.6079\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 20s 2s/step - loss: 0.9356 - val_loss: 1.5799\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 22s 2s/step - loss: 0.9227 - val_loss: 1.5856\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 0.9109 - val_loss: 1.5659\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 18s 2s/step - loss: 0.8993 - val_loss: 1.5750\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 18s 2s/step - loss: 0.8866 - val_loss: 1.5036\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 18s 2s/step - loss: 0.8757 - val_loss: 1.5348\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 0.8648 - val_loss: 1.5171\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 0.8550 - val_loss: 1.4659\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 18s 2s/step - loss: 0.8441 - val_loss: 1.5432\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 21s 2s/step - loss: 0.8342 - val_loss: 1.4878\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 0.8254 - val_loss: 1.4776\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 0.8157 - val_loss: 1.4611\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 0.8069 - val_loss: 1.4143\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 0.7976 - val_loss: 1.4789\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 0.7902 - val_loss: 1.4577\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 0.7815 - val_loss: 1.4129\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 0.7738 - val_loss: 1.4217\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 20s 2s/step - loss: 0.7663 - val_loss: 1.4389\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 0.7591 - val_loss: 1.4205\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 0.7521 - val_loss: 1.3859\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 20s 2s/step - loss: 0.7456 - val_loss: 1.3968\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 20s 2s/step - loss: 0.7395 - val_loss: 1.4199\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 0.7325 - val_loss: 1.3661\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 19s 2s/step - loss: 0.7267 - val_loss: 1.3756\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 20s 2s/step - loss: 0.7213 - val_loss: 1.3797\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 20s 2s/step - loss: 0.7154 - val_loss: 1.3580\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 22s 2s/step - loss: 0.7101 - val_loss: 1.3873\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 24s 2s/step - loss: 0.7052 - val_loss: 1.3517\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 24s 2s/step - loss: 0.7000 - val_loss: 1.3741\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 22s 2s/step - loss: 0.6953 - val_loss: 1.3013\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 22s 2s/step - loss: 0.6916 - val_loss: 1.3785\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 23s 2s/step - loss: 0.6861 - val_loss: 1.3362\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 22s 2s/step - loss: 0.6821 - val_loss: 1.3215\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6780Restoring model weights from the end of the best epoch: 96.\n",
      "12/12 [==============================] - 23s 2s/step - loss: 0.6780 - val_loss: 1.3546\n",
      "Epoch 00100: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0489b7c990>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience=4, verbose=1, restore_best_weights=True)\n",
    "model.fit(X_train_os, y_train_os, epochs=100, verbose=1, batch_size=128, validation_split=0.3, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c9d6a2e3-dc64-4f1e-9b1f-6df73e52cdc1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 2s 196ms/step - loss: 0.6244\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.29800254],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.37517145],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.29800254],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ],\n",
       "       [0.2980025 ]], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a7cfb11-9c13-415b-9167-0972b65d939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_same(arr):\n",
    "    return np.all(arr == arr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1200ad9-8b7e-4de8-89af-b9bab800eeb4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "11/11 [==============================] - 13s 1s/step - loss: 7.1348 - val_loss: 6.2245\n",
      "Epoch 2/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 5.6714 - val_loss: 5.0717\n",
      "Epoch 3/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 4.7085 - val_loss: 4.2816\n",
      "Epoch 4/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 4.0416 - val_loss: 3.7441\n",
      "Epoch 5/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 3.5809 - val_loss: 3.3725\n",
      "Epoch 6/300\n",
      "11/11 [==============================] - 11s 990ms/step - loss: 3.2613 - val_loss: 3.1141\n",
      "Epoch 7/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 3.0356 - val_loss: 2.9270\n",
      "Epoch 8/300\n",
      "11/11 [==============================] - 11s 991ms/step - loss: 2.8737 - val_loss: 2.7907\n",
      "Epoch 9/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 2.7517 - val_loss: 2.6865\n",
      "Epoch 10/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 2.6576 - val_loss: 2.6036\n",
      "Epoch 11/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 2.5813 - val_loss: 2.5339\n",
      "Epoch 12/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 2.5157 - val_loss: 2.4740\n",
      "Epoch 13/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 2.4578 - val_loss: 2.4197\n",
      "Epoch 14/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 2.4056 - val_loss: 2.3690\n",
      "Epoch 15/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 2.3564 - val_loss: 2.3216\n",
      "Epoch 16/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 2.3111 - val_loss: 2.2762\n",
      "Epoch 17/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 2.2649 - val_loss: 2.2319\n",
      "Epoch 18/300\n",
      "11/11 [==============================] - 11s 991ms/step - loss: 2.2208 - val_loss: 2.1895\n",
      "Epoch 19/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 2.1792 - val_loss: 2.1480\n",
      "Epoch 20/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 2.1383 - val_loss: 2.1066\n",
      "Epoch 21/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 2.0973 - val_loss: 2.0670\n",
      "Epoch 22/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 2.0578 - val_loss: 2.0279\n",
      "Epoch 23/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 2.0199 - val_loss: 1.9899\n",
      "Epoch 24/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 1.9820 - val_loss: 1.9527\n",
      "Epoch 25/300\n",
      "11/11 [==============================] - 11s 977ms/step - loss: 1.9442 - val_loss: 1.9157\n",
      "Epoch 26/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 1.9087 - val_loss: 1.8795\n",
      "Epoch 27/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 1.8729 - val_loss: 1.8447\n",
      "Epoch 28/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 1.8380 - val_loss: 1.8102\n",
      "Epoch 29/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 1.8030 - val_loss: 1.7762\n",
      "Epoch 30/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 1.7696 - val_loss: 1.7429\n",
      "Epoch 31/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 1.7367 - val_loss: 1.7104\n",
      "Epoch 32/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 1.7047 - val_loss: 1.6797\n",
      "Epoch 33/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 1.6732 - val_loss: 1.6479\n",
      "Epoch 34/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 1.6427 - val_loss: 1.6173\n",
      "Epoch 35/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 1.6129 - val_loss: 1.5873\n",
      "Epoch 36/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 1.5823 - val_loss: 1.5581\n",
      "Epoch 37/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 1.5535 - val_loss: 1.5293\n",
      "Epoch 38/300\n",
      "11/11 [==============================] - 11s 992ms/step - loss: 1.5250 - val_loss: 1.5012\n",
      "Epoch 39/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 1.4969 - val_loss: 1.4745\n",
      "Epoch 40/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 1.4705 - val_loss: 1.4477\n",
      "Epoch 41/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 1.4440 - val_loss: 1.4214\n",
      "Epoch 42/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 1.4172 - val_loss: 1.3951\n",
      "Epoch 43/300\n",
      "11/11 [==============================] - 11s 991ms/step - loss: 1.3916 - val_loss: 1.3700\n",
      "Epoch 44/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 1.3670 - val_loss: 1.3451\n",
      "Epoch 45/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 1.3422 - val_loss: 1.3212\n",
      "Epoch 46/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 1.3186 - val_loss: 1.2980\n",
      "Epoch 47/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 1.2952 - val_loss: 1.2746\n",
      "Epoch 48/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 1.2731 - val_loss: 1.2523\n",
      "Epoch 49/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 1.2516 - val_loss: 1.2319\n",
      "Epoch 50/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 1.2279 - val_loss: 1.2090\n",
      "Epoch 51/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 1.2113 - val_loss: 1.1887\n",
      "Epoch 52/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 1.1871 - val_loss: 1.1695\n",
      "Epoch 53/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 1.1669 - val_loss: 1.1479\n",
      "Epoch 54/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 1.1479 - val_loss: 1.1282\n",
      "Epoch 55/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 1.1279 - val_loss: 1.1109\n",
      "Epoch 56/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 1.1084 - val_loss: 1.0907\n",
      "Epoch 57/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 1.0926 - val_loss: 1.0729\n",
      "Epoch 58/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 1.0739 - val_loss: 1.0570\n",
      "Epoch 59/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 1.0550 - val_loss: 1.0381\n",
      "Epoch 60/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 1.0378 - val_loss: 1.0213\n",
      "Epoch 61/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 1.0212 - val_loss: 1.0048\n",
      "Epoch 62/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 1.0050 - val_loss: 0.9888\n",
      "Epoch 63/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 0.9892 - val_loss: 0.9730\n",
      "Epoch 64/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.9741 - val_loss: 0.9579\n",
      "Epoch 65/300\n",
      "11/11 [==============================] - 11s 996ms/step - loss: 0.9598 - val_loss: 0.9440\n",
      "Epoch 66/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.9445 - val_loss: 0.9290\n",
      "Epoch 67/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.9304 - val_loss: 0.9154\n",
      "Epoch 68/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.9169 - val_loss: 0.9017\n",
      "Epoch 69/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.9036 - val_loss: 0.8887\n",
      "Epoch 70/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.8899 - val_loss: 0.8755\n",
      "Epoch 71/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.8769 - val_loss: 0.8631\n",
      "Epoch 72/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.8650 - val_loss: 0.8506\n",
      "Epoch 73/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.8531 - val_loss: 0.8385\n",
      "Epoch 74/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.8408 - val_loss: 0.8273\n",
      "Epoch 75/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.8295 - val_loss: 0.8160\n",
      "Epoch 76/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.8183 - val_loss: 0.8053\n",
      "Epoch 77/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.8079 - val_loss: 0.7948\n",
      "Epoch 78/300\n",
      "11/11 [==============================] - 11s 990ms/step - loss: 0.7971 - val_loss: 0.7846\n",
      "Epoch 79/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.7872 - val_loss: 0.7749\n",
      "Epoch 80/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.7773 - val_loss: 0.7649\n",
      "Epoch 81/300\n",
      "11/11 [==============================] - 11s 991ms/step - loss: 0.7680 - val_loss: 0.7555\n",
      "Epoch 82/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.7620 - val_loss: 0.7482\n",
      "Epoch 83/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.7492 - val_loss: 0.7380\n",
      "Epoch 84/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.7419 - val_loss: 0.7290\n",
      "Epoch 85/300\n",
      "11/11 [==============================] - 11s 994ms/step - loss: 0.7331 - val_loss: 0.7209\n",
      "Epoch 86/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.7242 - val_loss: 0.7124\n",
      "Epoch 87/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.7165 - val_loss: 0.7050\n",
      "Epoch 88/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 0.7086 - val_loss: 0.6976\n",
      "Epoch 89/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.7014 - val_loss: 0.6901\n",
      "Epoch 90/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.6949 - val_loss: 0.6833\n",
      "Epoch 91/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.6868 - val_loss: 0.6762\n",
      "Epoch 92/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 0.6820 - val_loss: 0.6698\n",
      "Epoch 93/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.6733 - val_loss: 0.6630\n",
      "Epoch 94/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.6674 - val_loss: 0.6566\n",
      "Epoch 95/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.6611 - val_loss: 0.6512\n",
      "Epoch 96/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.6552 - val_loss: 0.6451\n",
      "Epoch 97/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.6495 - val_loss: 0.6396\n",
      "Epoch 98/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.6438 - val_loss: 0.6339\n",
      "Epoch 99/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.6387 - val_loss: 0.6287\n",
      "Epoch 100/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.6334 - val_loss: 0.6236\n",
      "Epoch 101/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 0.6291 - val_loss: 0.6187\n",
      "Epoch 102/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.6242 - val_loss: 0.6151\n",
      "Epoch 103/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.6188 - val_loss: 0.6095\n",
      "Epoch 104/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.6149 - val_loss: 0.6051\n",
      "Epoch 105/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.6103 - val_loss: 0.6011\n",
      "Epoch 106/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.6061 - val_loss: 0.5969\n",
      "Epoch 107/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 0.6016 - val_loss: 0.5936\n",
      "Epoch 108/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.5993 - val_loss: 0.5894\n",
      "Epoch 109/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 0.5944 - val_loss: 0.5857\n",
      "Epoch 110/300\n",
      "11/11 [==============================] - 11s 990ms/step - loss: 0.5907 - val_loss: 0.5824\n",
      "Epoch 111/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5876 - val_loss: 0.5791\n",
      "Epoch 112/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5843 - val_loss: 0.5757\n",
      "Epoch 113/300\n",
      "11/11 [==============================] - 11s 991ms/step - loss: 0.5813 - val_loss: 0.5724\n",
      "Epoch 114/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5778 - val_loss: 0.5698\n",
      "Epoch 115/300\n",
      "11/11 [==============================] - 11s 992ms/step - loss: 0.5753 - val_loss: 0.5666\n",
      "Epoch 116/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 0.5734 - val_loss: 0.5639\n",
      "Epoch 117/300\n",
      "11/11 [==============================] - 11s 995ms/step - loss: 0.5693 - val_loss: 0.5614\n",
      "Epoch 118/300\n",
      "11/11 [==============================] - 11s 977ms/step - loss: 0.5670 - val_loss: 0.5588\n",
      "Epoch 119/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.5642 - val_loss: 0.5561\n",
      "Epoch 120/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 0.5620 - val_loss: 0.5538\n",
      "Epoch 121/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5597 - val_loss: 0.5517\n",
      "Epoch 122/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5574 - val_loss: 0.5494\n",
      "Epoch 123/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.5551 - val_loss: 0.5474\n",
      "Epoch 124/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 0.5531 - val_loss: 0.5454\n",
      "Epoch 125/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5515 - val_loss: 0.5437\n",
      "Epoch 126/300\n",
      "11/11 [==============================] - 11s 992ms/step - loss: 0.5492 - val_loss: 0.5417\n",
      "Epoch 127/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 0.5479 - val_loss: 0.5398\n",
      "Epoch 128/300\n",
      "11/11 [==============================] - 11s 991ms/step - loss: 0.5460 - val_loss: 0.5384\n",
      "Epoch 129/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5459 - val_loss: 0.5367\n",
      "Epoch 130/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 0.5425 - val_loss: 0.5359\n",
      "Epoch 131/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5415 - val_loss: 0.5340\n",
      "Epoch 132/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 0.5404 - val_loss: 0.5326\n",
      "Epoch 133/300\n",
      "11/11 [==============================] - 11s 993ms/step - loss: 0.5384 - val_loss: 0.5309\n",
      "Epoch 134/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.5370 - val_loss: 0.5297\n",
      "Epoch 135/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.5361 - val_loss: 0.5287\n",
      "Epoch 136/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5348 - val_loss: 0.5273\n",
      "Epoch 137/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.5334 - val_loss: 0.5267\n",
      "Epoch 138/300\n",
      "11/11 [==============================] - 11s 990ms/step - loss: 0.5325 - val_loss: 0.5254\n",
      "Epoch 139/300\n",
      "11/11 [==============================] - 11s 976ms/step - loss: 0.5320 - val_loss: 0.5244\n",
      "Epoch 140/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.5310 - val_loss: 0.5242\n",
      "Epoch 141/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5295 - val_loss: 0.5224\n",
      "Epoch 142/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.5288 - val_loss: 0.5216\n",
      "Epoch 143/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5278 - val_loss: 0.5208\n",
      "Epoch 144/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.5271 - val_loss: 0.5199\n",
      "Epoch 145/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.5269 - val_loss: 0.5192\n",
      "Epoch 146/300\n",
      "11/11 [==============================] - 11s 990ms/step - loss: 0.5254 - val_loss: 0.5187\n",
      "Epoch 147/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.5249 - val_loss: 0.5182\n",
      "Epoch 148/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 0.5247 - val_loss: 0.5172\n",
      "Epoch 149/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5233 - val_loss: 0.5169\n",
      "Epoch 150/300\n",
      "11/11 [==============================] - 11s 991ms/step - loss: 0.5232 - val_loss: 0.5163\n",
      "Epoch 151/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 0.5230 - val_loss: 0.5155\n",
      "Epoch 152/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5219 - val_loss: 0.5153\n",
      "Epoch 153/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.5214 - val_loss: 0.5145\n",
      "Epoch 154/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 0.5212 - val_loss: 0.5140\n",
      "Epoch 155/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5203 - val_loss: 0.5140\n",
      "Epoch 156/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5204 - val_loss: 0.5133\n",
      "Epoch 157/300\n",
      "11/11 [==============================] - 11s 992ms/step - loss: 0.5203 - val_loss: 0.5128\n",
      "Epoch 158/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 0.5201 - val_loss: 0.5126\n",
      "Epoch 159/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 0.5188 - val_loss: 0.5120\n",
      "Epoch 160/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5187 - val_loss: 0.5119\n",
      "Epoch 161/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.5184 - val_loss: 0.5115\n",
      "Epoch 162/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5179 - val_loss: 0.5110\n",
      "Epoch 163/300\n",
      "11/11 [==============================] - 11s 990ms/step - loss: 0.5176 - val_loss: 0.5107\n",
      "Epoch 164/300\n",
      "11/11 [==============================] - 11s 992ms/step - loss: 0.5175 - val_loss: 0.5106\n",
      "Epoch 165/300\n",
      "11/11 [==============================] - 11s 990ms/step - loss: 0.5180 - val_loss: 0.5109\n",
      "Epoch 166/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 0.5170 - val_loss: 0.5102\n",
      "Epoch 167/300\n",
      "11/11 [==============================] - 11s 992ms/step - loss: 0.5169 - val_loss: 0.5099\n",
      "Epoch 168/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.5167 - val_loss: 0.5097\n",
      "Epoch 169/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5170 - val_loss: 0.5101\n",
      "Epoch 170/300\n",
      "11/11 [==============================] - 11s 992ms/step - loss: 0.5159 - val_loss: 0.5095\n",
      "Epoch 171/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5165 - val_loss: 0.5091\n",
      "Epoch 172/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5155 - val_loss: 0.5092\n",
      "Epoch 173/300\n",
      "11/11 [==============================] - 11s 990ms/step - loss: 0.5165 - val_loss: 0.5090\n",
      "Epoch 174/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5168 - val_loss: 0.5090\n",
      "Epoch 175/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5156 - val_loss: 0.5093\n",
      "Epoch 176/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5157 - val_loss: 0.5084\n",
      "Epoch 177/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 0.5152 - val_loss: 0.5086\n",
      "Epoch 178/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.5153 - val_loss: 0.5087\n",
      "Epoch 179/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5152 - val_loss: 0.5082\n",
      "Epoch 180/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5147 - val_loss: 0.5085\n",
      "Epoch 181/300\n",
      "11/11 [==============================] - 11s 991ms/step - loss: 0.5150 - val_loss: 0.5082\n",
      "Epoch 182/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5145 - val_loss: 0.5081\n",
      "Epoch 183/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5154 - val_loss: 0.5080\n",
      "Epoch 184/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.5151 - val_loss: 0.5086\n",
      "Epoch 185/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.5143 - val_loss: 0.5078\n",
      "Epoch 186/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.5154 - val_loss: 0.5077\n",
      "Epoch 187/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5146 - val_loss: 0.5093\n",
      "Epoch 188/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.5139 - val_loss: 0.5076\n",
      "Epoch 189/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5159 - val_loss: 0.5076\n",
      "Epoch 190/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 0.5135 - val_loss: 0.5083\n",
      "Epoch 191/300\n",
      "11/11 [==============================] - 11s 977ms/step - loss: 0.5147 - val_loss: 0.5084\n",
      "Epoch 192/300\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5146Restoring model weights from the end of the best epoch: 188.\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5146 - val_loss: 0.5078\n",
      "Epoch 00192: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_10/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf9bd65750> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf98c31b90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Epoch 1/300\n",
      "11/11 [==============================] - 41s 1s/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "11/11 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "11/11 [==============================] - 11s 982ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_11/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_11/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf80634dd0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf8096b5d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "Epoch 1/300\n",
      "11/11 [==============================] - 13s 1s/step - loss: 7.1409 - val_loss: 6.2313\n",
      "Epoch 2/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 5.6878 - val_loss: 5.0650\n",
      "Epoch 3/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 4.7129 - val_loss: 4.2874\n",
      "Epoch 4/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 4.0495 - val_loss: 3.7497\n",
      "Epoch 5/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 3.5860 - val_loss: 3.3772\n",
      "Epoch 6/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 3.2656 - val_loss: 3.1164\n",
      "Epoch 7/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 3.0392 - val_loss: 2.9300\n",
      "Epoch 8/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 2.8757 - val_loss: 2.7924\n",
      "Epoch 9/300\n",
      "11/11 [==============================] - 11s 976ms/step - loss: 2.7541 - val_loss: 2.6874\n",
      "Epoch 10/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 2.6588 - val_loss: 2.6045\n",
      "Epoch 11/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 2.5818 - val_loss: 2.5349\n",
      "Epoch 12/300\n",
      "11/11 [==============================] - 11s 990ms/step - loss: 2.5162 - val_loss: 2.4741\n",
      "Epoch 13/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 2.4588 - val_loss: 2.4198\n",
      "Epoch 14/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 2.4062 - val_loss: 2.3710\n",
      "Epoch 15/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 2.3565 - val_loss: 2.3214\n",
      "Epoch 16/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 2.3096 - val_loss: 2.2756\n",
      "Epoch 17/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 2.2656 - val_loss: 2.2318\n",
      "Epoch 18/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 2.2218 - val_loss: 2.1891\n",
      "Epoch 19/300\n",
      "11/11 [==============================] - 11s 976ms/step - loss: 2.1781 - val_loss: 2.1482\n",
      "Epoch 20/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 2.1392 - val_loss: 2.1064\n",
      "Epoch 21/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 2.0974 - val_loss: 2.0667\n",
      "Epoch 22/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 2.0580 - val_loss: 2.0280\n",
      "Epoch 23/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 2.0210 - val_loss: 1.9905\n",
      "Epoch 24/300\n",
      "11/11 [==============================] - 11s 994ms/step - loss: 1.9819 - val_loss: 1.9523\n",
      "Epoch 25/300\n",
      "11/11 [==============================] - 11s 974ms/step - loss: 1.9441 - val_loss: 1.9158\n",
      "Epoch 26/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 1.9091 - val_loss: 1.8796\n",
      "Epoch 27/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 1.8734 - val_loss: 1.8445\n",
      "Epoch 28/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 1.8374 - val_loss: 1.8103\n",
      "Epoch 29/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 1.8033 - val_loss: 1.7765\n",
      "Epoch 30/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 1.7704 - val_loss: 1.7433\n",
      "Epoch 31/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 1.7366 - val_loss: 1.7108\n",
      "Epoch 32/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 1.7051 - val_loss: 1.6791\n",
      "Epoch 33/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 1.6741 - val_loss: 1.6478\n",
      "Epoch 34/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 1.6422 - val_loss: 1.6180\n",
      "Epoch 35/300\n",
      "11/11 [==============================] - 11s 977ms/step - loss: 1.6127 - val_loss: 1.5868\n",
      "Epoch 36/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 1.5821 - val_loss: 1.5578\n",
      "Epoch 37/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 1.5534 - val_loss: 1.5292\n",
      "Epoch 38/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 1.5248 - val_loss: 1.5019\n",
      "Epoch 39/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 1.4977 - val_loss: 1.4735\n",
      "Epoch 40/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 1.4705 - val_loss: 1.4474\n",
      "Epoch 41/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 1.4432 - val_loss: 1.4206\n",
      "Epoch 42/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 1.4175 - val_loss: 1.3949\n",
      "Epoch 43/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 1.3917 - val_loss: 1.3705\n",
      "Epoch 44/300\n",
      "11/11 [==============================] - 11s 996ms/step - loss: 1.3669 - val_loss: 1.3452\n",
      "Epoch 45/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 1.3432 - val_loss: 1.3210\n",
      "Epoch 46/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 1.3189 - val_loss: 1.2982\n",
      "Epoch 47/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 1.2958 - val_loss: 1.2753\n",
      "Epoch 48/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 1.2727 - val_loss: 1.2522\n",
      "Epoch 49/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 1.2504 - val_loss: 1.2304\n",
      "Epoch 50/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 1.2285 - val_loss: 1.2093\n",
      "Epoch 51/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 1.2076 - val_loss: 1.1883\n",
      "Epoch 52/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 1.1875 - val_loss: 1.1676\n",
      "Epoch 53/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 1.1669 - val_loss: 1.1476\n",
      "Epoch 54/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 1.1473 - val_loss: 1.1279\n",
      "Epoch 55/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 1.1274 - val_loss: 1.1095\n",
      "Epoch 56/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 1.1088 - val_loss: 1.0906\n",
      "Epoch 57/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 1.0900 - val_loss: 1.0724\n",
      "Epoch 58/300\n",
      "11/11 [==============================] - 11s 992ms/step - loss: 1.0731 - val_loss: 1.0545\n",
      "Epoch 59/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 1.0552 - val_loss: 1.0370\n",
      "Epoch 60/300\n",
      "11/11 [==============================] - 11s 990ms/step - loss: 1.0382 - val_loss: 1.0213\n",
      "Epoch 61/300\n",
      "11/11 [==============================] - 11s 990ms/step - loss: 1.0224 - val_loss: 1.0054\n",
      "Epoch 62/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 1.0046 - val_loss: 0.9890\n",
      "Epoch 63/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.9901 - val_loss: 0.9730\n",
      "Epoch 64/300\n",
      "11/11 [==============================] - 11s 992ms/step - loss: 0.9742 - val_loss: 0.9583\n",
      "Epoch 65/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.9598 - val_loss: 0.9433\n",
      "Epoch 66/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.9441 - val_loss: 0.9289\n",
      "Epoch 67/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.9304 - val_loss: 0.9149\n",
      "Epoch 68/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.9163 - val_loss: 0.9018\n",
      "Epoch 69/300\n",
      "11/11 [==============================] - 11s 977ms/step - loss: 0.9031 - val_loss: 0.8880\n",
      "Epoch 70/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.8905 - val_loss: 0.8750\n",
      "Epoch 71/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.8771 - val_loss: 0.8633\n",
      "Epoch 72/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.8647 - val_loss: 0.8505\n",
      "Epoch 73/300\n",
      "11/11 [==============================] - 11s 992ms/step - loss: 0.8529 - val_loss: 0.8387\n",
      "Epoch 74/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.8417 - val_loss: 0.8282\n",
      "Epoch 75/300\n",
      "11/11 [==============================] - 11s 990ms/step - loss: 0.8298 - val_loss: 0.8162\n",
      "Epoch 76/300\n",
      "11/11 [==============================] - 11s 992ms/step - loss: 0.8186 - val_loss: 0.8052\n",
      "Epoch 77/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.8078 - val_loss: 0.7946\n",
      "Epoch 78/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.7974 - val_loss: 0.7841\n",
      "Epoch 79/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 0.7873 - val_loss: 0.7742\n",
      "Epoch 80/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.7781 - val_loss: 0.7646\n",
      "Epoch 81/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.7685 - val_loss: 0.7561\n",
      "Epoch 82/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.7585 - val_loss: 0.7462\n",
      "Epoch 83/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.7499 - val_loss: 0.7373\n",
      "Epoch 84/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.7413 - val_loss: 0.7292\n",
      "Epoch 85/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.7335 - val_loss: 0.7208\n",
      "Epoch 86/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.7249 - val_loss: 0.7141\n",
      "Epoch 87/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.7164 - val_loss: 0.7047\n",
      "Epoch 88/300\n",
      "11/11 [==============================] - 11s 992ms/step - loss: 0.7084 - val_loss: 0.6971\n",
      "Epoch 89/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.7011 - val_loss: 0.6912\n",
      "Epoch 90/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.6938 - val_loss: 0.6828\n",
      "Epoch 91/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.6875 - val_loss: 0.6760\n",
      "Epoch 92/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.6804 - val_loss: 0.6694\n",
      "Epoch 93/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.6736 - val_loss: 0.6629\n",
      "Epoch 94/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 0.6672 - val_loss: 0.6565\n",
      "Epoch 95/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.6610 - val_loss: 0.6508\n",
      "Epoch 96/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.6553 - val_loss: 0.6448\n",
      "Epoch 97/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.6497 - val_loss: 0.6395\n",
      "Epoch 98/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 0.6438 - val_loss: 0.6339\n",
      "Epoch 99/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 0.6389 - val_loss: 0.6285\n",
      "Epoch 100/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 0.6332 - val_loss: 0.6237\n",
      "Epoch 101/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.6287 - val_loss: 0.6186\n",
      "Epoch 102/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.6238 - val_loss: 0.6145\n",
      "Epoch 103/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.6193 - val_loss: 0.6097\n",
      "Epoch 104/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.6144 - val_loss: 0.6060\n",
      "Epoch 105/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.6102 - val_loss: 0.6009\n",
      "Epoch 106/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.6059 - val_loss: 0.5969\n",
      "Epoch 107/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.6022 - val_loss: 0.5929\n",
      "Epoch 108/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5978 - val_loss: 0.5891\n",
      "Epoch 109/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.5942 - val_loss: 0.5855\n",
      "Epoch 110/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.5910 - val_loss: 0.5820\n",
      "Epoch 111/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.5871 - val_loss: 0.5786\n",
      "Epoch 112/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.5842 - val_loss: 0.5753\n",
      "Epoch 113/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 0.5809 - val_loss: 0.5724\n",
      "Epoch 114/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 0.5778 - val_loss: 0.5692\n",
      "Epoch 115/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5750 - val_loss: 0.5663\n",
      "Epoch 116/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5725 - val_loss: 0.5636\n",
      "Epoch 117/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5709 - val_loss: 0.5621\n",
      "Epoch 118/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.5668 - val_loss: 0.5588\n",
      "Epoch 119/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 0.5641 - val_loss: 0.5564\n",
      "Epoch 120/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5620 - val_loss: 0.5541\n",
      "Epoch 121/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 0.5608 - val_loss: 0.5515\n",
      "Epoch 122/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 0.5578 - val_loss: 0.5510\n",
      "Epoch 123/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.5557 - val_loss: 0.5473\n",
      "Epoch 124/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 0.5540 - val_loss: 0.5456\n",
      "Epoch 125/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5527 - val_loss: 0.5441\n",
      "Epoch 126/300\n",
      "11/11 [==============================] - 11s 990ms/step - loss: 0.5502 - val_loss: 0.5425\n",
      "Epoch 127/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5481 - val_loss: 0.5398\n",
      "Epoch 128/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5458 - val_loss: 0.5386\n",
      "Epoch 129/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 0.5444 - val_loss: 0.5367\n",
      "Epoch 130/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.5424 - val_loss: 0.5350\n",
      "Epoch 131/300\n",
      "11/11 [==============================] - 11s 990ms/step - loss: 0.5419 - val_loss: 0.5336\n",
      "Epoch 132/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 0.5400 - val_loss: 0.5322\n",
      "Epoch 133/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5389 - val_loss: 0.5311\n",
      "Epoch 134/300\n",
      "11/11 [==============================] - 11s 996ms/step - loss: 0.5369 - val_loss: 0.5296\n",
      "Epoch 135/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.5365 - val_loss: 0.5284\n",
      "Epoch 136/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.5349 - val_loss: 0.5282\n",
      "Epoch 137/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5335 - val_loss: 0.5264\n",
      "Epoch 138/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5324 - val_loss: 0.5254\n",
      "Epoch 139/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5314 - val_loss: 0.5244\n",
      "Epoch 140/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5319 - val_loss: 0.5235\n",
      "Epoch 141/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5295 - val_loss: 0.5229\n",
      "Epoch 142/300\n",
      "11/11 [==============================] - 11s 990ms/step - loss: 0.5295 - val_loss: 0.5219\n",
      "Epoch 143/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5275 - val_loss: 0.5207\n",
      "Epoch 144/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.5276 - val_loss: 0.5199\n",
      "Epoch 145/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.5260 - val_loss: 0.5197\n",
      "Epoch 146/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5259 - val_loss: 0.5190\n",
      "Epoch 147/300\n",
      "11/11 [==============================] - 11s 990ms/step - loss: 0.5269 - val_loss: 0.5181\n",
      "Epoch 148/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 0.5243 - val_loss: 0.5182\n",
      "Epoch 149/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 0.5238 - val_loss: 0.5166\n",
      "Epoch 150/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5237 - val_loss: 0.5161\n",
      "Epoch 151/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5221 - val_loss: 0.5157\n",
      "Epoch 152/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5219 - val_loss: 0.5150\n",
      "Epoch 153/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5214 - val_loss: 0.5146\n",
      "Epoch 154/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5213 - val_loss: 0.5145\n",
      "Epoch 155/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5207 - val_loss: 0.5136\n",
      "Epoch 156/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.5200 - val_loss: 0.5134\n",
      "Epoch 157/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5197 - val_loss: 0.5130\n",
      "Epoch 158/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5192 - val_loss: 0.5126\n",
      "Epoch 159/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.5194 - val_loss: 0.5122\n",
      "Epoch 160/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.5185 - val_loss: 0.5117\n",
      "Epoch 161/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.5183 - val_loss: 0.5116\n",
      "Epoch 162/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 0.5178 - val_loss: 0.5112\n",
      "Epoch 163/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5177 - val_loss: 0.5110\n",
      "Epoch 164/300\n",
      "11/11 [==============================] - 11s 990ms/step - loss: 0.5172 - val_loss: 0.5107\n",
      "Epoch 165/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5173 - val_loss: 0.5108\n",
      "Epoch 166/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5171 - val_loss: 0.5103\n",
      "Epoch 167/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.5168 - val_loss: 0.5102\n",
      "Epoch 168/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5169 - val_loss: 0.5099\n",
      "Epoch 169/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.5164 - val_loss: 0.5099\n",
      "Epoch 170/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.5166 - val_loss: 0.5097\n",
      "Epoch 171/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5158 - val_loss: 0.5093\n",
      "Epoch 172/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 0.5155 - val_loss: 0.5090\n",
      "Epoch 173/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5154 - val_loss: 0.5089\n",
      "Epoch 174/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5157 - val_loss: 0.5090\n",
      "Epoch 175/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.5155 - val_loss: 0.5084\n",
      "Epoch 176/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.5152 - val_loss: 0.5088\n",
      "Epoch 177/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.5146 - val_loss: 0.5089\n",
      "Epoch 178/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.5156 - val_loss: 0.5093\n",
      "Epoch 179/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.5149 - val_loss: 0.5082\n",
      "Epoch 180/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.5153 - val_loss: 0.5084\n",
      "Epoch 181/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.5147 - val_loss: 0.5079\n",
      "Epoch 182/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.5144 - val_loss: 0.5078\n",
      "Epoch 183/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5144 - val_loss: 0.5079\n",
      "Epoch 184/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.5149 - val_loss: 0.5081\n",
      "Epoch 185/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.5152 - val_loss: 0.5078\n",
      "Epoch 186/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 0.5148 - val_loss: 0.5081\n",
      "Epoch 187/300\n",
      "11/11 [==============================] - 11s 990ms/step - loss: 0.5150 - val_loss: 0.5078\n",
      "Epoch 188/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.5167 - val_loss: 0.5083\n",
      "Epoch 189/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.5143 - val_loss: 0.5076\n",
      "Epoch 190/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5145 - val_loss: 0.5074\n",
      "Epoch 191/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 0.5139 - val_loss: 0.5075\n",
      "Epoch 192/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.5139 - val_loss: 0.5072\n",
      "Epoch 193/300\n",
      "11/11 [==============================] - 11s 993ms/step - loss: 0.5140 - val_loss: 0.5073\n",
      "Epoch 194/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5137 - val_loss: 0.5074\n",
      "Epoch 195/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5141 - val_loss: 0.5076\n",
      "Epoch 196/300\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5142Restoring model weights from the end of the best epoch: 192.\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.5142 - val_loss: 0.5076\n",
      "Epoch 00196: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_12/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_12/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf9b65ca10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcfa122ad90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "Epoch 1/300\n",
      "11/11 [==============================] - 13s 1s/step - loss: 7.1278 - val_loss: 6.2113\n",
      "Epoch 2/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 5.6628 - val_loss: 5.0633\n",
      "Epoch 3/300\n",
      "11/11 [==============================] - 11s 977ms/step - loss: 4.7065 - val_loss: 4.2758\n",
      "Epoch 4/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 4.0359 - val_loss: 3.7385\n",
      "Epoch 5/300\n",
      "11/11 [==============================] - 11s 975ms/step - loss: 3.5744 - val_loss: 3.3680\n",
      "Epoch 6/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 3.2576 - val_loss: 3.1087\n",
      "Epoch 7/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 3.0322 - val_loss: 2.9243\n",
      "Epoch 8/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 2.8705 - val_loss: 2.7882\n",
      "Epoch 9/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 2.7495 - val_loss: 2.6844\n",
      "Epoch 10/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 2.6563 - val_loss: 2.6019\n",
      "Epoch 11/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 2.5792 - val_loss: 2.5331\n",
      "Epoch 12/300\n",
      "11/11 [==============================] - 11s 976ms/step - loss: 2.5146 - val_loss: 2.4724\n",
      "Epoch 13/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 2.4569 - val_loss: 2.4187\n",
      "Epoch 14/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 2.4051 - val_loss: 2.3683\n",
      "Epoch 15/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 2.3563 - val_loss: 2.3211\n",
      "Epoch 16/300\n",
      "11/11 [==============================] - 11s 975ms/step - loss: 2.3091 - val_loss: 2.2750\n",
      "Epoch 17/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 2.2646 - val_loss: 2.2314\n",
      "Epoch 18/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 2.2218 - val_loss: 2.1887\n",
      "Epoch 19/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 2.1801 - val_loss: 2.1478\n",
      "Epoch 20/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 2.1379 - val_loss: 2.1062\n",
      "Epoch 21/300\n",
      "11/11 [==============================] - 11s 976ms/step - loss: 2.0969 - val_loss: 2.0665\n",
      "Epoch 22/300\n",
      "11/11 [==============================] - 11s 972ms/step - loss: 2.0579 - val_loss: 2.0270\n",
      "Epoch 23/300\n",
      "11/11 [==============================] - 11s 974ms/step - loss: 2.0198 - val_loss: 1.9894\n",
      "Epoch 24/300\n",
      "11/11 [==============================] - 11s 974ms/step - loss: 1.9815 - val_loss: 1.9521\n",
      "Epoch 25/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 1.9448 - val_loss: 1.9163\n",
      "Epoch 26/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 1.9076 - val_loss: 1.8797\n",
      "Epoch 27/300\n",
      "11/11 [==============================] - 11s 977ms/step - loss: 1.8718 - val_loss: 1.8446\n",
      "Epoch 28/300\n",
      "11/11 [==============================] - 11s 976ms/step - loss: 1.8373 - val_loss: 1.8097\n",
      "Epoch 29/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 1.8038 - val_loss: 1.7759\n",
      "Epoch 30/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 1.7709 - val_loss: 1.7428\n",
      "Epoch 31/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 1.7390 - val_loss: 1.7116\n",
      "Epoch 32/300\n",
      "11/11 [==============================] - 11s 973ms/step - loss: 1.7050 - val_loss: 1.6794\n",
      "Epoch 33/300\n",
      "11/11 [==============================] - 11s 975ms/step - loss: 1.6729 - val_loss: 1.6483\n",
      "Epoch 34/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 1.6422 - val_loss: 1.6168\n",
      "Epoch 35/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 1.6121 - val_loss: 1.5868\n",
      "Epoch 36/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 1.5823 - val_loss: 1.5580\n",
      "Epoch 37/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 1.5542 - val_loss: 1.5289\n",
      "Epoch 38/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 1.5253 - val_loss: 1.5017\n",
      "Epoch 39/300\n",
      "11/11 [==============================] - 11s 976ms/step - loss: 1.4969 - val_loss: 1.4737\n",
      "Epoch 40/300\n",
      "11/11 [==============================] - 11s 977ms/step - loss: 1.4709 - val_loss: 1.4470\n",
      "Epoch 41/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 1.4445 - val_loss: 1.4214\n",
      "Epoch 42/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 1.4176 - val_loss: 1.3949\n",
      "Epoch 43/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 1.3920 - val_loss: 1.3697\n",
      "Epoch 44/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 1.3667 - val_loss: 1.3448\n",
      "Epoch 45/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 1.3422 - val_loss: 1.3211\n",
      "Epoch 46/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 1.3205 - val_loss: 1.2976\n",
      "Epoch 47/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 1.2955 - val_loss: 1.2763\n",
      "Epoch 48/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 1.2725 - val_loss: 1.2522\n",
      "Epoch 49/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 1.2504 - val_loss: 1.2305\n",
      "Epoch 50/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 1.2286 - val_loss: 1.2094\n",
      "Epoch 51/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 1.2073 - val_loss: 1.1880\n",
      "Epoch 52/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 1.1864 - val_loss: 1.1679\n",
      "Epoch 53/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 1.1664 - val_loss: 1.1477\n",
      "Epoch 54/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 1.1466 - val_loss: 1.1282\n",
      "Epoch 55/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 1.1273 - val_loss: 1.1091\n",
      "Epoch 56/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 1.1092 - val_loss: 1.0910\n",
      "Epoch 57/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 1.0907 - val_loss: 1.0724\n",
      "Epoch 58/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 1.0724 - val_loss: 1.0552\n",
      "Epoch 59/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 1.0565 - val_loss: 1.0375\n",
      "Epoch 60/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 1.0382 - val_loss: 1.0218\n",
      "Epoch 61/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 1.0215 - val_loss: 1.0042\n",
      "Epoch 62/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 1.0060 - val_loss: 0.9888\n",
      "Epoch 63/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.9891 - val_loss: 0.9730\n",
      "Epoch 64/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.9739 - val_loss: 0.9581\n",
      "Epoch 65/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.9590 - val_loss: 0.9434\n",
      "Epoch 66/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.9445 - val_loss: 0.9289\n",
      "Epoch 67/300\n",
      "11/11 [==============================] - 11s 974ms/step - loss: 0.9301 - val_loss: 0.9147\n",
      "Epoch 68/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.9164 - val_loss: 0.9015\n",
      "Epoch 69/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.9028 - val_loss: 0.8879\n",
      "Epoch 70/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.8900 - val_loss: 0.8748\n",
      "Epoch 71/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.8771 - val_loss: 0.8625\n",
      "Epoch 72/300\n",
      "11/11 [==============================] - 11s 991ms/step - loss: 0.8649 - val_loss: 0.8505\n",
      "Epoch 73/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.8532 - val_loss: 0.8388\n",
      "Epoch 74/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.8414 - val_loss: 0.8273\n",
      "Epoch 75/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.8296 - val_loss: 0.8161\n",
      "Epoch 76/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.8186 - val_loss: 0.8048\n",
      "Epoch 77/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.8077 - val_loss: 0.7945\n",
      "Epoch 78/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.7972 - val_loss: 0.7842\n",
      "Epoch 79/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.7870 - val_loss: 0.7745\n",
      "Epoch 80/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.7775 - val_loss: 0.7647\n",
      "Epoch 81/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.7682 - val_loss: 0.7551\n",
      "Epoch 82/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.7586 - val_loss: 0.7462\n",
      "Epoch 83/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.7495 - val_loss: 0.7373\n",
      "Epoch 84/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.7405 - val_loss: 0.7292\n",
      "Epoch 85/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.7330 - val_loss: 0.7207\n",
      "Epoch 86/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.7250 - val_loss: 0.7125\n",
      "Epoch 87/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.7159 - val_loss: 0.7050\n",
      "Epoch 88/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.7084 - val_loss: 0.6975\n",
      "Epoch 89/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.7011 - val_loss: 0.6901\n",
      "Epoch 90/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.6937 - val_loss: 0.6828\n",
      "Epoch 91/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.6869 - val_loss: 0.6760\n",
      "Epoch 92/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.6797 - val_loss: 0.6698\n",
      "Epoch 93/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.6737 - val_loss: 0.6630\n",
      "Epoch 94/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.6670 - val_loss: 0.6566\n",
      "Epoch 95/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.6616 - val_loss: 0.6505\n",
      "Epoch 96/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.6553 - val_loss: 0.6452\n",
      "Epoch 97/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.6493 - val_loss: 0.6395\n",
      "Epoch 98/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.6438 - val_loss: 0.6339\n",
      "Epoch 99/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.6388 - val_loss: 0.6288\n",
      "Epoch 100/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.6332 - val_loss: 0.6235\n",
      "Epoch 101/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.6288 - val_loss: 0.6187\n",
      "Epoch 102/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.6234 - val_loss: 0.6141\n",
      "Epoch 103/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.6188 - val_loss: 0.6096\n",
      "Epoch 104/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.6144 - val_loss: 0.6052\n",
      "Epoch 105/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.6105 - val_loss: 0.6010\n",
      "Epoch 106/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.6066 - val_loss: 0.5973\n",
      "Epoch 107/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.6023 - val_loss: 0.5932\n",
      "Epoch 108/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.5983 - val_loss: 0.5894\n",
      "Epoch 109/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5945 - val_loss: 0.5855\n",
      "Epoch 110/300\n",
      "11/11 [==============================] - 11s 977ms/step - loss: 0.5909 - val_loss: 0.5822\n",
      "Epoch 111/300\n",
      "11/11 [==============================] - 11s 977ms/step - loss: 0.5875 - val_loss: 0.5791\n",
      "Epoch 112/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5844 - val_loss: 0.5756\n",
      "Epoch 113/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.5812 - val_loss: 0.5726\n",
      "Epoch 114/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5776 - val_loss: 0.5705\n",
      "Epoch 115/300\n",
      "11/11 [==============================] - 11s 976ms/step - loss: 0.5752 - val_loss: 0.5666\n",
      "Epoch 116/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5719 - val_loss: 0.5639\n",
      "Epoch 117/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5718 - val_loss: 0.5612\n",
      "Epoch 118/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.5662 - val_loss: 0.5595\n",
      "Epoch 119/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.5649 - val_loss: 0.5561\n",
      "Epoch 120/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.5622 - val_loss: 0.5537\n",
      "Epoch 121/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.5593 - val_loss: 0.5520\n",
      "Epoch 122/300\n",
      "11/11 [==============================] - 11s 975ms/step - loss: 0.5576 - val_loss: 0.5494\n",
      "Epoch 123/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.5555 - val_loss: 0.5472\n",
      "Epoch 124/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.5533 - val_loss: 0.5454\n",
      "Epoch 125/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.5516 - val_loss: 0.5439\n",
      "Epoch 126/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5493 - val_loss: 0.5416\n",
      "Epoch 127/300\n",
      "11/11 [==============================] - 11s 977ms/step - loss: 0.5474 - val_loss: 0.5399\n",
      "Epoch 128/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5458 - val_loss: 0.5386\n",
      "Epoch 129/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.5441 - val_loss: 0.5368\n",
      "Epoch 130/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.5426 - val_loss: 0.5352\n",
      "Epoch 131/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.5411 - val_loss: 0.5337\n",
      "Epoch 132/300\n",
      "11/11 [==============================] - 11s 977ms/step - loss: 0.5400 - val_loss: 0.5322\n",
      "Epoch 133/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5383 - val_loss: 0.5311\n",
      "Epoch 134/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5370 - val_loss: 0.5298\n",
      "Epoch 135/300\n",
      "11/11 [==============================] - 11s 974ms/step - loss: 0.5359 - val_loss: 0.5284\n",
      "Epoch 136/300\n",
      "11/11 [==============================] - 11s 975ms/step - loss: 0.5351 - val_loss: 0.5273\n",
      "Epoch 137/300\n",
      "11/11 [==============================] - 11s 977ms/step - loss: 0.5335 - val_loss: 0.5262\n",
      "Epoch 138/300\n",
      "11/11 [==============================] - 11s 973ms/step - loss: 0.5322 - val_loss: 0.5252\n",
      "Epoch 139/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5317 - val_loss: 0.5245\n",
      "Epoch 140/300\n",
      "11/11 [==============================] - 11s 977ms/step - loss: 0.5304 - val_loss: 0.5232\n",
      "Epoch 141/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.5298 - val_loss: 0.5224\n",
      "Epoch 142/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5287 - val_loss: 0.5214\n",
      "Epoch 143/300\n",
      "11/11 [==============================] - 11s 976ms/step - loss: 0.5284 - val_loss: 0.5208\n",
      "Epoch 144/300\n",
      "11/11 [==============================] - 11s 976ms/step - loss: 0.5281 - val_loss: 0.5208\n",
      "Epoch 145/300\n",
      "11/11 [==============================] - 11s 972ms/step - loss: 0.5280 - val_loss: 0.5193\n",
      "Epoch 146/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.5253 - val_loss: 0.5190\n",
      "Epoch 147/300\n",
      "11/11 [==============================] - 11s 975ms/step - loss: 0.5249 - val_loss: 0.5179\n",
      "Epoch 148/300\n",
      "11/11 [==============================] - 11s 972ms/step - loss: 0.5243 - val_loss: 0.5173\n",
      "Epoch 149/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5234 - val_loss: 0.5167\n",
      "Epoch 150/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.5234 - val_loss: 0.5162\n",
      "Epoch 151/300\n",
      "11/11 [==============================] - 11s 969ms/step - loss: 0.5223 - val_loss: 0.5157\n",
      "Epoch 152/300\n",
      "11/11 [==============================] - 11s 974ms/step - loss: 0.5220 - val_loss: 0.5150\n",
      "Epoch 153/300\n",
      "11/11 [==============================] - 11s 974ms/step - loss: 0.5214 - val_loss: 0.5146\n",
      "Epoch 154/300\n",
      "11/11 [==============================] - 11s 968ms/step - loss: 0.5210 - val_loss: 0.5140\n",
      "Epoch 155/300\n",
      "11/11 [==============================] - 11s 971ms/step - loss: 0.5214 - val_loss: 0.5142\n",
      "Epoch 156/300\n",
      "11/11 [==============================] - 11s 974ms/step - loss: 0.5204 - val_loss: 0.5131\n",
      "Epoch 157/300\n",
      "11/11 [==============================] - 11s 976ms/step - loss: 0.5200 - val_loss: 0.5129\n",
      "Epoch 158/300\n",
      "11/11 [==============================] - 11s 976ms/step - loss: 0.5205 - val_loss: 0.5126\n",
      "Epoch 159/300\n",
      "11/11 [==============================] - 11s 973ms/step - loss: 0.5197 - val_loss: 0.5132\n",
      "Epoch 160/300\n",
      "11/11 [==============================] - 11s 974ms/step - loss: 0.5181 - val_loss: 0.5119\n",
      "Epoch 161/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.5182 - val_loss: 0.5115\n",
      "Epoch 162/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5180 - val_loss: 0.5113\n",
      "Epoch 163/300\n",
      "11/11 [==============================] - 11s 970ms/step - loss: 0.5181 - val_loss: 0.5109\n",
      "Epoch 164/300\n",
      "11/11 [==============================] - 11s 972ms/step - loss: 0.5178 - val_loss: 0.5108\n",
      "Epoch 165/300\n",
      "11/11 [==============================] - 11s 971ms/step - loss: 0.5173 - val_loss: 0.5103\n",
      "Epoch 166/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5171 - val_loss: 0.5103\n",
      "Epoch 167/300\n",
      "11/11 [==============================] - 11s 976ms/step - loss: 0.5169 - val_loss: 0.5099\n",
      "Epoch 168/300\n",
      "11/11 [==============================] - 11s 974ms/step - loss: 0.5172 - val_loss: 0.5102\n",
      "Epoch 169/300\n",
      "11/11 [==============================] - 11s 972ms/step - loss: 0.5164 - val_loss: 0.5097\n",
      "Epoch 170/300\n",
      "11/11 [==============================] - 11s 971ms/step - loss: 0.5159 - val_loss: 0.5095\n",
      "Epoch 171/300\n",
      "11/11 [==============================] - 11s 972ms/step - loss: 0.5160 - val_loss: 0.5094\n",
      "Epoch 172/300\n",
      "11/11 [==============================] - 11s 976ms/step - loss: 0.5160 - val_loss: 0.5090\n",
      "Epoch 173/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5156 - val_loss: 0.5091\n",
      "Epoch 174/300\n",
      "11/11 [==============================] - 11s 971ms/step - loss: 0.5154 - val_loss: 0.5087\n",
      "Epoch 175/300\n",
      "11/11 [==============================] - 11s 968ms/step - loss: 0.5155 - val_loss: 0.5087\n",
      "Epoch 176/300\n",
      "11/11 [==============================] - 11s 970ms/step - loss: 0.5151 - val_loss: 0.5086\n",
      "Epoch 177/300\n",
      "11/11 [==============================] - 11s 969ms/step - loss: 0.5154 - val_loss: 0.5084\n",
      "Epoch 178/300\n",
      "11/11 [==============================] - 11s 967ms/step - loss: 0.5150 - val_loss: 0.5080\n",
      "Epoch 179/300\n",
      "11/11 [==============================] - 11s 966ms/step - loss: 0.5150 - val_loss: 0.5085\n",
      "Epoch 180/300\n",
      "11/11 [==============================] - 11s 968ms/step - loss: 0.5150 - val_loss: 0.5084\n",
      "Epoch 181/300\n",
      "11/11 [==============================] - 11s 966ms/step - loss: 0.5153 - val_loss: 0.5083\n",
      "Epoch 182/300\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5148Restoring model weights from the end of the best epoch: 178.\n",
      "11/11 [==============================] - 11s 967ms/step - loss: 0.5148 - val_loss: 0.5082\n",
      "Epoch 00182: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_13/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_13/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf80fa2290> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf80d17290> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "Epoch 1/300\n",
      "11/11 [==============================] - 13s 1s/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "11/11 [==============================] - 11s 972ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "11/11 [==============================] - 11s 973ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "11/11 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "11/11 [==============================] - 11s 970ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_14/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_14/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf80d3c510> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcfa1b02590> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "Epoch 1/300\n",
      "11/11 [==============================] - 13s 1s/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "11/11 [==============================] - 11s 972ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "11/11 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "11/11 [==============================] - 11s 977ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_15/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_15/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf80afae50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf9b464490> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "Epoch 1/300\n",
      "11/11 [==============================] - 13s 1s/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "11/11 [==============================] - 11s 977ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "11/11 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "11/11 [==============================] - 11s 975ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_16/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_16/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcfa010f110> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf984ab9d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "Epoch 1/300\n",
      "11/11 [==============================] - 13s 1s/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "11/11 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "11/11 [==============================] - 11s 979ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_17/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_17/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcfa12390d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcfa1fa6a10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "Epoch 1/300\n",
      "11/11 [==============================] - 13s 1s/step - loss: 7.1157 - val_loss: 6.2049\n",
      "Epoch 2/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 5.6557 - val_loss: 5.0572\n",
      "Epoch 3/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 4.7057 - val_loss: 4.2720\n",
      "Epoch 4/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 4.0357 - val_loss: 3.7392\n",
      "Epoch 5/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 3.5768 - val_loss: 3.3690\n",
      "Epoch 6/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 3.2587 - val_loss: 3.1113\n",
      "Epoch 7/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 3.0348 - val_loss: 2.9258\n",
      "Epoch 8/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 2.8722 - val_loss: 2.7889\n",
      "Epoch 9/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 2.7506 - val_loss: 2.6852\n",
      "Epoch 10/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 2.6567 - val_loss: 2.6026\n",
      "Epoch 11/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 2.5803 - val_loss: 2.5335\n",
      "Epoch 12/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 2.5148 - val_loss: 2.4736\n",
      "Epoch 13/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 2.4581 - val_loss: 2.4186\n",
      "Epoch 14/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 2.4047 - val_loss: 2.3685\n",
      "Epoch 15/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 2.3554 - val_loss: 2.3212\n",
      "Epoch 16/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 2.3096 - val_loss: 2.2758\n",
      "Epoch 17/300\n",
      "11/11 [==============================] - 11s 996ms/step - loss: 2.2652 - val_loss: 2.2311\n",
      "Epoch 18/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 2.2214 - val_loss: 2.1885\n",
      "Epoch 19/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 2.1792 - val_loss: 2.1467\n",
      "Epoch 20/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 2.1384 - val_loss: 2.1061\n",
      "Epoch 21/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 2.0978 - val_loss: 2.0672\n",
      "Epoch 22/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 2.0585 - val_loss: 2.0272\n",
      "Epoch 23/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 2.0191 - val_loss: 1.9897\n",
      "Epoch 24/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 1.9810 - val_loss: 1.9520\n",
      "Epoch 25/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 1.9441 - val_loss: 1.9151\n",
      "Epoch 26/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 1.9082 - val_loss: 1.8792\n",
      "Epoch 27/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 1.8726 - val_loss: 1.8440\n",
      "Epoch 28/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 1.8369 - val_loss: 1.8097\n",
      "Epoch 29/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 1.8029 - val_loss: 1.7758\n",
      "Epoch 30/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 1.7693 - val_loss: 1.7421\n",
      "Epoch 31/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 1.7365 - val_loss: 1.7100\n",
      "Epoch 32/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 1.7042 - val_loss: 1.6782\n",
      "Epoch 33/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 1.6731 - val_loss: 1.6470\n",
      "Epoch 34/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 1.6429 - val_loss: 1.6170\n",
      "Epoch 35/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 1.6114 - val_loss: 1.5870\n",
      "Epoch 36/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 1.5835 - val_loss: 1.5580\n",
      "Epoch 37/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 1.5529 - val_loss: 1.5299\n",
      "Epoch 38/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 1.5246 - val_loss: 1.5011\n",
      "Epoch 39/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 1.4976 - val_loss: 1.4735\n",
      "Epoch 40/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 1.4694 - val_loss: 1.4470\n",
      "Epoch 41/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 1.4432 - val_loss: 1.4203\n",
      "Epoch 42/300\n",
      "11/11 [==============================] - 11s 999ms/step - loss: 1.4174 - val_loss: 1.3948\n",
      "Epoch 43/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 1.3915 - val_loss: 1.3695\n",
      "Epoch 44/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 1.3674 - val_loss: 1.3454\n",
      "Epoch 45/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 1.3428 - val_loss: 1.3214\n",
      "Epoch 46/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 1.3200 - val_loss: 1.2981\n",
      "Epoch 47/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 1.2956 - val_loss: 1.2746\n",
      "Epoch 48/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 1.2726 - val_loss: 1.2521\n",
      "Epoch 49/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 1.2505 - val_loss: 1.2303\n",
      "Epoch 50/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 1.2286 - val_loss: 1.2091\n",
      "Epoch 51/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 1.2082 - val_loss: 1.1880\n",
      "Epoch 52/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 1.1866 - val_loss: 1.1681\n",
      "Epoch 53/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 1.1666 - val_loss: 1.1474\n",
      "Epoch 54/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 1.1469 - val_loss: 1.1283\n",
      "Epoch 55/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 1.1276 - val_loss: 1.1093\n",
      "Epoch 56/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 1.1086 - val_loss: 1.0909\n",
      "Epoch 57/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 1.0918 - val_loss: 1.0727\n",
      "Epoch 58/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 1.0723 - val_loss: 1.0554\n",
      "Epoch 59/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 1.0547 - val_loss: 1.0374\n",
      "Epoch 60/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 1.0379 - val_loss: 1.0206\n",
      "Epoch 61/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 1.0227 - val_loss: 1.0052\n",
      "Epoch 62/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 1.0048 - val_loss: 0.9888\n",
      "Epoch 63/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.9892 - val_loss: 0.9729\n",
      "Epoch 64/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.9742 - val_loss: 0.9580\n",
      "Epoch 65/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.9588 - val_loss: 0.9430\n",
      "Epoch 66/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.9445 - val_loss: 0.9287\n",
      "Epoch 67/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.9317 - val_loss: 0.9152\n",
      "Epoch 68/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.9162 - val_loss: 0.9016\n",
      "Epoch 69/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.9025 - val_loss: 0.8880\n",
      "Epoch 70/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.8904 - val_loss: 0.8756\n",
      "Epoch 71/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.8769 - val_loss: 0.8629\n",
      "Epoch 72/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.8649 - val_loss: 0.8504\n",
      "Epoch 73/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.8524 - val_loss: 0.8389\n",
      "Epoch 74/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.8413 - val_loss: 0.8275\n",
      "Epoch 75/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.8306 - val_loss: 0.8170\n",
      "Epoch 76/300\n",
      "11/11 [==============================] - 11s 991ms/step - loss: 0.8187 - val_loss: 0.8054\n",
      "Epoch 77/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.8077 - val_loss: 0.7950\n",
      "Epoch 78/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 0.7977 - val_loss: 0.7844\n",
      "Epoch 79/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.7871 - val_loss: 0.7744\n",
      "Epoch 80/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.7782 - val_loss: 0.7644\n",
      "Epoch 81/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.7678 - val_loss: 0.7551\n",
      "Epoch 82/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.7589 - val_loss: 0.7464\n",
      "Epoch 83/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.7494 - val_loss: 0.7371\n",
      "Epoch 84/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.7407 - val_loss: 0.7289\n",
      "Epoch 85/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.7321 - val_loss: 0.7207\n",
      "Epoch 86/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.7246 - val_loss: 0.7128\n",
      "Epoch 87/300\n",
      "11/11 [==============================] - 11s 977ms/step - loss: 0.7166 - val_loss: 0.7046\n",
      "Epoch 88/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.7084 - val_loss: 0.6970\n",
      "Epoch 89/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.7013 - val_loss: 0.6900\n",
      "Epoch 90/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.6940 - val_loss: 0.6830\n",
      "Epoch 91/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.6872 - val_loss: 0.6763\n",
      "Epoch 92/300\n",
      "11/11 [==============================] - 11s 977ms/step - loss: 0.6802 - val_loss: 0.6698\n",
      "Epoch 93/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.6733 - val_loss: 0.6630\n",
      "Epoch 94/300\n",
      "11/11 [==============================] - 11s 974ms/step - loss: 0.6682 - val_loss: 0.6567\n",
      "Epoch 95/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.6611 - val_loss: 0.6506\n",
      "Epoch 96/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.6550 - val_loss: 0.6447\n",
      "Epoch 97/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.6495 - val_loss: 0.6390\n",
      "Epoch 98/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.6439 - val_loss: 0.6339\n",
      "Epoch 99/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.6387 - val_loss: 0.6285\n",
      "Epoch 100/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.6331 - val_loss: 0.6240\n",
      "Epoch 101/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.6286 - val_loss: 0.6190\n",
      "Epoch 102/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.6234 - val_loss: 0.6141\n",
      "Epoch 103/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.6190 - val_loss: 0.6095\n",
      "Epoch 104/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.6153 - val_loss: 0.6053\n",
      "Epoch 105/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.6105 - val_loss: 0.6014\n",
      "Epoch 106/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.6072 - val_loss: 0.5975\n",
      "Epoch 107/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.6042 - val_loss: 0.5930\n",
      "Epoch 108/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5975 - val_loss: 0.5900\n",
      "Epoch 109/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5948 - val_loss: 0.5855\n",
      "Epoch 110/300\n",
      "11/11 [==============================] - 11s 977ms/step - loss: 0.5918 - val_loss: 0.5821\n",
      "Epoch 111/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5875 - val_loss: 0.5800\n",
      "Epoch 112/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.5845 - val_loss: 0.5753\n",
      "Epoch 113/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.5812 - val_loss: 0.5724\n",
      "Epoch 114/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5777 - val_loss: 0.5696\n",
      "Epoch 115/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.5755 - val_loss: 0.5664\n",
      "Epoch 116/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5725 - val_loss: 0.5643\n",
      "Epoch 117/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5693 - val_loss: 0.5611\n",
      "Epoch 118/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.5667 - val_loss: 0.5585\n",
      "Epoch 119/300\n",
      "11/11 [==============================] - 11s 990ms/step - loss: 0.5642 - val_loss: 0.5561\n",
      "Epoch 120/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5622 - val_loss: 0.5536\n",
      "Epoch 121/300\n",
      "11/11 [==============================] - 11s 991ms/step - loss: 0.5596 - val_loss: 0.5516\n",
      "Epoch 122/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 0.5573 - val_loss: 0.5494\n",
      "Epoch 123/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5550 - val_loss: 0.5473\n",
      "Epoch 124/300\n",
      "11/11 [==============================] - 11s 976ms/step - loss: 0.5534 - val_loss: 0.5453\n",
      "Epoch 125/300\n",
      "11/11 [==============================] - 11s 976ms/step - loss: 0.5521 - val_loss: 0.5437\n",
      "Epoch 126/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5494 - val_loss: 0.5427\n",
      "Epoch 127/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5479 - val_loss: 0.5400\n",
      "Epoch 128/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5466 - val_loss: 0.5383\n",
      "Epoch 129/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.5443 - val_loss: 0.5371\n",
      "Epoch 130/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5429 - val_loss: 0.5351\n",
      "Epoch 131/300\n",
      "11/11 [==============================] - 11s 974ms/step - loss: 0.5417 - val_loss: 0.5340\n",
      "Epoch 132/300\n",
      "11/11 [==============================] - 11s 976ms/step - loss: 0.5399 - val_loss: 0.5322\n",
      "Epoch 133/300\n",
      "11/11 [==============================] - 11s 977ms/step - loss: 0.5383 - val_loss: 0.5309\n",
      "Epoch 134/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5369 - val_loss: 0.5297\n",
      "Epoch 135/300\n",
      "11/11 [==============================] - 11s 975ms/step - loss: 0.5358 - val_loss: 0.5288\n",
      "Epoch 136/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5362 - val_loss: 0.5277\n",
      "Epoch 137/300\n",
      "11/11 [==============================] - 11s 972ms/step - loss: 0.5334 - val_loss: 0.5267\n",
      "Epoch 138/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5330 - val_loss: 0.5258\n",
      "Epoch 139/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5325 - val_loss: 0.5242\n",
      "Epoch 140/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.5301 - val_loss: 0.5235\n",
      "Epoch 141/300\n",
      "11/11 [==============================] - 11s 992ms/step - loss: 0.5303 - val_loss: 0.5227\n",
      "Epoch 142/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5289 - val_loss: 0.5215\n",
      "Epoch 143/300\n",
      "11/11 [==============================] - 11s 990ms/step - loss: 0.5278 - val_loss: 0.5209\n",
      "Epoch 144/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 0.5272 - val_loss: 0.5202\n",
      "Epoch 145/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5259 - val_loss: 0.5192\n",
      "Epoch 146/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5255 - val_loss: 0.5185\n",
      "Epoch 147/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5251 - val_loss: 0.5184\n",
      "Epoch 148/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 0.5240 - val_loss: 0.5171\n",
      "Epoch 149/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5237 - val_loss: 0.5166\n",
      "Epoch 150/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.5242 - val_loss: 0.5161\n",
      "Epoch 151/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5227 - val_loss: 0.5161\n",
      "Epoch 152/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.5224 - val_loss: 0.5148\n",
      "Epoch 153/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5216 - val_loss: 0.5147\n",
      "Epoch 154/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5213 - val_loss: 0.5141\n",
      "Epoch 155/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.5206 - val_loss: 0.5142\n",
      "Epoch 156/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5200 - val_loss: 0.5131\n",
      "Epoch 157/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.5199 - val_loss: 0.5130\n",
      "Epoch 158/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5196 - val_loss: 0.5126\n",
      "Epoch 159/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5186 - val_loss: 0.5134\n",
      "Epoch 160/300\n",
      "11/11 [==============================] - 11s 990ms/step - loss: 0.5195 - val_loss: 0.5117\n",
      "Epoch 161/300\n",
      "11/11 [==============================] - 11s 992ms/step - loss: 0.5182 - val_loss: 0.5117\n",
      "Epoch 162/300\n",
      "11/11 [==============================] - 11s 977ms/step - loss: 0.5178 - val_loss: 0.5112\n",
      "Epoch 163/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5176 - val_loss: 0.5111\n",
      "Epoch 164/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5174 - val_loss: 0.5106\n",
      "Epoch 165/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5171 - val_loss: 0.5107\n",
      "Epoch 166/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.5168 - val_loss: 0.5102\n",
      "Epoch 167/300\n",
      "11/11 [==============================] - 11s 977ms/step - loss: 0.5166 - val_loss: 0.5099\n",
      "Epoch 168/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.5162 - val_loss: 0.5102\n",
      "Epoch 169/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.5166 - val_loss: 0.5096\n",
      "Epoch 170/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5164 - val_loss: 0.5094\n",
      "Epoch 171/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.5162 - val_loss: 0.5097\n",
      "Epoch 172/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5166 - val_loss: 0.5090\n",
      "Epoch 173/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.5158 - val_loss: 0.5096\n",
      "Epoch 174/300\n",
      "11/11 [==============================] - 11s 990ms/step - loss: 0.5156 - val_loss: 0.5087\n",
      "Epoch 175/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5153 - val_loss: 0.5088\n",
      "Epoch 176/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5157 - val_loss: 0.5086\n",
      "Epoch 177/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5156 - val_loss: 0.5087\n",
      "Epoch 178/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.5152 - val_loss: 0.5083\n",
      "Epoch 179/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5152 - val_loss: 0.5088\n",
      "Epoch 180/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5149 - val_loss: 0.5080\n",
      "Epoch 181/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5149 - val_loss: 0.5080\n",
      "Epoch 182/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.5152 - val_loss: 0.5081\n",
      "Epoch 183/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 0.5145 - val_loss: 0.5079\n",
      "Epoch 184/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5143 - val_loss: 0.5079\n",
      "Epoch 185/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5149 - val_loss: 0.5080\n",
      "Epoch 186/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5145 - val_loss: 0.5076\n",
      "Epoch 187/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5146 - val_loss: 0.5075\n",
      "Epoch 188/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5144 - val_loss: 0.5075\n",
      "Epoch 189/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.5145 - val_loss: 0.5075\n",
      "Epoch 190/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.5141 - val_loss: 0.5078\n",
      "Epoch 191/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5139 - val_loss: 0.5073\n",
      "Epoch 192/300\n",
      "11/11 [==============================] - 11s 977ms/step - loss: 0.5144 - val_loss: 0.5072\n",
      "Epoch 193/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.5142 - val_loss: 0.5073\n",
      "Epoch 194/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 0.5136 - val_loss: 0.5075\n",
      "Epoch 195/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.5145 - val_loss: 0.5078\n",
      "Epoch 196/300\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5145Restoring model weights from the end of the best epoch: 192.\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5145 - val_loss: 0.5074\n",
      "Epoch 00196: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_18/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_18/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf98556b50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf80d36110> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "Epoch 1/300\n",
      "11/11 [==============================] - 13s 1s/step - loss: 7.1383 - val_loss: 6.2229\n",
      "Epoch 2/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 5.6851 - val_loss: 5.0692\n",
      "Epoch 3/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 4.7119 - val_loss: 4.2887\n",
      "Epoch 4/300\n",
      "11/11 [==============================] - 11s 974ms/step - loss: 4.0478 - val_loss: 3.7500\n",
      "Epoch 5/300\n",
      "11/11 [==============================] - 11s 992ms/step - loss: 3.5865 - val_loss: 3.3772\n",
      "Epoch 6/300\n",
      "11/11 [==============================] - 11s 976ms/step - loss: 3.2659 - val_loss: 3.1168\n",
      "Epoch 7/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 3.0397 - val_loss: 2.9306\n",
      "Epoch 8/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 2.8765 - val_loss: 2.7931\n",
      "Epoch 9/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 2.7548 - val_loss: 2.6882\n",
      "Epoch 10/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 2.6603 - val_loss: 2.6051\n",
      "Epoch 11/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 2.5823 - val_loss: 2.5351\n",
      "Epoch 12/300\n",
      "11/11 [==============================] - 11s 976ms/step - loss: 2.5192 - val_loss: 2.4759\n",
      "Epoch 13/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 2.4581 - val_loss: 2.4200\n",
      "Epoch 14/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 2.4076 - val_loss: 2.3699\n",
      "Epoch 15/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 2.3571 - val_loss: 2.3219\n",
      "Epoch 16/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 2.3102 - val_loss: 2.2765\n",
      "Epoch 17/300\n",
      "11/11 [==============================] - 11s 990ms/step - loss: 2.2652 - val_loss: 2.2320\n",
      "Epoch 18/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 2.2213 - val_loss: 2.1893\n",
      "Epoch 19/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 2.1810 - val_loss: 2.1483\n",
      "Epoch 20/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 2.1382 - val_loss: 2.1072\n",
      "Epoch 21/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 2.0978 - val_loss: 2.0670\n",
      "Epoch 22/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 2.0592 - val_loss: 2.0286\n",
      "Epoch 23/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 2.0195 - val_loss: 1.9894\n",
      "Epoch 24/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 1.9814 - val_loss: 1.9523\n",
      "Epoch 25/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 1.9455 - val_loss: 1.9159\n",
      "Epoch 26/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 1.9079 - val_loss: 1.8803\n",
      "Epoch 27/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 1.8723 - val_loss: 1.8445\n",
      "Epoch 28/300\n",
      "11/11 [==============================] - 11s 977ms/step - loss: 1.8386 - val_loss: 1.8101\n",
      "Epoch 29/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 1.8038 - val_loss: 1.7763\n",
      "Epoch 30/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 1.7698 - val_loss: 1.7429\n",
      "Epoch 31/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 1.7370 - val_loss: 1.7104\n",
      "Epoch 32/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 1.7049 - val_loss: 1.6789\n",
      "Epoch 33/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 1.6731 - val_loss: 1.6477\n",
      "Epoch 34/300\n",
      "11/11 [==============================] - 11s 977ms/step - loss: 1.6425 - val_loss: 1.6174\n",
      "Epoch 35/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 1.6123 - val_loss: 1.5875\n",
      "Epoch 36/300\n",
      "11/11 [==============================] - 11s 973ms/step - loss: 1.5823 - val_loss: 1.5576\n",
      "Epoch 37/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 1.5533 - val_loss: 1.5292\n",
      "Epoch 38/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 1.5251 - val_loss: 1.5012\n",
      "Epoch 39/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 1.4969 - val_loss: 1.4742\n",
      "Epoch 40/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 1.4705 - val_loss: 1.4473\n",
      "Epoch 41/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 1.4431 - val_loss: 1.4208\n",
      "Epoch 42/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 1.4176 - val_loss: 1.3951\n",
      "Epoch 43/300\n",
      "11/11 [==============================] - 12s 1s/step - loss: 1.3922 - val_loss: 1.3703\n",
      "Epoch 44/300\n",
      "11/11 [==============================] - 11s 1s/step - loss: 1.3674 - val_loss: 1.3451\n",
      "Epoch 45/300\n",
      "11/11 [==============================] - 11s 990ms/step - loss: 1.3428 - val_loss: 1.3215\n",
      "Epoch 46/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 1.3189 - val_loss: 1.2984\n",
      "Epoch 47/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 1.2954 - val_loss: 1.2749\n",
      "Epoch 48/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 1.2742 - val_loss: 1.2527\n",
      "Epoch 49/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 1.2530 - val_loss: 1.2321\n",
      "Epoch 50/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 1.2291 - val_loss: 1.2094\n",
      "Epoch 51/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 1.2074 - val_loss: 1.1886\n",
      "Epoch 52/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 1.1868 - val_loss: 1.1682\n",
      "Epoch 53/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 1.1664 - val_loss: 1.1479\n",
      "Epoch 54/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 1.1480 - val_loss: 1.1281\n",
      "Epoch 55/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 1.1274 - val_loss: 1.1092\n",
      "Epoch 56/300\n",
      "11/11 [==============================] - 11s 990ms/step - loss: 1.1090 - val_loss: 1.0908\n",
      "Epoch 57/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 1.0904 - val_loss: 1.0726\n",
      "Epoch 58/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 1.0728 - val_loss: 1.0552\n",
      "Epoch 59/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 1.0552 - val_loss: 1.0386\n",
      "Epoch 60/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 1.0389 - val_loss: 1.0211\n",
      "Epoch 61/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 1.0232 - val_loss: 1.0056\n",
      "Epoch 62/300\n",
      "11/11 [==============================] - 11s 977ms/step - loss: 1.0052 - val_loss: 0.9888\n",
      "Epoch 63/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.9897 - val_loss: 0.9737\n",
      "Epoch 64/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.9744 - val_loss: 0.9583\n",
      "Epoch 65/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.9591 - val_loss: 0.9433\n",
      "Epoch 66/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.9453 - val_loss: 0.9299\n",
      "Epoch 67/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.9298 - val_loss: 0.9153\n",
      "Epoch 68/300\n",
      "11/11 [==============================] - 11s 992ms/step - loss: 0.9178 - val_loss: 0.9018\n",
      "Epoch 69/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.9029 - val_loss: 0.8888\n",
      "Epoch 70/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.8899 - val_loss: 0.8757\n",
      "Epoch 71/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.8770 - val_loss: 0.8629\n",
      "Epoch 72/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.8647 - val_loss: 0.8510\n",
      "Epoch 73/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.8531 - val_loss: 0.8392\n",
      "Epoch 74/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.8406 - val_loss: 0.8274\n",
      "Epoch 75/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.8297 - val_loss: 0.8162\n",
      "Epoch 76/300\n",
      "11/11 [==============================] - 11s 972ms/step - loss: 0.8190 - val_loss: 0.8056\n",
      "Epoch 77/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.8077 - val_loss: 0.7946\n",
      "Epoch 78/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.7973 - val_loss: 0.7844\n",
      "Epoch 79/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.7884 - val_loss: 0.7750\n",
      "Epoch 80/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.7776 - val_loss: 0.7652\n",
      "Epoch 81/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.7686 - val_loss: 0.7560\n",
      "Epoch 82/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.7591 - val_loss: 0.7460\n",
      "Epoch 83/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.7513 - val_loss: 0.7379\n",
      "Epoch 84/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.7427 - val_loss: 0.7302\n",
      "Epoch 85/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 0.7324 - val_loss: 0.7210\n",
      "Epoch 86/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.7242 - val_loss: 0.7127\n",
      "Epoch 87/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.7169 - val_loss: 0.7053\n",
      "Epoch 88/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.7085 - val_loss: 0.6979\n",
      "Epoch 89/300\n",
      "11/11 [==============================] - 11s 991ms/step - loss: 0.7014 - val_loss: 0.6904\n",
      "Epoch 90/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.6944 - val_loss: 0.6832\n",
      "Epoch 91/300\n",
      "11/11 [==============================] - 11s 993ms/step - loss: 0.6880 - val_loss: 0.6762\n",
      "Epoch 92/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.6799 - val_loss: 0.6696\n",
      "Epoch 93/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.6740 - val_loss: 0.6632\n",
      "Epoch 94/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.6671 - val_loss: 0.6568\n",
      "Epoch 95/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.6611 - val_loss: 0.6509\n",
      "Epoch 96/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.6556 - val_loss: 0.6451\n",
      "Epoch 97/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.6496 - val_loss: 0.6394\n",
      "Epoch 98/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 0.6440 - val_loss: 0.6340\n",
      "Epoch 99/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.6391 - val_loss: 0.6290\n",
      "Epoch 100/300\n",
      "11/11 [==============================] - 11s 990ms/step - loss: 0.6339 - val_loss: 0.6241\n",
      "Epoch 101/300\n",
      "11/11 [==============================] - 11s 995ms/step - loss: 0.6282 - val_loss: 0.6189\n",
      "Epoch 102/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.6239 - val_loss: 0.6139\n",
      "Epoch 103/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.6193 - val_loss: 0.6100\n",
      "Epoch 104/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.6145 - val_loss: 0.6052\n",
      "Epoch 105/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.6108 - val_loss: 0.6007\n",
      "Epoch 106/300\n",
      "11/11 [==============================] - 11s 971ms/step - loss: 0.6064 - val_loss: 0.5974\n",
      "Epoch 107/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.6022 - val_loss: 0.5928\n",
      "Epoch 108/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.5980 - val_loss: 0.5895\n",
      "Epoch 109/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5945 - val_loss: 0.5856\n",
      "Epoch 110/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5911 - val_loss: 0.5819\n",
      "Epoch 111/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.5877 - val_loss: 0.5791\n",
      "Epoch 112/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5838 - val_loss: 0.5754\n",
      "Epoch 113/300\n",
      "11/11 [==============================] - 11s 991ms/step - loss: 0.5811 - val_loss: 0.5724\n",
      "Epoch 114/300\n",
      "11/11 [==============================] - 11s 976ms/step - loss: 0.5776 - val_loss: 0.5698\n",
      "Epoch 115/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.5761 - val_loss: 0.5672\n",
      "Epoch 116/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5722 - val_loss: 0.5639\n",
      "Epoch 117/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5698 - val_loss: 0.5613\n",
      "Epoch 118/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.5667 - val_loss: 0.5587\n",
      "Epoch 119/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.5646 - val_loss: 0.5565\n",
      "Epoch 120/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.5631 - val_loss: 0.5540\n",
      "Epoch 121/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 0.5596 - val_loss: 0.5518\n",
      "Epoch 122/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.5575 - val_loss: 0.5494\n",
      "Epoch 123/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5551 - val_loss: 0.5473\n",
      "Epoch 124/300\n",
      "11/11 [==============================] - 11s 976ms/step - loss: 0.5532 - val_loss: 0.5454\n",
      "Epoch 125/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.5511 - val_loss: 0.5439\n",
      "Epoch 126/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5499 - val_loss: 0.5417\n",
      "Epoch 127/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5476 - val_loss: 0.5406\n",
      "Epoch 128/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 0.5464 - val_loss: 0.5382\n",
      "Epoch 129/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5445 - val_loss: 0.5368\n",
      "Epoch 130/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5430 - val_loss: 0.5355\n",
      "Epoch 131/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.5420 - val_loss: 0.5338\n",
      "Epoch 132/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.5398 - val_loss: 0.5325\n",
      "Epoch 133/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5384 - val_loss: 0.5311\n",
      "Epoch 134/300\n",
      "11/11 [==============================] - 11s 995ms/step - loss: 0.5377 - val_loss: 0.5298\n",
      "Epoch 135/300\n",
      "11/11 [==============================] - 11s 976ms/step - loss: 0.5363 - val_loss: 0.5287\n",
      "Epoch 136/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5352 - val_loss: 0.5274\n",
      "Epoch 137/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5339 - val_loss: 0.5264\n",
      "Epoch 138/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 0.5326 - val_loss: 0.5262\n",
      "Epoch 139/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5319 - val_loss: 0.5244\n",
      "Epoch 140/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.5305 - val_loss: 0.5233\n",
      "Epoch 141/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.5296 - val_loss: 0.5223\n",
      "Epoch 142/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5286 - val_loss: 0.5216\n",
      "Epoch 143/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5277 - val_loss: 0.5206\n",
      "Epoch 144/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.5277 - val_loss: 0.5200\n",
      "Epoch 145/300\n",
      "11/11 [==============================] - 11s 998ms/step - loss: 0.5262 - val_loss: 0.5193\n",
      "Epoch 146/300\n",
      "11/11 [==============================] - 11s 974ms/step - loss: 0.5255 - val_loss: 0.5185\n",
      "Epoch 147/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.5250 - val_loss: 0.5179\n",
      "Epoch 148/300\n",
      "11/11 [==============================] - 11s 977ms/step - loss: 0.5251 - val_loss: 0.5177\n",
      "Epoch 149/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5239 - val_loss: 0.5166\n",
      "Epoch 150/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.5236 - val_loss: 0.5163\n",
      "Epoch 151/300\n",
      "11/11 [==============================] - 11s 996ms/step - loss: 0.5226 - val_loss: 0.5162\n",
      "Epoch 152/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5233 - val_loss: 0.5151\n",
      "Epoch 153/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5223 - val_loss: 0.5150\n",
      "Epoch 154/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5222 - val_loss: 0.5143\n",
      "Epoch 155/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5203 - val_loss: 0.5140\n",
      "Epoch 156/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5203 - val_loss: 0.5137\n",
      "Epoch 157/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5194 - val_loss: 0.5128\n",
      "Epoch 158/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.5193 - val_loss: 0.5125\n",
      "Epoch 159/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5187 - val_loss: 0.5123\n",
      "Epoch 160/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 0.5194 - val_loss: 0.5120\n",
      "Epoch 161/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.5184 - val_loss: 0.5116\n",
      "Epoch 162/300\n",
      "11/11 [==============================] - 11s 992ms/step - loss: 0.5181 - val_loss: 0.5114\n",
      "Epoch 163/300\n",
      "11/11 [==============================] - 11s 976ms/step - loss: 0.5176 - val_loss: 0.5109\n",
      "Epoch 164/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5184 - val_loss: 0.5106\n",
      "Epoch 165/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.5168 - val_loss: 0.5114\n",
      "Epoch 166/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.5171 - val_loss: 0.5103\n",
      "Epoch 167/300\n",
      "11/11 [==============================] - 11s 989ms/step - loss: 0.5177 - val_loss: 0.5101\n",
      "Epoch 168/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.5165 - val_loss: 0.5098\n",
      "Epoch 169/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5163 - val_loss: 0.5100\n",
      "Epoch 170/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5173 - val_loss: 0.5096\n",
      "Epoch 171/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 0.5167 - val_loss: 0.5104\n",
      "Epoch 172/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5162 - val_loss: 0.5092\n",
      "Epoch 173/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5157 - val_loss: 0.5090\n",
      "Epoch 174/300\n",
      "11/11 [==============================] - 11s 992ms/step - loss: 0.5171 - val_loss: 0.5108\n",
      "Epoch 175/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.5166 - val_loss: 0.5089\n",
      "Epoch 176/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5155 - val_loss: 0.5085\n",
      "Epoch 177/300\n",
      "11/11 [==============================] - 11s 986ms/step - loss: 0.5166 - val_loss: 0.5091\n",
      "Epoch 178/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.5144 - val_loss: 0.5086\n",
      "Epoch 179/300\n",
      "11/11 [==============================] - 11s 981ms/step - loss: 0.5155 - val_loss: 0.5083\n",
      "Epoch 180/300\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.5147 - val_loss: 0.5082\n",
      "Epoch 181/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5149 - val_loss: 0.5082\n",
      "Epoch 182/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.5146 - val_loss: 0.5083\n",
      "Epoch 183/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.5146 - val_loss: 0.5082\n",
      "Epoch 184/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.5147 - val_loss: 0.5077\n",
      "Epoch 185/300\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.5144 - val_loss: 0.5081\n",
      "Epoch 186/300\n",
      "11/11 [==============================] - 11s 990ms/step - loss: 0.5139 - val_loss: 0.5075\n",
      "Epoch 187/300\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.5148 - val_loss: 0.5074\n",
      "Epoch 188/300\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 0.5142 - val_loss: 0.5078\n",
      "Epoch 189/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.5140 - val_loss: 0.5075\n",
      "Epoch 190/300\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.5140 - val_loss: 0.5075\n",
      "Epoch 191/300\n",
      "11/11 [==============================] - 11s 979ms/step - loss: 0.5147 - val_loss: 0.5073\n",
      "Epoch 192/300\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5153 - val_loss: 0.5081\n",
      "Epoch 193/300\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.5141 - val_loss: 0.5078\n",
      "Epoch 194/300\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.5151 - val_loss: 0.5077\n",
      "Epoch 195/300\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5155Restoring model weights from the end of the best epoch: 191.\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.5155 - val_loss: 0.5074\n",
      "Epoch 00195: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_19/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_19/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcfa828a390> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcfa1d25cd0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "for i in range(10, 20):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, activation=\"relu\", input_shape=(max_length, 1), return_sequences=True,\n",
    "                  kernel_regularizer=\"l2\", recurrent_regularizer=\"l2\", bias_regularizer=\"l2\"))  \n",
    "    model.add(LSTM(128, activation=\"relu\", return_sequences=False,\n",
    "                  kernel_regularizer=\"l2\", recurrent_regularizer=\"l2\", bias_regularizer=\"l2\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer='adam', loss=\"binary_crossentropy\")\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', patience=4, verbose=1, restore_best_weights=True)\n",
    "    model.fit(X_train, y_train, epochs=300, verbose=1, batch_size=128, validation_data=(X_test, y_test), callbacks=[es])\n",
    "\n",
    "    try:\n",
    "        model.save(path.join(DATA_DIR, f\"models/LSTM5_{i}\"))\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        if not are_same(y_pred):\n",
    "            print(i)\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "056d793d-eab9-440e-91e7-b5938f783caf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "17/17 [==============================] - 19s 994ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 17s 979ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_21/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_21/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf9b0b0e10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf98379690> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "Epoch 1/300\n",
      "17/17 [==============================] - 19s 996ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 16s 968ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 17s 980ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_22/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_22/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf98dee3d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf80ad8a10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "Epoch 1/300\n",
      "17/17 [==============================] - 19s 998ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 17s 978ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_23/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_23/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf8072a090> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf9b04bd90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "Epoch 1/300\n",
      "17/17 [==============================] - 19s 999ms/step - loss: 6.7880 - val_loss: 5.6853\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 5.0399 - val_loss: 4.3840\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 4.0331 - val_loss: 3.6744\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 3.4634 - val_loss: 3.2566\n",
      "Epoch 5/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 3.1333 - val_loss: 3.0004\n",
      "Epoch 6/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 2.9295 - val_loss: 2.8536\n",
      "Epoch 7/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 2.7924 - val_loss: 2.7258\n",
      "Epoch 8/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 2.6883 - val_loss: 2.6435\n",
      "Epoch 9/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 2.6025 - val_loss: 2.5587\n",
      "Epoch 10/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 2.5264 - val_loss: 2.4892\n",
      "Epoch 11/300\n",
      "17/17 [==============================] - 17s 981ms/step - loss: 2.4564 - val_loss: 2.4153\n",
      "Epoch 12/300\n",
      "17/17 [==============================] - 17s 982ms/step - loss: 2.3884 - val_loss: 2.3505\n",
      "Epoch 13/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 2.3240 - val_loss: 2.2977\n",
      "Epoch 14/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 2.2620 - val_loss: 2.2272\n",
      "Epoch 15/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 2.2015 - val_loss: 2.1663\n",
      "Epoch 16/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 2.1435 - val_loss: 2.1005\n",
      "Epoch 17/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 2.0869 - val_loss: 2.0725\n",
      "Epoch 18/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 2.0324 - val_loss: 2.0052\n",
      "Epoch 19/300\n",
      "17/17 [==============================] - 17s 980ms/step - loss: 1.9791 - val_loss: 1.9460\n",
      "Epoch 20/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 1.9285 - val_loss: 1.8944\n",
      "Epoch 21/300\n",
      "17/17 [==============================] - 17s 982ms/step - loss: 1.8777 - val_loss: 1.8483\n",
      "Epoch 22/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 1.8299 - val_loss: 1.8023\n",
      "Epoch 23/300\n",
      "17/17 [==============================] - 17s 979ms/step - loss: 1.7826 - val_loss: 1.7624\n",
      "Epoch 24/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 1.7377 - val_loss: 1.7090\n",
      "Epoch 25/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 1.6938 - val_loss: 1.6700\n",
      "Epoch 26/300\n",
      "17/17 [==============================] - 17s 980ms/step - loss: 1.6513 - val_loss: 1.6262\n",
      "Epoch 27/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 1.6105 - val_loss: 1.5902\n",
      "Epoch 28/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 1.5708 - val_loss: 1.5504\n",
      "Epoch 29/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 1.5325 - val_loss: 1.5163\n",
      "Epoch 30/300\n",
      "17/17 [==============================] - 17s 979ms/step - loss: 1.4959 - val_loss: 1.4702\n",
      "Epoch 31/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 1.4600 - val_loss: 1.4432\n",
      "Epoch 32/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 1.4253 - val_loss: 1.4023\n",
      "Epoch 33/300\n",
      "17/17 [==============================] - 16s 970ms/step - loss: 1.3931 - val_loss: 1.3722\n",
      "Epoch 34/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 1.3604 - val_loss: 1.3312\n",
      "Epoch 35/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 1.3297 - val_loss: 1.3154\n",
      "Epoch 36/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 1.3000 - val_loss: 1.2872\n",
      "Epoch 37/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 1.2711 - val_loss: 1.2536\n",
      "Epoch 38/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 1.2439 - val_loss: 1.2322\n",
      "Epoch 39/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 1.2169 - val_loss: 1.1893\n",
      "Epoch 40/300\n",
      "17/17 [==============================] - 17s 979ms/step - loss: 1.1915 - val_loss: 1.1800\n",
      "Epoch 41/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 1.1670 - val_loss: 1.1531\n",
      "Epoch 42/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 1.1437 - val_loss: 1.1323\n",
      "Epoch 43/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 1.1215 - val_loss: 1.1040\n",
      "Epoch 44/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 1.0994 - val_loss: 1.0823\n",
      "Epoch 45/300\n",
      "17/17 [==============================] - 17s 979ms/step - loss: 1.0785 - val_loss: 1.0684\n",
      "Epoch 46/300\n",
      "17/17 [==============================] - 17s 983ms/step - loss: 1.0587 - val_loss: 1.0492\n",
      "Epoch 47/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 1.0397 - val_loss: 1.0282\n",
      "Epoch 48/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 1.0216 - val_loss: 1.0131\n",
      "Epoch 49/300\n",
      "17/17 [==============================] - 17s 985ms/step - loss: 1.0036 - val_loss: 0.9931\n",
      "Epoch 50/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 0.9873 - val_loss: 0.9814\n",
      "Epoch 51/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 0.9711 - val_loss: 0.9611\n",
      "Epoch 52/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 0.9555 - val_loss: 0.9399\n",
      "Epoch 53/300\n",
      "17/17 [==============================] - 17s 979ms/step - loss: 0.9410 - val_loss: 0.9306\n",
      "Epoch 54/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 0.9272 - val_loss: 0.9181\n",
      "Epoch 55/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 0.9141 - val_loss: 0.9002\n",
      "Epoch 56/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 0.9011 - val_loss: 0.9105\n",
      "Epoch 57/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 0.8894 - val_loss: 0.8752\n",
      "Epoch 58/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 0.8778 - val_loss: 0.8726\n",
      "Epoch 59/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 0.8665 - val_loss: 0.8492\n",
      "Epoch 60/300\n",
      "17/17 [==============================] - 16s 970ms/step - loss: 0.8564 - val_loss: 0.8542\n",
      "Epoch 61/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 0.8464 - val_loss: 0.8412\n",
      "Epoch 62/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 0.8371 - val_loss: 0.8313\n",
      "Epoch 63/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 0.8282 - val_loss: 0.8203\n",
      "Epoch 64/300\n",
      "17/17 [==============================] - 16s 969ms/step - loss: 0.8201 - val_loss: 0.8103\n",
      "Epoch 65/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 0.8120 - val_loss: 0.8190\n",
      "Epoch 66/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 0.8041 - val_loss: 0.7911\n",
      "Epoch 67/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 0.7973 - val_loss: 0.7946\n",
      "Epoch 68/300\n",
      "17/17 [==============================] - 17s 980ms/step - loss: 0.7903 - val_loss: 0.7778\n",
      "Epoch 69/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 0.7838 - val_loss: 0.7873\n",
      "Epoch 70/300\n",
      "17/17 [==============================] - 17s 983ms/step - loss: 0.7779 - val_loss: 0.7712\n",
      "Epoch 71/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 0.7720 - val_loss: 0.7701\n",
      "Epoch 72/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 0.7666 - val_loss: 0.7646\n",
      "Epoch 73/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 0.7616 - val_loss: 0.7562\n",
      "Epoch 74/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 0.7571 - val_loss: 0.7505\n",
      "Epoch 75/300\n",
      "17/17 [==============================] - 17s 979ms/step - loss: 0.7525 - val_loss: 0.7498\n",
      "Epoch 76/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 0.7485 - val_loss: 0.7497\n",
      "Epoch 77/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 0.7443 - val_loss: 0.7401\n",
      "Epoch 78/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 0.7407 - val_loss: 0.7283\n",
      "Epoch 79/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 0.7375 - val_loss: 0.7396\n",
      "Epoch 80/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 0.7336 - val_loss: 0.7287\n",
      "Epoch 81/300\n",
      "17/17 [==============================] - 16s 969ms/step - loss: 0.7307 - val_loss: 0.7205\n",
      "Epoch 82/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 0.7277 - val_loss: 0.7304\n",
      "Epoch 83/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 0.7252 - val_loss: 0.7248\n",
      "Epoch 84/300\n",
      "17/17 [==============================] - 17s 980ms/step - loss: 0.7227 - val_loss: 0.7232\n",
      "Epoch 85/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7204Restoring model weights from the end of the best epoch: 81.\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 0.7204 - val_loss: 0.7282\n",
      "Epoch 00085: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_24/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_24/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf80ed0850> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf984c8850> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "Epoch 1/300\n",
      "17/17 [==============================] - 19s 997ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 17s 975ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_25/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_25/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf9836ff10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf809c6e10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "Epoch 1/300\n",
      "17/17 [==============================] - 19s 997ms/step - loss: 6.7885 - val_loss: 5.6981\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 5.0416 - val_loss: 4.3961\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 4.0353 - val_loss: 3.6736\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 3.4659 - val_loss: 3.2474\n",
      "Epoch 5/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 3.1348 - val_loss: 3.0000\n",
      "Epoch 6/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 2.9312 - val_loss: 2.8443\n",
      "Epoch 7/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 2.7932 - val_loss: 2.7317\n",
      "Epoch 8/300\n",
      "17/17 [==============================] - 16s 967ms/step - loss: 2.6893 - val_loss: 2.6414\n",
      "Epoch 9/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 2.6033 - val_loss: 2.5646\n",
      "Epoch 10/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 2.5269 - val_loss: 2.4786\n",
      "Epoch 11/300\n",
      "17/17 [==============================] - 16s 970ms/step - loss: 2.4562 - val_loss: 2.4233\n",
      "Epoch 12/300\n",
      "17/17 [==============================] - 17s 979ms/step - loss: 2.3887 - val_loss: 2.3491\n",
      "Epoch 13/300\n",
      "17/17 [==============================] - 16s 968ms/step - loss: 2.3243 - val_loss: 2.2846\n",
      "Epoch 14/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 2.2619 - val_loss: 2.2248\n",
      "Epoch 15/300\n",
      "17/17 [==============================] - 17s 980ms/step - loss: 2.2019 - val_loss: 2.1706\n",
      "Epoch 16/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 2.1443 - val_loss: 2.1042\n",
      "Epoch 17/300\n",
      "17/17 [==============================] - 16s 971ms/step - loss: 2.0873 - val_loss: 2.0609\n",
      "Epoch 18/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 2.0328 - val_loss: 2.0013\n",
      "Epoch 19/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 1.9793 - val_loss: 1.9494\n",
      "Epoch 20/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 1.9278 - val_loss: 1.8940\n",
      "Epoch 21/300\n",
      "17/17 [==============================] - 16s 972ms/step - loss: 1.8784 - val_loss: 1.8443\n",
      "Epoch 22/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 1.8296 - val_loss: 1.8052\n",
      "Epoch 23/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 1.7833 - val_loss: 1.7639\n",
      "Epoch 24/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 1.7373 - val_loss: 1.7004\n",
      "Epoch 25/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 1.6939 - val_loss: 1.6685\n",
      "Epoch 26/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 1.6518 - val_loss: 1.6343\n",
      "Epoch 27/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 1.6104 - val_loss: 1.5797\n",
      "Epoch 28/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 1.5712 - val_loss: 1.5462\n",
      "Epoch 29/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 1.5327 - val_loss: 1.5150\n",
      "Epoch 30/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 1.4958 - val_loss: 1.4714\n",
      "Epoch 31/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 1.4604 - val_loss: 1.4433\n",
      "Epoch 32/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 1.4255 - val_loss: 1.3993\n",
      "Epoch 33/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 1.3923 - val_loss: 1.3750\n",
      "Epoch 34/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 1.3604 - val_loss: 1.3362\n",
      "Epoch 35/300\n",
      "17/17 [==============================] - 17s 971ms/step - loss: 1.3296 - val_loss: 1.3147\n",
      "Epoch 36/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 1.2997 - val_loss: 1.2873\n",
      "Epoch 37/300\n",
      "17/17 [==============================] - 17s 979ms/step - loss: 1.2716 - val_loss: 1.2547\n",
      "Epoch 38/300\n",
      "17/17 [==============================] - 16s 969ms/step - loss: 1.2435 - val_loss: 1.2235\n",
      "Epoch 39/300\n",
      "17/17 [==============================] - 17s 1s/step - loss: 1.2176 - val_loss: 1.1938\n",
      "Epoch 40/300\n",
      "17/17 [==============================] - 17s 970ms/step - loss: 1.1923 - val_loss: 1.1850\n",
      "Epoch 41/300\n",
      "17/17 [==============================] - 16s 968ms/step - loss: 1.1670 - val_loss: 1.1436\n",
      "Epoch 42/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 1.1440 - val_loss: 1.1296\n",
      "Epoch 43/300\n",
      "17/17 [==============================] - 17s 971ms/step - loss: 1.1218 - val_loss: 1.1172\n",
      "Epoch 44/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 1.0993 - val_loss: 1.0825\n",
      "Epoch 45/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 1.0785 - val_loss: 1.0640\n",
      "Epoch 46/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 1.0587 - val_loss: 1.0495\n",
      "Epoch 47/300\n",
      "17/17 [==============================] - 17s 979ms/step - loss: 1.0396 - val_loss: 1.0285\n",
      "Epoch 48/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 1.0212 - val_loss: 1.0019\n",
      "Epoch 49/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 1.0040 - val_loss: 0.9947\n",
      "Epoch 50/300\n",
      "17/17 [==============================] - 17s 979ms/step - loss: 0.9875 - val_loss: 0.9810\n",
      "Epoch 51/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 0.9709 - val_loss: 0.9494\n",
      "Epoch 52/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 0.9558 - val_loss: 0.9466\n",
      "Epoch 53/300\n",
      "17/17 [==============================] - 17s 980ms/step - loss: 0.9413 - val_loss: 0.9262\n",
      "Epoch 54/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 0.9273 - val_loss: 0.9303\n",
      "Epoch 55/300\n",
      "17/17 [==============================] - 17s 980ms/step - loss: 0.9142 - val_loss: 0.8956\n",
      "Epoch 56/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 0.9012 - val_loss: 0.8948\n",
      "Epoch 57/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 0.8892 - val_loss: 0.8884\n",
      "Epoch 58/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 0.8776 - val_loss: 0.8653\n",
      "Epoch 59/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 0.8667 - val_loss: 0.8599\n",
      "Epoch 60/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 0.8563 - val_loss: 0.8482\n",
      "Epoch 61/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 0.8463 - val_loss: 0.8348\n",
      "Epoch 62/300\n",
      "17/17 [==============================] - 17s 981ms/step - loss: 0.8371 - val_loss: 0.8269\n",
      "Epoch 63/300\n",
      "17/17 [==============================] - 16s 970ms/step - loss: 0.8280 - val_loss: 0.8261\n",
      "Epoch 64/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 0.8200 - val_loss: 0.8188\n",
      "Epoch 65/300\n",
      "17/17 [==============================] - 17s 981ms/step - loss: 0.8117 - val_loss: 0.7971\n",
      "Epoch 66/300\n",
      "17/17 [==============================] - 17s 970ms/step - loss: 0.8042 - val_loss: 0.8056\n",
      "Epoch 67/300\n",
      "17/17 [==============================] - 17s 981ms/step - loss: 0.7974 - val_loss: 0.7910\n",
      "Epoch 68/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 0.7905 - val_loss: 0.7950\n",
      "Epoch 69/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 0.7844 - val_loss: 0.7706\n",
      "Epoch 70/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 0.7777 - val_loss: 0.7782\n",
      "Epoch 71/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 0.7720 - val_loss: 0.7692\n",
      "Epoch 72/300\n",
      "17/17 [==============================] - 17s 981ms/step - loss: 0.7671 - val_loss: 0.7615\n",
      "Epoch 73/300\n",
      "17/17 [==============================] - 16s 967ms/step - loss: 0.7621 - val_loss: 0.7644\n",
      "Epoch 74/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 0.7570 - val_loss: 0.7477\n",
      "Epoch 75/300\n",
      "17/17 [==============================] - 16s 970ms/step - loss: 0.7524 - val_loss: 0.7468\n",
      "Epoch 76/300\n",
      "17/17 [==============================] - 16s 970ms/step - loss: 0.7483 - val_loss: 0.7499\n",
      "Epoch 77/300\n",
      "17/17 [==============================] - 16s 968ms/step - loss: 0.7442 - val_loss: 0.7366\n",
      "Epoch 78/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 0.7405 - val_loss: 0.7351\n",
      "Epoch 79/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 0.7369 - val_loss: 0.7361\n",
      "Epoch 80/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 0.7339 - val_loss: 0.7302\n",
      "Epoch 81/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 0.7306 - val_loss: 0.7262\n",
      "Epoch 82/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 0.7277 - val_loss: 0.7274\n",
      "Epoch 83/300\n",
      "17/17 [==============================] - 17s 979ms/step - loss: 0.7253 - val_loss: 0.7335\n",
      "Epoch 84/300\n",
      "17/17 [==============================] - 16s 967ms/step - loss: 0.7225 - val_loss: 0.7099\n",
      "Epoch 85/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 0.7205 - val_loss: 0.7155\n",
      "Epoch 86/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 0.7183 - val_loss: 0.7145\n",
      "Epoch 87/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 0.7165 - val_loss: 0.7234\n",
      "Epoch 88/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 0.7140 - val_loss: 0.7036\n",
      "Epoch 89/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 0.7126 - val_loss: 0.7005\n",
      "Epoch 90/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 0.7109 - val_loss: 0.7164\n",
      "Epoch 91/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 0.7096 - val_loss: 0.7087\n",
      "Epoch 92/300\n",
      "17/17 [==============================] - 16s 969ms/step - loss: 0.7082 - val_loss: 0.7065\n",
      "Epoch 93/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7069Restoring model weights from the end of the best epoch: 89.\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 0.7069 - val_loss: 0.7069\n",
      "Epoch 00093: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_26/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_26/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf98511ed0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf804b34d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "Epoch 1/300\n",
      "17/17 [==============================] - 19s 992ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 16s 971ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 17s 974ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_27/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_27/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf981ebbd0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf9b7899d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "Epoch 1/300\n",
      "17/17 [==============================] - 19s 996ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 17s 980ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 17s 973ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_28/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_28/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf8063ac10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf9b252f50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "Epoch 1/300\n",
      "17/17 [==============================] - 19s 999ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 17s 979ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 17s 975ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_29/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_29/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf9bde5910> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf809c6ad0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "Epoch 1/300\n",
      "17/17 [==============================] - 19s 984ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 17s 982ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 16s 967ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_30/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_30/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf804eeb10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf8067e9d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "Epoch 1/300\n",
      "17/17 [==============================] - 19s 997ms/step - loss: 6.7889 - val_loss: 5.6893\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 5.0379 - val_loss: 4.3989\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 4.0298 - val_loss: 3.6619\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 3.4609 - val_loss: 3.2522\n",
      "Epoch 5/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 3.1311 - val_loss: 3.0052\n",
      "Epoch 6/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 2.9287 - val_loss: 2.8466\n",
      "Epoch 7/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 2.7913 - val_loss: 2.7246\n",
      "Epoch 8/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 2.6879 - val_loss: 2.6468\n",
      "Epoch 9/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 2.6028 - val_loss: 2.5639\n",
      "Epoch 10/300\n",
      "17/17 [==============================] - 17s 971ms/step - loss: 2.5261 - val_loss: 2.4751\n",
      "Epoch 11/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 2.4557 - val_loss: 2.4193\n",
      "Epoch 12/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 2.3883 - val_loss: 2.3506\n",
      "Epoch 13/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 2.3238 - val_loss: 2.2829\n",
      "Epoch 14/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 2.2614 - val_loss: 2.2364\n",
      "Epoch 15/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 2.2016 - val_loss: 2.1633\n",
      "Epoch 16/300\n",
      "17/17 [==============================] - 16s 969ms/step - loss: 2.1442 - val_loss: 2.1137\n",
      "Epoch 17/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 2.0870 - val_loss: 2.0537\n",
      "Epoch 18/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 2.0328 - val_loss: 2.0037\n",
      "Epoch 19/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 1.9789 - val_loss: 1.9447\n",
      "Epoch 20/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 1.9279 - val_loss: 1.8969\n",
      "Epoch 21/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 1.8779 - val_loss: 1.8409\n",
      "Epoch 22/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 1.8301 - val_loss: 1.8122\n",
      "Epoch 23/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 1.7827 - val_loss: 1.7536\n",
      "Epoch 24/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 1.7374 - val_loss: 1.7095\n",
      "Epoch 25/300\n",
      "17/17 [==============================] - 16s 971ms/step - loss: 1.6939 - val_loss: 1.6679\n",
      "Epoch 26/300\n",
      "17/17 [==============================] - 17s 982ms/step - loss: 1.6515 - val_loss: 1.6284\n",
      "Epoch 27/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 1.6105 - val_loss: 1.5826\n",
      "Epoch 28/300\n",
      "17/17 [==============================] - 16s 968ms/step - loss: 1.5708 - val_loss: 1.5599\n",
      "Epoch 29/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 1.5325 - val_loss: 1.5069\n",
      "Epoch 30/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 1.4955 - val_loss: 1.4754\n",
      "Epoch 31/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 1.4606 - val_loss: 1.4494\n",
      "Epoch 32/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 1.4254 - val_loss: 1.3948\n",
      "Epoch 33/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 1.3925 - val_loss: 1.3767\n",
      "Epoch 34/300\n",
      "17/17 [==============================] - 16s 971ms/step - loss: 1.3603 - val_loss: 1.3491\n",
      "Epoch 35/300\n",
      "17/17 [==============================] - 17s 971ms/step - loss: 1.3292 - val_loss: 1.3086\n",
      "Epoch 36/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 1.2999 - val_loss: 1.2799\n",
      "Epoch 37/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 1.2709 - val_loss: 1.2570\n",
      "Epoch 38/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 1.2435 - val_loss: 1.2275\n",
      "Epoch 39/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 1.2169 - val_loss: 1.1959\n",
      "Epoch 40/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 1.1916 - val_loss: 1.1680\n",
      "Epoch 41/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 1.1669 - val_loss: 1.1663\n",
      "Epoch 42/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 1.1436 - val_loss: 1.1321\n",
      "Epoch 43/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 1.1212 - val_loss: 1.1028\n",
      "Epoch 44/300\n",
      "17/17 [==============================] - 16s 968ms/step - loss: 1.0993 - val_loss: 1.0944\n",
      "Epoch 45/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 1.0785 - val_loss: 1.0624\n",
      "Epoch 46/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 1.0585 - val_loss: 1.0494\n",
      "Epoch 47/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 1.0398 - val_loss: 1.0349\n",
      "Epoch 48/300\n",
      "17/17 [==============================] - 16s 970ms/step - loss: 1.0211 - val_loss: 0.9995\n",
      "Epoch 49/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 1.0037 - val_loss: 0.9949\n",
      "Epoch 50/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 0.9870 - val_loss: 0.9769\n",
      "Epoch 51/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 0.9710 - val_loss: 0.9597\n",
      "Epoch 52/300\n",
      "17/17 [==============================] - 17s 981ms/step - loss: 0.9555 - val_loss: 0.9436\n",
      "Epoch 53/300\n",
      "17/17 [==============================] - 17s 981ms/step - loss: 0.9412 - val_loss: 0.9308\n",
      "Epoch 54/300\n",
      "17/17 [==============================] - 16s 969ms/step - loss: 0.9273 - val_loss: 0.9210\n",
      "Epoch 55/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 0.9138 - val_loss: 0.9071\n",
      "Epoch 56/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 0.9011 - val_loss: 0.8867\n",
      "Epoch 57/300\n",
      "17/17 [==============================] - 17s 979ms/step - loss: 0.8894 - val_loss: 0.8844\n",
      "Epoch 58/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 0.8778 - val_loss: 0.8776\n",
      "Epoch 59/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 0.8666 - val_loss: 0.8594\n",
      "Epoch 60/300\n",
      "17/17 [==============================] - 17s 979ms/step - loss: 0.8562 - val_loss: 0.8429\n",
      "Epoch 61/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 0.8463 - val_loss: 0.8377\n",
      "Epoch 62/300\n",
      "17/17 [==============================] - 16s 969ms/step - loss: 0.8370 - val_loss: 0.8302\n",
      "Epoch 63/300\n",
      "17/17 [==============================] - 16s 968ms/step - loss: 0.8281 - val_loss: 0.8229\n",
      "Epoch 64/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 0.8200 - val_loss: 0.8121\n",
      "Epoch 65/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 0.8119 - val_loss: 0.7967\n",
      "Epoch 66/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 0.8041 - val_loss: 0.7983\n",
      "Epoch 67/300\n",
      "17/17 [==============================] - 17s 981ms/step - loss: 0.7968 - val_loss: 0.7957\n",
      "Epoch 68/300\n",
      "17/17 [==============================] - 17s 971ms/step - loss: 0.7902 - val_loss: 0.7901\n",
      "Epoch 69/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 0.7837 - val_loss: 0.7765\n",
      "Epoch 70/300\n",
      "17/17 [==============================] - 16s 971ms/step - loss: 0.7780 - val_loss: 0.7817\n",
      "Epoch 71/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 0.7718 - val_loss: 0.7640\n",
      "Epoch 72/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 0.7668 - val_loss: 0.7643\n",
      "Epoch 73/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 0.7618 - val_loss: 0.7511\n",
      "Epoch 74/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 0.7568 - val_loss: 0.7504\n",
      "Epoch 75/300\n",
      "17/17 [==============================] - 16s 970ms/step - loss: 0.7524 - val_loss: 0.7547\n",
      "Epoch 76/300\n",
      "17/17 [==============================] - 17s 979ms/step - loss: 0.7481 - val_loss: 0.7426\n",
      "Epoch 77/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 0.7443 - val_loss: 0.7390\n",
      "Epoch 78/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 0.7406 - val_loss: 0.7326\n",
      "Epoch 79/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 0.7370 - val_loss: 0.7382\n",
      "Epoch 80/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 0.7338 - val_loss: 0.7367\n",
      "Epoch 81/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 0.7306 - val_loss: 0.7270\n",
      "Epoch 82/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 0.7277 - val_loss: 0.7243\n",
      "Epoch 83/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 0.7250 - val_loss: 0.7213\n",
      "Epoch 84/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 0.7226 - val_loss: 0.7234\n",
      "Epoch 85/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 0.7205 - val_loss: 0.7213\n",
      "Epoch 86/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 0.7181 - val_loss: 0.7162\n",
      "Epoch 87/300\n",
      "17/17 [==============================] - 17s 971ms/step - loss: 0.7164 - val_loss: 0.7182\n",
      "Epoch 88/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 0.7147 - val_loss: 0.6988\n",
      "Epoch 89/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 0.7124 - val_loss: 0.7097\n",
      "Epoch 90/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 0.7109 - val_loss: 0.7089\n",
      "Epoch 91/300\n",
      "17/17 [==============================] - 17s 979ms/step - loss: 0.7096 - val_loss: 0.7160\n",
      "Epoch 92/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 0.7080 - val_loss: 0.6986\n",
      "Epoch 93/300\n",
      "17/17 [==============================] - 17s 979ms/step - loss: 0.7068 - val_loss: 0.7055\n",
      "Epoch 94/300\n",
      "17/17 [==============================] - 17s 981ms/step - loss: 0.7055 - val_loss: 0.6968\n",
      "Epoch 95/300\n",
      "17/17 [==============================] - 17s 981ms/step - loss: 0.7044 - val_loss: 0.7094\n",
      "Epoch 96/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 0.7035 - val_loss: 0.7002\n",
      "Epoch 97/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 0.7024 - val_loss: 0.7036\n",
      "Epoch 98/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7015Restoring model weights from the end of the best epoch: 94.\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 0.7015 - val_loss: 0.7025\n",
      "Epoch 00098: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_31/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_31/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf8044cbd0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf9b3cf250> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "Epoch 1/300\n",
      "17/17 [==============================] - 19s 990ms/step - loss: 6.7981 - val_loss: 5.6993\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 5.0464 - val_loss: 4.4000\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 4.0371 - val_loss: 3.6730\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - 17s 979ms/step - loss: 3.4652 - val_loss: 3.2564\n",
      "Epoch 5/300\n",
      "17/17 [==============================] - 17s 979ms/step - loss: 3.1340 - val_loss: 3.0024\n",
      "Epoch 6/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 2.9302 - val_loss: 2.8435\n",
      "Epoch 7/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 2.7919 - val_loss: 2.7271\n",
      "Epoch 8/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 2.6889 - val_loss: 2.6331\n",
      "Epoch 9/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 2.6029 - val_loss: 2.5469\n",
      "Epoch 10/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 2.5273 - val_loss: 2.4850\n",
      "Epoch 11/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 2.4557 - val_loss: 2.4091\n",
      "Epoch 12/300\n",
      "17/17 [==============================] - 17s 980ms/step - loss: 2.3895 - val_loss: 2.3626\n",
      "Epoch 13/300\n",
      "17/17 [==============================] - 17s 971ms/step - loss: 2.3238 - val_loss: 2.2818\n",
      "Epoch 14/300\n",
      "17/17 [==============================] - 16s 967ms/step - loss: 2.2620 - val_loss: 2.2226\n",
      "Epoch 15/300\n",
      "17/17 [==============================] - 16s 965ms/step - loss: 2.2017 - val_loss: 2.1713\n",
      "Epoch 16/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 2.1433 - val_loss: 2.1067\n",
      "Epoch 17/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 2.0871 - val_loss: 2.0560\n",
      "Epoch 18/300\n",
      "17/17 [==============================] - 16s 971ms/step - loss: 2.0326 - val_loss: 2.0098\n",
      "Epoch 19/300\n",
      "17/17 [==============================] - 16s 968ms/step - loss: 1.9791 - val_loss: 1.9403\n",
      "Epoch 20/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 1.9277 - val_loss: 1.9013\n",
      "Epoch 21/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 1.8779 - val_loss: 1.8487\n",
      "Epoch 22/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 1.8297 - val_loss: 1.7963\n",
      "Epoch 23/300\n",
      "17/17 [==============================] - 16s 967ms/step - loss: 1.7829 - val_loss: 1.7668\n",
      "Epoch 24/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 1.7378 - val_loss: 1.7118\n",
      "Epoch 25/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 1.6941 - val_loss: 1.6546\n",
      "Epoch 26/300\n",
      "17/17 [==============================] - 16s 969ms/step - loss: 1.6512 - val_loss: 1.6351\n",
      "Epoch 27/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 1.6109 - val_loss: 1.5848\n",
      "Epoch 28/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 1.5708 - val_loss: 1.5506\n",
      "Epoch 29/300\n",
      "17/17 [==============================] - 16s 968ms/step - loss: 1.5328 - val_loss: 1.5191\n",
      "Epoch 30/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 1.4959 - val_loss: 1.4609\n",
      "Epoch 31/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 1.4607 - val_loss: 1.4406\n",
      "Epoch 32/300\n",
      "17/17 [==============================] - 16s 968ms/step - loss: 1.4255 - val_loss: 1.4012\n",
      "Epoch 33/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 1.3924 - val_loss: 1.3726\n",
      "Epoch 34/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 1.3605 - val_loss: 1.3404\n",
      "Epoch 35/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 1.3295 - val_loss: 1.3072\n",
      "Epoch 36/300\n",
      "17/17 [==============================] - 16s 969ms/step - loss: 1.2997 - val_loss: 1.2821\n",
      "Epoch 37/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 1.2711 - val_loss: 1.2599\n",
      "Epoch 38/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 1.2439 - val_loss: 1.2357\n",
      "Epoch 39/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 1.2171 - val_loss: 1.1952\n",
      "Epoch 40/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 1.1916 - val_loss: 1.1815\n",
      "Epoch 41/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 1.1672 - val_loss: 1.1528\n",
      "Epoch 42/300\n",
      "17/17 [==============================] - 16s 970ms/step - loss: 1.1443 - val_loss: 1.1310\n",
      "Epoch 43/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 1.1219 - val_loss: 1.0947\n",
      "Epoch 44/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 1.0996 - val_loss: 1.0960\n",
      "Epoch 45/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 1.0786 - val_loss: 1.0679\n",
      "Epoch 46/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 1.0586 - val_loss: 1.0438\n",
      "Epoch 47/300\n",
      "17/17 [==============================] - 16s 971ms/step - loss: 1.0395 - val_loss: 1.0287\n",
      "Epoch 48/300\n",
      "17/17 [==============================] - 16s 968ms/step - loss: 1.0212 - val_loss: 1.0134\n",
      "Epoch 49/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 1.0038 - val_loss: 0.9916\n",
      "Epoch 50/300\n",
      "17/17 [==============================] - 16s 971ms/step - loss: 0.9870 - val_loss: 0.9839\n",
      "Epoch 51/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 0.9710 - val_loss: 0.9580\n",
      "Epoch 52/300\n",
      "17/17 [==============================] - 17s 971ms/step - loss: 0.9558 - val_loss: 0.9516\n",
      "Epoch 53/300\n",
      "17/17 [==============================] - 16s 970ms/step - loss: 0.9412 - val_loss: 0.9319\n",
      "Epoch 54/300\n",
      "17/17 [==============================] - 16s 970ms/step - loss: 0.9272 - val_loss: 0.9164\n",
      "Epoch 55/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 0.9140 - val_loss: 0.9002\n",
      "Epoch 56/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 0.9011 - val_loss: 0.8949\n",
      "Epoch 57/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 0.8895 - val_loss: 0.8880\n",
      "Epoch 58/300\n",
      "17/17 [==============================] - 17s 971ms/step - loss: 0.8774 - val_loss: 0.8622\n",
      "Epoch 59/300\n",
      "17/17 [==============================] - 17s 971ms/step - loss: 0.8668 - val_loss: 0.8536\n",
      "Epoch 60/300\n",
      "17/17 [==============================] - 16s 967ms/step - loss: 0.8563 - val_loss: 0.8544\n",
      "Epoch 61/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 0.8464 - val_loss: 0.8436\n",
      "Epoch 62/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 0.8370 - val_loss: 0.8297\n",
      "Epoch 63/300\n",
      "17/17 [==============================] - 16s 966ms/step - loss: 0.8282 - val_loss: 0.8176\n",
      "Epoch 64/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 0.8199 - val_loss: 0.8121\n",
      "Epoch 65/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 0.8118 - val_loss: 0.8129\n",
      "Epoch 66/300\n",
      "17/17 [==============================] - 16s 967ms/step - loss: 0.8040 - val_loss: 0.7943\n",
      "Epoch 67/300\n",
      "17/17 [==============================] - 16s 965ms/step - loss: 0.7976 - val_loss: 0.7945\n",
      "Epoch 68/300\n",
      "17/17 [==============================] - 16s 968ms/step - loss: 0.7902 - val_loss: 0.7757\n",
      "Epoch 69/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 0.7842 - val_loss: 0.7842\n",
      "Epoch 70/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 0.7777 - val_loss: 0.7611\n",
      "Epoch 71/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 0.7721 - val_loss: 0.7679\n",
      "Epoch 72/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 0.7666 - val_loss: 0.7661\n",
      "Epoch 73/300\n",
      "17/17 [==============================] - 16s 972ms/step - loss: 0.7616 - val_loss: 0.7562\n",
      "Epoch 74/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 0.7571 - val_loss: 0.7478\n",
      "Epoch 75/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 0.7527 - val_loss: 0.7557\n",
      "Epoch 76/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 0.7483 - val_loss: 0.7426\n",
      "Epoch 77/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 0.7443 - val_loss: 0.7356\n",
      "Epoch 78/300\n",
      "17/17 [==============================] - 17s 979ms/step - loss: 0.7407 - val_loss: 0.7348\n",
      "Epoch 79/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 0.7372 - val_loss: 0.7367\n",
      "Epoch 80/300\n",
      "17/17 [==============================] - 17s 971ms/step - loss: 0.7337 - val_loss: 0.7250\n",
      "Epoch 81/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 0.7307 - val_loss: 0.7298\n",
      "Epoch 82/300\n",
      "17/17 [==============================] - 16s 970ms/step - loss: 0.7279 - val_loss: 0.7249\n",
      "Epoch 83/300\n",
      "17/17 [==============================] - 17s 981ms/step - loss: 0.7260 - val_loss: 0.7101\n",
      "Epoch 84/300\n",
      "17/17 [==============================] - 17s 989ms/step - loss: 0.7227 - val_loss: 0.7306\n",
      "Epoch 85/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 0.7203 - val_loss: 0.7190\n",
      "Epoch 86/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 0.7182 - val_loss: 0.7164\n",
      "Epoch 87/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7164Restoring model weights from the end of the best epoch: 83.\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 0.7164 - val_loss: 0.7144\n",
      "Epoch 00087: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_32/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_32/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf80684450> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf806bb990> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "Epoch 1/300\n",
      "17/17 [==============================] - 19s 991ms/step - loss: 6.7882 - val_loss: 5.6807\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 16s 972ms/step - loss: 5.0413 - val_loss: 4.3998\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 16s 970ms/step - loss: 4.0341 - val_loss: 3.6629\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 3.4643 - val_loss: 3.2634\n",
      "Epoch 5/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 3.1335 - val_loss: 3.0048\n",
      "Epoch 6/300\n",
      "17/17 [==============================] - 16s 965ms/step - loss: 2.9310 - val_loss: 2.8310\n",
      "Epoch 7/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 2.7934 - val_loss: 2.7300\n",
      "Epoch 8/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 2.6897 - val_loss: 2.6517\n",
      "Epoch 9/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 2.6035 - val_loss: 2.5513\n",
      "Epoch 10/300\n",
      "17/17 [==============================] - 16s 965ms/step - loss: 2.5267 - val_loss: 2.4918\n",
      "Epoch 11/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 2.4554 - val_loss: 2.4182\n",
      "Epoch 12/300\n",
      "17/17 [==============================] - 16s 970ms/step - loss: 2.3883 - val_loss: 2.3458\n",
      "Epoch 13/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 2.3243 - val_loss: 2.2893\n",
      "Epoch 14/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 2.2617 - val_loss: 2.2251\n",
      "Epoch 15/300\n",
      "17/17 [==============================] - 17s 1s/step - loss: 2.2015 - val_loss: 2.1639\n",
      "Epoch 16/300\n",
      "17/17 [==============================] - 17s 979ms/step - loss: 2.1432 - val_loss: 2.1147\n",
      "Epoch 17/300\n",
      "17/17 [==============================] - 16s 970ms/step - loss: 2.0866 - val_loss: 2.0541\n",
      "Epoch 18/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 2.0321 - val_loss: 1.9980\n",
      "Epoch 19/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 1.9790 - val_loss: 1.9416\n",
      "Epoch 20/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 1.9274 - val_loss: 1.9022\n",
      "Epoch 21/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 1.8779 - val_loss: 1.8456\n",
      "Epoch 22/300\n",
      "17/17 [==============================] - 16s 969ms/step - loss: 1.8294 - val_loss: 1.8088\n",
      "Epoch 23/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 1.7828 - val_loss: 1.7600\n",
      "Epoch 24/300\n",
      "17/17 [==============================] - 16s 965ms/step - loss: 1.7375 - val_loss: 1.7088\n",
      "Epoch 25/300\n",
      "17/17 [==============================] - 17s 1s/step - loss: 1.6938 - val_loss: 1.6641\n",
      "Epoch 26/300\n",
      "17/17 [==============================] - 18s 1s/step - loss: 1.6512 - val_loss: 1.6302\n",
      "Epoch 27/300\n",
      "17/17 [==============================] - 17s 993ms/step - loss: 1.6106 - val_loss: 1.5888\n",
      "Epoch 28/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 1.5707 - val_loss: 1.5571\n",
      "Epoch 29/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 1.5325 - val_loss: 1.5052\n",
      "Epoch 30/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 1.4955 - val_loss: 1.4749\n",
      "Epoch 31/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 1.4599 - val_loss: 1.4498\n",
      "Epoch 32/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 1.4254 - val_loss: 1.4005\n",
      "Epoch 33/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 1.3921 - val_loss: 1.3714\n",
      "Epoch 34/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 1.3606 - val_loss: 1.3342\n",
      "Epoch 35/300\n",
      "17/17 [==============================] - 17s 979ms/step - loss: 1.3293 - val_loss: 1.3186\n",
      "Epoch 36/300\n",
      "17/17 [==============================] - 16s 969ms/step - loss: 1.2996 - val_loss: 1.2783\n",
      "Epoch 37/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 1.2711 - val_loss: 1.2525\n",
      "Epoch 38/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 1.2435 - val_loss: 1.2298\n",
      "Epoch 39/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 1.2171 - val_loss: 1.2009\n",
      "Epoch 40/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 1.1918 - val_loss: 1.1770\n",
      "Epoch 41/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 1.1669 - val_loss: 1.1510\n",
      "Epoch 42/300\n",
      "17/17 [==============================] - 17s 971ms/step - loss: 1.1434 - val_loss: 1.1303\n",
      "Epoch 43/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 1.1211 - val_loss: 1.1105\n",
      "Epoch 44/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: 1.0991 - val_loss: 1.0851\n",
      "Epoch 45/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 1.0784 - val_loss: 1.0650\n",
      "Epoch 46/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 1.0587 - val_loss: 1.0426\n",
      "Epoch 47/300\n",
      "17/17 [==============================] - 17s 979ms/step - loss: 1.0395 - val_loss: 1.0363\n",
      "Epoch 48/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 1.0211 - val_loss: 1.0087\n",
      "Epoch 49/300\n",
      "17/17 [==============================] - 17s 981ms/step - loss: 1.0035 - val_loss: 0.9891\n",
      "Epoch 50/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: 0.9869 - val_loss: 0.9746\n",
      "Epoch 51/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 0.9708 - val_loss: 0.9610\n",
      "Epoch 52/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 0.9557 - val_loss: 0.9531\n",
      "Epoch 53/300\n",
      "17/17 [==============================] - 16s 971ms/step - loss: 0.9409 - val_loss: 0.9286\n",
      "Epoch 54/300\n",
      "17/17 [==============================] - 16s 970ms/step - loss: 0.9270 - val_loss: 0.9199\n",
      "Epoch 55/300\n",
      "17/17 [==============================] - 16s 970ms/step - loss: 0.9140 - val_loss: 0.8970\n",
      "Epoch 56/300\n",
      "17/17 [==============================] - 16s 962ms/step - loss: 0.9011 - val_loss: 0.8979\n",
      "Epoch 57/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 0.8892 - val_loss: 0.8777\n",
      "Epoch 58/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 0.8775 - val_loss: 0.8706\n",
      "Epoch 59/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 0.8669 - val_loss: 0.8607\n",
      "Epoch 60/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 0.8565 - val_loss: 0.8455\n",
      "Epoch 61/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 0.8464 - val_loss: 0.8421\n",
      "Epoch 62/300\n",
      "17/17 [==============================] - 17s 979ms/step - loss: 0.8368 - val_loss: 0.8302\n",
      "Epoch 63/300\n",
      "17/17 [==============================] - 17s 971ms/step - loss: 0.8283 - val_loss: 0.8259\n",
      "Epoch 64/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 0.8196 - val_loss: 0.8031\n",
      "Epoch 65/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 0.8117 - val_loss: 0.8043\n",
      "Epoch 66/300\n",
      "17/17 [==============================] - 17s 980ms/step - loss: 0.8039 - val_loss: 0.8014\n",
      "Epoch 67/300\n",
      "17/17 [==============================] - 16s 971ms/step - loss: 0.7971 - val_loss: 0.7911\n",
      "Epoch 68/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 0.7901 - val_loss: 0.7764\n",
      "Epoch 69/300\n",
      "17/17 [==============================] - 17s 980ms/step - loss: 0.7836 - val_loss: 0.7832\n",
      "Epoch 70/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 0.7780 - val_loss: 0.7732\n",
      "Epoch 71/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 0.7727 - val_loss: 0.7805\n",
      "Epoch 72/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 0.7673 - val_loss: 0.7511\n",
      "Epoch 73/300\n",
      "17/17 [==============================] - 17s 971ms/step - loss: 0.7618 - val_loss: 0.7595\n",
      "Epoch 74/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 0.7570 - val_loss: 0.7503\n",
      "Epoch 75/300\n",
      "17/17 [==============================] - 17s 971ms/step - loss: 0.7523 - val_loss: 0.7477\n",
      "Epoch 76/300\n",
      "17/17 [==============================] - 17s 1s/step - loss: 0.7480 - val_loss: 0.7508\n",
      "Epoch 77/300\n",
      "17/17 [==============================] - 17s 1s/step - loss: 0.7443 - val_loss: 0.7454\n",
      "Epoch 78/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 0.7407 - val_loss: 0.7413\n",
      "Epoch 79/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: 0.7371 - val_loss: 0.7214\n",
      "Epoch 80/300\n",
      "17/17 [==============================] - 16s 969ms/step - loss: 0.7337 - val_loss: 0.7308\n",
      "Epoch 81/300\n",
      "17/17 [==============================] - 16s 967ms/step - loss: 0.7309 - val_loss: 0.7317\n",
      "Epoch 82/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 0.7277 - val_loss: 0.7232\n",
      "Epoch 83/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 0.7252 - val_loss: 0.7208\n",
      "Epoch 84/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 0.7226 - val_loss: 0.7140\n",
      "Epoch 85/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 0.7204 - val_loss: 0.7216\n",
      "Epoch 86/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: 0.7181 - val_loss: 0.7159\n",
      "Epoch 87/300\n",
      "17/17 [==============================] - 16s 971ms/step - loss: 0.7161 - val_loss: 0.7060\n",
      "Epoch 88/300\n",
      "17/17 [==============================] - 16s 970ms/step - loss: 0.7142 - val_loss: 0.7109\n",
      "Epoch 89/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 0.7127 - val_loss: 0.7151\n",
      "Epoch 90/300\n",
      "17/17 [==============================] - 16s 971ms/step - loss: 0.7110 - val_loss: 0.7035\n",
      "Epoch 91/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: 0.7093 - val_loss: 0.7068\n",
      "Epoch 92/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: 0.7081 - val_loss: 0.7075\n",
      "Epoch 93/300\n",
      "17/17 [==============================] - 16s 967ms/step - loss: 0.7071 - val_loss: 0.7148\n",
      "Epoch 94/300\n",
      "17/17 [==============================] - 16s 968ms/step - loss: 0.7053 - val_loss: 0.6965\n",
      "Epoch 95/300\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 0.7044 - val_loss: 0.7005\n",
      "Epoch 96/300\n",
      "17/17 [==============================] - 17s 984ms/step - loss: 0.7034 - val_loss: 0.7034\n",
      "Epoch 97/300\n",
      "17/17 [==============================] - 16s 972ms/step - loss: 0.7027 - val_loss: 0.7008\n",
      "Epoch 98/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7017Restoring model weights from the end of the best epoch: 94.\n",
      "17/17 [==============================] - 17s 975ms/step - loss: 0.7017 - val_loss: 0.6997\n",
      "Epoch 00098: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_33/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_33/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcfa1d69c50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcfadfe8dd0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "Epoch 1/300\n",
      "17/17 [==============================] - 19s 992ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 17s 976ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 17s 973ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 16s 967ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_34/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_34/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf98078550> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf980bbc90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "Epoch 1/300\n",
      "17/17 [==============================] - 19s 992ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 17s 989ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 17s 972ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_35/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_35/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf80500cd0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf9b618cd0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "Epoch 1/300\n",
      "17/17 [==============================] - 19s 1s/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 16s 969ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 17s 976ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_36/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_36/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf98f829d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf98f44590> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "Epoch 1/300\n",
      "17/17 [==============================] - 19s 987ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 17s 977ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 16s 969ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_37/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_37/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf98b9e550> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcfa1f04990> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "Epoch 1/300\n",
      "17/17 [==============================] - 19s 996ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 17s 978ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 17s 974ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_38/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_38/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf988c8590> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf9815a450> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "Epoch 1/300\n",
      "17/17 [==============================] - 19s 996ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 17s 971ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 17s 971ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_39/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_39/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcfa1a9a9d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf80523110> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "Epoch 1/300\n",
      "17/17 [==============================] - 19s 992ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 17s 974ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 17s 972ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 17s 971ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_40/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_40/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf98c8b710> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf807ffe50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "for i in range(21,41):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, activation=\"relu\", input_shape=(max_length, 1), return_sequences=True,\n",
    "                  kernel_regularizer=\"l2\", recurrent_regularizer=\"l2\", bias_regularizer=\"l2\"))  \n",
    "    model.add(LSTM(128, activation=\"relu\", return_sequences=False,\n",
    "                  kernel_regularizer=\"l2\", recurrent_regularizer=\"l2\", bias_regularizer=\"l2\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer='adam', loss=\"binary_crossentropy\")\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', patience=4, verbose=1, restore_best_weights=True)\n",
    "    model.fit(X_train_os, y_train_os, epochs=300, verbose=1, batch_size=128, validation_data=(X_test, y_test), callbacks=[es])\n",
    "\n",
    "    try:\n",
    "        model.save(path.join(DATA_DIR, f\"models/LSTM5_{i}\"))\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        if not are_same(y_pred):\n",
    "            print(i)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532027f8-d971-498c-a9a4-232f01440f79",
   "metadata": {},
   "source": [
    "Predictions of each model (models, which predict nan, were skipped)  \n",
    "5 - all same  \n",
    "7 - all same  \n",
    "9 - one different: 114  \n",
    "10 - 114  \n",
    "12 - 114  \n",
    "13 - 114  \n",
    "18 - 114  \n",
    "19 - 114  \n",
    "24 - 114  \n",
    "26 - 114, 196  \n",
    "31 - 114  \n",
    "32 - 114  \n",
    "33 - 114  \n",
    "Conclusion: Models produce the same results for all sequences except one, which is the longest.  \n",
    "\n",
    "Attempt at solving this issue: trim long sequences.\n",
    "\n",
    "# \"Short\" data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2840aec8-2fe7-42ae-b206-ac1a86a9428d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ab_ID</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2073</th>\n",
       "      <td>6aod</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>4yny</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>5xcv</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2070</th>\n",
       "      <td>6and</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>2xqy</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 236 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Ab_ID   0   1   2   3   4   5   6  7  8  ...  225  226  227  228  229  \\\n",
       "2073  6aod   4  18  14  10  18  14  16  6  1  ...    6   17    9   18    3   \n",
       "1517  4yny   4  18  14  10  18   4  16  6  6  ...    6    6    6   17    9   \n",
       "2025  5xcv   4  18  14  10  18   4  16  6  6  ...    6    6    6   17    9   \n",
       "2070  6and   4  18  14  10  18   4  16  6  6  ...   20   17    5    6   14   \n",
       "666   2xqy  14  18  14  10  14  14  13  6  1  ...   17    5    6    6    6   \n",
       "\n",
       "      230  231  232  233  234  \n",
       "2073    8    9    0    0    0  \n",
       "1517   10   17   18   10    0  \n",
       "2025   10   17   18   10    0  \n",
       "2070    6   17    9   18    4  \n",
       "666    17    9   10    4    8  \n",
       "\n",
       "[5 rows x 236 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_chen_short = pd.read_csv(path.join(DATA_DIR, \"chen/integer_encoding/chen_integer_encoded_separate_short.csv\"), index_col=0)\n",
    "x_chen_train = x_chen_short.loc[chen_train.index]\n",
    "x_chen_test = x_chen_short.loc[chen_valid.index]\n",
    "x_chen_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7e307803-88ad-408e-99d0-1a869a912880",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = x_chen_train.drop(\"Ab_ID\", axis=1).to_numpy()\n",
    "X_test = x_chen_test.drop(\"Ab_ID\", axis=1).to_numpy()\n",
    "\n",
    "y_train = chen_train[\"Y\"].to_numpy()\n",
    "y_test = chen_valid[\"Y\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f753cbbc-6294-4dea-bc5b-db2c37d2f5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235\n"
     ]
    }
   ],
   "source": [
    "max_length = len(X_train[0])\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366f9771-6c3a-44db-ab58-65fa2059667c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-05 15:37:45.540512: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      " 3/11 [=======>......................] - ETA: 6s - loss: nan     "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/pbs.33248.lich-compute.vscht.cz/ipykernel_3499/2801180039.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(40,50):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, activation=\"relu\", input_shape=(max_length, 1), return_sequences=True,\n",
    "                  kernel_regularizer=\"l2\", recurrent_regularizer=\"l2\", bias_regularizer=\"l2\"))  \n",
    "    model.add(LSTM(128, activation=\"relu\", return_sequences=False,\n",
    "                  kernel_regularizer=\"l2\", recurrent_regularizer=\"l2\", bias_regularizer=\"l2\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer='adam', loss=\"binary_crossentropy\")\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', patience=4, verbose=1, restore_best_weights=True)\n",
    "    model.fit(X_train, y_train, epochs=300, verbose=1, batch_size=128, validation_data=(X_test, y_test), callbacks=[es])\n",
    "\n",
    "    try:\n",
    "        model.save(path.join(DATA_DIR, f\"models/LSTM5_{i}\"))\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        if not are_same(y_pred) and not np.isna(y_pred[0]):\n",
    "            print(f\"####### {i} ########\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "67d25d38-ebe3-40ac-bb46-10d74789f652",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06899455],\n",
       "       [0.27158046],\n",
       "       [0.20779726],\n",
       "       [0.22978708],\n",
       "       [0.22980142],\n",
       "       [0.16201907],\n",
       "       [0.07136723],\n",
       "       [0.19466239],\n",
       "       [0.17164588],\n",
       "       [0.17296824],\n",
       "       [0.2725097 ],\n",
       "       [0.06542116],\n",
       "       [0.10364473],\n",
       "       [0.09012645],\n",
       "       [0.09777227],\n",
       "       [0.0771991 ],\n",
       "       [0.17190406],\n",
       "       [0.09285197],\n",
       "       [0.08243164],\n",
       "       [0.05294067],\n",
       "       [0.05282646],\n",
       "       [0.05291408],\n",
       "       [0.05291408],\n",
       "       [0.1364786 ],\n",
       "       [0.5581246 ],\n",
       "       [0.42886806],\n",
       "       [0.03805584],\n",
       "       [0.09586623],\n",
       "       [0.3369332 ],\n",
       "       [0.0834963 ],\n",
       "       [0.08299831],\n",
       "       [0.11299083],\n",
       "       [0.11645439],\n",
       "       [0.10437143],\n",
       "       [0.10437432],\n",
       "       [0.22223398],\n",
       "       [0.2897813 ],\n",
       "       [0.41543114],\n",
       "       [0.08049372],\n",
       "       [0.36722237],\n",
       "       [0.3755607 ],\n",
       "       [0.34420067],\n",
       "       [0.14052877],\n",
       "       [0.23319945],\n",
       "       [0.3133432 ],\n",
       "       [0.09434327],\n",
       "       [0.06814182],\n",
       "       [0.07146752],\n",
       "       [0.07144582],\n",
       "       [0.2610513 ],\n",
       "       [0.30419058],\n",
       "       [0.17003438],\n",
       "       [0.3492315 ],\n",
       "       [0.20221171],\n",
       "       [0.08672154],\n",
       "       [0.14515293],\n",
       "       [0.28863257],\n",
       "       [0.29130304],\n",
       "       [0.19230601],\n",
       "       [0.26740623],\n",
       "       [0.31116188],\n",
       "       [0.10047984],\n",
       "       [0.13708383],\n",
       "       [0.3553874 ],\n",
       "       [0.35731393],\n",
       "       [0.31106204],\n",
       "       [0.09779754],\n",
       "       [0.09549224],\n",
       "       [0.28163368],\n",
       "       [0.14106277],\n",
       "       [0.09355134],\n",
       "       [0.09398985],\n",
       "       [0.09525412],\n",
       "       [0.10029107],\n",
       "       [0.09875482],\n",
       "       [0.09007761],\n",
       "       [0.07335952],\n",
       "       [0.07127151],\n",
       "       [0.11098975],\n",
       "       [0.28306752],\n",
       "       [0.36548936],\n",
       "       [0.23569879],\n",
       "       [0.22961223],\n",
       "       [0.12927988],\n",
       "       [0.25848827],\n",
       "       [0.22624663],\n",
       "       [0.17130962],\n",
       "       [0.17294368],\n",
       "       [0.32254267],\n",
       "       [0.10641441],\n",
       "       [0.08865628],\n",
       "       [0.09896106],\n",
       "       [0.27196914],\n",
       "       [0.27212507],\n",
       "       [0.1052351 ],\n",
       "       [0.10532147],\n",
       "       [0.10523367],\n",
       "       [0.17609006],\n",
       "       [0.24888793],\n",
       "       [0.09633625],\n",
       "       [0.35323298],\n",
       "       [0.35329878],\n",
       "       [0.3549235 ],\n",
       "       [0.07696727],\n",
       "       [0.31116503],\n",
       "       [0.16051778],\n",
       "       [0.08400136],\n",
       "       [0.08886877],\n",
       "       [0.14122158],\n",
       "       [0.32671714],\n",
       "       [0.24326196],\n",
       "       [0.07956704],\n",
       "       [0.4317983 ],\n",
       "       [0.0688194 ],\n",
       "       [0.08097175],\n",
       "       [0.11429343],\n",
       "       [0.43686837],\n",
       "       [0.07395035],\n",
       "       [0.10529619],\n",
       "       [0.37875077],\n",
       "       [0.12487841],\n",
       "       [0.05290303],\n",
       "       [0.0529044 ],\n",
       "       [0.32853982],\n",
       "       [0.10821131],\n",
       "       [0.17238373],\n",
       "       [0.10499877],\n",
       "       [0.15184283],\n",
       "       [0.21416813],\n",
       "       [0.09669033],\n",
       "       [0.10783464],\n",
       "       [0.08467066],\n",
       "       [0.09677684],\n",
       "       [0.05617687],\n",
       "       [0.06289235],\n",
       "       [0.45673248],\n",
       "       [0.10640723],\n",
       "       [0.11987203],\n",
       "       [0.1405671 ],\n",
       "       [0.13812453],\n",
       "       [0.17563078],\n",
       "       [0.2577766 ],\n",
       "       [0.28289086],\n",
       "       [0.04994985],\n",
       "       [0.12736467],\n",
       "       [0.08324394],\n",
       "       [0.08328864],\n",
       "       [0.1242471 ],\n",
       "       [0.12598372],\n",
       "       [0.12409744],\n",
       "       [0.08183497],\n",
       "       [0.09478828],\n",
       "       [0.3803588 ],\n",
       "       [0.30345634],\n",
       "       [0.13534594],\n",
       "       [0.31461078],\n",
       "       [0.09110135],\n",
       "       [0.36543196],\n",
       "       [0.17470545],\n",
       "       [0.1728248 ],\n",
       "       [0.26752526],\n",
       "       [0.06946969],\n",
       "       [0.07481313],\n",
       "       [0.07619071],\n",
       "       [0.15426603],\n",
       "       [0.31415653],\n",
       "       [0.08966345],\n",
       "       [0.08276016],\n",
       "       [0.36537957],\n",
       "       [0.08679003],\n",
       "       [0.12871519],\n",
       "       [0.11084795],\n",
       "       [0.11359519],\n",
       "       [0.09337708],\n",
       "       [0.15333337],\n",
       "       [0.3032248 ],\n",
       "       [0.11147064],\n",
       "       [0.16522372],\n",
       "       [0.31495303],\n",
       "       [0.33384287],\n",
       "       [0.31498146],\n",
       "       [0.3364281 ],\n",
       "       [0.31301498],\n",
       "       [0.10116446],\n",
       "       [0.37096086],\n",
       "       [0.08614099],\n",
       "       [0.35514647],\n",
       "       [0.08371028],\n",
       "       [0.09270468],\n",
       "       [0.4317842 ],\n",
       "       [0.2757881 ],\n",
       "       [0.3863973 ],\n",
       "       [0.08670908],\n",
       "       [0.11092901],\n",
       "       [0.07976279],\n",
       "       [0.09260073],\n",
       "       [0.12777933],\n",
       "       [0.08747828],\n",
       "       [0.13918528],\n",
       "       [0.14170709],\n",
       "       [0.13586882],\n",
       "       [0.12163427],\n",
       "       [0.2871001 ],\n",
       "       [0.19592959],\n",
       "       [0.04784399],\n",
       "       [0.04784378],\n",
       "       [0.18129286],\n",
       "       [0.3576864 ],\n",
       "       [0.09482914],\n",
       "       [0.09482914],\n",
       "       [0.3784886 ],\n",
       "       [0.46944705],\n",
       "       [0.31107777],\n",
       "       [0.13801622],\n",
       "       [0.3274017 ],\n",
       "       [0.05004978],\n",
       "       [0.14079759],\n",
       "       [0.29869568],\n",
       "       [0.16800717],\n",
       "       [0.10960171],\n",
       "       [0.06713164],\n",
       "       [0.06713164],\n",
       "       [0.1954639 ],\n",
       "       [0.19142371],\n",
       "       [0.10338148],\n",
       "       [0.33399743],\n",
       "       [0.11831698],\n",
       "       [0.22888637],\n",
       "       [0.18825018],\n",
       "       [0.24991861],\n",
       "       [0.24018121],\n",
       "       [0.2783237 ],\n",
       "       [0.22139841],\n",
       "       [0.34041917],\n",
       "       [0.11595273],\n",
       "       [0.1123825 ],\n",
       "       [0.05289567],\n",
       "       [0.11129966],\n",
       "       [0.3376087 ]], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 42\n",
    "model = keras.models.load_model(path.join(DATA_DIR, f\"models/LSTM5_{i}\"))\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5a1202c3-694d-407b-a90a-480b3a854fde",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49350649350649345\n",
      "0.16500000000000006\n",
      "\n",
      "\n",
      "MCC: 0.3439881846156861\n",
      "Acc: 0.6736401673640168\n"
     ]
    }
   ],
   "source": [
    "best_f1 = 0\n",
    "best_t = 0\n",
    "for t in np.arange(0.1, 0.3, 0.005):\n",
    "    bin_pred = binarize(y_pred, t)\n",
    "    f1 = metrics.f1_score(y_test, bin_pred)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_t = t\n",
    "print(best_f1)\n",
    "print(best_t)\n",
    "print(\"\\n\")\n",
    "bin_pred = binarize(y_pred, best_t)\n",
    "print(f\"MCC: {metrics.matthews_corrcoef(y_test, bin_pred)}\")\n",
    "print(f\"Acc: {metrics.accuracy_score(y_test, bin_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f800a5c1-b507-4ac1-a945-bfd2e3dc57ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = []\n",
    "thresholds = []\n",
    "for t in np.arange(0.1, 0.8, 0.005):\n",
    "    bin_pred = binarize(y_pred, t)\n",
    "    f1 = metrics.f1_score(y_test, bin_pred)\n",
    "    scores.append(f1)\n",
    "    thresholds.append(t)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.ylabel(\"F1\")\n",
    "plt.xlabel(\"threshold\")\n",
    "plt.plot(thresholds, scores)\n",
    "plt.savefig(path.join(DATA_DIR, 'images/model_42_threshold_dependence.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b57a74b-d9b5-445f-bc0d-fd851306e766",
   "metadata": {},
   "source": [
    "Performances of each model (models, which predict nan, were skipped)  \n",
    "41: F1=0.46; threshold=0.195  \n",
    "42: F1=0.4935; threshold=0.165  \n",
    "49: F1=0.4837; threshold=0.185   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef8044b-985b-402e-815f-97849873c3c2",
   "metadata": {},
   "source": [
    "# Short data + Over-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "05c329cb-560c-405a-9f83-ec12729da6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape Counter({0: 1057, 1: 1057})\n",
      "Resampled dataset shape Counter({0: 190, 1: 190})\n"
     ]
    }
   ],
   "source": [
    "sampler = RandomOverSampler(random_state=42)\n",
    "\n",
    "X_train_os, y_train_os = sampler.fit_resample(X_train, y_train) \n",
    "print('Resampled dataset shape %s' % Counter(y_train_os)) \n",
    "\n",
    "X_test_os, y_test_os = sampler.fit_resample(X_test, y_test)\n",
    "print('Resampled dataset shape %s' % Counter(y_test_os)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "1267c649-bd75-48b8-b938-c5439c8a86d6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "17/17 [==============================] - 16s 825ms/step - loss: 7.5465 - val_loss: 7.0321\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 14s 806ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - 14s 801ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/300\n",
      "17/17 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 14s 806ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00005: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_50/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_50/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf9b2520d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf9b25e2d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "17/17 [==============================] - 16s 823ms/step - loss: 6.8530 - val_loss: 5.7239\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 14s 804ms/step - loss: 5.1595 - val_loss: 4.4865\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 14s 806ms/step - loss: 4.1555 - val_loss: 3.6923\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - 14s 805ms/step - loss: 3.5696 - val_loss: 3.2833\n",
      "Epoch 5/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 3.2140 - val_loss: 3.0516\n",
      "Epoch 6/300\n",
      "17/17 [==============================] - 14s 799ms/step - loss: 2.9952 - val_loss: 2.8918\n",
      "Epoch 7/300\n",
      "17/17 [==============================] - 14s 806ms/step - loss: 2.8344 - val_loss: 2.6761\n",
      "Epoch 8/300\n",
      "17/17 [==============================] - 14s 804ms/step - loss: 2.7310 - val_loss: 2.5982\n",
      "Epoch 9/300\n",
      "17/17 [==============================] - 14s 803ms/step - loss: 2.6302 - val_loss: 2.4901\n",
      "Epoch 10/300\n",
      "17/17 [==============================] - 14s 806ms/step - loss: 2.5425 - val_loss: 2.3641\n",
      "Epoch 11/300\n",
      "17/17 [==============================] - 14s 806ms/step - loss: 2.4668 - val_loss: 2.2882\n",
      "Epoch 12/300\n",
      "17/17 [==============================] - 14s 803ms/step - loss: 2.3964 - val_loss: 2.3145\n",
      "Epoch 13/300\n",
      "17/17 [==============================] - 14s 804ms/step - loss: 2.3279 - val_loss: 2.1375\n",
      "Epoch 14/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 2.2830 - val_loss: 2.2233\n",
      "Epoch 15/300\n",
      "17/17 [==============================] - 14s 805ms/step - loss: 2.2206 - val_loss: 2.1472\n",
      "Epoch 16/300\n",
      "17/17 [==============================] - 14s 805ms/step - loss: 2.1668 - val_loss: 2.0268\n",
      "Epoch 17/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 2.1127 - val_loss: 2.0065\n",
      "Epoch 18/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 2.0490 - val_loss: 1.9857\n",
      "Epoch 19/300\n",
      "17/17 [==============================] - 14s 801ms/step - loss: 1.9972 - val_loss: 1.8903\n",
      "Epoch 20/300\n",
      "17/17 [==============================] - 14s 806ms/step - loss: 1.9384 - val_loss: 1.8268\n",
      "Epoch 21/300\n",
      "17/17 [==============================] - 14s 803ms/step - loss: 1.8897 - val_loss: 1.8237\n",
      "Epoch 22/300\n",
      "17/17 [==============================] - 14s 802ms/step - loss: 1.8425 - val_loss: 1.7995\n",
      "Epoch 23/300\n",
      "17/17 [==============================] - 14s 806ms/step - loss: 1.7918 - val_loss: 1.7488\n",
      "Epoch 24/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 1.7466 - val_loss: 1.6591\n",
      "Epoch 25/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 1.7001 - val_loss: 1.5796\n",
      "Epoch 26/300\n",
      "17/17 [==============================] - 14s 806ms/step - loss: 1.6542 - val_loss: 1.6394\n",
      "Epoch 27/300\n",
      "17/17 [==============================] - 14s 804ms/step - loss: 1.6156 - val_loss: 1.4699\n",
      "Epoch 28/300\n",
      "17/17 [==============================] - 14s 806ms/step - loss: 1.5772 - val_loss: 1.4616\n",
      "Epoch 29/300\n",
      "17/17 [==============================] - 14s 836ms/step - loss: 1.5359 - val_loss: 1.4918\n",
      "Epoch 30/300\n",
      "17/17 [==============================] - 14s 806ms/step - loss: 1.4982 - val_loss: 1.4222\n",
      "Epoch 31/300\n",
      "17/17 [==============================] - 14s 810ms/step - loss: 1.4545 - val_loss: 1.4181\n",
      "Epoch 32/300\n",
      "17/17 [==============================] - 14s 805ms/step - loss: 1.4224 - val_loss: 1.3890\n",
      "Epoch 33/300\n",
      "17/17 [==============================] - 14s 803ms/step - loss: 1.3984 - val_loss: 1.2509\n",
      "Epoch 34/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 1.3617 - val_loss: 1.2616\n",
      "Epoch 35/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 1.3206 - val_loss: 1.2759\n",
      "Epoch 36/300\n",
      "17/17 [==============================] - 14s 806ms/step - loss: 1.2898 - val_loss: 1.1818\n",
      "Epoch 37/300\n",
      "17/17 [==============================] - 14s 805ms/step - loss: 1.2631 - val_loss: 1.2020\n",
      "Epoch 38/300\n",
      "17/17 [==============================] - 14s 805ms/step - loss: 1.2293 - val_loss: 1.1880\n",
      "Epoch 39/300\n",
      "17/17 [==============================] - 14s 804ms/step - loss: 1.2053 - val_loss: 1.1443\n",
      "Epoch 40/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 1.1816 - val_loss: 1.1720\n",
      "Epoch 41/300\n",
      "17/17 [==============================] - 14s 802ms/step - loss: 1.1572 - val_loss: 1.1154\n",
      "Epoch 42/300\n",
      "17/17 [==============================] - 14s 802ms/step - loss: 1.1210 - val_loss: 1.0815\n",
      "Epoch 43/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 1.0981 - val_loss: 1.1107\n",
      "Epoch 44/300\n",
      "17/17 [==============================] - 14s 805ms/step - loss: 1.0844 - val_loss: 1.0138\n",
      "Epoch 45/300\n",
      "17/17 [==============================] - 14s 806ms/step - loss: 1.0666 - val_loss: 0.9912\n",
      "Epoch 46/300\n",
      "17/17 [==============================] - 14s 806ms/step - loss: 1.0350 - val_loss: 0.9555\n",
      "Epoch 47/300\n",
      "17/17 [==============================] - 14s 804ms/step - loss: 1.0125 - val_loss: 0.9883\n",
      "Epoch 48/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 0.9953 - val_loss: 0.9198\n",
      "Epoch 49/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 0.9786 - val_loss: 0.9068\n",
      "Epoch 50/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 0.9647 - val_loss: 0.9514\n",
      "Epoch 51/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 0.9514 - val_loss: 0.9107\n",
      "Epoch 52/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 0.9272 - val_loss: 0.9370\n",
      "Epoch 53/300\n",
      "17/17 [==============================] - 14s 805ms/step - loss: 0.9191 - val_loss: 0.8784\n",
      "Epoch 54/300\n",
      "17/17 [==============================] - 14s 806ms/step - loss: 0.8996 - val_loss: 0.9199\n",
      "Epoch 55/300\n",
      "17/17 [==============================] - 14s 806ms/step - loss: 0.8957 - val_loss: 0.7940\n",
      "Epoch 56/300\n",
      "17/17 [==============================] - 14s 803ms/step - loss: 0.8727 - val_loss: 0.8706\n",
      "Epoch 57/300\n",
      "17/17 [==============================] - 14s 805ms/step - loss: 0.8633 - val_loss: 0.8056\n",
      "Epoch 58/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 0.8444 - val_loss: 0.8244\n",
      "Epoch 59/300\n",
      "17/17 [==============================] - 14s 804ms/step - loss: 0.8304 - val_loss: 0.7931\n",
      "Epoch 60/300\n",
      "17/17 [==============================] - 14s 803ms/step - loss: 0.9902 - val_loss: 0.7759\n",
      "Epoch 61/300\n",
      "17/17 [==============================] - 14s 805ms/step - loss: 0.9010 - val_loss: 0.8849\n",
      "Epoch 62/300\n",
      "17/17 [==============================] - 14s 804ms/step - loss: 0.9000 - val_loss: 0.8766\n",
      "Epoch 63/300\n",
      "17/17 [==============================] - 14s 809ms/step - loss: 0.8881 - val_loss: 0.8373\n",
      "Epoch 64/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8785Restoring model weights from the end of the best epoch: 60.\n",
      "17/17 [==============================] - 14s 803ms/step - loss: 0.8785 - val_loss: 0.8254\n",
      "Epoch 00064: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_51/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_51/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf6cb81e50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcfa8089f50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "17/17 [==============================] - 16s 827ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 14s 802ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 14s 805ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_52/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_52/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf98424510> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf80a62a90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "17/17 [==============================] - 16s 828ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 14s 806ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 14s 809ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 14s 812ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_53/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_53/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcfa153a410> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcfadb81810> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "17/17 [==============================] - 16s 828ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 14s 810ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 14s 807ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_54/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_54/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf9b38e890> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcfaeb1b4d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "17/17 [==============================] - 16s 825ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 14s 805ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 14s 805ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 14s 808ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_55/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_55/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcfadce77d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf982a55d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "17/17 [==============================] - 16s 829ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 14s 810ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_56/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_56/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf8055ccd0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf9842e510> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "17/17 [==============================] - 16s 828ms/step - loss: 6.8614 - val_loss: 5.8301\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 14s 809ms/step - loss: 5.1740 - val_loss: 4.5126\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 4.1614 - val_loss: 3.7176\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 3.5714 - val_loss: 3.2488\n",
      "Epoch 5/300\n",
      "17/17 [==============================] - 14s 809ms/step - loss: 3.2167 - val_loss: 2.9637\n",
      "Epoch 6/300\n",
      "17/17 [==============================] - 14s 805ms/step - loss: 2.9990 - val_loss: 2.9209\n",
      "Epoch 7/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 2.8444 - val_loss: 2.8066\n",
      "Epoch 8/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 2.7246 - val_loss: 2.4988\n",
      "Epoch 9/300\n",
      "17/17 [==============================] - 14s 812ms/step - loss: 1117.8434 - val_loss: 2.6255\n",
      "Epoch 10/300\n",
      "17/17 [==============================] - 14s 810ms/step - loss: 2.6776 - val_loss: 2.6214\n",
      "Epoch 11/300\n",
      "17/17 [==============================] - 14s 806ms/step - loss: 2.6347 - val_loss: 2.5978\n",
      "Epoch 12/300\n",
      "17/17 [==============================] - 14s 806ms/step - loss: 2.5880 - val_loss: 2.4778\n",
      "Epoch 13/300\n",
      "17/17 [==============================] - 14s 806ms/step - loss: 2.5470 - val_loss: 2.4765\n",
      "Epoch 14/300\n",
      "17/17 [==============================] - 14s 803ms/step - loss: 2.5037 - val_loss: 2.4972\n",
      "Epoch 15/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 2.4623 - val_loss: 2.4612\n",
      "Epoch 16/300\n",
      "17/17 [==============================] - 14s 805ms/step - loss: 27653502976.0000 - val_loss: 2.4377\n",
      "Epoch 17/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 2.4478 - val_loss: 2.4307\n",
      "Epoch 18/300\n",
      "17/17 [==============================] - 14s 809ms/step - loss: 2.4385 - val_loss: 2.4108\n",
      "Epoch 19/300\n",
      "17/17 [==============================] - 14s 810ms/step - loss: 2.4135 - val_loss: 2.3763\n",
      "Epoch 20/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 2.3881 - val_loss: 2.3773\n",
      "Epoch 21/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 2.3608 - val_loss: 2.3190\n",
      "Epoch 22/300\n",
      "17/17 [==============================] - 14s 804ms/step - loss: 2.3344 - val_loss: 2.2859\n",
      "Epoch 23/300\n",
      "17/17 [==============================] - 14s 810ms/step - loss: 2.3089 - val_loss: 2.2577\n",
      "Epoch 24/300\n",
      "17/17 [==============================] - 14s 809ms/step - loss: 2.2850 - val_loss: 2.2842\n",
      "Epoch 25/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 2.2623 - val_loss: 2.2066\n",
      "Epoch 26/300\n",
      "17/17 [==============================] - 14s 809ms/step - loss: 2.2410 - val_loss: 2.2230\n",
      "Epoch 27/300\n",
      "17/17 [==============================] - 14s 813ms/step - loss: 2.2180 - val_loss: 2.2333\n",
      "Epoch 28/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 2.1964 - val_loss: 2.1405\n",
      "Epoch 29/300\n",
      "17/17 [==============================] - 14s 805ms/step - loss: 2.1750 - val_loss: 2.1169\n",
      "Epoch 30/300\n",
      "17/17 [==============================] - 14s 804ms/step - loss: 2.1549 - val_loss: 2.1189\n",
      "Epoch 31/300\n",
      "17/17 [==============================] - 14s 809ms/step - loss: 2.1350 - val_loss: 2.0854\n",
      "Epoch 32/300\n",
      "17/17 [==============================] - 14s 803ms/step - loss: 2.1168 - val_loss: 2.0945\n",
      "Epoch 33/300\n",
      "17/17 [==============================] - 14s 805ms/step - loss: 2.0974 - val_loss: 2.0318\n",
      "Epoch 34/300\n",
      "17/17 [==============================] - 14s 806ms/step - loss: 2.0805 - val_loss: 2.0371\n",
      "Epoch 35/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 2.0629 - val_loss: 2.0395\n",
      "Epoch 36/300\n",
      "17/17 [==============================] - 14s 812ms/step - loss: 2.0465 - val_loss: 2.0089\n",
      "Epoch 37/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 2.0305 - val_loss: 1.9665\n",
      "Epoch 38/300\n",
      "17/17 [==============================] - 14s 803ms/step - loss: 2.0147 - val_loss: 2.0209\n",
      "Epoch 39/300\n",
      "17/17 [==============================] - 14s 809ms/step - loss: 2.0025 - val_loss: 2.0016\n",
      "Epoch 40/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 1.9906 - val_loss: 1.9662\n",
      "Epoch 41/300\n",
      "17/17 [==============================] - 14s 811ms/step - loss: 1.9780 - val_loss: 1.9309\n",
      "Epoch 42/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 1.9596 - val_loss: 1.9226\n",
      "Epoch 43/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 1.9458 - val_loss: 1.8852\n",
      "Epoch 44/300\n",
      "17/17 [==============================] - 14s 809ms/step - loss: 1.9352 - val_loss: 1.8646\n",
      "Epoch 45/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 1.9212 - val_loss: 1.8934\n",
      "Epoch 46/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 1.9091 - val_loss: 1.8774\n",
      "Epoch 47/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 1.8982 - val_loss: 1.8568\n",
      "Epoch 48/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 1.8871 - val_loss: 1.8835\n",
      "Epoch 49/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 1.8779 - val_loss: 1.8232\n",
      "Epoch 50/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 1.8666 - val_loss: 1.7851\n",
      "Epoch 51/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 1.8593 - val_loss: 1.8162\n",
      "Epoch 52/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 1.8470 - val_loss: 1.7881\n",
      "Epoch 53/300\n",
      "17/17 [==============================] - 14s 806ms/step - loss: 1.8379 - val_loss: 1.8243\n",
      "Epoch 54/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.8319Restoring model weights from the end of the best epoch: 50.\n",
      "17/17 [==============================] - 14s 815ms/step - loss: 1.8319 - val_loss: 1.7886\n",
      "Epoch 00054: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_57/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_57/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf9b38a610> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcfa8133bd0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "17/17 [==============================] - 16s 824ms/step - loss: 6.8417 - val_loss: 5.7822\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 14s 806ms/step - loss: 5.1152 - val_loss: 4.3645\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 14s 809ms/step - loss: 4.1020 - val_loss: 3.7612\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - 14s 821ms/step - loss: 3.5254 - val_loss: 3.3531\n",
      "Epoch 5/300\n",
      "17/17 [==============================] - 14s 809ms/step - loss: 3.1848 - val_loss: 3.0595\n",
      "Epoch 6/300\n",
      "17/17 [==============================] - 14s 814ms/step - loss: 2.9647 - val_loss: 2.7939\n",
      "Epoch 7/300\n",
      "17/17 [==============================] - 14s 810ms/step - loss: 2.8187 - val_loss: 2.6978\n",
      "Epoch 8/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 2.7050 - val_loss: 2.5454\n",
      "Epoch 9/300\n",
      "17/17 [==============================] - 14s 805ms/step - loss: 2.6141 - val_loss: 2.5176\n",
      "Epoch 10/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 2.5313 - val_loss: 2.4032\n",
      "Epoch 11/300\n",
      "17/17 [==============================] - 14s 805ms/step - loss: 2.4580 - val_loss: 2.4348\n",
      "Epoch 12/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 2.3878 - val_loss: 2.2941\n",
      "Epoch 13/300\n",
      "17/17 [==============================] - 14s 809ms/step - loss: 2.3171 - val_loss: 2.2664\n",
      "Epoch 14/300\n",
      "17/17 [==============================] - 14s 810ms/step - loss: 2.2541 - val_loss: 2.2253\n",
      "Epoch 15/300\n",
      "17/17 [==============================] - 14s 809ms/step - loss: 2.1943 - val_loss: 2.1259\n",
      "Epoch 16/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 2.1317 - val_loss: 1.9563\n",
      "Epoch 17/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 2.0729 - val_loss: 1.8981\n",
      "Epoch 18/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 2.0225 - val_loss: 2.0454\n",
      "Epoch 19/300\n",
      "17/17 [==============================] - 14s 806ms/step - loss: 1.9646 - val_loss: 1.8527\n",
      "Epoch 20/300\n",
      "17/17 [==============================] - 14s 809ms/step - loss: 1.9087 - val_loss: 1.8455\n",
      "Epoch 21/300\n",
      "17/17 [==============================] - 14s 810ms/step - loss: 1.8706 - val_loss: 1.8081\n",
      "Epoch 22/300\n",
      "17/17 [==============================] - 14s 812ms/step - loss: 1.8207 - val_loss: 1.7823\n",
      "Epoch 23/300\n",
      "17/17 [==============================] - 14s 809ms/step - loss: 1.7685 - val_loss: 1.6414\n",
      "Epoch 24/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 1.7298 - val_loss: 1.6930\n",
      "Epoch 25/300\n",
      "17/17 [==============================] - 14s 812ms/step - loss: 1.6847 - val_loss: 1.6393\n",
      "Epoch 26/300\n",
      "17/17 [==============================] - 14s 814ms/step - loss: 1.6337 - val_loss: 1.5673\n",
      "Epoch 27/300\n",
      "17/17 [==============================] - 14s 806ms/step - loss: 1.5991 - val_loss: 1.4953\n",
      "Epoch 28/300\n",
      "17/17 [==============================] - 14s 809ms/step - loss: 1.5572 - val_loss: 1.4783\n",
      "Epoch 29/300\n",
      "17/17 [==============================] - 14s 809ms/step - loss: 1.5179 - val_loss: 1.4172\n",
      "Epoch 30/300\n",
      "17/17 [==============================] - 14s 809ms/step - loss: 1.4745 - val_loss: 1.4052\n",
      "Epoch 31/300\n",
      "17/17 [==============================] - 14s 806ms/step - loss: 1.4366 - val_loss: 1.3003\n",
      "Epoch 32/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 1.4010 - val_loss: 1.3762\n",
      "Epoch 33/300\n",
      "17/17 [==============================] - 14s 809ms/step - loss: 1.3695 - val_loss: 1.2638\n",
      "Epoch 34/300\n",
      "17/17 [==============================] - 14s 803ms/step - loss: 1.3310 - val_loss: 1.3224\n",
      "Epoch 35/300\n",
      "17/17 [==============================] - 14s 803ms/step - loss: 1.2956 - val_loss: 1.2410\n",
      "Epoch 36/300\n",
      "17/17 [==============================] - 14s 803ms/step - loss: 1.2636 - val_loss: 1.1507\n",
      "Epoch 37/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 1.2327 - val_loss: 1.2401\n",
      "Epoch 38/300\n",
      "17/17 [==============================] - 14s 813ms/step - loss: 1.1994 - val_loss: 1.0901\n",
      "Epoch 39/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 1.1911 - val_loss: 1.1220\n",
      "Epoch 40/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 1.1684 - val_loss: 1.0652\n",
      "Epoch 41/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 1.1412 - val_loss: 1.1484\n",
      "Epoch 42/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 1.1145 - val_loss: 1.0265\n",
      "Epoch 43/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 1.0817 - val_loss: 1.0919\n",
      "Epoch 44/300\n",
      "17/17 [==============================] - 14s 806ms/step - loss: 1.0635 - val_loss: 1.0053\n",
      "Epoch 45/300\n",
      "17/17 [==============================] - 14s 805ms/step - loss: 1.0375 - val_loss: 0.9406\n",
      "Epoch 46/300\n",
      "17/17 [==============================] - 14s 805ms/step - loss: 1.0161 - val_loss: 0.9372\n",
      "Epoch 47/300\n",
      "17/17 [==============================] - 14s 809ms/step - loss: 1.0084 - val_loss: 1.0551\n",
      "Epoch 48/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 1.0002 - val_loss: 0.9405\n",
      "Epoch 49/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 0.9593 - val_loss: 0.9573\n",
      "Epoch 50/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 0.9396 - val_loss: 0.8972\n",
      "Epoch 51/300\n",
      "17/17 [==============================] - 14s 805ms/step - loss: 0.9320 - val_loss: 0.8457\n",
      "Epoch 52/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 0.9159 - val_loss: 0.8971\n",
      "Epoch 53/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 0.9101 - val_loss: 0.8373\n",
      "Epoch 54/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 0.8902 - val_loss: 0.8799\n",
      "Epoch 55/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 0.8793 - val_loss: 0.8358\n",
      "Epoch 56/300\n",
      "17/17 [==============================] - 14s 809ms/step - loss: 0.8628 - val_loss: 0.7963\n",
      "Epoch 57/300\n",
      "17/17 [==============================] - 14s 806ms/step - loss: 0.8466 - val_loss: 0.8813\n",
      "Epoch 58/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 0.8397 - val_loss: 0.7587\n",
      "Epoch 59/300\n",
      "17/17 [==============================] - 14s 810ms/step - loss: 0.8373 - val_loss: 0.7464\n",
      "Epoch 60/300\n",
      "17/17 [==============================] - 14s 811ms/step - loss: 0.8132 - val_loss: 0.9261\n",
      "Epoch 61/300\n",
      "17/17 [==============================] - 14s 838ms/step - loss: 0.8119 - val_loss: 0.7286\n",
      "Epoch 62/300\n",
      "17/17 [==============================] - 14s 806ms/step - loss: 0.7890 - val_loss: 0.7609\n",
      "Epoch 63/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 0.7803 - val_loss: 0.7397\n",
      "Epoch 64/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 0.7718 - val_loss: 0.6992\n",
      "Epoch 65/300\n",
      "17/17 [==============================] - 14s 812ms/step - loss: 0.7845 - val_loss: 0.7922\n",
      "Epoch 66/300\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 0.7595 - val_loss: 0.6862\n",
      "Epoch 67/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 0.7521 - val_loss: 0.7421\n",
      "Epoch 68/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 0.7415 - val_loss: 0.7156\n",
      "Epoch 69/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 0.7448 - val_loss: 0.6446\n",
      "Epoch 70/300\n",
      "17/17 [==============================] - 14s 814ms/step - loss: 0.7307 - val_loss: 0.7738\n",
      "Epoch 71/300\n",
      "17/17 [==============================] - 14s 810ms/step - loss: 0.7334 - val_loss: 0.6760\n",
      "Epoch 72/300\n",
      "17/17 [==============================] - 14s 806ms/step - loss: 0.7130 - val_loss: 0.6954\n",
      "Epoch 73/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7109Restoring model weights from the end of the best epoch: 69.\n",
      "17/17 [==============================] - 14s 808ms/step - loss: 0.7109 - val_loss: 0.6828\n",
      "Epoch 00073: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_58/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_58/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf9be1b810> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf80639510> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "17/17 [==============================] - 16s 824ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 14s 804ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 14s 808ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 14s 806ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_59/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_59/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf98e08e50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf984d2890> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "for i in range(50,60):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, activation=\"relu\", input_shape=(max_length, 1), return_sequences=True,\n",
    "                  kernel_regularizer=\"l2\", recurrent_regularizer=\"l2\", bias_regularizer=\"l2\"))  \n",
    "    model.add(LSTM(128, activation=\"relu\", return_sequences=False,\n",
    "                  kernel_regularizer=\"l2\", recurrent_regularizer=\"l2\", bias_regularizer=\"l2\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer='adam', loss=\"binary_crossentropy\")\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', patience=4, verbose=1, restore_best_weights=True)\n",
    "    model.fit(X_train_os, y_train_os, epochs=300, verbose=1, batch_size=128, validation_data=(X_test, y_test), callbacks=[es])\n",
    "\n",
    "    try:\n",
    "        model.save(path.join(DATA_DIR, f\"models/LSTM5_{i}\"))\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d348e5-f4c7-43cb-b6b6-db3fad14ed2a",
   "metadata": {},
   "source": [
    "51: F1=0.466; threshold=0.41  \n",
    "57: F1=0.47; threshold=0.495  \n",
    "58: F1=0.504; threshold=0.505\n",
    "\n",
    "### Smaller model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "61f6103e-6d41-4acb-996f-d02f891a1a0a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "17/17 [==============================] - 12s 594ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 10s 572ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 10s 572ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 10s 572ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_60/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_60/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcfa825af10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf80574290> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "17/17 [==============================] - 12s 596ms/step - loss: 5.5744 - val_loss: 4.7704\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 10s 579ms/step - loss: 4.3309 - val_loss: 3.7689\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 10s 580ms/step - loss: 3.5594 - val_loss: 3.2043\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - 10s 581ms/step - loss: 3.0753 - val_loss: 2.8296\n",
      "Epoch 5/300\n",
      "17/17 [==============================] - 10s 581ms/step - loss: 2.7649 - val_loss: 2.6141\n",
      "Epoch 6/300\n",
      "17/17 [==============================] - 10s 578ms/step - loss: 2.5601 - val_loss: 2.3938\n",
      "Epoch 7/300\n",
      "17/17 [==============================] - 10s 580ms/step - loss: 2.4146 - val_loss: 2.2926\n",
      "Epoch 8/300\n",
      "17/17 [==============================] - 10s 578ms/step - loss: 2.3133 - val_loss: 2.2320\n",
      "Epoch 9/300\n",
      "17/17 [==============================] - 10s 583ms/step - loss: 2.2230 - val_loss: 2.1952\n",
      "Epoch 10/300\n",
      "17/17 [==============================] - 10s 580ms/step - loss: 2.1467 - val_loss: 1.9864\n",
      "Epoch 11/300\n",
      "17/17 [==============================] - 11s 622ms/step - loss: 2.0884 - val_loss: 2.0071\n",
      "Epoch 12/300\n",
      "17/17 [==============================] - 10s 580ms/step - loss: 2.0260 - val_loss: 2.0101\n",
      "Epoch 13/300\n",
      "17/17 [==============================] - 10s 582ms/step - loss: 1.9682 - val_loss: 1.9271\n",
      "Epoch 14/300\n",
      "17/17 [==============================] - 10s 576ms/step - loss: 1.9258 - val_loss: 1.8619\n",
      "Epoch 15/300\n",
      "17/17 [==============================] - 10s 582ms/step - loss: 1.8934 - val_loss: 1.7902\n",
      "Epoch 16/300\n",
      "17/17 [==============================] - 10s 581ms/step - loss: 1.8344 - val_loss: 1.7613\n",
      "Epoch 17/300\n",
      "17/17 [==============================] - 10s 581ms/step - loss: 1.7808 - val_loss: 1.6524\n",
      "Epoch 18/300\n",
      "17/17 [==============================] - 10s 581ms/step - loss: 1.7304 - val_loss: 1.6300\n",
      "Epoch 19/300\n",
      "17/17 [==============================] - 10s 583ms/step - loss: 1.6984 - val_loss: 1.6488\n",
      "Epoch 20/300\n",
      "17/17 [==============================] - 10s 588ms/step - loss: 1.6616 - val_loss: 1.5948\n",
      "Epoch 21/300\n",
      "17/17 [==============================] - 10s 580ms/step - loss: 1.6268 - val_loss: 1.4775\n",
      "Epoch 22/300\n",
      "17/17 [==============================] - 10s 582ms/step - loss: 1.5813 - val_loss: 1.5428\n",
      "Epoch 23/300\n",
      "17/17 [==============================] - 10s 584ms/step - loss: 1.5454 - val_loss: 1.4592\n",
      "Epoch 24/300\n",
      "17/17 [==============================] - 10s 585ms/step - loss: 1.5012 - val_loss: 1.3903\n",
      "Epoch 25/300\n",
      "17/17 [==============================] - 10s 585ms/step - loss: 1.4659 - val_loss: 1.3286\n",
      "Epoch 26/300\n",
      "17/17 [==============================] - 10s 580ms/step - loss: 1.4246 - val_loss: 1.3648\n",
      "Epoch 27/300\n",
      "17/17 [==============================] - 10s 578ms/step - loss: 1.3938 - val_loss: 1.3533\n",
      "Epoch 28/300\n",
      "17/17 [==============================] - 10s 580ms/step - loss: 1.3826 - val_loss: 1.4345\n",
      "Epoch 29/300\n",
      "17/17 [==============================] - 10s 574ms/step - loss: 1.3432 - val_loss: 1.2759\n",
      "Epoch 30/300\n",
      "17/17 [==============================] - 10s 582ms/step - loss: 1.2979 - val_loss: 1.2533\n",
      "Epoch 31/300\n",
      "17/17 [==============================] - 10s 580ms/step - loss: 1.2648 - val_loss: 1.1474\n",
      "Epoch 32/300\n",
      "17/17 [==============================] - 10s 579ms/step - loss: 1.2361 - val_loss: 1.1803\n",
      "Epoch 33/300\n",
      "17/17 [==============================] - 10s 578ms/step - loss: 1.2053 - val_loss: 1.1148\n",
      "Epoch 34/300\n",
      "17/17 [==============================] - 10s 578ms/step - loss: 1.1849 - val_loss: 1.0352\n",
      "Epoch 35/300\n",
      "17/17 [==============================] - 10s 585ms/step - loss: 1.1711 - val_loss: 1.0913\n",
      "Epoch 36/300\n",
      "17/17 [==============================] - 10s 580ms/step - loss: 1.1454 - val_loss: 1.1086\n",
      "Epoch 37/300\n",
      "17/17 [==============================] - 10s 584ms/step - loss: 1.1110 - val_loss: 1.0511\n",
      "Epoch 38/300\n",
      "17/17 [==============================] - 10s 580ms/step - loss: 1.0883 - val_loss: 1.0197\n",
      "Epoch 39/300\n",
      "17/17 [==============================] - 10s 588ms/step - loss: 1.0692 - val_loss: 1.0087\n",
      "Epoch 40/300\n",
      "17/17 [==============================] - 10s 579ms/step - loss: 1.0446 - val_loss: 0.9867\n",
      "Epoch 41/300\n",
      "17/17 [==============================] - 10s 586ms/step - loss: 1.0327 - val_loss: 1.0126\n",
      "Epoch 42/300\n",
      "17/17 [==============================] - 10s 581ms/step - loss: 1.0045 - val_loss: 0.9133\n",
      "Epoch 43/300\n",
      "17/17 [==============================] - 10s 586ms/step - loss: 1.0020 - val_loss: 0.9281\n",
      "Epoch 44/300\n",
      "17/17 [==============================] - 10s 591ms/step - loss: 0.9771 - val_loss: 0.8986\n",
      "Epoch 45/300\n",
      "17/17 [==============================] - 10s 581ms/step - loss: 0.9554 - val_loss: 0.9324\n",
      "Epoch 46/300\n",
      "17/17 [==============================] - 10s 607ms/step - loss: 0.9356 - val_loss: 0.9049\n",
      "Epoch 47/300\n",
      "17/17 [==============================] - 10s 584ms/step - loss: 0.9297 - val_loss: 0.8187\n",
      "Epoch 48/300\n",
      "17/17 [==============================] - 10s 586ms/step - loss: 0.9174 - val_loss: 0.8567\n",
      "Epoch 49/300\n",
      "17/17 [==============================] - 10s 578ms/step - loss: 0.8981 - val_loss: 0.8418\n",
      "Epoch 50/300\n",
      "17/17 [==============================] - 10s 597ms/step - loss: 0.8833 - val_loss: 0.7977\n",
      "Epoch 51/300\n",
      "17/17 [==============================] - 10s 580ms/step - loss: 0.8669 - val_loss: 0.8194\n",
      "Epoch 52/300\n",
      "17/17 [==============================] - 10s 581ms/step - loss: 0.8551 - val_loss: 0.8398\n",
      "Epoch 53/300\n",
      "17/17 [==============================] - 10s 583ms/step - loss: 0.8558 - val_loss: 0.7737\n",
      "Epoch 54/300\n",
      "17/17 [==============================] - 10s 582ms/step - loss: 0.8391 - val_loss: 0.7663\n",
      "Epoch 55/300\n",
      "17/17 [==============================] - 10s 583ms/step - loss: 0.8261 - val_loss: 0.7540\n",
      "Epoch 56/300\n",
      "17/17 [==============================] - 10s 585ms/step - loss: 0.8143 - val_loss: 0.7281\n",
      "Epoch 57/300\n",
      "17/17 [==============================] - 10s 584ms/step - loss: 0.8043 - val_loss: 0.7941\n",
      "Epoch 58/300\n",
      "17/17 [==============================] - 10s 587ms/step - loss: 0.8176 - val_loss: 0.7691\n",
      "Epoch 59/300\n",
      "17/17 [==============================] - 10s 588ms/step - loss: 0.7944 - val_loss: 0.7341\n",
      "Epoch 60/300\n",
      "17/17 [==============================] - 11s 627ms/step - loss: 0.7793 - val_loss: 0.7090\n",
      "Epoch 61/300\n",
      "17/17 [==============================] - 11s 624ms/step - loss: 0.7679 - val_loss: 0.7570\n",
      "Epoch 62/300\n",
      "17/17 [==============================] - 11s 626ms/step - loss: 0.7595 - val_loss: 0.7545\n",
      "Epoch 63/300\n",
      "17/17 [==============================] - 11s 634ms/step - loss: 0.7787 - val_loss: 0.6156\n",
      "Epoch 64/300\n",
      "17/17 [==============================] - 11s 630ms/step - loss: 0.7547 - val_loss: 0.6308\n",
      "Epoch 65/300\n",
      "17/17 [==============================] - 11s 621ms/step - loss: 0.7426 - val_loss: 0.6755\n",
      "Epoch 66/300\n",
      "17/17 [==============================] - 11s 620ms/step - loss: 0.7284 - val_loss: 0.6810\n",
      "Epoch 67/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7234Restoring model weights from the end of the best epoch: 63.\n",
      "17/17 [==============================] - 10s 586ms/step - loss: 0.7234 - val_loss: 0.7328\n",
      "Epoch 00067: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_61/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_61/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf98aeb690> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf983208d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "17/17 [==============================] - 12s 606ms/step - loss: 49.3877 - val_loss: 5.6467\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 10s 585ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 10s 585ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - 10s 580ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/300\n",
      "17/17 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 10s 583ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00005: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_62/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_62/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf9b4bf050> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf80cc5f10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "17/17 [==============================] - 12s 601ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 10s 584ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 10s 588ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 10s 587ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_63/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_63/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf802e9890> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf80bc2790> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "17/17 [==============================] - 13s 653ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 10s 592ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 10s 603ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 10s 589ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_64/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_64/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcfa00c8610> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf98722c50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "17/17 [==============================] - 12s 597ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 10s 594ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 10s 590ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 10s 588ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_65/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_65/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcfa191fb10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf80ea22d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "17/17 [==============================] - 12s 600ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 10s 594ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 10s 588ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 10s 586ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_66/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_66/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf983c2f10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf98820710> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "17/17 [==============================] - 12s 611ms/step - loss: 5.6279 - val_loss: 4.8405\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 10s 580ms/step - loss: 4.3919 - val_loss: 3.8892\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 10s 581ms/step - loss: 3.6045 - val_loss: 3.3333\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - 10s 578ms/step - loss: 3.1126 - val_loss: 2.8362\n",
      "Epoch 5/300\n",
      "17/17 [==============================] - 10s 590ms/step - loss: 2.7975 - val_loss: 2.6240\n",
      "Epoch 6/300\n",
      "17/17 [==============================] - 10s 579ms/step - loss: 2.5944 - val_loss: 2.5505\n",
      "Epoch 7/300\n",
      "17/17 [==============================] - 10s 585ms/step - loss: 2.4556 - val_loss: 2.4119\n",
      "Epoch 8/300\n",
      "17/17 [==============================] - 10s 585ms/step - loss: 2.3456 - val_loss: 2.2906\n",
      "Epoch 9/300\n",
      "17/17 [==============================] - 10s 584ms/step - loss: 2.2543 - val_loss: 2.1134\n",
      "Epoch 10/300\n",
      "17/17 [==============================] - 10s 587ms/step - loss: 2.1759 - val_loss: 2.1542\n",
      "Epoch 11/300\n",
      "17/17 [==============================] - 10s 588ms/step - loss: 2.1156 - val_loss: 1.9655\n",
      "Epoch 12/300\n",
      "17/17 [==============================] - 10s 587ms/step - loss: 2.0581 - val_loss: 1.9397\n",
      "Epoch 13/300\n",
      "17/17 [==============================] - 10s 581ms/step - loss: 1.9913 - val_loss: 1.9625\n",
      "Epoch 14/300\n",
      "17/17 [==============================] - 10s 583ms/step - loss: 1.9469 - val_loss: 1.8910\n",
      "Epoch 15/300\n",
      "17/17 [==============================] - 10s 583ms/step - loss: 30948.0254 - val_loss: 1.7655\n",
      "Epoch 16/300\n",
      "17/17 [==============================] - 10s 585ms/step - loss: 1.9328 - val_loss: 1.9451\n",
      "Epoch 17/300\n",
      "17/17 [==============================] - 10s 593ms/step - loss: 1.9581 - val_loss: 1.8973\n",
      "Epoch 18/300\n",
      "17/17 [==============================] - 10s 583ms/step - loss: 1.9476 - val_loss: 1.9863\n",
      "Epoch 19/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.9376Restoring model weights from the end of the best epoch: 15.\n",
      "17/17 [==============================] - 10s 581ms/step - loss: 1.9376 - val_loss: 2.0099\n",
      "Epoch 00019: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_67/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_67/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf987f3ed0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf989b74d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "17/17 [==============================] - 12s 611ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 10s 586ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 10s 600ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 10s 589ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_68/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_68/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf6c777d50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcfa1b31d90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "17/17 [==============================] - 13s 607ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 10s 576ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 10s 590ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - ETA: 0s - loss: nanRestoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 10s 596ms/step - loss: nan - val_loss: nan\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_69/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/models/LSTM5_69/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf9879dcd0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcfade57610> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "for i in range(60, 70):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, activation=\"relu\", input_shape=(max_length, 1), return_sequences=True,\n",
    "                  kernel_regularizer=\"l2\", recurrent_regularizer=\"l2\", bias_regularizer=\"l2\"))  \n",
    "    model.add(LSTM(100, activation=\"relu\", return_sequences=False,\n",
    "                  kernel_regularizer=\"l2\", recurrent_regularizer=\"l2\", bias_regularizer=\"l2\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer='adam', loss=\"binary_crossentropy\")\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', patience=4, verbose=1, restore_best_weights=True)\n",
    "    model.fit(X_train_os, y_train_os, epochs=300, verbose=1, batch_size=128, validation_data=(X_test[:120], y_test[:120]), callbacks=[es])\n",
    "\n",
    "    try:\n",
    "        model.save(path.join(DATA_DIR, f\"models/LSTM5_{i}\"))\n",
    "\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe451cf-4241-4c25-a5d1-ae7e7e72f2a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e1ac59-89d2-4d0f-9ec7-a0c8cc51b9b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82a516d-83ab-4833-a091-47cbed1153e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
