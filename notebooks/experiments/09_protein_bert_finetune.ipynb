{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff8a9d94-4216-4b46-92e0-67298b77bf45",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from os import path\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3394e398-aa33-45b9-981f-ee3ead567624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from proteinbert.finetuning import encode_train_and_valid_sets, encode_dataset\n",
    "from proteinbert import OutputType, OutputSpec, evaluate_by_len, load_pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a869012f-8189-4cd3-b7a7-f614ce87bb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from proteinbert import OutputType, OutputSpec, FinetuningModelGenerator, load_pretrained_model, \\\n",
    "finetune, evaluate_by_len\n",
    "\n",
    "from proteinbert.conv_and_global_attention_model import get_model_with_hidden_layers_as_outputs\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d2ed9b0-3023-4f0c-b93e-3330a86f35ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4798e75-9168-4aa2-b76c-bf37d692efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33c6481c-b3ce-47c3-86c4-5258a9553fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5db7bda-d751-4876-916f-23e824fd7b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_TYPE = OutputType(False, 'binary')\n",
    "UNIQUE_LABELS = [0, 1]\n",
    "OUTPUT_SPEC = OutputSpec(OUTPUT_TYPE, UNIQUE_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78d4d0ff-52b0-4152-9447-182709e45923",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_generator, input_encoder = load_pretrained_model(\"../../data/protein_bert/\", \"epoch_92400_sample_23500000.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59ffc4b8-405d-4244-a4bc-f0ab0190c73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbdaf9de-0881-43dc-8d2e-317db6302948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkvetab\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/March_finetune/runs/g4l8kydo\" target=\"_blank\">elated-bush-9</a></strong> to <a href=\"https://wandb.ai/kvetab/March_finetune\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/kvetab/March_finetune/runs/g4l8kydo?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f51bf51bd10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=f\"March_finetune\", entity=\"kvetab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1743c445-302b-4f4e-9325-a6ef4fb17aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antibody_ID</th>\n",
       "      <th>heavy</th>\n",
       "      <th>light</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2073</th>\n",
       "      <td>6aod</td>\n",
       "      <td>EVQLVQSGAEVKKPGASVKVSCKASGYTFTGYYMHWVRQAPGQGLE...</td>\n",
       "      <td>DIVMTKSPSSLSASVGDRVTITCRASQGIRNDLGWYQQKPGKAPKR...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>4yny</td>\n",
       "      <td>EVQLVESGGGLVQPGRSLKLSCAASGFTFSNYGMAWVRQTPTKGLE...</td>\n",
       "      <td>EFVLTQPNSVSTNLGSTVKLSCKRSTGNIGSNYVNWYQQHEGRSPT...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>5xcv</td>\n",
       "      <td>EVQLVESGGGLVQPGRSLKLSCAASGFTFSNYGMAWVRQTPTKGLE...</td>\n",
       "      <td>QFVLTQPNSVSTNLGSTVKLSCKRSTGNIGSNYVNWYQQHEGRSPT...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2070</th>\n",
       "      <td>6and</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGYEFSRSWMNWVRQAPGKGLE...</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCRSSQSIVHSVGNTFLEWYQQKPG...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>2xqy</td>\n",
       "      <td>QVQLQQPGAELVKPGASVKMSCKASGYSFTSYWMNWVKQRPGRGLE...</td>\n",
       "      <td>DIVLTQSPASLALSLGQRATISCRASKSVSTSGYSYMYWYQQKPGQ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Antibody_ID                                              heavy  \\\n",
       "2073        6aod  EVQLVQSGAEVKKPGASVKVSCKASGYTFTGYYMHWVRQAPGQGLE...   \n",
       "1517        4yny  EVQLVESGGGLVQPGRSLKLSCAASGFTFSNYGMAWVRQTPTKGLE...   \n",
       "2025        5xcv  EVQLVESGGGLVQPGRSLKLSCAASGFTFSNYGMAWVRQTPTKGLE...   \n",
       "2070        6and  EVQLVESGGGLVQPGGSLRLSCAASGYEFSRSWMNWVRQAPGKGLE...   \n",
       "666         2xqy  QVQLQQPGAELVKPGASVKMSCKASGYSFTSYWMNWVKQRPGRGLE...   \n",
       "\n",
       "                                                  light  Y  \n",
       "2073  DIVMTKSPSSLSASVGDRVTITCRASQGIRNDLGWYQQKPGKAPKR...  0  \n",
       "1517  EFVLTQPNSVSTNLGSTVKLSCKRSTGNIGSNYVNWYQQHEGRSPT...  1  \n",
       "2025  QFVLTQPNSVSTNLGSTVKLSCKRSTGNIGSNYVNWYQQHEGRSPT...  1  \n",
       "2070  DIQMTQSPSSLSASVGDRVTITCRSSQSIVHSVGNTFLEWYQQKPG...  1  \n",
       "666   DIVLTQSPASLALSLGQRATISCRASKSVSTSGYSYMYWYQQKPGQ...  0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_train_data.csv\"), index_col=0)\n",
    "valid_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_valid_data.csv\"), index_col=0)\n",
    "test_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_test_data.csv\"), index_col=0)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c09b813-ffc8-48f7-99a5-586c76ac33f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"seq\"] = train_data[\"heavy\"] + train_data[\"light\"]\n",
    "valid_data[\"seq\"] = valid_data[\"heavy\"] + valid_data[\"light\"]\n",
    "test_data[\"seq\"] = test_data[\"heavy\"] + test_data[\"light\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fe63df3-96bf-40ed-9065-216a8e67b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(patience = 3, factor = 0.25, min_lr = 1e-07, verbose = 1),\n",
    "    keras.callbacks.EarlyStopping(patience = 6, restore_best_weights = True),\n",
    "    WandbCallback()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faeffd00-5c33-40ed-ad02-1c71cb739527",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 50\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd45319f-0dd8-489a-b105-7163d1679298",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config = {\n",
    "      \"learning_rate\": learning_rate,\n",
    "      \"epochs\": epoch_num * 2,\n",
    "      \"batch_size\": batch_size\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72bd9e53-8469-410c-9d42-6daa3811fc7e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_03_07-12:38:15] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_07-12:38:15] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_07-12:38:15] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-07 12:38:16.543610: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-07 12:38:17.659971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9656 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:41:00.0, compute capability: 7.5\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "2022-03-07 12:38:19.665660: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-07 12:38:28.305649: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 7605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 12s 327ms/step - loss: 0.7412 - val_loss: 0.5121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4909 - val_loss: 0.5763\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.5073 - val_loss: 0.4869\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4595 - val_loss: 0.4831\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 1s 113ms/step - loss: 0.4504 - val_loss: 0.4515\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4326 - val_loss: 0.4645\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4454 - val_loss: 0.4529\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4213 - val_loss: 0.4423\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4188 - val_loss: 0.4585\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4261 - val_loss: 0.4314\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.4088 - val_loss: 0.4268\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4094 - val_loss: 0.4289\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4032 - val_loss: 0.4308\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3802 - val_loss: 0.4154\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4017 - val_loss: 0.4205\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3875 - val_loss: 0.4044\n",
      "Epoch 17/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3809 - val_loss: 0.4275\n",
      "Epoch 18/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3835 - val_loss: 0.4081\n",
      "Epoch 19/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3918 - val_loss: 0.4321\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_03_07-12:38:54] Training the entire fine-tuned model...\n",
      "[2022_03_07-12:39:15] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/50\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3837WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1104s vs `on_train_batch_end` time: 0.1410s). Check your callbacks.\n",
      "11/11 [==============================] - 10s 426ms/step - loss: 0.3943 - val_loss: 0.4341\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 3s 252ms/step - loss: 0.4131 - val_loss: 0.4013\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3731 - val_loss: 0.3983\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3651 - val_loss: 0.4034\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 3s 252ms/step - loss: 0.3586 - val_loss: 0.3861\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3600 - val_loss: 0.3832\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3548 - val_loss: 0.3901\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3420 - val_loss: 0.3800\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3313 - val_loss: 0.3861\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3222 - val_loss: 0.3749\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3149 - val_loss: 0.4259\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3196 - val_loss: 0.3826\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.2905 - val_loss: 0.3761\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_03_07-12:40:00] Training on final epochs of sequence length 1024...\n",
      "[2022_03_07-12:40:00] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_03_07-12:40:02] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 1022.\n",
      " 6/21 [=======>......................] - ETA: 3s - loss: 0.3362WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1086s vs `on_train_batch_end` time: 0.1356s). Check your callbacks.\n",
      "21/21 [==============================] - 13s 371ms/step - loss: 0.3236 - val_loss: 0.3795\n"
     ]
    }
   ],
   "source": [
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data['seq'], train_data['Y'], valid_data['seq'], valid_data['Y'], \\\n",
    "        seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = learning_rate, begin_with_frozen_pretrained_layers = True, \\\n",
    "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = learning_rate / 10, callbacks = training_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f7e7d4f-4ca7-47fd-ad0f-cedd9de6c46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    }
   ],
   "source": [
    "results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "        start_seq_len = 512, start_batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d14dac3-1751-4172-96e6-7636bb9ad8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-set performance:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># records</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model seq len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>119</td>\n",
       "      <td>0.946784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>119</td>\n",
       "      <td>0.946784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               # records       AUC\n",
       "Model seq len                     \n",
       "512                  119  0.946784\n",
       "All                  119  0.946784"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  89   7\n",
       "1   8  15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Test-set performance:')\n",
    "display(results)\n",
    "\n",
    "print('Confusion matrix:')\n",
    "display(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "601cc41a-442a-4b5a-a651-a22f09e5ad2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c444b8d-daf7-4e24-a86c-c97f47b80d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "2022-03-07 12:45:22.012863: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_07__01.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_07__01.pkl/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "mod = model_generator.create_model(seq_len = 512)\n",
    "mod.save(path.join(DATA_DIR, \"protein_bert/2022_03_07__01.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce15989-ee5f-4ed4-b224-abc9c810f855",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a28a6b4-2cfe-4604-bde0-0b293d2378aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf. __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f85b7eba-b91b-4e09-b8c7-b1fbf7569ab6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_03_07-12:50:19] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_07-12:50:19] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_07-12:50:19] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "11/11 [==============================] - 10s 280ms/step - loss: 0.7594 - val_loss: 0.5533\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.5586 - val_loss: 0.5529\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.5223 - val_loss: 0.4892\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4937 - val_loss: 0.5385\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4691 - val_loss: 0.4616\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4576 - val_loss: 0.5050\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4565 - val_loss: 0.4543\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4226 - val_loss: 0.4396\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4127 - val_loss: 0.4365\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4285 - val_loss: 0.5902\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4698 - val_loss: 0.4295\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4224 - val_loss: 0.4316\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.4295 - val_loss: 0.4294\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4129 - val_loss: 0.4117\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3852 - val_loss: 0.4120\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3974 - val_loss: 0.4122\n",
      "Epoch 17/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3847 - val_loss: 0.4065\n",
      "Epoch 18/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3860 - val_loss: 0.4279\n",
      "Epoch 19/50\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.3889 - val_loss: 0.4500\n",
      "Epoch 20/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4077 - val_loss: 0.4027\n",
      "Epoch 21/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3842 - val_loss: 0.3978\n",
      "Epoch 22/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3741 - val_loss: 0.3973\n",
      "Epoch 23/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3829 - val_loss: 0.4179\n",
      "Epoch 24/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3685 - val_loss: 0.4051\n",
      "Epoch 25/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3869 - val_loss: 0.4138\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 26/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3766 - val_loss: 0.3943\n",
      "Epoch 27/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3692 - val_loss: 0.3961\n",
      "Epoch 28/50\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3790 - val_loss: 0.4137\n",
      "Epoch 29/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3750 - val_loss: 0.3991\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 30/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3683 - val_loss: 0.3963\n",
      "Epoch 31/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3710 - val_loss: 0.3952\n",
      "Epoch 32/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3564 - val_loss: 0.3968\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_03_07-12:51:09] Training the entire fine-tuned model...\n",
      "[2022_03_07-12:51:58] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/50\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3927WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1122s vs `on_train_batch_end` time: 0.1443s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1122s vs `on_train_batch_end` time: 0.1443s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 424ms/step - loss: 0.3950 - val_loss: 0.3896\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 3s 251ms/step - loss: 0.3596 - val_loss: 0.3884\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 3s 252ms/step - loss: 0.3535 - val_loss: 0.4016\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3590 - val_loss: 0.4105\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3456 - val_loss: 0.3785\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3587 - val_loss: 0.3791\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3341 - val_loss: 0.3717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3297 - val_loss: 0.3746\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3338 - val_loss: 0.3943\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3164 - val_loss: 0.3641\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.2924 - val_loss: 0.3959\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.2997 - val_loss: 0.3778\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.2747 - val_loss: 0.3684\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.2774 - val_loss: 0.3766\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.2629 - val_loss: 0.3693\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.2639 - val_loss: 0.3693\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_03_07-12:52:51] Training on final epochs of sequence length 1024...\n",
      "[2022_03_07-12:52:51] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_03_07-12:52:51] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 1022.\n",
      " 6/21 [=======>......................] - ETA: 3s - loss: 0.3263WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1071s vs `on_train_batch_end` time: 0.1419s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1071s vs `on_train_batch_end` time: 0.1419s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 13s 345ms/step - loss: 0.3221 - val_loss: 0.3717\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 50\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4\n",
    "wandb.config = {\n",
    "      \"learning_rate\": learning_rate,\n",
    "      \"epochs\": epoch_num * 2,\n",
    "      \"batch_size\": batch_size\n",
    "    }\n",
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data['seq'], train_data['Y'], valid_data['seq'], valid_data['Y'], \\\n",
    "        seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = learning_rate, begin_with_frozen_pretrained_layers = True, \\\n",
    "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = learning_rate / 10, callbacks = training_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc6f941c-f6c4-4ec0-9ba9-bdbbb942963d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  85  11\n",
       "1   5  18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6923076923076923"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "        start_seq_len = 512, start_batch_size = 32)\n",
    "print('Confusion matrix:')\n",
    "display(confusion_matrix)\n",
    "fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b52a48a5-ff77-497f-8b0f-798b8351a213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_07__02.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_07__02.pkl/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "mod = model_generator.create_model(seq_len = 512)\n",
    "mod.save(path.join(DATA_DIR, \"protein_bert/2022_03_07__02.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c95384a-4309-40d6-9284-8a350af85835",
   "metadata": {},
   "source": [
    "# 5, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "827fdb5f-9d8d-4d9d-be00-15e1a62901f0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ho0tbk51) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 43170... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▃▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▅▄▄▄▃▅▃▄▃▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▃▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>17</td></tr><tr><td>best_val_loss</td><td>0.36364</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31084</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.36835</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">feasible-silence-4</strong>: <a href=\"https://wandb.ai/kvetab/March_finetune/runs/ho0tbk51\" target=\"_blank\">https://wandb.ai/kvetab/March_finetune/runs/ho0tbk51</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220307_131420-ho0tbk51/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ho0tbk51). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/March_finetune/runs/wll3fv47\" target=\"_blank\">solar-microwave-5</a></strong> to <a href=\"https://wandb.ai/kvetab/March_finetune\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/kvetab/March_finetune/runs/wll3fv47?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f49601a02d0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=f\"March_finetune\", entity=\"kvetab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfc5ccdf-5ad1-46f7-aadb-49c84deb5214",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e05c685d-665b-433b-87ed-22199c891e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(patience = 4, factor = 0.25, min_lr = 1e-07, verbose = 1),\n",
    "    keras.callbacks.EarlyStopping(patience = 8, restore_best_weights = True),\n",
    "    WandbCallback()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "531b454a-3ba9-4d8f-bcfd-7bc2aa9df881",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_03_30-14:56:34] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_30-14:56:34] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_30-14:56:34] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 14:56:37.927515: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-30 14:56:41.171695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9656 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:21:00.0, compute capability: 7.5\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "2022-03-30 14:56:43.472165: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 14:56:54.924980: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 7605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 15s 327ms/step - loss: 0.8103 - val_loss: 0.8175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.5943 - val_loss: 0.5337\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.5033 - val_loss: 0.4976\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.4616 - val_loss: 0.4737\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.4389 - val_loss: 0.4583\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4436 - val_loss: 0.4796\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4345 - val_loss: 0.4501\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4449 - val_loss: 0.4451\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.4531 - val_loss: 0.4949\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.4386 - val_loss: 0.4279\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.4162 - val_loss: 0.4250\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.4026 - val_loss: 0.4192\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3967 - val_loss: 0.4138\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3924 - val_loss: 0.4193\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4007 - val_loss: 0.4435\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3982 - val_loss: 0.4190\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.4016 - val_loss: 0.4053\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3952 - val_loss: 0.4058\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3872 - val_loss: 0.4287\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.4059 - val_loss: 0.4765\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4295 - val_loss: 0.4292\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3867 - val_loss: 0.4060\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3762 - val_loss: 0.4008\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3806 - val_loss: 0.4004\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3729 - val_loss: 0.3989\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.3772 - val_loss: 0.4001\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.3640 - val_loss: 0.3986\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.3655 - val_loss: 0.3993\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3646 - val_loss: 0.3993\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3621 - val_loss: 0.4049\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3661 - val_loss: 0.3986\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3610 - val_loss: 0.4002\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3630 - val_loss: 0.3982\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3691 - val_loss: 0.4011\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3715 - val_loss: 0.3993\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3706 - val_loss: 0.3986\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3672 - val_loss: 0.3980\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3675 - val_loss: 0.3983\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3657 - val_loss: 0.3994\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3607 - val_loss: 0.3974\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3583 - val_loss: 0.3990\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3700 - val_loss: 0.3974\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 1s 113ms/step - loss: 0.3671 - val_loss: 0.4000\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3593 - val_loss: 0.3975\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3717 - val_loss: 0.3988\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3667 - val_loss: 0.3989\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3616 - val_loss: 0.3984\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3629 - val_loss: 0.3978\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_03_30-14:57:56] Training the entire fine-tuned model...\n",
      "[2022_03_30-14:58:20] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3506WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1119s vs `on_train_batch_end` time: 0.1388s). Check your callbacks.\n",
      "11/11 [==============================] - 10s 421ms/step - loss: 0.3678 - val_loss: 0.3989\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3688 - val_loss: 0.4128\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3570 - val_loss: 0.3924\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3560 - val_loss: 0.3881\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3546 - val_loss: 0.3854\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3528 - val_loss: 0.3980\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3462 - val_loss: 0.4047\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3441 - val_loss: 0.3816\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3457 - val_loss: 0.3784\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3471 - val_loss: 0.3760\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3418 - val_loss: 0.3782\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3369 - val_loss: 0.3821\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3308 - val_loss: 0.3809\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3257 - val_loss: 0.3824\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3246 - val_loss: 0.3757\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3176 - val_loss: 0.3757\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3186 - val_loss: 0.3741\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3184 - val_loss: 0.3775\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3236 - val_loss: 0.3733\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3243 - val_loss: 0.3745\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3105 - val_loss: 0.3730\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3192 - val_loss: 0.3719\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3092 - val_loss: 0.3726\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3221 - val_loss: 0.3707\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3058 - val_loss: 0.3724\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3055 - val_loss: 0.3715\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 3s 261ms/step - loss: 0.3102 - val_loss: 0.3682\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3010 - val_loss: 0.3723\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3000 - val_loss: 0.3698\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3046 - val_loss: 0.3752\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3005 - val_loss: 0.3690\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3012 - val_loss: 0.3669\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 0.3051 - val_loss: 0.3713\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.2996 - val_loss: 0.3679\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3003 - val_loss: 0.3672\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3002 - val_loss: 0.3678\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3056 - val_loss: 0.3678\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.2903 - val_loss: 0.3685\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3019 - val_loss: 0.3681\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.2977 - val_loss: 0.3680\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "[2022_03_30-15:00:21] Training on final epochs of sequence length 512...\n",
      "[2022_03_30-15:00:21] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_30-15:00:48] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.2924WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1118s vs `on_train_batch_end` time: 0.1454s). Check your callbacks.\n",
      "11/11 [==============================] - 10s 475ms/step - loss: 0.2956 - val_loss: 0.3692\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 100\n",
    "batch_size = 128\n",
    "learning_rate = 5e-5\n",
    "wandb.config = {\n",
    "      \"learning_rate\": learning_rate,\n",
    "      \"epochs\": epoch_num * 2,\n",
    "      \"batch_size\": batch_size\n",
    "    }\n",
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data['seq'], train_data['Y'], valid_data['seq'], valid_data['Y'], \\\n",
    "        seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = learning_rate, begin_with_frozen_pretrained_layers = True, \\\n",
    "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 512, final_lr = learning_rate / 10, callbacks = training_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2df83ea-5adc-473c-ae52-3042fd95d68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  94   2\n",
       "1  11  12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6486486486486487"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "        start_seq_len = 512, start_batch_size = 32)\n",
    "print('Confusion matrix:')\n",
    "display(confusion_matrix)\n",
    "fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5eed3d57-a38c-4428-98bf-2cdfb31a3fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "2022-03-30 15:01:21.837698: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_30__05.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_30__05.pkl/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "mod = model_generator.create_model(seq_len = 512)\n",
    "mod.save(path.join(DATA_DIR, \"protein_bert/2022_03_30__05.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920495fb-16a9-48e2-afc9-37da80b5593c",
   "metadata": {},
   "source": [
    "# 7, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9196c285-2b22-4c13-b6a6-8378b10e2785",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:llc049yo) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 44058... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▄▃▃▄▄▃▃▃▃▃▃▃▂▂▂▂▃▂▂▂▂▂▂▅▃▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇▅▅▆█▄▄▃▄▆▃▃▂▃▃▂▂▂▂▂▂▂▂▂▆▄▂▂▁▁▁▁▄▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.37816</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34296</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.37942</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">major-sponge-6</strong>: <a href=\"https://wandb.ai/kvetab/March_finetune/runs/llc049yo\" target=\"_blank\">https://wandb.ai/kvetab/March_finetune/runs/llc049yo</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220307_132508-llc049yo/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:llc049yo). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/March_finetune/runs/rji5d1uv\" target=\"_blank\">noble-blaze-7</a></strong> to <a href=\"https://wandb.ai/kvetab/March_finetune\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/kvetab/March_finetune/runs/rji5d1uv?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f4980bbce10>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=f\"March_finetune\", entity=\"kvetab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4ae0008a-f230-49d3-8aa5-aa1b73a10e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b6f98896-ba67-4b9a-93a9-bc48c73334c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(patience = 3, factor = 0.25, min_lr = 1e-07, verbose = 1),\n",
    "    keras.callbacks.EarlyStopping(patience = 6, restore_best_weights = True),\n",
    "    WandbCallback()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b82a7194-d5e4-421a-b8a2-15f3d5cc53ad",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_03_07-13:31:05] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_07-13:31:05] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_07-13:31:06] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 9s 285ms/step - loss: 0.7924 - val_loss: 0.7399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.5823 - val_loss: 0.5075\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.4952 - val_loss: 0.4969\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4732 - val_loss: 0.5046\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4584 - val_loss: 0.4563\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4429 - val_loss: 0.4501\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4343 - val_loss: 0.4438\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4186 - val_loss: 0.4389\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4136 - val_loss: 0.4374\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4113 - val_loss: 0.4322\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4250 - val_loss: 0.4265\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4376 - val_loss: 0.4377\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4126 - val_loss: 0.4315\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3882 - val_loss: 0.4181\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3927 - val_loss: 0.4129\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3940 - val_loss: 0.4181\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4014 - val_loss: 0.4724\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3949 - val_loss: 0.4450\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3950 - val_loss: 0.4074\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3837 - val_loss: 0.4036\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3735 - val_loss: 0.4018\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3791 - val_loss: 0.4028\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3752 - val_loss: 0.4021\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3771 - val_loss: 0.4018\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3672 - val_loss: 0.4012\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3687 - val_loss: 0.4014\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3702 - val_loss: 0.4016\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3780 - val_loss: 0.4020\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3632 - val_loss: 0.4015\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3741 - val_loss: 0.4009\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3789 - val_loss: 0.4012\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3679 - val_loss: 0.4025\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3786 - val_loss: 0.4018\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3711 - val_loss: 0.4018\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3694 - val_loss: 0.4016\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3732 - val_loss: 0.4015\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "[2022_03_07-13:32:00] Training the entire fine-tuned model...\n",
      "[2022_03_07-13:32:09] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3840WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1115s vs `on_train_batch_end` time: 0.1456s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1115s vs `on_train_batch_end` time: 0.1456s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 419ms/step - loss: 0.3756 - val_loss: 0.3989\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3735 - val_loss: 0.3986\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3734 - val_loss: 0.3865\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3646 - val_loss: 0.3801\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3616 - val_loss: 0.3768\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3611 - val_loss: 0.3848\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3646 - val_loss: 0.4085\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3483 - val_loss: 0.3732\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3511 - val_loss: 0.3730\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3188 - val_loss: 0.3716\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3214 - val_loss: 0.3730\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3113 - val_loss: 0.3907\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3045 - val_loss: 0.3656\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3090 - val_loss: 0.3662\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.2913 - val_loss: 0.3847\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.2888 - val_loss: 0.3798\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.2692 - val_loss: 0.3748\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 0.2657 - val_loss: 0.3855\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.2579 - val_loss: 0.3799\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_03_07-13:33:11] Training on final epochs of sequence length 1024...\n",
      "[2022_03_07-13:33:11] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_03_07-13:33:37] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 1022.\n",
      " 6/21 [=======>......................] - ETA: 3s - loss: 0.3442WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1085s vs `on_train_batch_end` time: 0.1420s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1085s vs `on_train_batch_end` time: 0.1420s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 13s 344ms/step - loss: 0.3268 - val_loss: 0.3726\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 100\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4\n",
    "wandb.config = {\n",
    "      \"learning_rate\": learning_rate,\n",
    "      \"epochs\": epoch_num * 2,\n",
    "      \"batch_size\": batch_size\n",
    "    }\n",
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data['seq'], train_data['Y'], valid_data['seq'], valid_data['Y'], \\\n",
    "        seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = learning_rate, begin_with_frozen_pretrained_layers = True, \\\n",
    "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = learning_rate / 10, callbacks = training_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "06871eab-05de-471e-9dcb-d6a181e0f0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  88   8\n",
       "1   8  15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6521739130434783"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "        start_seq_len = 512, start_batch_size = 32)\n",
    "print('Confusion matrix:')\n",
    "display(confusion_matrix)\n",
    "fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eb309fad-1915-4445-bac2-339f564d415f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_07__08.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_07__08.pkl/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "mod = model_generator.create_model(seq_len = 512)\n",
    "mod.save(path.join(DATA_DIR, \"protein_bert/2022_03_07__08.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66d6a11-28fb-4e69-8009-0f4319468101",
   "metadata": {},
   "source": [
    "# Oversampling\n",
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12ef5f5c-f067-4216-a133-d47b2ccdcd42",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:30hgao08) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 41955... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▁▂▂▃▃▄▁▁▂▂▃▃▄▄▅▅▆▆▇▇██▁▂▂▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▃▃▃▃▃▃▂▂▂▂▁█▅▄▄▃▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂</td></tr><tr><td>lr</td><td>██████████▁▁▁▁▁▁████████████▃▃▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▆▆▄▄▄▃▃▃▃▄▂▂▂▂▃▁█▆▅▄▄▃▃▃▃▄▂▃▂▃▂▂▂▃▂▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>9</td></tr><tr><td>best_val_loss</td><td>0.36406</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32212</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.37174</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dry-pine-1</strong>: <a href=\"https://wandb.ai/kvetab/March_finetune/runs/30hgao08\" target=\"_blank\">https://wandb.ai/kvetab/March_finetune/runs/30hgao08</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220307_123717-30hgao08/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:30hgao08). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/March_finetune/runs/3gwj2i9r\" target=\"_blank\">fallen-bird-2</a></strong> to <a href=\"https://wandb.ai/kvetab/March_finetune\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/kvetab/March_finetune/runs/3gwj2i9r?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f49a3880750>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=f\"March_finetune\", entity=\"kvetab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "323dcf35-93be-40e1-9673-f16a79787ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = RandomOverSampler(random_state=42)\n",
    "x_train, y_train = sampler.fit_resample(train_data.drop(\"Y\", axis=1), train_data['Y'])\n",
    "x_valid, y_valid = sampler.fit_resample(valid_data.drop(\"Y\", axis=1), valid_data['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be33458b-17ed-42e1-ae31-c6ec8fabf23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5149694a-1a1c-4e39-8034-ed1946e1386c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_03_07-12:59:26] Training set: Filtered out 0 of 2114 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_07-12:59:26] Validation set: Filtered out 0 of 188 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_07-12:59:26] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 11s 228ms/step - loss: 0.8073 - val_loss: 0.7109\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.6141 - val_loss: 0.6363\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.5803 - val_loss: 0.5773\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.5528 - val_loss: 0.5728\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.5507 - val_loss: 0.5749\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.5257 - val_loss: 0.5750\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.5299 - val_loss: 0.5670\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.5130 - val_loss: 0.5943\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.5069 - val_loss: 0.5351\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4862 - val_loss: 0.5462\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4800 - val_loss: 0.6190\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4807 - val_loss: 0.5217\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.4766 - val_loss: 0.5228\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4650 - val_loss: 0.5245\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4603 - val_loss: 0.5321\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4503 - val_loss: 0.5169\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4509 - val_loss: 0.5160\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4433 - val_loss: 0.5403\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4494 - val_loss: 0.5254\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4418 - val_loss: 0.5152\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4517 - val_loss: 0.5163\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4483 - val_loss: 0.5264\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4437 - val_loss: 0.5156\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4452 - val_loss: 0.5287\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.4393 - val_loss: 0.5158\n",
      "Epoch 26/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4417 - val_loss: 0.5171\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_03_07-13:00:26] Training the entire fine-tuned model...\n",
      "[2022_03_07-13:00:35] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/17 [=========>....................] - ETA: 2s - loss: 0.4709WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1105s vs `on_train_batch_end` time: 0.1444s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1105s vs `on_train_batch_end` time: 0.1444s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 12s 367ms/step - loss: 0.4657 - val_loss: 0.5102\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 4s 258ms/step - loss: 0.4443 - val_loss: 0.5507\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 4s 259ms/step - loss: 0.4225 - val_loss: 0.4919\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 4s 259ms/step - loss: 0.4076 - val_loss: 0.5101\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 4s 259ms/step - loss: 0.4103 - val_loss: 0.5304\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 4s 259ms/step - loss: 0.3643 - val_loss: 0.4958\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 4s 259ms/step - loss: 0.3532 - val_loss: 0.4965\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 4s 258ms/step - loss: 0.3483 - val_loss: 0.4983\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 4s 259ms/step - loss: 0.3399 - val_loss: 0.5005\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_03_07-13:01:23] Training on final epochs of sequence length 1024...\n",
      "[2022_03_07-13:01:23] Training set: Filtered out 0 of 2114 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_03_07-13:02:03] Validation set: Filtered out 0 of 188 (0.0%) records of lengths exceeding 1022.\n",
      " 6/34 [====>.........................] - ETA: 6s - loss: 0.4210WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1075s vs `on_train_batch_end` time: 0.1397s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1075s vs `on_train_batch_end` time: 0.1397s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 16s 301ms/step - loss: 0.4216 - val_loss: 0.5015\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 100\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4\n",
    "wandb.config = {\n",
    "      \"learning_rate\": learning_rate,\n",
    "      \"epochs\": epoch_num * 2,\n",
    "      \"batch_size\": batch_size\n",
    "    }\n",
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, x_train[\"seq\"], y_train, x_valid['seq'], y_valid, \\\n",
    "        seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = learning_rate, begin_with_frozen_pretrained_layers = True, \\\n",
    "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = learning_rate / 10, callbacks = training_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b1c5cab-20f9-4159-81a5-e045f973a502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  75  21\n",
       "1   1  22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "        start_seq_len = 512, start_batch_size = 32)\n",
    "print('Confusion matrix:')\n",
    "display(confusion_matrix)\n",
    "fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56030912-58b9-4285-a7a3-aa30be61ca2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_07__03.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_07__03.pkl/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "mod = model_generator.create_model(seq_len = 512)\n",
    "mod.save(path.join(DATA_DIR, \"protein_bert/2022_03_07__03.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fd9619-1fb9-4771-92a7-442833414c94",
   "metadata": {},
   "source": [
    "# 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cf4d552b-1a11-4ad9-99a0-0fc989333b15",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:rji5d1uv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 44342... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▄▄▄▄▁</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▃▃▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>12</td></tr><tr><td>best_val_loss</td><td>0.36563</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32678</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.37264</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">noble-blaze-7</strong>: <a href=\"https://wandb.ai/kvetab/March_finetune/runs/rji5d1uv\" target=\"_blank\">https://wandb.ai/kvetab/March_finetune/runs/rji5d1uv</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220307_132949-rji5d1uv/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:rji5d1uv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/March_finetune/runs/25jtz98x\" target=\"_blank\">dutiful-aardvark-8</a></strong> to <a href=\"https://wandb.ai/kvetab/March_finetune\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/kvetab/March_finetune/runs/25jtz98x?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f48f4719450>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=f\"March_finetune\", entity=\"kvetab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5c5d485e-ea47-4836-b6a7-58822f0764be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "60b18dee-772d-4ec0-8836-0b11623b3c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(patience = 3, factor = 0.25, min_lr = 1e-07, verbose = 1),\n",
    "    keras.callbacks.EarlyStopping(patience = 6, restore_best_weights = True),\n",
    "    WandbCallback()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "461da1ef-9e3a-4120-bf72-e96c69bb5970",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_03_07-13:37:58] Training set: Filtered out 0 of 2114 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_07-13:37:58] Validation set: Filtered out 0 of 188 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_07-13:37:58] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 11s 223ms/step - loss: 0.7751 - val_loss: 0.6406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.5977 - val_loss: 0.6238\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.5741 - val_loss: 0.5723\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.5726 - val_loss: 0.6539\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.5250 - val_loss: 0.5448\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.5171 - val_loss: 0.5468\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4923 - val_loss: 0.5443\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4926 - val_loss: 0.5471\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4977 - val_loss: 0.5363\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4853 - val_loss: 0.5426\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4822 - val_loss: 0.5434\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4633 - val_loss: 0.5383\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4610 - val_loss: 0.5227\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4558 - val_loss: 0.5229\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4501 - val_loss: 0.5229\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4683 - val_loss: 0.5177\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4600 - val_loss: 0.5240\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4750 - val_loss: 0.5154\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.4505 - val_loss: 0.5179\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4503 - val_loss: 0.5134\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4580 - val_loss: 0.5170\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4523 - val_loss: 0.5251\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4525 - val_loss: 0.5141\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4476 - val_loss: 0.5213\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4457 - val_loss: 0.5161\n",
      "Epoch 26/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4447 - val_loss: 0.5147\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_03_07-13:38:59] Training the entire fine-tuned model...\n",
      "[2022_03_07-13:39:37] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/17 [=========>....................] - ETA: 2s - loss: 0.4542WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1113s vs `on_train_batch_end` time: 0.1447s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1113s vs `on_train_batch_end` time: 0.1447s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 12s 370ms/step - loss: 0.4499 - val_loss: 0.5293\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 4s 258ms/step - loss: 0.4416 - val_loss: 0.5522\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 4s 259ms/step - loss: 0.4483 - val_loss: 0.6439\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 4s 258ms/step - loss: 0.4372 - val_loss: 0.4974\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 4s 258ms/step - loss: 0.4028 - val_loss: 0.4906\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 4s 259ms/step - loss: 0.3774 - val_loss: 0.4947\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 4s 259ms/step - loss: 0.3641 - val_loss: 0.4910\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 4s 259ms/step - loss: 0.3407 - val_loss: 0.4917\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 4s 259ms/step - loss: 0.3216 - val_loss: 0.5227\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 4s 259ms/step - loss: 0.3201 - val_loss: 0.5121\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 4s 259ms/step - loss: 0.3066 - val_loss: 0.5313\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_03_07-13:40:34] Training on final epochs of sequence length 1024...\n",
      "[2022_03_07-13:40:34] Training set: Filtered out 0 of 2114 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_03_07-13:40:34] Validation set: Filtered out 0 of 188 (0.0%) records of lengths exceeding 1022.\n",
      " 6/34 [====>.........................] - ETA: 6s - loss: 0.4192WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1085s vs `on_train_batch_end` time: 0.1423s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1085s vs `on_train_batch_end` time: 0.1423s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 16s 318ms/step - loss: 0.4017 - val_loss: 0.5252\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 100\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4\n",
    "wandb.config = {\n",
    "      \"learning_rate\": learning_rate,\n",
    "      \"epochs\": epoch_num * 2,\n",
    "      \"batch_size\": batch_size\n",
    "    }\n",
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, x_train[\"seq\"], y_train, x_valid['seq'], y_valid, \\\n",
    "        seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = learning_rate, begin_with_frozen_pretrained_layers = True, \\\n",
    "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = learning_rate / 10, callbacks = training_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a21ec67c-10c2-4574-88c9-5550808e5ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  78  18\n",
       "1   1  22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6984126984126984"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "        start_seq_len = 512, start_batch_size = 32)\n",
    "print('Confusion matrix:')\n",
    "display(confusion_matrix)\n",
    "fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "648f0f97-bb1c-4ad8-b53a-2a5bfba2026f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_07__09.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_07__09.pkl/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "mod = model_generator.create_model(seq_len = 512)\n",
    "mod.save(path.join(DATA_DIR, \"protein_bert/2022_03_07__09.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c68ec8-a7e1-490a-a459-09a47fd872e5",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "acccd8fa-3c79-4d97-89cf-f24b8562b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = RandomUnderSampler(random_state=42)\n",
    "x_train_u, y_train_u = sampler.fit_resample(train_data.drop(\"Y\", axis=1), train_data['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52610218-cc4f-4fa2-af39-ed570e8b551b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3gwj2i9r) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 42629... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▃▃▃▂▂▂▁▁▁▁▂</td></tr><tr><td>lr</td><td>███████████████▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▄▄▄▄▃▄▂▃▅▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▃▁▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.4216</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.50154</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fallen-bird-2</strong>: <a href=\"https://wandb.ai/kvetab/March_finetune/runs/3gwj2i9r\" target=\"_blank\">https://wandb.ai/kvetab/March_finetune/runs/3gwj2i9r</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220307_125721-3gwj2i9r/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3gwj2i9r). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/March_finetune/runs/weln1d3d\" target=\"_blank\">avid-deluge-3</a></strong> to <a href=\"https://wandb.ai/kvetab/March_finetune\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/kvetab/March_finetune/runs/weln1d3d?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f4980277d90>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)\n",
    "\n",
    "wandb.init(project=f\"March_finetune\", entity=\"kvetab\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "efa8989a-ea12-42fa-9877-fbffc5ecb069",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_03_07-13:07:54] Training set: Filtered out 0 of 562 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_07-13:07:54] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_07-13:07:54] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 543ms/step - loss: 1.2386 - val_loss: 1.6062\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 123ms/step - loss: 1.0543 - val_loss: 0.5909\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 120ms/step - loss: 0.8385 - val_loss: 0.7112\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 118ms/step - loss: 0.8166 - val_loss: 0.7024\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 120ms/step - loss: 0.7512 - val_loss: 0.5992\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 120ms/step - loss: 0.6350 - val_loss: 0.5411\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 118ms/step - loss: 0.6238 - val_loss: 0.7253\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 118ms/step - loss: 0.6080 - val_loss: 0.7608\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 119ms/step - loss: 0.5872 - val_loss: 0.6103\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 119ms/step - loss: 0.5958 - val_loss: 0.6030\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 119ms/step - loss: 0.5955 - val_loss: 0.6402\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 131ms/step - loss: 0.5893 - val_loss: 0.6657\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_03_07-13:08:12] Training the entire fine-tuned model...\n",
      "[2022_03_07-13:08:21] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 672ms/step - loss: 0.6632 - val_loss: 0.5136\n",
      "Epoch 2/100\n",
      "2/5 [===========>..................] - ETA: 0s - loss: 0.6432WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1107s vs `on_train_batch_end` time: 0.1291s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1107s vs `on_train_batch_end` time: 0.1291s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 248ms/step - loss: 0.6278 - val_loss: 0.7628\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 247ms/step - loss: 0.6260 - val_loss: 0.5725\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 245ms/step - loss: 0.5982 - val_loss: 0.6099\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 245ms/step - loss: 0.5832 - val_loss: 0.6256\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 245ms/step - loss: 0.5755 - val_loss: 0.6147\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 245ms/step - loss: 0.5628 - val_loss: 0.6059\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_03_07-13:08:38] Training on final epochs of sequence length 1024...\n",
      "[2022_03_07-13:08:38] Training set: Filtered out 0 of 562 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_03_07-13:08:38] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 1022.\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 0.6559WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1074s vs `on_train_batch_end` time: 0.1399s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1074s vs `on_train_batch_end` time: 0.1399s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 10s 485ms/step - loss: 0.6393 - val_loss: 0.6304\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 100\n",
    "batch_size = 128\n",
    "learning_rate = 5e-4\n",
    "wandb.config = {\n",
    "      \"learning_rate\": learning_rate,\n",
    "      \"epochs\": epoch_num * 2,\n",
    "      \"batch_size\": batch_size\n",
    "    }\n",
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, x_train_u[\"seq\"], y_train_u, valid_data['seq'], valid_data['Y'], \\\n",
    "        seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = learning_rate, begin_with_frozen_pretrained_layers = True, \\\n",
    "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = learning_rate / 10, callbacks = training_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3607bfa-0aee-4272-86bc-5d82dd6373f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  71  25\n",
       "1   5  18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5454545454545454"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "        start_seq_len = 512, start_batch_size = 32)\n",
    "print('Confusion matrix:')\n",
    "display(confusion_matrix)\n",
    "fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fa07d3ca-f793-4a7d-b2e8-3febe8ea0299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_07__04.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_07__04.pkl/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "# protein_bert/2022_03_07__04.pkl\n",
    "mod = model_generator.create_model(seq_len = 512)\n",
    "mod.save(path.join(DATA_DIR, \"protein_bert/2022_03_07__04.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec00e839-cf6f-400c-9cba-3fc6e08f52b4",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56644fd0-ccff-434a-87a3-6a053d7f1416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e816cc74-29d5-4c85-9fcc-1db0f5042900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_03_30-15:06:19] Test set: Filtered out 0 of 119 (0.0%) records of lengths exceeding 510.\n"
     ]
    }
   ],
   "source": [
    "seq_len = 512\n",
    "encoded_test_set = encode_dataset(test_data[\"seq\"], test_data[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'Test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ebf1fb3-80bd-423c-a6af-2c51d82675ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X, test_Y, test_sample_weigths = encoded_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49e9d36a-63d9-45b6-bcf8-b66e75fd506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = path.join(DATA_DIR, \"protein_bert/2022_03_30__05.pkl\")\n",
    "model = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1959c14b-c779-4741-9a0a-b2a453fd1fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6486486486486486"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(test_X, batch_size=32)\n",
    "y_pred_classes = (y_pred >= 0.5)\n",
    "f1_score(test_Y, y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76d7099d-ada5-4464-8337-e64f1eaa7fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antibody_ID</th>\n",
       "      <th>heavy</th>\n",
       "      <th>light</th>\n",
       "      <th>CDR_length</th>\n",
       "      <th>PSH</th>\n",
       "      <th>PPC</th>\n",
       "      <th>PNC</th>\n",
       "      <th>SFvCSP</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abagovomab</td>\n",
       "      <td>QVKLQESGAELARPGASVKLSCKASGYTFTNYWMQWVKQRPGQGLD...</td>\n",
       "      <td>DIELTQSPASLSASVGETVTITCQASENIYSYLAWHQQKQGKSPQL...</td>\n",
       "      <td>46</td>\n",
       "      <td>129.7603</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abituzumab</td>\n",
       "      <td>QVQLQQSGGELAKPGASVKVSCKASGYTFSSFWMHWVRQAPGQGLE...</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCRASQDISNYLAWYQQKPGKAPKL...</td>\n",
       "      <td>45</td>\n",
       "      <td>115.9106</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0421</td>\n",
       "      <td>-3.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abrilumab</td>\n",
       "      <td>QVQLVQSGAEVKKPGASVKVSCKVSGYTLSDLSIHWVRQAPGKGLE...</td>\n",
       "      <td>DIQMTQSPSSVSASVGDRVTITCRASQGISSWLAWYQQKPGKAPKL...</td>\n",
       "      <td>45</td>\n",
       "      <td>109.6995</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8965</td>\n",
       "      <td>-4.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Actoxumab</td>\n",
       "      <td>QVQLVESGGGVVQPGRSLRLSCAASGFSFSNYGMHWVRQAPGKGLE...</td>\n",
       "      <td>DIQMTQSPSSVSASVGDRVTITCRASQGISSWLAWYQHKPGKAPKL...</td>\n",
       "      <td>49</td>\n",
       "      <td>112.6290</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.1247</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adalimumab</td>\n",
       "      <td>EVQLVESGGGLVQPGRSLRLSCAASGFTFDDYAMHWVRQAPGKGLE...</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCRASQGIRNYLAWYQQKPGKAPKL...</td>\n",
       "      <td>48</td>\n",
       "      <td>111.2512</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>1.1364</td>\n",
       "      <td>-19.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Antibody_ID                                              heavy  \\\n",
       "0  Abagovomab  QVKLQESGAELARPGASVKLSCKASGYTFTNYWMQWVKQRPGQGLD...   \n",
       "1  Abituzumab  QVQLQQSGGELAKPGASVKVSCKASGYTFSSFWMHWVRQAPGQGLE...   \n",
       "2   Abrilumab  QVQLVQSGAEVKKPGASVKVSCKVSGYTLSDLSIHWVRQAPGKGLE...   \n",
       "3   Actoxumab  QVQLVESGGGVVQPGRSLRLSCAASGFSFSNYGMHWVRQAPGKGLE...   \n",
       "4  Adalimumab  EVQLVESGGGLVQPGRSLRLSCAASGFTFDDYAMHWVRQAPGKGLE...   \n",
       "\n",
       "                                               light  CDR_length       PSH  \\\n",
       "0  DIELTQSPASLSASVGETVTITCQASENIYSYLAWHQQKQGKSPQL...          46  129.7603   \n",
       "1  DIQMTQSPSSLSASVGDRVTITCRASQDISNYLAWYQQKPGKAPKL...          45  115.9106   \n",
       "2  DIQMTQSPSSVSASVGDRVTITCRASQGISSWLAWYQQKPGKAPKL...          45  109.6995   \n",
       "3  DIQMTQSPSSVSASVGDRVTITCRASQGISSWLAWYQHKPGKAPKL...          49  112.6290   \n",
       "4  DIQMTQSPSSLSASVGDRVTITCRASQGIRNYLAWYQQKPGKAPKL...          48  111.2512   \n",
       "\n",
       "      PPC     PNC  SFvCSP  Y  \n",
       "0  0.0000  0.0000   16.32  1  \n",
       "1  0.0954  0.0421   -3.10  1  \n",
       "2  0.0000  0.8965   -4.00  1  \n",
       "3  0.0000  1.1247    3.10  1  \n",
       "4  0.0485  1.1364  -19.50  1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tap_data = pd.read_csv(path.join(DATA_DIR, \"tap/TAP_data.csv\"))\n",
    "tap_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cc5b7cb-1cc4-450d-a52d-2a027f00a1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tap_data[\"seq\"] = tap_data[\"heavy\"] +  tap_data[\"light\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b06ed7a5-5e02-48e8-8e69-6355c776ef1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_03_30-15:06:39] TAP set: Filtered out 0 of 241 (0.0%) records of lengths exceeding 510.\n"
     ]
    }
   ],
   "source": [
    "encoded_tap_set = encode_dataset(tap_data[\"seq\"], tap_data[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'TAP set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e96bc721-e479-400a-8d09-e0b6c9635a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tap_X, tap_Y, tap_sample_weigths = encoded_tap_settap_X, tap_Y, tap_sample_weigths = encoded_tap_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c95a893-f467-405f-81b6-59b0277c0a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34285714285714286"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(tap_X, batch_size=32)\n",
    "y_pred_classes = (y_pred >= 0.5)\n",
    "f1_score(tap_Y, y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "345121cc-6ccb-46c2-aef0-5b173263f069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23651452282157676"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(tap_Y, y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "566d023a-b6e2-48e9-b884-c48037cc54bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_03_10-10:20:09] Test set: Filtered out 0 of 119 (0.0%) records of lengths exceeding 510.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "seq_len = 512\n",
    "encoded_test_set = encode_dataset(test_data[\"seq\"], test_data[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'Test set')\n",
    "test_X, test_Y, test_sample_weigths = encoded_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb44bed8-ab18-4ae3-b16e-8c90e38c027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_name):\n",
    "    model_path = path.join(DATA_DIR, f\"protein_bert/{model_name}\")\n",
    "    model = keras.models.load_model(model_path)\n",
    "    y_pred = model.predict(test_X, batch_size=32)\n",
    "    y_pred_classes = (y_pred >= 0.5)\n",
    "    f1 = f1_score(test_Y, y_pred_classes)\n",
    "    print(f\"Model {model_name}\")\n",
    "    print(f\"Test F1: {f1}\")\n",
    "    \n",
    "    y_pred = model.predict(tap_X, batch_size=32)\n",
    "    y_pred_classes = (y_pred >= 0.5)\n",
    "    f1 = f1_score(tap_Y, y_pred_classes)\n",
    "    acc = accuracy_score(tap_Y, y_pred_classes)\n",
    "    print(f\"TAP F1: {f1}\")\n",
    "    print(f\"TAP Acc: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9f0a4cc-7b7a-4fcd-acb3-cc5f097855e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2022_03_10__09_78.pkl\n",
      "Test F1: 0.6666666666666666\n",
      "TAP F1: 0.7774936061381073\n",
      "TAP Acc: 0.6390041493775933\n"
     ]
    }
   ],
   "source": [
    "test_model(f\"2022_03_10__09_78.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8383aa7a-717f-49f2-ace0-4d5ecf4c7eb3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2022_03_10__09_02.pkl\n",
      "Test F1: 0.7333333333333334\n",
      "TAP F1: 0.7277628032345014\n",
      "TAP Acc: 0.5809128630705395\n",
      "\n",
      "\n",
      "Model 2022_03_10__09_13.pkl\n",
      "Test F1: 0.7301587301587301\n",
      "TAP F1: 0.7374005305039788\n",
      "TAP Acc: 0.5892116182572614\n",
      "\n",
      "\n",
      "Model 2022_03_10__09_19.pkl\n",
      "Test F1: 0.7301587301587301\n",
      "TAP F1: 0.7413333333333334\n",
      "TAP Acc: 0.5975103734439834\n",
      "\n",
      "\n",
      "Model 2022_03_10__09_27.pkl\n",
      "Test F1: 0.7457627118644068\n",
      "TAP F1: 0.7119565217391305\n",
      "TAP Acc: 0.5601659751037344\n",
      "\n",
      "\n",
      "Model 2022_03_10__09_38.pkl\n",
      "Test F1: 0.6571428571428571\n",
      "TAP F1: 0.7552083333333333\n",
      "TAP Acc: 0.6099585062240664\n",
      "\n",
      "\n",
      "Model 2022_03_10__09_42.pkl\n",
      "Test F1: 0.6999999999999998\n",
      "TAP F1: 0.688888888888889\n",
      "TAP Acc: 0.5352697095435685\n",
      "\n",
      "\n",
      "Model 2022_03_10__09_56.pkl\n",
      "Test F1: 0.6666666666666666\n",
      "TAP F1: 0.805\n",
      "TAP Acc: 0.6763485477178424\n",
      "\n",
      "\n",
      "Model 2022_03_10__09_63.pkl\n",
      "Test F1: 0.7096774193548387\n",
      "TAP F1: 0.7119565217391305\n",
      "TAP Acc: 0.5601659751037344\n",
      "\n",
      "\n",
      "Model 2022_03_10__09_06.pkl\n",
      "Test F1: 0.7931034482758621\n",
      "TAP F1: 0.6704225352112676\n",
      "TAP Acc: 0.5145228215767634\n",
      "\n",
      "\n",
      "Model 2022_03_10__09_78.pkl\n",
      "Test F1: 0.6666666666666666\n",
      "TAP F1: 0.7774936061381073\n",
      "TAP Acc: 0.6390041493775933\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seed in seeds:\n",
    "    test_model(f\"2022_03_10__09_{seed:02d}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e0df81b3-32ad-4a43-b06a-ce192cabfe87",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2022_03_07__01\n",
      "Test F1: 0.6666666666666666\n",
      "TAP F1: 0.5345911949685533\n",
      "TAP Acc: 0.38589211618257263\n",
      "Model 2022_03_07__02\n",
      "Test F1: 0.6923076923076923\n",
      "TAP F1: 0.5970149253731343\n",
      "TAP Acc: 0.43983402489626555\n",
      "Model 2022_03_07__03\n",
      "Test F1: 0.6666666666666667\n",
      "TAP F1: 0.7552083333333333\n",
      "TAP Acc: 0.6099585062240664\n",
      "Model 2022_03_07__04\n",
      "Test F1: 0.5454545454545454\n",
      "TAP F1: 0.703601108033241\n",
      "TAP Acc: 0.5560165975103735\n",
      "Model 2022_03_07__06\n",
      "Test F1: 0.5945945945945946\n",
      "TAP F1: 0.3604240282685512\n",
      "TAP Acc: 0.24896265560165975\n",
      "Model 2022_03_07__07\n",
      "Test F1: 0.5454545454545454\n",
      "TAP F1: 0.3237410071942446\n",
      "TAP Acc: 0.21991701244813278\n",
      "Model 2022_03_07__08\n",
      "Test F1: 0.6521739130434783\n",
      "TAP F1: 0.529968454258675\n",
      "TAP Acc: 0.3817427385892116\n",
      "Model 2022_03_07__09\n",
      "Test F1: 0.6984126984126985\n",
      "TAP F1: 0.7154471544715447\n",
      "TAP Acc: 0.5643153526970954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 31574... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    if i == 4:\n",
    "        continue\n",
    "    test_model(f\"2022_03_07__{i + 1:02d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55b10c4-7210-4f32-8e3e-169cb8a74f30",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a0524d8-6ace-4979-b20d-978c8b70ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eced430e-d702-4158-a94a-1b1161f6026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [2, 13, 19, 27, 38, 42, 56, 63, 6, 78]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "307df681-b2c2-4de1-9425-75f027651531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_for_seed(seed):\n",
    "    chen_train = pd.read_csv(path.join(DATA_DIR, f\"chen/deduplicated/crossval/chen_train_{seed}.csv\"), index_col=0)\n",
    "    chen_test = pd.read_csv(path.join(DATA_DIR, f\"chen/deduplicated/crossval/chen_test_{seed}.csv\"), index_col=0)\n",
    "    chen_valid, chen_test = train_test_split(chen_test, test_size=0.5, random_state=3)\n",
    "    return chen_train, chen_valid, chen_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcce6540-bef9-4050-881d-6893cf8bd343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkvetab\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Cross-val/runs/3jsdc35f\" target=\"_blank\">glowing-flower-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Cross-val\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=f\"Cross-val\", entity=\"kvetab\")\n",
    "training_callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(patience = 3, factor = 0.25, min_lr = 1e-07, verbose = 1),\n",
    "    keras.callbacks.EarlyStopping(patience = 6, restore_best_weights = True),\n",
    "    WandbCallback()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec761c93-5653-41bc-9ea1-f9ad0afb187f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "epoch_num = 100\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4\n",
    "wandb.config = {\n",
    "      \"learning_rate\": learning_rate,\n",
    "      \"epochs\": epoch_num * 2,\n",
    "      \"batch_size\": batch_size\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "568eed52-6ff1-4d13-a1f6-32afffe762de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_split(seed):\n",
    "    train, valid, test = read_data_for_seed(seed)\n",
    "    train[\"seq\"] = train[\"heavy\"] + train[\"light\"]\n",
    "    valid[\"seq\"] = valid[\"heavy\"] + valid[\"light\"]\n",
    "    test[\"seq\"] = test[\"heavy\"] + test[\"light\"]\n",
    "    sampler = RandomOverSampler(random_state=42)\n",
    "    x_train, y_train = sampler.fit_resample(train.drop(\"Y\", axis=1), train['Y'])\n",
    "    x_valid, y_valid = sampler.fit_resample(valid.drop(\"Y\", axis=1), valid['Y'])\n",
    "    wandb.init(project=f\"Cross-val\", entity=\"kvetab\")\n",
    "    model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)\n",
    "    \n",
    "    finetune(model_generator, input_encoder, OUTPUT_SPEC, x_train[\"seq\"], y_train, x_valid['seq'], y_valid, \\\n",
    "        seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = learning_rate, begin_with_frozen_pretrained_layers = True, \\\n",
    "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = learning_rate / 10, callbacks = training_callbacks)\n",
    "    \n",
    "    results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test['seq'], test['Y'], \\\n",
    "            start_seq_len = 512, start_batch_size = 32)\n",
    "    print(f\"Training split {seed}\")\n",
    "    print('Confusion matrix:')\n",
    "    display(confusion_matrix)\n",
    "    fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "    f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "    print(f1)\n",
    "    \n",
    "    mod = model_generator.create_model(seq_len = 512)\n",
    "    mod.save(path.join(DATA_DIR, f\"protein_bert/2022_03_10__09_{seed:02d}.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "389e94ac-4458-48d7-9fa1-79b9f9a0f5ce",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3jsdc35f) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 28880... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">glowing-flower-6</strong>: <a href=\"https://wandb.ai/kvetab/Cross-val/runs/3jsdc35f\" target=\"_blank\">https://wandb.ai/kvetab/Cross-val/runs/3jsdc35f</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220310_093056-3jsdc35f/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3jsdc35f). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Cross-val/runs/339y3n8w\" target=\"_blank\">distinctive-yogurt-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Cross-val\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_03_10-09:31:24] Training set: Filtered out 0 of 2002 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_10-09:31:24] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_10-09:31:24] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-10 09:31:25.040869: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-10 09:31:25.598857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9656 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:21:00.0, compute capability: 7.5\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "2022-03-10 09:31:27.522882: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-10 09:31:35.358668: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 7605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 12s 239ms/step - loss: 1.0635 - val_loss: 0.7519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.6555 - val_loss: 0.7067\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.5791 - val_loss: 0.6767\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 2s 121ms/step - loss: 0.5429 - val_loss: 0.6685\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.5226 - val_loss: 0.6691\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 2s 121ms/step - loss: 0.5050 - val_loss: 0.6646\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.4951 - val_loss: 0.6573\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.4826 - val_loss: 0.6662\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.4774 - val_loss: 0.6575\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.4785 - val_loss: 0.6577\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.4481 - val_loss: 0.6587\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.4558 - val_loss: 0.6554\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.4537 - val_loss: 0.6559\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.4503 - val_loss: 0.6573\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.4534 - val_loss: 0.6610\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.4565 - val_loss: 0.6611\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 2s 122ms/step - loss: 0.4437 - val_loss: 0.6591\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.4431 - val_loss: 0.6571\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_03_10-09:32:11] Training the entire fine-tuned model...\n",
      "[2022_03_10-09:32:23] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/16 [==========>...................] - ETA: 2s - loss: 0.4603WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1106s vs `on_train_batch_end` time: 0.1415s). Check your callbacks.\n",
      "16/16 [==============================] - 12s 383ms/step - loss: 0.4576 - val_loss: 0.6662\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.4420 - val_loss: 0.6614\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 4s 271ms/step - loss: 0.4226 - val_loss: 0.6707\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.3996 - val_loss: 0.7281\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.3872 - val_loss: 0.6969\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 4s 273ms/step - loss: 0.3702 - val_loss: 0.6989\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.3555 - val_loss: 0.7012\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.3433 - val_loss: 0.7099\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_03_10-09:33:06] Training on final epochs of sequence length 1024...\n",
      "[2022_03_10-09:33:06] Training set: Filtered out 0 of 2002 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_03_10-09:33:06] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 1022.\n",
      " 6/32 [====>.........................] - ETA: 6s - loss: 0.4541WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1096s vs `on_train_batch_end` time: 0.1364s). Check your callbacks.\n",
      "32/32 [==============================] - 16s 309ms/step - loss: 0.4335 - val_loss: 0.6491\n",
      "Training split 2\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  74  49\n",
       "1   5  30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5263157894736842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "2022-03-10 09:33:40.257984: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_10__09_02.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_10__09_02.pkl/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:339y3n8w) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 28933... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██▁▁▂▂▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂</td></tr><tr><td>lr</td><td>██████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▂▂▂▂▂▂▁▁▂▂▂▂▂▂▂▂▆▄▄▅▅▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.64906</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.43348</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.64906</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">distinctive-yogurt-7</strong>: <a href=\"https://wandb.ai/kvetab/Cross-val/runs/339y3n8w\" target=\"_blank\">https://wandb.ai/kvetab/Cross-val/runs/339y3n8w</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220310_093113-339y3n8w/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:339y3n8w). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Cross-val/runs/tsc921ki\" target=\"_blank\">dainty-monkey-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Cross-val\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_03_10-09:34:00] Training set: Filtered out 0 of 2048 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_10-09:34:00] Validation set: Filtered out 0 of 226 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_10-09:34:00] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 6/16 [==========>...................] - ETA: 0s - loss: 0.8205WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0090s vs `on_train_batch_end` time: 0.0847s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0090s vs `on_train_batch_end` time: 0.0847s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 9s 222ms/step - loss: 0.7267 - val_loss: 0.6293\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.5996 - val_loss: 0.6031\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.5659 - val_loss: 0.6056\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.5575 - val_loss: 0.5710\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.5617 - val_loss: 0.5923\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.5373 - val_loss: 0.5304\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.5031 - val_loss: 0.5243\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.5330 - val_loss: 0.5641\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.4960 - val_loss: 0.5144\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.4667 - val_loss: 0.5352\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.4843 - val_loss: 0.5373\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.4843 - val_loss: 0.5044\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.4464 - val_loss: 0.5150\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.4548 - val_loss: 0.5205\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.4498 - val_loss: 0.5562\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.4523 - val_loss: 0.5045\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.4431 - val_loss: 0.4997\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.4404 - val_loss: 0.4960\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.4296 - val_loss: 0.5001\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.4341 - val_loss: 0.5026\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.4339 - val_loss: 0.4953\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.4356 - val_loss: 0.5069\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.4391 - val_loss: 0.4937\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.4466 - val_loss: 0.4960\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.4249 - val_loss: 0.4986\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.4357 - val_loss: 0.5108\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.4323 - val_loss: 0.4949\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.4270 - val_loss: 0.4962\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.4244 - val_loss: 0.4967\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_03_10-09:34:59] Training the entire fine-tuned model...\n",
      "[2022_03_10-09:35:38] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/16 [==========>...................] - ETA: 2s - loss: 0.4537WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1215s vs `on_train_batch_end` time: 0.1325s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1215s vs `on_train_batch_end` time: 0.1325s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 10s 374ms/step - loss: 0.4466 - val_loss: 0.5001\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.4237 - val_loss: 0.5166\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.4343 - val_loss: 0.5188\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.4066 - val_loss: 0.4996\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.3804 - val_loss: 0.5485\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.3797 - val_loss: 0.5389\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.3777 - val_loss: 0.5087\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.3308 - val_loss: 0.5053\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.3376 - val_loss: 0.5039\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.3227 - val_loss: 0.5052\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_03_10-09:36:27] Training on final epochs of sequence length 1024...\n",
      "[2022_03_10-09:36:27] Training set: Filtered out 0 of 2048 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_03_10-09:37:05] Validation set: Filtered out 0 of 226 (0.0%) records of lengths exceeding 1022.\n",
      "32/32 [==============================] - 14s 309ms/step - loss: 0.4166 - val_loss: 0.5070\n",
      "Training split 13\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  71  39\n",
       "1   1  35"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6363636363636364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_10__09_13.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_10__09_13.pkl/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:tsc921ki) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 29177... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██▁▁▁▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▆▅▅▅▅▄▅▄▃▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▁▁▁▃</td></tr><tr><td>lr</td><td>███████████████▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▇▅▆▃▃▅▂▃▃▂▂▂▄▂▁▁▁▁▁▂▁▁▁▂▁▁▁▁▂▂▁▄▃▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>22</td></tr><tr><td>best_val_loss</td><td>0.49373</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.41662</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.50702</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dainty-monkey-8</strong>: <a href=\"https://wandb.ai/kvetab/Cross-val/runs/tsc921ki\" target=\"_blank\">https://wandb.ai/kvetab/Cross-val/runs/tsc921ki</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220310_093348-tsc921ki/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:tsc921ki). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Cross-val/runs/3qiluxbx\" target=\"_blank\">serene-rain-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Cross-val\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_03_10-09:37:57] Training set: Filtered out 0 of 2088 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_10-09:37:57] Validation set: Filtered out 0 of 202 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_10-09:37:57] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 11s 220ms/step - loss: 1.0361 - val_loss: 0.8343\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.6786 - val_loss: 0.6541\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.5959 - val_loss: 0.6670\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.5500 - val_loss: 0.6147\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.5424 - val_loss: 0.5946\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.5246 - val_loss: 0.5984\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.5042 - val_loss: 0.6310\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.5106 - val_loss: 0.5989\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.4902 - val_loss: 0.5709\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.4785 - val_loss: 0.5705\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.4848 - val_loss: 0.5693\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4880 - val_loss: 0.5682\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.4787 - val_loss: 0.5757\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.4756 - val_loss: 0.5637\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.4740 - val_loss: 0.5647\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.4702 - val_loss: 0.5633\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.4649 - val_loss: 0.5747\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.4752 - val_loss: 0.5664\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.4627 - val_loss: 0.5581\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.4591 - val_loss: 0.5583\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.4570 - val_loss: 0.5586\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4542 - val_loss: 0.5573\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.4553 - val_loss: 0.5546\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.4549 - val_loss: 0.5538\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.4569 - val_loss: 0.5558\n",
      "Epoch 26/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.4584 - val_loss: 0.5595\n",
      "Epoch 27/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4597 - val_loss: 0.5483\n",
      "Epoch 28/100\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.4461 - val_loss: 0.6193\n",
      "Epoch 29/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4622 - val_loss: 0.5505\n",
      "Epoch 30/100\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.4547 - val_loss: 0.5549\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 31/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.4520 - val_loss: 0.5495\n",
      "Epoch 32/100\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.4459 - val_loss: 0.5501\n",
      "Epoch 33/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.4461 - val_loss: 0.5539\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_03_10-09:39:12] Training the entire fine-tuned model...\n",
      "[2022_03_10-09:39:20] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/17 [=========>....................] - ETA: 2s - loss: 0.4418WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1102s vs `on_train_batch_end` time: 0.1466s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1102s vs `on_train_batch_end` time: 0.1466s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 12s 375ms/step - loss: 0.4515 - val_loss: 0.5481\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 4s 256ms/step - loss: 0.4318 - val_loss: 0.5490\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 4s 257ms/step - loss: 0.4215 - val_loss: 0.5484\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 4s 256ms/step - loss: 0.4107 - val_loss: 0.5448\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 4s 257ms/step - loss: 0.3749 - val_loss: 0.5318\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 5s 266ms/step - loss: 0.3650 - val_loss: 0.5280\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 4s 257ms/step - loss: 0.3381 - val_loss: 0.5921\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 4s 257ms/step - loss: 0.3358 - val_loss: 0.5810\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 4s 257ms/step - loss: 0.3061 - val_loss: 0.6612\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 4s 258ms/step - loss: 0.2813 - val_loss: 0.5739\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 4s 258ms/step - loss: 0.2650 - val_loss: 0.6482\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 4s 258ms/step - loss: 0.2604 - val_loss: 0.6389\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_03_10-09:40:21] Training on final epochs of sequence length 1024...\n",
      "[2022_03_10-09:40:21] Training set: Filtered out 0 of 2088 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_03_10-09:40:21] Validation set: Filtered out 0 of 202 (0.0%) records of lengths exceeding 1022.\n",
      " 6/33 [====>.........................] - ETA: 6s - loss: 0.3596WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1082s vs `on_train_batch_end` time: 0.1412s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1082s vs `on_train_batch_end` time: 0.1412s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 16s 311ms/step - loss: 0.3673 - val_loss: 0.5290\n",
      "Training split 19\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  69  33\n",
       "1   5  18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4864864864864865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_10__09_19.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_10__09_19.pkl/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3qiluxbx) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 29479... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▁▂▂▂▃▃▃▃▃</td></tr><tr><td>loss</td><td>█▅▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>lr</td><td>███████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▃▂▂▂▂▁▁▁▁▁▂▂▄▂▄▄</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3673</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.52903</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">serene-rain-9</strong>: <a href=\"https://wandb.ai/kvetab/Cross-val/runs/3qiluxbx\" target=\"_blank\">https://wandb.ai/kvetab/Cross-val/runs/3qiluxbx</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220310_093745-3qiluxbx/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3qiluxbx). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Cross-val/runs/k9son0j5\" target=\"_blank\">spring-dust-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Cross-val\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_03_10-09:41:26] Training set: Filtered out 0 of 2048 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_10-09:41:26] Validation set: Filtered out 0 of 206 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_10-09:41:26] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 6/16 [==========>...................] - ETA: 0s - loss: 1.3363WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0130s vs `on_train_batch_end` time: 0.0819s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0130s vs `on_train_batch_end` time: 0.0819s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 9s 226ms/step - loss: 1.0688 - val_loss: 0.6082\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.6834 - val_loss: 0.6426\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.5909 - val_loss: 0.5618\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.5523 - val_loss: 0.5651\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.5326 - val_loss: 0.5506\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.5246 - val_loss: 0.5175\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.5103 - val_loss: 0.5254\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.4995 - val_loss: 0.5044\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.4763 - val_loss: 0.5018\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.4892 - val_loss: 0.4999\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.4792 - val_loss: 0.4915\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.4622 - val_loss: 0.4974\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.4634 - val_loss: 0.4954\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.4587 - val_loss: 0.4774\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.4577 - val_loss: 0.4806\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.4426 - val_loss: 0.4754\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.4448 - val_loss: 0.4862\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.4415 - val_loss: 0.4819\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.4458 - val_loss: 0.4743\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.4275 - val_loss: 0.4879\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.4341 - val_loss: 0.5064\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.4528 - val_loss: 0.4966\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.4357 - val_loss: 0.4680\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.4324 - val_loss: 0.4690\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.4174 - val_loss: 0.4754\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.4239 - val_loss: 0.4707\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.4234 - val_loss: 0.4680\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.4300 - val_loss: 0.4676\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.4283 - val_loss: 0.4679\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.4233 - val_loss: 0.4677\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.4250 - val_loss: 0.4677\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.4237 - val_loss: 0.4676\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.4194 - val_loss: 0.4676\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.4211 - val_loss: 0.4676\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_03_10-09:42:33] Training the entire fine-tuned model...\n",
      "[2022_03_10-09:42:41] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/16 [==========>...................] - ETA: 2s - loss: 0.4307WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1142s vs `on_train_batch_end` time: 0.1412s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1142s vs `on_train_batch_end` time: 0.1412s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 10s 381ms/step - loss: 0.4309 - val_loss: 0.4845\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.4345 - val_loss: 0.4930\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.4067 - val_loss: 0.4678\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.3996 - val_loss: 0.4683\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 4s 275ms/step - loss: 0.3771 - val_loss: 0.4596\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.3608 - val_loss: 0.4588\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.3484 - val_loss: 0.4685\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.3389 - val_loss: 0.4873\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.3312 - val_loss: 0.4788\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.3060 - val_loss: 0.4768\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.2916 - val_loss: 0.4782\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.2831 - val_loss: 0.4764\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_03_10-09:43:38] Training on final epochs of sequence length 1024...\n",
      "[2022_03_10-09:43:38] Training set: Filtered out 0 of 2048 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_03_10-09:44:07] Validation set: Filtered out 0 of 206 (0.0%) records of lengths exceeding 1022.\n",
      "32/32 [==============================] - 14s 307ms/step - loss: 0.3848 - val_loss: 0.4720\n",
      "Training split 27\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  81  39\n",
       "1   7  11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3235294117647059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_10__09_27.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_10__09_27.pkl/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:k9son0j5) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 29819... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂</td></tr><tr><td>lr</td><td>███████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇█▅▅▄▃▃▃▃▂▂▂▂▂▂▂▂▃▂▁▁▂▁▁▁▁▁▁▁▂▂▁▁▁▁▂▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.45876</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.38477</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.47199</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">spring-dust-10</strong>: <a href=\"https://wandb.ai/kvetab/Cross-val/runs/k9son0j5\" target=\"_blank\">https://wandb.ai/kvetab/Cross-val/runs/k9son0j5</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220310_094114-k9son0j5/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:k9son0j5). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Cross-val/runs/55f9y1hl\" target=\"_blank\">sage-gorge-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Cross-val\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_03_10-09:45:00] Training set: Filtered out 0 of 1910 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_10-09:45:00] Validation set: Filtered out 0 of 288 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_10-09:45:00] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 [==============================] - 11s 252ms/step - loss: 0.8717 - val_loss: 0.6647\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 2s 118ms/step - loss: 0.6412 - val_loss: 0.6248\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 2s 119ms/step - loss: 0.5673 - val_loss: 0.6344\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 2s 119ms/step - loss: 0.5674 - val_loss: 0.6032\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 2s 118ms/step - loss: 0.5272 - val_loss: 0.5957\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 2s 118ms/step - loss: 0.5401 - val_loss: 0.6938\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 2s 118ms/step - loss: 0.5181 - val_loss: 0.6387\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 2s 118ms/step - loss: 0.4925 - val_loss: 0.6015\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 2s 118ms/step - loss: 0.4733 - val_loss: 0.5956\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 2s 119ms/step - loss: 0.4749 - val_loss: 0.5994\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 2s 118ms/step - loss: 0.4717 - val_loss: 0.5976\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 2s 119ms/step - loss: 0.4661 - val_loss: 0.5964\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 2s 118ms/step - loss: 0.4602 - val_loss: 0.5965\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 2s 118ms/step - loss: 0.4675 - val_loss: 0.5964\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 2s 119ms/step - loss: 0.4700 - val_loss: 0.5962\n",
      "[2022_03_10-09:45:38] Training the entire fine-tuned model...\n",
      "[2022_03_10-09:45:58] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/15 [===========>..................] - ETA: 2s - loss: 0.4808WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1114s vs `on_train_batch_end` time: 0.1441s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1114s vs `on_train_batch_end` time: 0.1441s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 12s 432ms/step - loss: 0.4792 - val_loss: 0.6099\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 4s 271ms/step - loss: 0.4619 - val_loss: 0.5993\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 4s 273ms/step - loss: 0.4439 - val_loss: 0.6015\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 4s 271ms/step - loss: 0.4271 - val_loss: 0.6003\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 4s 273ms/step - loss: 0.4044 - val_loss: 0.6059\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 4s 273ms/step - loss: 0.3879 - val_loss: 0.6146\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 4s 272ms/step - loss: 0.3857 - val_loss: 0.6168\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 4s 273ms/step - loss: 0.3796 - val_loss: 0.6240\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_03_10-09:46:39] Training on final epochs of sequence length 1024...\n",
      "[2022_03_10-09:46:39] Training set: Filtered out 0 of 1910 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_03_10-09:46:39] Validation set: Filtered out 0 of 288 (0.0%) records of lengths exceeding 1022.\n",
      " 6/30 [=====>........................] - ETA: 5s - loss: 0.5014WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1091s vs `on_train_batch_end` time: 0.1414s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1091s vs `on_train_batch_end` time: 0.1414s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 16s 323ms/step - loss: 0.4786 - val_loss: 0.5948\n",
      "Training split 38\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  92  56\n",
       "1   4  29"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4915254237288136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_10__09_38.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_10__09_38.pkl/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:55f9y1hl) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 30148... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█▁▁▂▃▃▃▄▅▁</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂</td></tr><tr><td>lr</td><td>████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▆▃▄▂▁█▄▁▁▁▁▁▁▁▁▂▁▁▁▂▂▃▃▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.47858</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.59479</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">sage-gorge-11</strong>: <a href=\"https://wandb.ai/kvetab/Cross-val/runs/55f9y1hl\" target=\"_blank\">https://wandb.ai/kvetab/Cross-val/runs/55f9y1hl</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220310_094448-55f9y1hl/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:55f9y1hl). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Cross-val/runs/3hd75ris\" target=\"_blank\">vague-resonance-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Cross-val\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_03_10-09:47:34] Training set: Filtered out 0 of 2114 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_10-09:47:34] Validation set: Filtered out 0 of 194 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_10-09:47:34] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 11s 235ms/step - loss: 0.7989 - val_loss: 0.6101\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.6513 - val_loss: 0.6316\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.6076 - val_loss: 0.6586\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.5588 - val_loss: 0.5522\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.5434 - val_loss: 0.5334\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.5205 - val_loss: 0.5170\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.5255 - val_loss: 0.5307\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.5118 - val_loss: 0.5121\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.5075 - val_loss: 0.4880\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4777 - val_loss: 0.4834\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4742 - val_loss: 0.4801\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4899 - val_loss: 0.4980\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4682 - val_loss: 0.4775\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4739 - val_loss: 0.5532\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4903 - val_loss: 0.4638\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4575 - val_loss: 0.4720\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4660 - val_loss: 0.4721\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4637 - val_loss: 0.4895\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4616 - val_loss: 0.4697\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4379 - val_loss: 0.4606\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4394 - val_loss: 0.4821\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.4464 - val_loss: 0.4506\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4420 - val_loss: 0.4507\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4372 - val_loss: 0.4500\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4372 - val_loss: 0.4551\n",
      "Epoch 26/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.4279 - val_loss: 0.4481\n",
      "Epoch 27/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4297 - val_loss: 0.4496\n",
      "Epoch 28/100\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.4310 - val_loss: 0.4555\n",
      "Epoch 29/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4406 - val_loss: 0.4566\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 30/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4234 - val_loss: 0.4552\n",
      "Epoch 31/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4298 - val_loss: 0.4486\n",
      "Epoch 32/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4374 - val_loss: 0.4540\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_03_10-09:48:46] Training the entire fine-tuned model...\n",
      "[2022_03_10-09:48:54] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/17 [=========>....................] - ETA: 2s - loss: 0.4837WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1110s vs `on_train_batch_end` time: 0.1478s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1110s vs `on_train_batch_end` time: 0.1478s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 12s 377ms/step - loss: 0.4470 - val_loss: 0.4619\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 4s 260ms/step - loss: 0.4233 - val_loss: 0.4372\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 4s 260ms/step - loss: 0.4125 - val_loss: 0.4473\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 4s 260ms/step - loss: 0.4163 - val_loss: 0.4354\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 5s 270ms/step - loss: 0.3811 - val_loss: 0.4211\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 4s 261ms/step - loss: 0.3834 - val_loss: 0.4276\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 4s 261ms/step - loss: 0.3600 - val_loss: 0.4301\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 4s 260ms/step - loss: 0.3426 - val_loss: 0.4284\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 4s 260ms/step - loss: 0.3203 - val_loss: 0.4273\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 4s 261ms/step - loss: 0.3158 - val_loss: 0.4226\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 5s 270ms/step - loss: 0.3152 - val_loss: 0.4378\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_03_10-09:49:51] Training on final epochs of sequence length 1024...\n",
      "[2022_03_10-09:49:51] Training set: Filtered out 0 of 2114 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_03_10-09:49:52] Validation set: Filtered out 0 of 194 (0.0%) records of lengths exceeding 1022.\n",
      " 6/34 [====>.........................] - ETA: 6s - loss: 0.3787WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1100s vs `on_train_batch_end` time: 0.1420s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1100s vs `on_train_batch_end` time: 0.1420s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 16s 303ms/step - loss: 0.3942 - val_loss: 0.4476\n",
      "Training split 42\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  75  18\n",
       "1   5  22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6567164179104478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_10__09_42.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_10__09_42.pkl/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3hd75ris) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 30369... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▆▅▅▄▄▄▄▄▃▄▃▃▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▁▁▁▂</td></tr><tr><td>lr</td><td>█████████████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇▇█▅▄▄▄▄▃▃▃▃▅▂▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>4</td></tr><tr><td>best_val_loss</td><td>0.42111</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.39418</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.44756</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">vague-resonance-12</strong>: <a href=\"https://wandb.ai/kvetab/Cross-val/runs/3hd75ris\" target=\"_blank\">https://wandb.ai/kvetab/Cross-val/runs/3hd75ris</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220310_094721-3hd75ris/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3hd75ris). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Cross-val/runs/alhoqjes\" target=\"_blank\">frosty-tree-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Cross-val\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_03_10-09:50:56] Training set: Filtered out 0 of 2070 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_10-09:50:56] Validation set: Filtered out 0 of 206 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_10-09:50:56] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 10s 221ms/step - loss: 0.8814 - val_loss: 0.7471\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.6259 - val_loss: 0.6532\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.5891 - val_loss: 0.6118\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.5578 - val_loss: 0.6039\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.5520 - val_loss: 0.6274\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.5491 - val_loss: 0.6002\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.5233 - val_loss: 0.5769\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.5043 - val_loss: 0.6324\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.5007 - val_loss: 0.5728\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.5272 - val_loss: 0.5688\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.5272 - val_loss: 0.5785\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4707 - val_loss: 0.5674\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.4770 - val_loss: 0.6305\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.4612 - val_loss: 0.6161\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.4736 - val_loss: 0.5622\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.4560 - val_loss: 0.5973\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.4578 - val_loss: 0.5561\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.4581 - val_loss: 0.5619\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.4613 - val_loss: 0.5767\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.4857 - val_loss: 0.6919\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.4978 - val_loss: 0.5518\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.4471 - val_loss: 0.5495\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.4362 - val_loss: 0.5571\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.4407 - val_loss: 0.5503\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.4371 - val_loss: 0.5498\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 26/100\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.4550 - val_loss: 0.5658\n",
      "Epoch 27/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.4433 - val_loss: 0.5481\n",
      "Epoch 28/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.4310 - val_loss: 0.5580\n",
      "Epoch 29/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.4400 - val_loss: 0.5490\n",
      "Epoch 30/100\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.4440 - val_loss: 0.5523\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 31/100\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.4327 - val_loss: 0.5505\n",
      "Epoch 32/100\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.4318 - val_loss: 0.5522\n",
      "Epoch 33/100\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.4393 - val_loss: 0.5529\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_03_10-09:52:09] Training the entire fine-tuned model...\n",
      "[2022_03_10-09:52:17] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/17 [=========>....................] - ETA: 2s - loss: 0.4515WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1133s vs `on_train_batch_end` time: 0.1454s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1133s vs `on_train_batch_end` time: 0.1454s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 12s 375ms/step - loss: 0.4468 - val_loss: 0.5479\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 4s 256ms/step - loss: 0.4365 - val_loss: 0.5557\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 4s 255ms/step - loss: 0.4447 - val_loss: 0.5426\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 4s 256ms/step - loss: 0.3986 - val_loss: 0.5954\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 4s 256ms/step - loss: 0.3927 - val_loss: 0.5889\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 4s 255ms/step - loss: 0.3818 - val_loss: 0.6125\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 4s 256ms/step - loss: 0.3587 - val_loss: 0.5925\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 4s 256ms/step - loss: 0.3476 - val_loss: 0.5654\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 5s 265ms/step - loss: 0.3445 - val_loss: 0.5733\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_03_10-09:53:06] Training on final epochs of sequence length 1024...\n",
      "[2022_03_10-09:53:06] Training set: Filtered out 0 of 2070 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_03_10-09:53:46] Validation set: Filtered out 0 of 206 (0.0%) records of lengths exceeding 1022.\n",
      " 6/33 [====>.........................] - ETA: 6s - loss: 0.3720WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1087s vs `on_train_batch_end` time: 0.1420s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1087s vs `on_train_batch_end` time: 0.1420s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 16s 307ms/step - loss: 0.4194 - val_loss: 0.5241\n",
      "Training split 56\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  66  43\n",
       "1   0  20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4819277108433735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_10__09_56.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_10__09_56.pkl/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:alhoqjes) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 30690... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇███▁▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▅▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂</td></tr><tr><td>lr</td><td>███████████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▄▄▄▃▃▄▃▂▃▂▄▂▃▂▂▃▆▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▄▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.41938</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.52414</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">frosty-tree-13</strong>: <a href=\"https://wandb.ai/kvetab/Cross-val/runs/alhoqjes\" target=\"_blank\">https://wandb.ai/kvetab/Cross-val/runs/alhoqjes</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220310_095045-alhoqjes/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:alhoqjes). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Cross-val/runs/34jzcyg6\" target=\"_blank\">autumn-firebrand-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Cross-val\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_03_10-09:54:50] Training set: Filtered out 0 of 1864 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_10-09:54:50] Validation set: Filtered out 0 of 316 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_10-09:54:50] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 [==============================] - 11s 246ms/step - loss: 0.8128 - val_loss: 0.6786\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 2s 117ms/step - loss: 0.6524 - val_loss: 0.6504\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 2s 117ms/step - loss: 0.5744 - val_loss: 0.5822\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 2s 117ms/step - loss: 0.5462 - val_loss: 0.5659\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 2s 117ms/step - loss: 0.5144 - val_loss: 0.5529\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 2s 117ms/step - loss: 0.5052 - val_loss: 0.5486\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 2s 117ms/step - loss: 0.4889 - val_loss: 0.5607\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 2s 116ms/step - loss: 0.4962 - val_loss: 0.5869\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 2s 116ms/step - loss: 0.4826 - val_loss: 0.5291\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 2s 117ms/step - loss: 0.4715 - val_loss: 0.5426\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 2s 117ms/step - loss: 0.4904 - val_loss: 0.5388\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 2s 117ms/step - loss: 0.4723 - val_loss: 0.5415\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 2s 118ms/step - loss: 0.4458 - val_loss: 0.5237\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 2s 118ms/step - loss: 0.4372 - val_loss: 0.5222\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 2s 117ms/step - loss: 0.4351 - val_loss: 0.5227\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 2s 118ms/step - loss: 0.4360 - val_loss: 0.5323\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 2s 117ms/step - loss: 0.4443 - val_loss: 0.5225\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 2s 117ms/step - loss: 0.4351 - val_loss: 0.5261\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 2s 117ms/step - loss: 0.4315 - val_loss: 0.5206\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 2s 117ms/step - loss: 0.4207 - val_loss: 0.5250\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 2s 118ms/step - loss: 0.4377 - val_loss: 0.5202\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 2s 117ms/step - loss: 0.4272 - val_loss: 0.5243\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 2s 117ms/step - loss: 0.4391 - val_loss: 0.5208\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 2s 117ms/step - loss: 0.4277 - val_loss: 0.5210\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 2s 119ms/step - loss: 0.4297 - val_loss: 0.5210\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 2s 117ms/step - loss: 0.4193 - val_loss: 0.5216\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 2s 117ms/step - loss: 0.4288 - val_loss: 0.5209\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_03_10-09:55:49] Training the entire fine-tuned model...\n",
      "[2022_03_10-09:55:57] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/15 [===========>..................] - ETA: 2s - loss: 0.4561WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1129s vs `on_train_batch_end` time: 0.1438s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1129s vs `on_train_batch_end` time: 0.1438s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 12s 393ms/step - loss: 0.4462 - val_loss: 0.5153\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 4s 267ms/step - loss: 0.4198 - val_loss: 0.5143\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 4s 268ms/step - loss: 0.3964 - val_loss: 0.5673\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 4s 268ms/step - loss: 0.3951 - val_loss: 0.5329\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 4s 268ms/step - loss: 0.3729 - val_loss: 0.5249\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 4s 268ms/step - loss: 0.3687 - val_loss: 0.5250\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 4s 268ms/step - loss: 0.3626 - val_loss: 0.5253\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 4s 268ms/step - loss: 0.3436 - val_loss: 0.5256\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_03_10-09:56:38] Training on final epochs of sequence length 1024...\n",
      "[2022_03_10-09:56:38] Training set: Filtered out 0 of 1864 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_03_10-09:56:45] Validation set: Filtered out 0 of 316 (0.0%) records of lengths exceeding 1022.\n",
      " 6/30 [=====>........................] - ETA: 5s - loss: 0.4272WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1077s vs `on_train_batch_end` time: 0.1423s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1077s vs `on_train_batch_end` time: 0.1423s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 15s 320ms/step - loss: 0.4209 - val_loss: 0.5225\n",
      "Training split 63\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  100  57\n",
       "1    9  38"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5352112676056338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_10__09_63.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_10__09_63.pkl/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:34jzcyg6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 31016... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▆▄▄▄▃▃▃▃▃▃▃▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▃▂▂▂▁▁▁▁▂</td></tr><tr><td>lr</td><td>████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▄▃▃▂▃▄▂▂▂▂▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▃▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.42088</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.52247</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">autumn-firebrand-14</strong>: <a href=\"https://wandb.ai/kvetab/Cross-val/runs/34jzcyg6\" target=\"_blank\">https://wandb.ai/kvetab/Cross-val/runs/34jzcyg6</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220310_095438-34jzcyg6/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:34jzcyg6). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Cross-val/runs/20b16cpl\" target=\"_blank\">dainty-sponge-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Cross-val\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_03_10-09:57:40] Training set: Filtered out 0 of 2022 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_10-09:57:40] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_10-09:57:40] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 [==============================] - 11s 237ms/step - loss: 0.8089 - val_loss: 0.6160\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.6389 - val_loss: 0.5767\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.5796 - val_loss: 0.5731\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.5515 - val_loss: 0.5574\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.5328 - val_loss: 0.5262\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.5151 - val_loss: 0.5184\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.5083 - val_loss: 0.5270\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.4809 - val_loss: 0.5082\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.4720 - val_loss: 0.5114\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.4705 - val_loss: 0.4997\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.4660 - val_loss: 0.5058\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.4797 - val_loss: 0.5671\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.4653 - val_loss: 0.4867\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.4685 - val_loss: 0.5167\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.4685 - val_loss: 0.5120\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.4467 - val_loss: 0.4982\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.4394 - val_loss: 0.5052\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.4491 - val_loss: 0.5295\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.4317 - val_loss: 0.5027\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_03_10-09:58:26] Training the entire fine-tuned model...\n",
      "[2022_03_10-09:58:34] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/16 [==========>...................] - ETA: 2s - loss: 0.4515WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1113s vs `on_train_batch_end` time: 0.1455s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1113s vs `on_train_batch_end` time: 0.1455s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 12s 381ms/step - loss: 0.4562 - val_loss: 0.4831\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.4379 - val_loss: 0.4738\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 4s 267ms/step - loss: 0.4087 - val_loss: 0.5090\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 4s 267ms/step - loss: 0.4035 - val_loss: 0.4699\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 4s 267ms/step - loss: 0.4035 - val_loss: 0.4660\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.3844 - val_loss: 0.4671\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.3617 - val_loss: 0.4629\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.3379 - val_loss: 0.4592\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 4s 267ms/step - loss: 0.3224 - val_loss: 0.4544\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.3080 - val_loss: 0.4870\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 4s 276ms/step - loss: 0.2917 - val_loss: 0.4658\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 4s 275ms/step - loss: 0.2518 - val_loss: 0.5726\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 4s 267ms/step - loss: 0.2494 - val_loss: 0.4981\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.2263 - val_loss: 0.5022\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 4s 269ms/step - loss: 0.2098 - val_loss: 0.5121\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_03_10-09:59:47] Training on final epochs of sequence length 1024...\n",
      "[2022_03_10-09:59:47] Training set: Filtered out 0 of 2022 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_03_10-09:59:47] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 1022.\n",
      " 6/32 [====>.........................] - ETA: 6s - loss: 0.3418WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1091s vs `on_train_batch_end` time: 0.1435s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1091s vs `on_train_batch_end` time: 0.1435s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 16s 322ms/step - loss: 0.3391 - val_loss: 0.4907\n",
      "Training split 6\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  79  33\n",
       "1   9  20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4878048780487805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_10__09_06.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_10__09_06.pkl/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:20b16cpl) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 31297... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▁</td></tr><tr><td>loss</td><td>█▆▅▅▅▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁▁▃</td></tr><tr><td>lr</td><td>████████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▆▅▄▄▄▃▃▃▃▆▂▄▃▃▃▄▃▂▂▃▂▂▂▁▁▁▂▁▆▃▃▄▃</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33906</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.49065</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dainty-sponge-15</strong>: <a href=\"https://wandb.ai/kvetab/Cross-val/runs/20b16cpl\" target=\"_blank\">https://wandb.ai/kvetab/Cross-val/runs/20b16cpl</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220310_095727-20b16cpl/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:20b16cpl). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Cross-val/runs/2fadx8dz\" target=\"_blank\">dauntless-grass-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Cross-val\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_03_10-10:00:41] Training set: Filtered out 0 of 1948 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_10-10:00:41] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_10-10:00:41] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 [==============================] - 10s 235ms/step - loss: 0.8347 - val_loss: 0.6885\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.6079 - val_loss: 0.6775\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.5728 - val_loss: 0.6469\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.5484 - val_loss: 0.6408\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.5204 - val_loss: 0.6462\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.5205 - val_loss: 0.6422\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.5009 - val_loss: 0.6877\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.4793 - val_loss: 0.6658\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 2s 131ms/step - loss: 0.4705 - val_loss: 0.6723\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.4660 - val_loss: 0.6562\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_03_10-10:01:10] Training the entire fine-tuned model...\n",
      "[2022_03_10-10:01:18] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/16 [==========>...................] - ETA: 2s - loss: 0.5099WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1115s vs `on_train_batch_end` time: 0.1444s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1115s vs `on_train_batch_end` time: 0.1444s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 12s 372ms/step - loss: 0.5152 - val_loss: 0.6358\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.5068 - val_loss: 0.6364\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.4748 - val_loss: 0.6360\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.4506 - val_loss: 0.6566\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.4415 - val_loss: 0.6463\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.4205 - val_loss: 0.6428\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.4141 - val_loss: 0.6585\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_03_10-10:01:56] Training on final epochs of sequence length 1024...\n",
      "[2022_03_10-10:01:56] Training set: Filtered out 0 of 1948 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_03_10-10:02:06] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 1022.\n",
      " 6/31 [====>.........................] - ETA: 6s - loss: 0.4982WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1073s vs `on_train_batch_end` time: 0.1433s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1073s vs `on_train_batch_end` time: 0.1433s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 15s 315ms/step - loss: 0.5037 - val_loss: 0.6254\n",
      "Training split 78\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  75  64\n",
       "1   6  25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4166666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_10__09_78.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_10__09_78.pkl/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "for s in seeds:\n",
    "    train_on_split(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeec339-00e8-4af2-8481-c87c18baf230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
