{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff8a9d94-4216-4b46-92e0-67298b77bf45",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "\n",
    "from proteinbert import OutputType, OutputSpec, FinetuningModelGenerator, load_pretrained_model, \\\n",
    "finetune, evaluate_by_len\n",
    "from proteinbert.finetuning import encode_train_and_valid_sets, encode_dataset\n",
    "from proteinbert.conv_and_global_attention_model import get_model_with_hidden_layers_as_outputs\n",
    "from os import path\n",
    "import pickle\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d2ed9b0-3023-4f0c-b93e-3330a86f35ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4798e75-9168-4aa2-b76c-bf37d692efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33c6481c-b3ce-47c3-86c4-5258a9553fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5db7bda-d751-4876-916f-23e824fd7b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_TYPE = OutputType(False, 'binary')\n",
    "UNIQUE_LABELS = [0, 1]\n",
    "OUTPUT_SPEC = OutputSpec(OUTPUT_TYPE, UNIQUE_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78d4d0ff-52b0-4152-9447-182709e45923",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_generator, input_encoder = load_pretrained_model(\"../../data/protein_bert/\", \"epoch_92400_sample_23500000.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59ffc4b8-405d-4244-a4bc-f0ab0190c73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbdaf9de-0881-43dc-8d2e-317db6302948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkvetab\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/March_finetune/runs/30hgao08\" target=\"_blank\">dry-pine-1</a></strong> to <a href=\"https://wandb.ai/kvetab/March_finetune\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/kvetab/March_finetune/runs/30hgao08?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f49a3adfbd0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=f\"March_finetune\", entity=\"kvetab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1743c445-302b-4f4e-9325-a6ef4fb17aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antibody_ID</th>\n",
       "      <th>heavy</th>\n",
       "      <th>light</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2073</th>\n",
       "      <td>6aod</td>\n",
       "      <td>EVQLVQSGAEVKKPGASVKVSCKASGYTFTGYYMHWVRQAPGQGLE...</td>\n",
       "      <td>DIVMTKSPSSLSASVGDRVTITCRASQGIRNDLGWYQQKPGKAPKR...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>4yny</td>\n",
       "      <td>EVQLVESGGGLVQPGRSLKLSCAASGFTFSNYGMAWVRQTPTKGLE...</td>\n",
       "      <td>EFVLTQPNSVSTNLGSTVKLSCKRSTGNIGSNYVNWYQQHEGRSPT...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>5xcv</td>\n",
       "      <td>EVQLVESGGGLVQPGRSLKLSCAASGFTFSNYGMAWVRQTPTKGLE...</td>\n",
       "      <td>QFVLTQPNSVSTNLGSTVKLSCKRSTGNIGSNYVNWYQQHEGRSPT...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2070</th>\n",
       "      <td>6and</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGYEFSRSWMNWVRQAPGKGLE...</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCRSSQSIVHSVGNTFLEWYQQKPG...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>2xqy</td>\n",
       "      <td>QVQLQQPGAELVKPGASVKMSCKASGYSFTSYWMNWVKQRPGRGLE...</td>\n",
       "      <td>DIVLTQSPASLALSLGQRATISCRASKSVSTSGYSYMYWYQQKPGQ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Antibody_ID                                              heavy  \\\n",
       "2073        6aod  EVQLVQSGAEVKKPGASVKVSCKASGYTFTGYYMHWVRQAPGQGLE...   \n",
       "1517        4yny  EVQLVESGGGLVQPGRSLKLSCAASGFTFSNYGMAWVRQTPTKGLE...   \n",
       "2025        5xcv  EVQLVESGGGLVQPGRSLKLSCAASGFTFSNYGMAWVRQTPTKGLE...   \n",
       "2070        6and  EVQLVESGGGLVQPGGSLRLSCAASGYEFSRSWMNWVRQAPGKGLE...   \n",
       "666         2xqy  QVQLQQPGAELVKPGASVKMSCKASGYSFTSYWMNWVKQRPGRGLE...   \n",
       "\n",
       "                                                  light  Y  \n",
       "2073  DIVMTKSPSSLSASVGDRVTITCRASQGIRNDLGWYQQKPGKAPKR...  0  \n",
       "1517  EFVLTQPNSVSTNLGSTVKLSCKRSTGNIGSNYVNWYQQHEGRSPT...  1  \n",
       "2025  QFVLTQPNSVSTNLGSTVKLSCKRSTGNIGSNYVNWYQQHEGRSPT...  1  \n",
       "2070  DIQMTQSPSSLSASVGDRVTITCRSSQSIVHSVGNTFLEWYQQKPG...  1  \n",
       "666   DIVLTQSPASLALSLGQRATISCRASKSVSTSGYSYMYWYQQKPGQ...  0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_train_data.csv\"), index_col=0)\n",
    "valid_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_valid_data.csv\"), index_col=0)\n",
    "test_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_test_data.csv\"), index_col=0)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c09b813-ffc8-48f7-99a5-586c76ac33f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"seq\"] = train_data[\"heavy\"] + train_data[\"light\"]\n",
    "valid_data[\"seq\"] = valid_data[\"heavy\"] + valid_data[\"light\"]\n",
    "test_data[\"seq\"] = test_data[\"heavy\"] + test_data[\"light\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fe63df3-96bf-40ed-9065-216a8e67b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(patience = 3, factor = 0.25, min_lr = 1e-07, verbose = 1),\n",
    "    keras.callbacks.EarlyStopping(patience = 6, restore_best_weights = True),\n",
    "    WandbCallback()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faeffd00-5c33-40ed-ad02-1c71cb739527",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 50\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd45319f-0dd8-489a-b105-7163d1679298",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config = {\n",
    "      \"learning_rate\": learning_rate,\n",
    "      \"epochs\": epoch_num * 2,\n",
    "      \"batch_size\": batch_size\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72bd9e53-8469-410c-9d42-6daa3811fc7e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_03_07-12:38:15] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_07-12:38:15] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_07-12:38:15] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-07 12:38:16.543610: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-07 12:38:17.659971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9656 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:41:00.0, compute capability: 7.5\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "2022-03-07 12:38:19.665660: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-07 12:38:28.305649: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 7605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 12s 327ms/step - loss: 0.7412 - val_loss: 0.5121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4909 - val_loss: 0.5763\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.5073 - val_loss: 0.4869\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4595 - val_loss: 0.4831\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 1s 113ms/step - loss: 0.4504 - val_loss: 0.4515\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4326 - val_loss: 0.4645\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4454 - val_loss: 0.4529\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4213 - val_loss: 0.4423\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4188 - val_loss: 0.4585\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4261 - val_loss: 0.4314\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.4088 - val_loss: 0.4268\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4094 - val_loss: 0.4289\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4032 - val_loss: 0.4308\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3802 - val_loss: 0.4154\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4017 - val_loss: 0.4205\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3875 - val_loss: 0.4044\n",
      "Epoch 17/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3809 - val_loss: 0.4275\n",
      "Epoch 18/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3835 - val_loss: 0.4081\n",
      "Epoch 19/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3918 - val_loss: 0.4321\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_03_07-12:38:54] Training the entire fine-tuned model...\n",
      "[2022_03_07-12:39:15] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/50\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3837WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1104s vs `on_train_batch_end` time: 0.1410s). Check your callbacks.\n",
      "11/11 [==============================] - 10s 426ms/step - loss: 0.3943 - val_loss: 0.4341\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 3s 252ms/step - loss: 0.4131 - val_loss: 0.4013\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3731 - val_loss: 0.3983\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3651 - val_loss: 0.4034\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 3s 252ms/step - loss: 0.3586 - val_loss: 0.3861\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3600 - val_loss: 0.3832\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3548 - val_loss: 0.3901\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3420 - val_loss: 0.3800\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3313 - val_loss: 0.3861\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3222 - val_loss: 0.3749\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3149 - val_loss: 0.4259\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3196 - val_loss: 0.3826\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.2905 - val_loss: 0.3761\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_03_07-12:40:00] Training on final epochs of sequence length 1024...\n",
      "[2022_03_07-12:40:00] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_03_07-12:40:02] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 1022.\n",
      " 6/21 [=======>......................] - ETA: 3s - loss: 0.3362WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1086s vs `on_train_batch_end` time: 0.1356s). Check your callbacks.\n",
      "21/21 [==============================] - 13s 371ms/step - loss: 0.3236 - val_loss: 0.3795\n"
     ]
    }
   ],
   "source": [
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data['seq'], train_data['Y'], valid_data['seq'], valid_data['Y'], \\\n",
    "        seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = learning_rate, begin_with_frozen_pretrained_layers = True, \\\n",
    "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = learning_rate / 10, callbacks = training_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f7e7d4f-4ca7-47fd-ad0f-cedd9de6c46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    }
   ],
   "source": [
    "results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "        start_seq_len = 512, start_batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d14dac3-1751-4172-96e6-7636bb9ad8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-set performance:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># records</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model seq len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>119</td>\n",
       "      <td>0.946784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>119</td>\n",
       "      <td>0.946784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               # records       AUC\n",
       "Model seq len                     \n",
       "512                  119  0.946784\n",
       "All                  119  0.946784"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  89   7\n",
       "1   8  15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Test-set performance:')\n",
    "display(results)\n",
    "\n",
    "print('Confusion matrix:')\n",
    "display(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "601cc41a-442a-4b5a-a651-a22f09e5ad2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c444b8d-daf7-4e24-a86c-c97f47b80d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "2022-03-07 12:45:22.012863: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_07__01.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_07__01.pkl/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "mod = model_generator.create_model(seq_len = 512)\n",
    "mod.save(path.join(DATA_DIR, \"protein_bert/2022_03_07__01.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce15989-ee5f-4ed4-b224-abc9c810f855",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a28a6b4-2cfe-4604-bde0-0b293d2378aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf. __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f85b7eba-b91b-4e09-b8c7-b1fbf7569ab6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_03_07-12:50:19] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_07-12:50:19] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_07-12:50:19] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "11/11 [==============================] - 10s 280ms/step - loss: 0.7594 - val_loss: 0.5533\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.5586 - val_loss: 0.5529\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.5223 - val_loss: 0.4892\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4937 - val_loss: 0.5385\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4691 - val_loss: 0.4616\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4576 - val_loss: 0.5050\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4565 - val_loss: 0.4543\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4226 - val_loss: 0.4396\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4127 - val_loss: 0.4365\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4285 - val_loss: 0.5902\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4698 - val_loss: 0.4295\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4224 - val_loss: 0.4316\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.4295 - val_loss: 0.4294\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4129 - val_loss: 0.4117\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3852 - val_loss: 0.4120\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3974 - val_loss: 0.4122\n",
      "Epoch 17/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3847 - val_loss: 0.4065\n",
      "Epoch 18/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3860 - val_loss: 0.4279\n",
      "Epoch 19/50\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.3889 - val_loss: 0.4500\n",
      "Epoch 20/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4077 - val_loss: 0.4027\n",
      "Epoch 21/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3842 - val_loss: 0.3978\n",
      "Epoch 22/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3741 - val_loss: 0.3973\n",
      "Epoch 23/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3829 - val_loss: 0.4179\n",
      "Epoch 24/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3685 - val_loss: 0.4051\n",
      "Epoch 25/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3869 - val_loss: 0.4138\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 26/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3766 - val_loss: 0.3943\n",
      "Epoch 27/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3692 - val_loss: 0.3961\n",
      "Epoch 28/50\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3790 - val_loss: 0.4137\n",
      "Epoch 29/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3750 - val_loss: 0.3991\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 30/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3683 - val_loss: 0.3963\n",
      "Epoch 31/50\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3710 - val_loss: 0.3952\n",
      "Epoch 32/50\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3564 - val_loss: 0.3968\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_03_07-12:51:09] Training the entire fine-tuned model...\n",
      "[2022_03_07-12:51:58] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/50\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3927WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1122s vs `on_train_batch_end` time: 0.1443s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1122s vs `on_train_batch_end` time: 0.1443s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 424ms/step - loss: 0.3950 - val_loss: 0.3896\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 3s 251ms/step - loss: 0.3596 - val_loss: 0.3884\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 3s 252ms/step - loss: 0.3535 - val_loss: 0.4016\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3590 - val_loss: 0.4105\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3456 - val_loss: 0.3785\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3587 - val_loss: 0.3791\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3341 - val_loss: 0.3717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3297 - val_loss: 0.3746\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3338 - val_loss: 0.3943\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3164 - val_loss: 0.3641\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.2924 - val_loss: 0.3959\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.2997 - val_loss: 0.3778\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.2747 - val_loss: 0.3684\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.2774 - val_loss: 0.3766\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.2629 - val_loss: 0.3693\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.2639 - val_loss: 0.3693\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_03_07-12:52:51] Training on final epochs of sequence length 1024...\n",
      "[2022_03_07-12:52:51] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_03_07-12:52:51] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 1022.\n",
      " 6/21 [=======>......................] - ETA: 3s - loss: 0.3263WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1071s vs `on_train_batch_end` time: 0.1419s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1071s vs `on_train_batch_end` time: 0.1419s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 13s 345ms/step - loss: 0.3221 - val_loss: 0.3717\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 50\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4\n",
    "wandb.config = {\n",
    "      \"learning_rate\": learning_rate,\n",
    "      \"epochs\": epoch_num * 2,\n",
    "      \"batch_size\": batch_size\n",
    "    }\n",
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data['seq'], train_data['Y'], valid_data['seq'], valid_data['Y'], \\\n",
    "        seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = learning_rate, begin_with_frozen_pretrained_layers = True, \\\n",
    "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = learning_rate / 10, callbacks = training_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc6f941c-f6c4-4ec0-9ba9-bdbbb942963d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  85  11\n",
       "1   5  18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6923076923076923"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "        start_seq_len = 512, start_batch_size = 32)\n",
    "print('Confusion matrix:')\n",
    "display(confusion_matrix)\n",
    "fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b52a48a5-ff77-497f-8b0f-798b8351a213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_07__02.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_07__02.pkl/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "mod = model_generator.create_model(seq_len = 512)\n",
    "mod.save(path.join(DATA_DIR, \"protein_bert/2022_03_07__02.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c95384a-4309-40d6-9284-8a350af85835",
   "metadata": {},
   "source": [
    "# 5, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "827fdb5f-9d8d-4d9d-be00-15e1a62901f0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ho0tbk51) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 43170... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▃▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▅▄▄▄▃▅▃▄▃▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▃▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>17</td></tr><tr><td>best_val_loss</td><td>0.36364</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31084</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.36835</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">feasible-silence-4</strong>: <a href=\"https://wandb.ai/kvetab/March_finetune/runs/ho0tbk51\" target=\"_blank\">https://wandb.ai/kvetab/March_finetune/runs/ho0tbk51</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220307_131420-ho0tbk51/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ho0tbk51). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/March_finetune/runs/wll3fv47\" target=\"_blank\">solar-microwave-5</a></strong> to <a href=\"https://wandb.ai/kvetab/March_finetune\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/kvetab/March_finetune/runs/wll3fv47?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f49601a02d0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=f\"March_finetune\", entity=\"kvetab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bfc5ccdf-5ad1-46f7-aadb-49c84deb5214",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e05c685d-665b-433b-87ed-22199c891e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(patience = 4, factor = 0.25, min_lr = 1e-07, verbose = 1),\n",
    "    keras.callbacks.EarlyStopping(patience = 8, restore_best_weights = True),\n",
    "    WandbCallback()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "531b454a-3ba9-4d8f-bcfd-7bc2aa9df881",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_03_07-13:19:58] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_07-13:19:58] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_07-13:19:58] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 9s 274ms/step - loss: 0.7816 - val_loss: 0.5922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.5647 - val_loss: 0.5360\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.5495 - val_loss: 0.6187\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.5039 - val_loss: 0.5358\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.4566 - val_loss: 0.4879\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.4420 - val_loss: 0.4583\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.4283 - val_loss: 0.4464\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.4119 - val_loss: 0.4437\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.4265 - val_loss: 0.4567\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.4146 - val_loss: 0.4448\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.4179 - val_loss: 0.4916\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.4148 - val_loss: 0.4386\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.4101 - val_loss: 0.4187\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.4046 - val_loss: 0.4196\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.3972 - val_loss: 0.4339\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.3907 - val_loss: 0.4153\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.3918 - val_loss: 0.4093\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3856 - val_loss: 0.4493\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.4188 - val_loss: 0.5339\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.4277 - val_loss: 0.4114\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.4024 - val_loss: 0.4000\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.3694 - val_loss: 0.3999\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.3737 - val_loss: 0.4066\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.3640 - val_loss: 0.3944\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3793 - val_loss: 0.3907\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3723 - val_loss: 0.3952\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3756 - val_loss: 0.4346\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3705 - val_loss: 0.3924\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3691 - val_loss: 0.4548\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3873 - val_loss: 0.3857\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3582 - val_loss: 0.3983\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3763 - val_loss: 0.4118\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3610 - val_loss: 0.4038\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3599 - val_loss: 0.3863\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3602 - val_loss: 0.3915\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3533 - val_loss: 0.3855\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3592 - val_loss: 0.3851\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3543 - val_loss: 0.3879\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3634 - val_loss: 0.3856\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3637 - val_loss: 0.3863\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3509 - val_loss: 0.3852\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3595 - val_loss: 0.3870\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3572 - val_loss: 0.3881\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3521 - val_loss: 0.3870\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3534 - val_loss: 0.3862\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_03_07-13:21:03] Training the entire fine-tuned model...\n",
      "[2022_03_07-13:21:37] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3963WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1120s vs `on_train_batch_end` time: 0.1458s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1120s vs `on_train_batch_end` time: 0.1458s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 426ms/step - loss: 0.3939 - val_loss: 0.3853\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3732 - val_loss: 0.3772\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3611 - val_loss: 0.3844\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3700 - val_loss: 0.3762\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3465 - val_loss: 0.3691\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3380 - val_loss: 0.3725\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3369 - val_loss: 0.3634\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3237 - val_loss: 0.3720\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3314 - val_loss: 0.3637\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3186 - val_loss: 0.3715\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3005 - val_loss: 0.3693\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3022 - val_loss: 0.3639\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.2999 - val_loss: 0.3684\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.2882 - val_loss: 0.3590\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3046 - val_loss: 0.3959\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.2966 - val_loss: 0.3588\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.2930 - val_loss: 0.3624\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.2923 - val_loss: 0.3577\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.2730 - val_loss: 0.3568\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.2753 - val_loss: 0.3690\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 0.2770 - val_loss: 0.3604\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.2705 - val_loss: 0.3654\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.2652 - val_loss: 0.3665\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.2690 - val_loss: 0.3620\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.2572 - val_loss: 0.3664\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.2654 - val_loss: 0.3625\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 3s 261ms/step - loss: 0.2577 - val_loss: 0.3686\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "[2022_03_07-13:23:01] Training on final epochs of sequence length 512...\n",
      "[2022_03_07-13:23:01] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_07-13:23:01] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.2911WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1143s vs `on_train_batch_end` time: 0.1442s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1143s vs `on_train_batch_end` time: 0.1442s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 430ms/step - loss: 0.2841 - val_loss: 0.3639\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 100\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4\n",
    "wandb.config = {\n",
    "      \"learning_rate\": learning_rate,\n",
    "      \"epochs\": epoch_num * 2,\n",
    "      \"batch_size\": batch_size\n",
    "    }\n",
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data['seq'], train_data['Y'], valid_data['seq'], valid_data['Y'], \\\n",
    "        seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = learning_rate, begin_with_frozen_pretrained_layers = True, \\\n",
    "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 512, final_lr = learning_rate / 10, callbacks = training_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e2df83ea-5adc-473c-ae52-3042fd95d68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  93   3\n",
       "1  12  11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5945945945945946"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "        start_seq_len = 512, start_batch_size = 32)\n",
    "print('Confusion matrix:')\n",
    "display(confusion_matrix)\n",
    "fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5eed3d57-a38c-4428-98bf-2cdfb31a3fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_07__06.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_07__06.pkl/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "mod = model_generator.create_model(seq_len = 512)\n",
    "mod.save(path.join(DATA_DIR, \"protein_bert/2022_03_07__06.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920495fb-16a9-48e2-afc9-37da80b5593c",
   "metadata": {},
   "source": [
    "# 7, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9196c285-2b22-4c13-b6a6-8378b10e2785",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:llc049yo) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 44058... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▄▃▃▄▄▃▃▃▃▃▃▃▂▂▂▂▃▂▂▂▂▂▂▅▃▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇▅▅▆█▄▄▃▄▆▃▃▂▃▃▂▂▂▂▂▂▂▂▂▆▄▂▂▁▁▁▁▄▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.37816</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34296</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.37942</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">major-sponge-6</strong>: <a href=\"https://wandb.ai/kvetab/March_finetune/runs/llc049yo\" target=\"_blank\">https://wandb.ai/kvetab/March_finetune/runs/llc049yo</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220307_132508-llc049yo/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:llc049yo). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/March_finetune/runs/rji5d1uv\" target=\"_blank\">noble-blaze-7</a></strong> to <a href=\"https://wandb.ai/kvetab/March_finetune\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/kvetab/March_finetune/runs/rji5d1uv?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f4980bbce10>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=f\"March_finetune\", entity=\"kvetab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4ae0008a-f230-49d3-8aa5-aa1b73a10e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b6f98896-ba67-4b9a-93a9-bc48c73334c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(patience = 3, factor = 0.25, min_lr = 1e-07, verbose = 1),\n",
    "    keras.callbacks.EarlyStopping(patience = 6, restore_best_weights = True),\n",
    "    WandbCallback()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b82a7194-d5e4-421a-b8a2-15f3d5cc53ad",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_03_07-13:31:05] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_07-13:31:05] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_07-13:31:06] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 9s 285ms/step - loss: 0.7924 - val_loss: 0.7399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.5823 - val_loss: 0.5075\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.4952 - val_loss: 0.4969\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4732 - val_loss: 0.5046\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4584 - val_loss: 0.4563\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4429 - val_loss: 0.4501\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4343 - val_loss: 0.4438\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4186 - val_loss: 0.4389\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4136 - val_loss: 0.4374\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4113 - val_loss: 0.4322\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4250 - val_loss: 0.4265\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4376 - val_loss: 0.4377\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4126 - val_loss: 0.4315\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3882 - val_loss: 0.4181\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3927 - val_loss: 0.4129\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3940 - val_loss: 0.4181\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4014 - val_loss: 0.4724\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3949 - val_loss: 0.4450\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3950 - val_loss: 0.4074\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3837 - val_loss: 0.4036\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3735 - val_loss: 0.4018\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3791 - val_loss: 0.4028\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3752 - val_loss: 0.4021\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3771 - val_loss: 0.4018\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3672 - val_loss: 0.4012\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3687 - val_loss: 0.4014\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3702 - val_loss: 0.4016\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3780 - val_loss: 0.4020\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3632 - val_loss: 0.4015\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3741 - val_loss: 0.4009\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3789 - val_loss: 0.4012\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3679 - val_loss: 0.4025\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3786 - val_loss: 0.4018\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3711 - val_loss: 0.4018\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3694 - val_loss: 0.4016\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3732 - val_loss: 0.4015\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "[2022_03_07-13:32:00] Training the entire fine-tuned model...\n",
      "[2022_03_07-13:32:09] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3840WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1115s vs `on_train_batch_end` time: 0.1456s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1115s vs `on_train_batch_end` time: 0.1456s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 419ms/step - loss: 0.3756 - val_loss: 0.3989\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3735 - val_loss: 0.3986\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3734 - val_loss: 0.3865\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3646 - val_loss: 0.3801\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3616 - val_loss: 0.3768\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3611 - val_loss: 0.3848\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3646 - val_loss: 0.4085\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3483 - val_loss: 0.3732\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3511 - val_loss: 0.3730\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3188 - val_loss: 0.3716\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3214 - val_loss: 0.3730\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3113 - val_loss: 0.3907\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3045 - val_loss: 0.3656\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3090 - val_loss: 0.3662\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.2913 - val_loss: 0.3847\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.2888 - val_loss: 0.3798\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.2692 - val_loss: 0.3748\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 0.2657 - val_loss: 0.3855\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.2579 - val_loss: 0.3799\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_03_07-13:33:11] Training on final epochs of sequence length 1024...\n",
      "[2022_03_07-13:33:11] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_03_07-13:33:37] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 1022.\n",
      " 6/21 [=======>......................] - ETA: 3s - loss: 0.3442WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1085s vs `on_train_batch_end` time: 0.1420s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1085s vs `on_train_batch_end` time: 0.1420s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 13s 344ms/step - loss: 0.3268 - val_loss: 0.3726\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 100\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4\n",
    "wandb.config = {\n",
    "      \"learning_rate\": learning_rate,\n",
    "      \"epochs\": epoch_num * 2,\n",
    "      \"batch_size\": batch_size\n",
    "    }\n",
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data['seq'], train_data['Y'], valid_data['seq'], valid_data['Y'], \\\n",
    "        seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = learning_rate, begin_with_frozen_pretrained_layers = True, \\\n",
    "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = learning_rate / 10, callbacks = training_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "06871eab-05de-471e-9dcb-d6a181e0f0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  88   8\n",
       "1   8  15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6521739130434783"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "        start_seq_len = 512, start_batch_size = 32)\n",
    "print('Confusion matrix:')\n",
    "display(confusion_matrix)\n",
    "fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eb309fad-1915-4445-bac2-339f564d415f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_07__08.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_07__08.pkl/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "mod = model_generator.create_model(seq_len = 512)\n",
    "mod.save(path.join(DATA_DIR, \"protein_bert/2022_03_07__08.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66d6a11-28fb-4e69-8009-0f4319468101",
   "metadata": {},
   "source": [
    "# Oversampling\n",
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12ef5f5c-f067-4216-a133-d47b2ccdcd42",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:30hgao08) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 41955... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▁▂▂▃▃▄▁▁▂▂▃▃▄▄▅▅▆▆▇▇██▁▂▂▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▃▃▃▃▃▃▂▂▂▂▁█▅▄▄▃▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂</td></tr><tr><td>lr</td><td>██████████▁▁▁▁▁▁████████████▃▃▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▆▆▄▄▄▃▃▃▃▄▂▂▂▂▃▁█▆▅▄▄▃▃▃▃▄▂▃▂▃▂▂▂▃▂▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>9</td></tr><tr><td>best_val_loss</td><td>0.36406</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32212</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.37174</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dry-pine-1</strong>: <a href=\"https://wandb.ai/kvetab/March_finetune/runs/30hgao08\" target=\"_blank\">https://wandb.ai/kvetab/March_finetune/runs/30hgao08</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220307_123717-30hgao08/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:30hgao08). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/March_finetune/runs/3gwj2i9r\" target=\"_blank\">fallen-bird-2</a></strong> to <a href=\"https://wandb.ai/kvetab/March_finetune\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/kvetab/March_finetune/runs/3gwj2i9r?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f49a3880750>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=f\"March_finetune\", entity=\"kvetab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "323dcf35-93be-40e1-9673-f16a79787ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = RandomOverSampler(random_state=42)\n",
    "x_train, y_train = sampler.fit_resample(train_data.drop(\"Y\", axis=1), train_data['Y'])\n",
    "x_valid, y_valid = sampler.fit_resample(valid_data.drop(\"Y\", axis=1), valid_data['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be33458b-17ed-42e1-ae31-c6ec8fabf23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5149694a-1a1c-4e39-8034-ed1946e1386c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_03_07-12:59:26] Training set: Filtered out 0 of 2114 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_07-12:59:26] Validation set: Filtered out 0 of 188 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_07-12:59:26] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 11s 228ms/step - loss: 0.8073 - val_loss: 0.7109\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.6141 - val_loss: 0.6363\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.5803 - val_loss: 0.5773\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.5528 - val_loss: 0.5728\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.5507 - val_loss: 0.5749\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.5257 - val_loss: 0.5750\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.5299 - val_loss: 0.5670\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.5130 - val_loss: 0.5943\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.5069 - val_loss: 0.5351\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4862 - val_loss: 0.5462\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4800 - val_loss: 0.6190\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4807 - val_loss: 0.5217\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.4766 - val_loss: 0.5228\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4650 - val_loss: 0.5245\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4603 - val_loss: 0.5321\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4503 - val_loss: 0.5169\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4509 - val_loss: 0.5160\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4433 - val_loss: 0.5403\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4494 - val_loss: 0.5254\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4418 - val_loss: 0.5152\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4517 - val_loss: 0.5163\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4483 - val_loss: 0.5264\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4437 - val_loss: 0.5156\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4452 - val_loss: 0.5287\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.4393 - val_loss: 0.5158\n",
      "Epoch 26/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4417 - val_loss: 0.5171\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_03_07-13:00:26] Training the entire fine-tuned model...\n",
      "[2022_03_07-13:00:35] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/17 [=========>....................] - ETA: 2s - loss: 0.4709WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1105s vs `on_train_batch_end` time: 0.1444s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1105s vs `on_train_batch_end` time: 0.1444s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 12s 367ms/step - loss: 0.4657 - val_loss: 0.5102\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 4s 258ms/step - loss: 0.4443 - val_loss: 0.5507\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 4s 259ms/step - loss: 0.4225 - val_loss: 0.4919\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 4s 259ms/step - loss: 0.4076 - val_loss: 0.5101\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 4s 259ms/step - loss: 0.4103 - val_loss: 0.5304\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 4s 259ms/step - loss: 0.3643 - val_loss: 0.4958\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 4s 259ms/step - loss: 0.3532 - val_loss: 0.4965\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 4s 258ms/step - loss: 0.3483 - val_loss: 0.4983\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 4s 259ms/step - loss: 0.3399 - val_loss: 0.5005\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_03_07-13:01:23] Training on final epochs of sequence length 1024...\n",
      "[2022_03_07-13:01:23] Training set: Filtered out 0 of 2114 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_03_07-13:02:03] Validation set: Filtered out 0 of 188 (0.0%) records of lengths exceeding 1022.\n",
      " 6/34 [====>.........................] - ETA: 6s - loss: 0.4210WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1075s vs `on_train_batch_end` time: 0.1397s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1075s vs `on_train_batch_end` time: 0.1397s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 16s 301ms/step - loss: 0.4216 - val_loss: 0.5015\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 100\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4\n",
    "wandb.config = {\n",
    "      \"learning_rate\": learning_rate,\n",
    "      \"epochs\": epoch_num * 2,\n",
    "      \"batch_size\": batch_size\n",
    "    }\n",
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, x_train[\"seq\"], y_train, x_valid['seq'], y_valid, \\\n",
    "        seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = learning_rate, begin_with_frozen_pretrained_layers = True, \\\n",
    "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = learning_rate / 10, callbacks = training_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b1c5cab-20f9-4159-81a5-e045f973a502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  75  21\n",
       "1   1  22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "        start_seq_len = 512, start_batch_size = 32)\n",
    "print('Confusion matrix:')\n",
    "display(confusion_matrix)\n",
    "fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56030912-58b9-4285-a7a3-aa30be61ca2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_07__03.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_07__03.pkl/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "mod = model_generator.create_model(seq_len = 512)\n",
    "mod.save(path.join(DATA_DIR, \"protein_bert/2022_03_07__03.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fd9619-1fb9-4771-92a7-442833414c94",
   "metadata": {},
   "source": [
    "# 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cf4d552b-1a11-4ad9-99a0-0fc989333b15",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:rji5d1uv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 44342... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▄▄▄▄▁</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▃▃▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>12</td></tr><tr><td>best_val_loss</td><td>0.36563</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32678</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.37264</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">noble-blaze-7</strong>: <a href=\"https://wandb.ai/kvetab/March_finetune/runs/rji5d1uv\" target=\"_blank\">https://wandb.ai/kvetab/March_finetune/runs/rji5d1uv</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220307_132949-rji5d1uv/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:rji5d1uv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/March_finetune/runs/25jtz98x\" target=\"_blank\">dutiful-aardvark-8</a></strong> to <a href=\"https://wandb.ai/kvetab/March_finetune\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/kvetab/March_finetune/runs/25jtz98x?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f48f4719450>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=f\"March_finetune\", entity=\"kvetab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5c5d485e-ea47-4836-b6a7-58822f0764be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "60b18dee-772d-4ec0-8836-0b11623b3c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(patience = 3, factor = 0.25, min_lr = 1e-07, verbose = 1),\n",
    "    keras.callbacks.EarlyStopping(patience = 6, restore_best_weights = True),\n",
    "    WandbCallback()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "461da1ef-9e3a-4120-bf72-e96c69bb5970",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_03_07-13:37:58] Training set: Filtered out 0 of 2114 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_07-13:37:58] Validation set: Filtered out 0 of 188 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_07-13:37:58] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 11s 223ms/step - loss: 0.7751 - val_loss: 0.6406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.5977 - val_loss: 0.6238\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.5741 - val_loss: 0.5723\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.5726 - val_loss: 0.6539\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.5250 - val_loss: 0.5448\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.5171 - val_loss: 0.5468\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4923 - val_loss: 0.5443\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4926 - val_loss: 0.5471\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4977 - val_loss: 0.5363\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4853 - val_loss: 0.5426\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4822 - val_loss: 0.5434\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4633 - val_loss: 0.5383\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4610 - val_loss: 0.5227\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4558 - val_loss: 0.5229\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4501 - val_loss: 0.5229\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4683 - val_loss: 0.5177\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4600 - val_loss: 0.5240\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4750 - val_loss: 0.5154\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.4505 - val_loss: 0.5179\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4503 - val_loss: 0.5134\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4580 - val_loss: 0.5170\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4523 - val_loss: 0.5251\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4525 - val_loss: 0.5141\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4476 - val_loss: 0.5213\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.4457 - val_loss: 0.5161\n",
      "Epoch 26/100\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.4447 - val_loss: 0.5147\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_03_07-13:38:59] Training the entire fine-tuned model...\n",
      "[2022_03_07-13:39:37] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/17 [=========>....................] - ETA: 2s - loss: 0.4542WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1113s vs `on_train_batch_end` time: 0.1447s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1113s vs `on_train_batch_end` time: 0.1447s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 12s 370ms/step - loss: 0.4499 - val_loss: 0.5293\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 4s 258ms/step - loss: 0.4416 - val_loss: 0.5522\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 4s 259ms/step - loss: 0.4483 - val_loss: 0.6439\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 4s 258ms/step - loss: 0.4372 - val_loss: 0.4974\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 4s 258ms/step - loss: 0.4028 - val_loss: 0.4906\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 4s 259ms/step - loss: 0.3774 - val_loss: 0.4947\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 4s 259ms/step - loss: 0.3641 - val_loss: 0.4910\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 4s 259ms/step - loss: 0.3407 - val_loss: 0.4917\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 4s 259ms/step - loss: 0.3216 - val_loss: 0.5227\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 4s 259ms/step - loss: 0.3201 - val_loss: 0.5121\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 4s 259ms/step - loss: 0.3066 - val_loss: 0.5313\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_03_07-13:40:34] Training on final epochs of sequence length 1024...\n",
      "[2022_03_07-13:40:34] Training set: Filtered out 0 of 2114 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_03_07-13:40:34] Validation set: Filtered out 0 of 188 (0.0%) records of lengths exceeding 1022.\n",
      " 6/34 [====>.........................] - ETA: 6s - loss: 0.4192WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1085s vs `on_train_batch_end` time: 0.1423s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1085s vs `on_train_batch_end` time: 0.1423s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 16s 318ms/step - loss: 0.4017 - val_loss: 0.5252\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 100\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4\n",
    "wandb.config = {\n",
    "      \"learning_rate\": learning_rate,\n",
    "      \"epochs\": epoch_num * 2,\n",
    "      \"batch_size\": batch_size\n",
    "    }\n",
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, x_train[\"seq\"], y_train, x_valid['seq'], y_valid, \\\n",
    "        seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = learning_rate, begin_with_frozen_pretrained_layers = True, \\\n",
    "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = learning_rate / 10, callbacks = training_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a21ec67c-10c2-4574-88c9-5550808e5ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  78  18\n",
       "1   1  22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6984126984126984"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "        start_seq_len = 512, start_batch_size = 32)\n",
    "print('Confusion matrix:')\n",
    "display(confusion_matrix)\n",
    "fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "648f0f97-bb1c-4ad8-b53a-2a5bfba2026f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_07__09.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_07__09.pkl/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "mod = model_generator.create_model(seq_len = 512)\n",
    "mod.save(path.join(DATA_DIR, \"protein_bert/2022_03_07__09.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c68ec8-a7e1-490a-a459-09a47fd872e5",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "acccd8fa-3c79-4d97-89cf-f24b8562b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = RandomUnderSampler(random_state=42)\n",
    "x_train_u, y_train_u = sampler.fit_resample(train_data.drop(\"Y\", axis=1), train_data['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52610218-cc4f-4fa2-af39-ed570e8b551b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3gwj2i9r) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 42629... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▃▃▃▂▂▂▁▁▁▁▂</td></tr><tr><td>lr</td><td>███████████████▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▄▄▄▄▃▄▂▃▅▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▃▁▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.4216</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.50154</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fallen-bird-2</strong>: <a href=\"https://wandb.ai/kvetab/March_finetune/runs/3gwj2i9r\" target=\"_blank\">https://wandb.ai/kvetab/March_finetune/runs/3gwj2i9r</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220307_125721-3gwj2i9r/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3gwj2i9r). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/March_finetune/runs/weln1d3d\" target=\"_blank\">avid-deluge-3</a></strong> to <a href=\"https://wandb.ai/kvetab/March_finetune\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/kvetab/March_finetune/runs/weln1d3d?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f4980277d90>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)\n",
    "\n",
    "wandb.init(project=f\"March_finetune\", entity=\"kvetab\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "efa8989a-ea12-42fa-9877-fbffc5ecb069",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_03_07-13:07:54] Training set: Filtered out 0 of 562 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_07-13:07:54] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_03_07-13:07:54] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 543ms/step - loss: 1.2386 - val_loss: 1.6062\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 123ms/step - loss: 1.0543 - val_loss: 0.5909\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 120ms/step - loss: 0.8385 - val_loss: 0.7112\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 118ms/step - loss: 0.8166 - val_loss: 0.7024\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 120ms/step - loss: 0.7512 - val_loss: 0.5992\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 120ms/step - loss: 0.6350 - val_loss: 0.5411\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 118ms/step - loss: 0.6238 - val_loss: 0.7253\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 118ms/step - loss: 0.6080 - val_loss: 0.7608\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 119ms/step - loss: 0.5872 - val_loss: 0.6103\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 119ms/step - loss: 0.5958 - val_loss: 0.6030\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 119ms/step - loss: 0.5955 - val_loss: 0.6402\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 131ms/step - loss: 0.5893 - val_loss: 0.6657\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_03_07-13:08:12] Training the entire fine-tuned model...\n",
      "[2022_03_07-13:08:21] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 672ms/step - loss: 0.6632 - val_loss: 0.5136\n",
      "Epoch 2/100\n",
      "2/5 [===========>..................] - ETA: 0s - loss: 0.6432WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1107s vs `on_train_batch_end` time: 0.1291s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1107s vs `on_train_batch_end` time: 0.1291s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 248ms/step - loss: 0.6278 - val_loss: 0.7628\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 247ms/step - loss: 0.6260 - val_loss: 0.5725\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 245ms/step - loss: 0.5982 - val_loss: 0.6099\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 245ms/step - loss: 0.5832 - val_loss: 0.6256\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 245ms/step - loss: 0.5755 - val_loss: 0.6147\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 245ms/step - loss: 0.5628 - val_loss: 0.6059\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_03_07-13:08:38] Training on final epochs of sequence length 1024...\n",
      "[2022_03_07-13:08:38] Training set: Filtered out 0 of 562 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_03_07-13:08:38] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 1022.\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 0.6559WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1074s vs `on_train_batch_end` time: 0.1399s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1074s vs `on_train_batch_end` time: 0.1399s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 10s 485ms/step - loss: 0.6393 - val_loss: 0.6304\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 100\n",
    "batch_size = 128\n",
    "learning_rate = 5e-4\n",
    "wandb.config = {\n",
    "      \"learning_rate\": learning_rate,\n",
    "      \"epochs\": epoch_num * 2,\n",
    "      \"batch_size\": batch_size\n",
    "    }\n",
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, x_train_u[\"seq\"], y_train_u, valid_data['seq'], valid_data['Y'], \\\n",
    "        seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = learning_rate, begin_with_frozen_pretrained_layers = True, \\\n",
    "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = learning_rate / 10, callbacks = training_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3607bfa-0aee-4272-86bc-5d82dd6373f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  71  25\n",
       "1   5  18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5454545454545454"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "        start_seq_len = 512, start_batch_size = 32)\n",
    "print('Confusion matrix:')\n",
    "display(confusion_matrix)\n",
    "fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fa07d3ca-f793-4a7d-b2e8-3febe8ea0299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_07__04.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_03_07__04.pkl/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "# protein_bert/2022_03_07__04.pkl\n",
    "mod = model_generator.create_model(seq_len = 512)\n",
    "mod.save(path.join(DATA_DIR, \"protein_bert/2022_03_07__04.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec00e839-cf6f-400c-9cba-3fc6e08f52b4",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "49e9d36a-63d9-45b6-bcf8-b66e75fd506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(path.join(DATA_DIR, \"protein_bert/batch_128_lr_1e-5_OS1_2022_02_13.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e816cc74-29d5-4c85-9fcc-1db0f5042900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_02_15-11:15:28] Test set: Filtered out 0 of 119 (0.0%) records of lengths exceeding 510.\n"
     ]
    }
   ],
   "source": [
    "seq_len = 512\n",
    "encoded_test_set = encode_dataset(test_data[\"seq\"], test_data[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'Test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4ebf1fb3-80bd-423c-a6af-2c51d82675ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "test_X, test_Y, test_sample_weigths = encoded_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1959c14b-c779-4741-9a0a-b2a453fd1fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.676470588235294"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(test_X, batch_size=32)\n",
    "y_pred_classes = (y_pred >= 0.5)\n",
    "f1_score(test_Y, y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d7099d-ada5-4464-8337-e64f1eaa7fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
