{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff8a9d94-4216-4b46-92e0-67298b77bf45",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from os import path\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3394e398-aa33-45b9-981f-ee3ead567624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from proteinbert.finetuning import encode_train_and_valid_sets, encode_dataset\n",
    "from proteinbert import OutputType, OutputSpec, evaluate_by_len, load_pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a869012f-8189-4cd3-b7a7-f614ce87bb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from proteinbert import OutputType, OutputSpec, FinetuningModelGenerator, load_pretrained_model, \\\n",
    "finetune, evaluate_by_len\n",
    "\n",
    "from proteinbert.conv_and_global_attention_model import get_model_with_hidden_layers_as_outputs\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d2ed9b0-3023-4f0c-b93e-3330a86f35ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4798e75-9168-4aa2-b76c-bf37d692efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33c6481c-b3ce-47c3-86c4-5258a9553fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5db7bda-d751-4876-916f-23e824fd7b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_TYPE = OutputType(False, 'binary')\n",
    "UNIQUE_LABELS = [0, 1]\n",
    "OUTPUT_SPEC = OutputSpec(OUTPUT_TYPE, UNIQUE_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78d4d0ff-52b0-4152-9447-182709e45923",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_generator, input_encoder = load_pretrained_model(\"../../data/protein_bert/\", \"epoch_92400_sample_23500000.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59ffc4b8-405d-4244-a4bc-f0ab0190c73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbdaf9de-0881-43dc-8d2e-317db6302948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkvetab\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Old_split_finetune/runs/vv174smt\" target=\"_blank\">glamorous-dawn-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Old_split_finetune\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/kvetab/Old_split_finetune/runs/vv174smt?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fe74148e9d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=f\"Old_split_finetune\", entity=\"kvetab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c5d33f-1465-4902-b24e-9a0e45c99d44",
   "metadata": {},
   "source": [
    "# Split 4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1743c445-302b-4f4e-9325-a6ef4fb17aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antibody_ID</th>\n",
       "      <th>heavy</th>\n",
       "      <th>light</th>\n",
       "      <th>Y</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6aod</td>\n",
       "      <td>EVQLVQSGAEVKKPGASVKVSCKASGYTFTGYYMHWVRQAPGQGLE...</td>\n",
       "      <td>DIVMTKSPSSLSASVGDRVTITCRASQGIRNDLGWYQQKPGKAPKR...</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6and</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGYEFSRSWMNWVRQAPGKGLE...</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCRSSQSIVHSVGNTFLEWYQQKPG...</td>\n",
       "      <td>1</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3esu</td>\n",
       "      <td>EVQLQQSGPELVKPGASVKISCKDSGYAFSSSWMNWVKQRPGQGPE...</td>\n",
       "      <td>DIVLIQSTSSLSASLGDRVTISCRASQDIRNYLNWYQQKPDGTVKL...</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3esv</td>\n",
       "      <td>EVQLQQSGPELVKPGASVKISCKDSGYAFNSSWMNWVKQRPGQGLE...</td>\n",
       "      <td>DIQMTQTTSSLSASLGDRVTVSCRASQDIRNYLNWYQQKPDGTVKF...</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3et9</td>\n",
       "      <td>EVQLQQSGPELVKPGASVKISCKDSGYAFSSSWMNWVKQRPGQGPE...</td>\n",
       "      <td>DIVLIQSTSSLSASLGDRVTISCRASQDIRNYLNWYQQKPDGTVKL...</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Antibody_ID                                              heavy  \\\n",
       "0         6aod  EVQLVQSGAEVKKPGASVKVSCKASGYTFTGYYMHWVRQAPGQGLE...   \n",
       "2         6and  EVQLVESGGGLVQPGGSLRLSCAASGYEFSRSWMNWVRQAPGKGLE...   \n",
       "9         3esu  EVQLQQSGPELVKPGASVKISCKDSGYAFSSSWMNWVKQRPGQGPE...   \n",
       "10        3esv  EVQLQQSGPELVKPGASVKISCKDSGYAFNSSWMNWVKQRPGQGLE...   \n",
       "11        3et9  EVQLQQSGPELVKPGASVKISCKDSGYAFSSSWMNWVKQRPGQGPE...   \n",
       "\n",
       "                                                light  Y  cluster  \n",
       "0   DIVMTKSPSSLSASVGDRVTITCRASQGIRNDLGWYQQKPGKAPKR...  0      165  \n",
       "2   DIQMTQSPSSLSASVGDRVTITCRSSQSIVHSVGNTFLEWYQQKPG...  1      135  \n",
       "9   DIVLIQSTSSLSASLGDRVTISCRASQDIRNYLNWYQQKPDGTVKL...  0       46  \n",
       "10  DIQMTQTTSSLSASLGDRVTVSCRASQDIRNYLNWYQQKPDGTVKF...  0       46  \n",
       "11  DIVLIQSTSSLSASLGDRVTISCRASQDIRNYLNWYQQKPDGTVKL...  0       46  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/crossval/chen_4_a.csv\"), index_col=0)\n",
    "test_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/crossval/chen_4_b.csv\"), index_col=0)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c09b813-ffc8-48f7-99a5-586c76ac33f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"seq\"] = train_data[\"heavy\"] + train_data[\"light\"]\n",
    "test_data[\"seq\"] = test_data[\"heavy\"] + test_data[\"light\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fe63df3-96bf-40ed-9065-216a8e67b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(patience = 3, factor = 0.25, min_lr = 1e-07, verbose = 1),\n",
    "    keras.callbacks.EarlyStopping(patience = 6, restore_best_weights = True),\n",
    "    WandbCallback()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faeffd00-5c33-40ed-ad02-1c71cb739527",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 50\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd45319f-0dd8-489a-b105-7163d1679298",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config = {\n",
    "      \"learning_rate\": learning_rate,\n",
    "      \"epochs\": epoch_num * 2,\n",
    "      \"batch_size\": batch_size\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16a259a0-5115-4000-a24a-42193248ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Validating using test data!!! #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72bd9e53-8469-410c-9d42-6daa3811fc7e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_20-11:55:49] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-11:55:49] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-11:55:49] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-20 11:55:49.746183: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-20 11:55:50.305130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9656 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:41:00.0, compute capability: 7.5\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "2022-04-20 11:55:52.250985: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-20 11:55:59.787175: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 7605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 11s 791ms/step - loss: 0.8133 - val_loss: 0.6563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.6111 - val_loss: 0.7012\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.5588 - val_loss: 0.5636\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.5089 - val_loss: 0.5047\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4533 - val_loss: 0.4726\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4418 - val_loss: 0.4622\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4370 - val_loss: 0.4592\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4249 - val_loss: 0.4527\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4141 - val_loss: 0.4612\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4101 - val_loss: 0.4456\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3975 - val_loss: 0.4485\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3892 - val_loss: 0.4472\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3928 - val_loss: 0.4430\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3757 - val_loss: 0.4401\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3700 - val_loss: 0.4430\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3719 - val_loss: 0.4494\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3749 - val_loss: 0.4394\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3670 - val_loss: 0.4486\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3763 - val_loss: 0.4381\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3568 - val_loss: 0.4341\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3690 - val_loss: 0.4543\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3591 - val_loss: 0.4769\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3581 - val_loss: 0.4336\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3468 - val_loss: 0.4326\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3443 - val_loss: 0.4397\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3428 - val_loss: 0.4531\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3618 - val_loss: 0.4275\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3829 - val_loss: 0.5017\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3678 - val_loss: 0.4766\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3617 - val_loss: 0.4267\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3323 - val_loss: 0.4459\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3367 - val_loss: 0.4600\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3325 - val_loss: 0.4285\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3188 - val_loss: 0.4303\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3208 - val_loss: 0.4289\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3086 - val_loss: 0.4255\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3090 - val_loss: 0.4251\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3159 - val_loss: 0.4251\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3163 - val_loss: 0.4269\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3255 - val_loss: 0.4280\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3359 - val_loss: 0.4250\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3107 - val_loss: 0.4245\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3069 - val_loss: 0.4248\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3077 - val_loss: 0.4244\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3211 - val_loss: 0.4242\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3201 - val_loss: 0.4241\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3036 - val_loss: 0.4242\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3213 - val_loss: 0.4241\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3172 - val_loss: 0.4241\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3154 - val_loss: 0.4240\n",
      "[2022_04_20-11:57:20] Training the entire fine-tuned model...\n",
      "[2022_04_20-11:57:29] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 9s 854ms/step - loss: 0.3087 - val_loss: 0.4627\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3090 - val_loss: 0.4285\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.2965 - val_loss: 0.4371\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3196 - val_loss: 0.4361\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 2s 372ms/step - loss: 0.3072 - val_loss: 0.4292\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.2870 - val_loss: 0.4215\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.2831 - val_loss: 0.4237\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.2804 - val_loss: 0.4200\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.2918 - val_loss: 0.4206\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.2800 - val_loss: 0.4196\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 2s 372ms/step - loss: 0.2767 - val_loss: 0.4213\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.2830 - val_loss: 0.4196\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2852 - val_loss: 0.4211\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.2743 - val_loss: 0.4205\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2669 - val_loss: 0.4201\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.2741 - val_loss: 0.4210\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.2610 - val_loss: 0.4209\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.2736 - val_loss: 0.4206\n",
      "[2022_04_20-11:58:09] Training on final epochs of sequence length 1024...\n",
      "[2022_04_20-11:58:09] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_20-11:58:09] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 1022.\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.3028WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1113s vs `on_train_batch_end` time: 0.1379s). Check your callbacks.\n",
      "10/10 [==============================] - 11s 603ms/step - loss: 0.2931 - val_loss: 0.4196\n"
     ]
    }
   ],
   "source": [
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data['seq'], train_data['Y'], test_data['seq'], test_data['Y'], \\\n",
    "        seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = learning_rate, begin_with_frozen_pretrained_layers = True, \\\n",
    "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = learning_rate / 10, callbacks = training_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f7e7d4f-4ca7-47fd-ad0f-cedd9de6c46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "        start_seq_len = 512, start_batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d14dac3-1751-4172-96e6-7636bb9ad8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-set performance:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># records</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model seq len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>733</td>\n",
       "      <td>0.785587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>733</td>\n",
       "      <td>0.785587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               # records       AUC\n",
       "Model seq len                     \n",
       "512                  733  0.785587\n",
       "All                  733  0.785587"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>496</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  496  86\n",
       "1   78  73"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Test-set performance:')\n",
    "display(results)\n",
    "\n",
    "print('Confusion matrix:')\n",
    "display(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "601cc41a-442a-4b5a-a651-a22f09e5ad2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47096774193548385"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c444b8d-daf7-4e24-a86c-c97f47b80d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "2022-04-09 11:32:18.361248: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_01/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_01/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "mod = model_generator.create_model(seq_len = 512)\n",
    "mod.save(path.join(DATA_DIR, \"protein_bert/2022_04_09_01_x\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8230234c-f488-4a51-91d1-14a68aa24d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = [(3,3), (4,3), (6,3), (8,4)]\n",
    "learning_rate = [1e-5, 5e-5, 1e-4, 5e-4]\n",
    "prepro = [\"scaling\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4969f0aa-0567-4f3c-aa9f-c9681cf164d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_by_settings(patience, lr, project_name):\n",
    "    wandb.init(project=project_name, entity=\"kvetab\")\n",
    "    model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)\n",
    "    training_callbacks = [\n",
    "        keras.callbacks.ReduceLROnPlateau(patience = patience[1], factor = 0.25, min_lr = 1e-07, verbose = 1),\n",
    "        keras.callbacks.EarlyStopping(patience = patience[0], restore_best_weights = True),\n",
    "        WandbCallback()\n",
    "    ]\n",
    "    epoch_num = 100\n",
    "    batch_size = 128\n",
    "    wandb.config = {\n",
    "          \"learning_rate\": lr,\n",
    "          \"epochs\": epoch_num * 2,\n",
    "          \"batch_size\": batch_size\n",
    "        }\n",
    "    print(type(lr))\n",
    "    print(lr)\n",
    "    finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data['seq'], train_data['Y'], test_data['seq'], test_data['Y'], \\\n",
    "            seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = lr, begin_with_frozen_pretrained_layers = True, \\\n",
    "            lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 512, final_lr = lr / 10, callbacks = training_callbacks)\n",
    "    \n",
    "    results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "            start_seq_len = 512, start_batch_size = 32)\n",
    "    print('Confusion matrix:')\n",
    "    display(confusion_matrix)\n",
    "    fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "    f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "    print(f1)\n",
    "    \n",
    "    mod = model_generator.create_model(seq_len = 512)\n",
    "    mod.save(path.join(DATA_DIR, f\"protein_bert/4b/2022_04_20_{patience[0]}_{patience[1]}_{lr}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "591b1a01-6aee-4329-936e-30c6cdfa6091",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:vv174smt) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 18232... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇█▃▂▂▂▂▂▂▂▂▁▂▁▁▁▃▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>11</td></tr><tr><td>best_val_loss</td><td>0.41962</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.29309</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.41963</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">glamorous-dawn-1</strong>: <a href=\"https://wandb.ai/kvetab/Old_split_finetune/runs/vv174smt\" target=\"_blank\">https://wandb.ai/kvetab/Old_split_finetune/runs/vv174smt</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_115539-vv174smt/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:vv174smt). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/lrlucoz2\" target=\"_blank\">dry-forest-18</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-12:01:14] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:01:14] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:01:14] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 710ms/step - loss: 0.9246 - val_loss: 0.5503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.5783 - val_loss: 0.7069\n",
      "Epoch 3/100\n",
      "2/5 [===========>..................] - ETA: 0s - loss: 0.6474"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/pbs.42056.lich-compute.vscht.cz/ipykernel_18199/780394418.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mfinetune_by_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Split 4a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/tmp/pbs.42056.lich-compute.vscht.cz/ipykernel_18199/1053265744.py\u001b[0m in \u001b[0;36mfinetune_by_settings\u001b[0;34m(patience, lr, project_name)\u001b[0m\n\u001b[1;32m     19\u001b[0m     finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data['seq'], train_data['Y'], test_data['seq'], test_data['Y'], \\\n\u001b[1;32m     20\u001b[0m             \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs_per_stage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbegin_with_frozen_pretrained_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 512, final_lr = lr / 10, callbacks = training_callbacks)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/proteinbert/finetuning.py\u001b[0m in \u001b[0;36mfinetune\u001b[0;34m(model_generator, input_encoder, output_spec, train_seqs, train_raw_Y, valid_seqs, valid_raw_Y, seq_len, batch_size, max_epochs_per_stage, lr, begin_with_frozen_pretrained_layers, lr_with_frozen_pretrained_layers, n_final_epochs, final_seq_len, final_lr, callbacks)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training with frozen pretrained layers...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         model_generator.train(encoded_train_set, encoded_valid_set, seq_len, batch_size, max_epochs_per_stage, lr = lr_with_frozen_pretrained_layers, \\\n\u001b[0;32m---> 53\u001b[0;31m                 callbacks = callbacks, freeze_pretrained_layers = True)\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training the entire fine-tuned model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/proteinbert/model_generation.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, encoded_train_set, encoded_valid_set, seq_len, batch_size, n_epochs, lr, callbacks, **create_model_kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         model.fit(train_X, train_Y, sample_weight = train_sample_weigths, batch_size = batch_size, epochs = n_epochs, validation_data = encoded_valid_set, \\\n\u001b[0;32m---> 30\u001b[0;31m                 callbacks = callbacks)\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/wandb/integration/keras/keras.py\u001b[0m in \u001b[0;36mnew_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0mset_wandb_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mold_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0mtraining_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_fit_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for pat in patience:\n",
    "    for lr in learning_rate:\n",
    "        finetune_by_settings(pat, lr, \"Split 4a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ffa85cd4-1674-44cb-9f4b-47fed90ec037",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/iq605716\" target=\"_blank\">proud-dragon-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-17:22:57] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:22:57] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:22:57] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 54s 581ms/step - loss: 0.8790 - val_loss: 0.4708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.6815 - val_loss: 0.5763\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5838 - val_loss: 0.4740\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5411 - val_loss: 0.5429\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-17:23:56] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:24:08] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5247WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1173s vs `on_train_batch_end` time: 0.1209s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1173s vs `on_train_batch_end` time: 0.1209s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 682ms/step - loss: 0.5247 - val_loss: 0.4662\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.5210 - val_loss: 0.4659\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.5201 - val_loss: 0.4630\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.5215 - val_loss: 0.4605\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.5191 - val_loss: 0.4575\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 0.5173 - val_loss: 0.4555\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.5183 - val_loss: 0.4542\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.5246 - val_loss: 0.4540\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.5200 - val_loss: 0.4555\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 343ms/step - loss: 0.5233 - val_loss: 0.4566\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 0.5164 - val_loss: 0.4576\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "[2022_04_09-17:24:37] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:24:37] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:24:37] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.5157WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0956s vs `on_train_batch_end` time: 0.1247s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0956s vs `on_train_batch_end` time: 0.1247s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 673ms/step - loss: 0.5177 - val_loss: 0.4539\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>529</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  529  0\n",
       "1  108  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:iq605716) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 34491... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▁▂▂▃▄▅▅▆▇▇█▁</td></tr><tr><td>loss</td><td>█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▂█▂▆▂▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.45389</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.51768</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.45389</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">proud-dragon-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/iq605716\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/iq605716</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_172247-iq605716/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:iq605716). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/uualznsv\" target=\"_blank\">sandy-planet-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-17:25:28] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:25:28] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:25:28] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 571ms/step - loss: 0.9884 - val_loss: 0.8532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.7348 - val_loss: 0.4670\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.5681 - val_loss: 0.4281\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5252 - val_loss: 0.5680\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5567 - val_loss: 0.4570\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4968 - val_loss: 0.5835\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-17:25:45] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:25:55] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4882WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0981s vs `on_train_batch_end` time: 0.1216s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0981s vs `on_train_batch_end` time: 0.1216s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 699ms/step - loss: 0.4896 - val_loss: 0.4518\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4907 - val_loss: 0.4375\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4800 - val_loss: 0.4252\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.4774 - val_loss: 0.4387\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4732 - val_loss: 0.4328\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4716 - val_loss: 0.4195\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4676 - val_loss: 0.4244\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.4605 - val_loss: 0.4311\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4517 - val_loss: 0.4116\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.4455 - val_loss: 0.4132\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4344 - val_loss: 0.4121\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4168 - val_loss: 0.4165\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_09-17:26:27] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:26:27] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:26:27] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4424WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0978s vs `on_train_batch_end` time: 0.1227s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0978s vs `on_train_batch_end` time: 0.1227s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 674ms/step - loss: 0.4387 - val_loss: 0.4123\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>521</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  521  8\n",
       "1  105  3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05042016806722689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:uualznsv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 34667... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▄▁▂▂▃▄▄▅▅▆▇▇█▁</td></tr><tr><td>loss</td><td>█▅▃▂▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▁▃▂▄▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_val_loss</td><td>0.41161</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.43875</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41233</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">sandy-planet-2</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/uualznsv\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/uualznsv</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_172513-uualznsv/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:uualznsv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/1yigexn9\" target=\"_blank\">breezy-durian-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-17:27:21] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:27:21] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:27:21] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 564ms/step - loss: 0.9048 - val_loss: 0.7479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.7445 - val_loss: 0.4370\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.6348 - val_loss: 0.4283\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.5251 - val_loss: 0.4871\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4881 - val_loss: 0.4153\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.4796 - val_loss: 0.5000\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4763 - val_loss: 0.4090\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4636 - val_loss: 0.4435\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4476 - val_loss: 0.4009\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4436 - val_loss: 0.4027\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4501 - val_loss: 0.4195\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4368 - val_loss: 0.4038\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-17:27:44] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:27:52] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4465WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0964s vs `on_train_batch_end` time: 0.1238s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0964s vs `on_train_batch_end` time: 0.1238s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 708ms/step - loss: 0.4487 - val_loss: 0.4005\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4485 - val_loss: 0.4159\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4274 - val_loss: 0.4109\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4474 - val_loss: 0.4227\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_09-17:28:09] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:28:09] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:28:09] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4393WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0952s vs `on_train_batch_end` time: 0.1246s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0952s vs `on_train_batch_end` time: 0.1246s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 703ms/step - loss: 0.4399 - val_loss: 0.4003\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>520</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  520  9\n",
       "1  104  4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06611570247933884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1yigexn9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 34856... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█▁▂▂▃▁</td></tr><tr><td>loss</td><td>█▆▄▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▃▁▃▁▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.40032</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.43986</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.40032</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">breezy-durian-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/1yigexn9\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/1yigexn9</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_172704-1yigexn9/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1yigexn9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/3ca7hirr\" target=\"_blank\">volcanic-wood-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-17:29:02] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:29:02] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:29:02] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 571ms/step - loss: 1.0263 - val_loss: 1.1799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.7311 - val_loss: 0.5796\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.6268 - val_loss: 0.4795\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.5593 - val_loss: 0.4428\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5221 - val_loss: 0.4226\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4706 - val_loss: 0.4370\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4593 - val_loss: 0.4257\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.4614 - val_loss: 0.4040\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4459 - val_loss: 0.3997\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4459 - val_loss: 0.4188\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4327 - val_loss: 0.4026\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4237 - val_loss: 0.4009\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-17:29:26] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:29:40] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.5546WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0954s vs `on_train_batch_end` time: 0.1235s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0954s vs `on_train_batch_end` time: 0.1235s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 706ms/step - loss: 0.5525 - val_loss: 0.4014\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.4700 - val_loss: 0.4042\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4854 - val_loss: 0.4126\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4412 - val_loss: 0.3991\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4482 - val_loss: 0.4337\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4410 - val_loss: 0.4020\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4531 - val_loss: 0.4382\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_09-17:30:01] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:30:01] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:30:01] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4324WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1004s vs `on_train_batch_end` time: 0.1234s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1004s vs `on_train_batch_end` time: 0.1234s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 700ms/step - loss: 0.4296 - val_loss: 0.4038\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>516</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  516  13\n",
       "1  102   6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09448818897637795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3ca7hirr) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 35036... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█▁▂▂▃▄▄▅▁</td></tr><tr><td>loss</td><td>█▅▃▃▂▂▁▁▁▁▁▁▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>3</td></tr><tr><td>best_val_loss</td><td>0.39912</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.42957</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.40377</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">volcanic-wood-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/3ca7hirr\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/3ca7hirr</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_172847-3ca7hirr/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3ca7hirr). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/3j7id2qm\" target=\"_blank\">ancient-valley-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-17:30:55] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:30:55] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:30:55] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 590ms/step - loss: 0.9398 - val_loss: 0.9604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.6932 - val_loss: 0.5022\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.5938 - val_loss: 0.4416\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.5174 - val_loss: 0.5099\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.5201 - val_loss: 0.4230\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4692 - val_loss: 0.4910\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4686 - val_loss: 0.4065\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4658 - val_loss: 0.4110\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4389 - val_loss: 0.4138\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4437 - val_loss: 0.4018\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4348 - val_loss: 0.3964\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4743 - val_loss: 0.4227\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4383 - val_loss: 0.3959\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4220 - val_loss: 0.3964\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4226 - val_loss: 0.3885\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4229 - val_loss: 0.3913\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4037 - val_loss: 0.4050\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4144 - val_loss: 0.4456\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4331 - val_loss: 0.3800\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4022 - val_loss: 0.3808\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4080 - val_loss: 0.3983\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3934 - val_loss: 0.3794\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3896 - val_loss: 0.4036\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4079 - val_loss: 0.3862\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4035 - val_loss: 0.3833\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4048 - val_loss: 0.3857\n",
      "[2022_04_09-17:31:35] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:31:54] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4081WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0968s vs `on_train_batch_end` time: 0.1243s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0968s vs `on_train_batch_end` time: 0.1243s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 658ms/step - loss: 0.4082 - val_loss: 0.3896\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3987 - val_loss: 0.3897\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4128 - val_loss: 0.3941\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4005 - val_loss: 0.3888\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3961 - val_loss: 0.3837\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3811 - val_loss: 0.3792\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3948 - val_loss: 0.3797\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3915 - val_loss: 0.3840\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3982 - val_loss: 0.3820\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3873 - val_loss: 0.3825\n",
      "[2022_04_09-17:32:20] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:32:20] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:32:20] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3958WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1003s vs `on_train_batch_end` time: 0.1247s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1003s vs `on_train_batch_end` time: 0.1247s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 688ms/step - loss: 0.3958 - val_loss: 0.3797\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>510</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  510  19\n",
       "1   89  19"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2602739726027397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3j7id2qm) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 35237... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▃▂▂▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.37917</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.39584</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.37969</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">ancient-valley-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/3j7id2qm\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/3j7id2qm</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_173039-3j7id2qm/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3j7id2qm). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/xxj6t3tk\" target=\"_blank\">major-cosmos-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-17:33:15] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:33:15] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:33:15] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 572ms/step - loss: 0.9796 - val_loss: 0.7330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.6496 - val_loss: 0.4352\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5586 - val_loss: 0.4319\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5078 - val_loss: 0.4981\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.5081 - val_loss: 0.4373\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4765 - val_loss: 0.4085\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4544 - val_loss: 0.4543\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.4662 - val_loss: 0.4008\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4473 - val_loss: 0.4415\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4573 - val_loss: 0.4020\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.4528 - val_loss: 0.3908\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4710 - val_loss: 0.4208\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4241 - val_loss: 0.4625\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4563 - val_loss: 0.4297\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4758 - val_loss: 0.4018\n",
      "[2022_04_09-17:33:41] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:33:49] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4313WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0995s vs `on_train_batch_end` time: 0.1222s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0995s vs `on_train_batch_end` time: 0.1222s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 670ms/step - loss: 0.4301 - val_loss: 0.4042\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 0.4243 - val_loss: 0.3918\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4270 - val_loss: 0.3901\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4166 - val_loss: 0.4090\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4090 - val_loss: 0.3861\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4108 - val_loss: 0.3929\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.4020 - val_loss: 0.4064\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4006 - val_loss: 0.3833\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3936 - val_loss: 0.3902\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3896 - val_loss: 0.3843\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3890 - val_loss: 0.3873\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3851 - val_loss: 0.3857\n",
      "[2022_04_09-17:34:21] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:34:21] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:34:21] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3908WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0956s vs `on_train_batch_end` time: 0.1242s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0956s vs `on_train_batch_end` time: 0.1242s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 672ms/step - loss: 0.3907 - val_loss: 0.3837\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>515</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  515  14\n",
       "1   95  13"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1925925925925926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:xxj6t3tk) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 35518... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█▁▁▂▃▃▃▄▅▅▅▆▇▁</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▃▂▂▂▁▂▁▁▂▃▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.38329</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.39074</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.38366</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">major-cosmos-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/xxj6t3tk\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/xxj6t3tk</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_173300-xxj6t3tk/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:xxj6t3tk). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/3pflob8e\" target=\"_blank\">easy-night-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-17:35:31] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:35:31] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:35:31] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 568ms/step - loss: 0.8316 - val_loss: 0.7136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 0.6507 - val_loss: 0.4898\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.5513 - val_loss: 0.4772\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.5333 - val_loss: 0.4468\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 0.4890 - val_loss: 0.4713\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 0.4778 - val_loss: 0.4139\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.4720 - val_loss: 0.4745\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 0.5251 - val_loss: 0.5055\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 0.4613 - val_loss: 0.4186\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 0.4424 - val_loss: 0.4154\n",
      "[2022_04_09-17:35:53] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:36:01] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4531WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0971s vs `on_train_batch_end` time: 0.1240s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0971s vs `on_train_batch_end` time: 0.1240s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 667ms/step - loss: 0.4557 - val_loss: 0.4999\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4774 - val_loss: 0.4085\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4567 - val_loss: 0.4199\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.4388 - val_loss: 0.4136\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.4282 - val_loss: 0.4091\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.4232 - val_loss: 0.4053\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4296 - val_loss: 0.4149\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4278 - val_loss: 0.4170\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4273 - val_loss: 0.4134\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4183 - val_loss: 0.4123\n",
      "[2022_04_09-17:36:28] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:36:28] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:36:28] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4286WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0940s vs `on_train_batch_end` time: 0.1273s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0940s vs `on_train_batch_end` time: 0.1273s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 673ms/step - loss: 0.4269 - val_loss: 0.4068\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>513</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  513  16\n",
       "1   99   9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13533834586466165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3pflob8e) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 35754... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█▁▂▃▃▄▅▆▆▇█▁</td></tr><tr><td>loss</td><td>█▅▃▃▂▂▂▃▂▁▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▂▂▁▃▃▁▁▃▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.40534</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.42693</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.4068</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">easy-night-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/3pflob8e\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/3pflob8e</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_173515-3pflob8e/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3pflob8e). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/am0668wt\" target=\"_blank\">exalted-lake-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-17:37:21] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:37:21] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:37:21] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 573ms/step - loss: 0.8241 - val_loss: 0.5921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.6161 - val_loss: 0.5308\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5331 - val_loss: 0.4581\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.5192 - val_loss: 0.4286\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.5814 - val_loss: 0.6019\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5168 - val_loss: 0.4159\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4813 - val_loss: 0.4081\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4614 - val_loss: 0.4573\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4681 - val_loss: 0.4743\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4738 - val_loss: 0.4788\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.5501 - val_loss: 0.4040\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4613 - val_loss: 0.4878\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4567 - val_loss: 0.3983\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4427 - val_loss: 0.4172\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4302 - val_loss: 0.4084\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4274 - val_loss: 0.4012\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4249 - val_loss: 0.3993\n",
      "[2022_04_09-17:37:50] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:38:00] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4812WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1234s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1234s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 670ms/step - loss: 0.4771 - val_loss: 0.3990\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.4190 - val_loss: 0.3873\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.4155 - val_loss: 0.4008\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 308ms/step - loss: 0.4072 - val_loss: 0.4027\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.3951 - val_loss: 0.4106\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4177 - val_loss: 0.4133\n",
      "[2022_04_09-17:38:19] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:38:19] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:38:26] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4220WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0986s vs `on_train_batch_end` time: 0.1212s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0986s vs `on_train_batch_end` time: 0.1212s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 687ms/step - loss: 0.4212 - val_loss: 0.4009\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>507</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  507  22\n",
       "1   89  19"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2550335570469799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:am0668wt) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 35954... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██▁▁▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▅▃▃▄▃▂▂▂▂▄▂▂▂▂▂▁▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▃▂█▂▂▃▄▄▂▄▁▂▂▁▁▁▁▁▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_val_loss</td><td>0.38733</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.42122</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.40093</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">exalted-lake-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/am0668wt\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/am0668wt</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_173706-am0668wt/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:am0668wt). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/1waglaf7\" target=\"_blank\">vibrant-glade-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-17:39:21] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:39:21] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:39:21] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 547ms/step - loss: 0.8813 - val_loss: 1.3722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.8270 - val_loss: 0.6514\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.6990 - val_loss: 0.4738\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 0.5945 - val_loss: 0.4451\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.6084 - val_loss: 0.5054\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 0.7313 - val_loss: 0.4767\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.6031 - val_loss: 0.4671\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 0.5647 - val_loss: 0.4207\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4659 - val_loss: 0.4939\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4907 - val_loss: 0.4297\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4549 - val_loss: 0.4138\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4590 - val_loss: 0.4253\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4567 - val_loss: 0.4437\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 180ms/step - loss: 0.4493 - val_loss: 0.4103\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 0.4511 - val_loss: 0.4091\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4416 - val_loss: 0.4166\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4461 - val_loss: 0.4071\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4406 - val_loss: 0.4127\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4436 - val_loss: 0.4111\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4474 - val_loss: 0.4060\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4320 - val_loss: 0.4198\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4525 - val_loss: 0.4177\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4349 - val_loss: 0.4000\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4296 - val_loss: 0.4420\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 177ms/step - loss: 0.4490 - val_loss: 0.4078\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 175ms/step - loss: 0.4428 - val_loss: 0.3960\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 177ms/step - loss: 0.4451 - val_loss: 0.4473\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4302 - val_loss: 0.3960\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4342 - val_loss: 0.3981\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4173 - val_loss: 0.4009\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 0.4186 - val_loss: 0.3993\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 0.4270 - val_loss: 0.4003\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 180ms/step - loss: 0.4207 - val_loss: 0.4012\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 181ms/step - loss: 0.4155 - val_loss: 0.4017\n",
      "[2022_04_09-17:40:07] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:40:20] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4370WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0953s vs `on_train_batch_end` time: 0.1259s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0953s vs `on_train_batch_end` time: 0.1259s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 694ms/step - loss: 0.4364 - val_loss: 0.3974\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4286 - val_loss: 0.4092\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 309ms/step - loss: 0.4229 - val_loss: 0.4043\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 0.4315 - val_loss: 0.3986\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 309ms/step - loss: 0.4223 - val_loss: 0.3988\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.4256 - val_loss: 0.4004\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4203 - val_loss: 0.4025\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_09-17:40:40] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:40:40] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:40:40] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4156WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0981s vs `on_train_batch_end` time: 0.1216s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0981s vs `on_train_batch_end` time: 0.1216s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 684ms/step - loss: 0.4188 - val_loss: 0.3979\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>512</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  512  17\n",
       "1   97  11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16176470588235295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1waglaf7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 36169... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███▁▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▇▅▄▄▆▄▃▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>27</td></tr><tr><td>best_val_loss</td><td>0.39603</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.41884</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.39785</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">vibrant-glade-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/1waglaf7\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/1waglaf7</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_173904-1waglaf7/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1waglaf7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/2or1kl6t\" target=\"_blank\">kind-field-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-17:41:35] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:41:35] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:41:35] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 558ms/step - loss: 0.9627 - val_loss: 0.9321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.6830 - val_loss: 0.4828\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.5887 - val_loss: 0.4431\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.5013 - val_loss: 0.5086\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.5022 - val_loss: 0.4210\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4904 - val_loss: 0.4119\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4621 - val_loss: 0.4129\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4486 - val_loss: 0.4035\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.4616 - val_loss: 0.4299\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.4311 - val_loss: 0.4123\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4397 - val_loss: 0.3968\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 177ms/step - loss: 0.4395 - val_loss: 0.5503\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 0.4825 - val_loss: 0.3960\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 175ms/step - loss: 0.4686 - val_loss: 0.4359\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4728 - val_loss: 0.4150\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4481 - val_loss: 0.5008\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 175ms/step - loss: 0.4669 - val_loss: 0.3878\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 0.4155 - val_loss: 0.3822\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4139 - val_loss: 0.3899\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4179 - val_loss: 0.4030\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 181ms/step - loss: 0.4028 - val_loss: 0.3818\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4135 - val_loss: 0.4052\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4042 - val_loss: 0.3802\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4154 - val_loss: 0.3915\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3968 - val_loss: 0.3884\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.4075 - val_loss: 0.3823\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3929 - val_loss: 0.3935\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.3917 - val_loss: 0.3924\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.3904 - val_loss: 0.3830\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-17:42:16] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:42:33] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4045WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0964s vs `on_train_batch_end` time: 0.1234s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0964s vs `on_train_batch_end` time: 0.1234s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 682ms/step - loss: 0.4030 - val_loss: 0.3806\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 308ms/step - loss: 0.4105 - val_loss: 0.3821\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 0.3894 - val_loss: 0.3884\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3938 - val_loss: 0.3789\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4137 - val_loss: 0.3990\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3957 - val_loss: 0.3781\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3891 - val_loss: 0.3770\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3812 - val_loss: 0.4235\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3794 - val_loss: 0.3738\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3936 - val_loss: 0.3832\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3830 - val_loss: 0.3974\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3778 - val_loss: 0.3726\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3656 - val_loss: 0.3913\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3797 - val_loss: 0.3874\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3649 - val_loss: 0.3724\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3569 - val_loss: 0.4053\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3546 - val_loss: 0.3738\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3794 - val_loss: 0.3765\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3409 - val_loss: 0.3894\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3320 - val_loss: 0.3893\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3446 - val_loss: 0.3805\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_09-17:43:20] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:43:20] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:43:20] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3592WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1219s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1219s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 662ms/step - loss: 0.3617 - val_loss: 0.3724\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>506</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  506  23\n",
       "1   87  21"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27631578947368424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2or1kl6t) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 36480... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▇▇▇▇█▁▁▂▂▂▃▃▃▃▄▄▅▅▅▅▆▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▂▂▂▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▃▁▂▁▂▁▃▁▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.3724</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36172</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.3724</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">kind-field-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/2or1kl6t\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/2or1kl6t</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_174118-2or1kl6t/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2or1kl6t). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/34c02zic\" target=\"_blank\">driven-sea-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-17:44:54] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:44:54] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:44:54] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 571ms/step - loss: 0.8470 - val_loss: 0.6149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.6706 - val_loss: 0.4347\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5361 - val_loss: 0.4264\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4792 - val_loss: 0.4379\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4780 - val_loss: 0.4633\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4828 - val_loss: 0.4182\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4631 - val_loss: 0.4058\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4560 - val_loss: 0.4204\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4425 - val_loss: 0.4475\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4525 - val_loss: 0.3960\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4579 - val_loss: 0.3936\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4329 - val_loss: 0.4101\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4620 - val_loss: 0.5389\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4499 - val_loss: 0.3891\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4290 - val_loss: 0.3879\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4475 - val_loss: 0.4246\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4289 - val_loss: 0.3934\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4178 - val_loss: 0.3801\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4044 - val_loss: 0.3972\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.3904 - val_loss: 0.3935\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3899 - val_loss: 0.3865\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4282 - val_loss: 0.3901\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3987 - val_loss: 0.3957\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4032 - val_loss: 0.3731\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3962 - val_loss: 0.3869\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3840 - val_loss: 0.3778\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3964 - val_loss: 0.3884\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3966 - val_loss: 0.3846\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3818 - val_loss: 0.3746\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3880 - val_loss: 0.3725\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3791 - val_loss: 0.3785\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3869 - val_loss: 0.3827\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3806 - val_loss: 0.3744\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3774 - val_loss: 0.3734\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3815 - val_loss: 0.3739\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3889 - val_loss: 0.3741\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_09-17:45:44] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:45:53] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3966WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0960s vs `on_train_batch_end` time: 0.1237s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0960s vs `on_train_batch_end` time: 0.1237s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 680ms/step - loss: 0.4024 - val_loss: 0.4048\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4004 - val_loss: 0.3693\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3745 - val_loss: 0.3774\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3822 - val_loss: 0.3780\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3625 - val_loss: 0.3702\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3687 - val_loss: 0.3734\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3679 - val_loss: 0.3869\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3555 - val_loss: 0.3766\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_09-17:46:16] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:46:16] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:46:16] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3781WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0954s vs `on_train_batch_end` time: 0.1268s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0954s vs `on_train_batch_end` time: 0.1268s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 680ms/step - loss: 0.3780 - val_loss: 0.3702\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>508</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  508  21\n",
       "1   84  24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3137254901960784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:34c02zic) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 36845... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███▁▁▁▂▂▂▂▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▃▄▂▂▂▂▂▂▆▂▂▃▂▂▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_val_loss</td><td>0.36927</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37796</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.37018</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">driven-sea-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/34c02zic\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/34c02zic</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_174439-34c02zic/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:34c02zic). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/12czo9bj\" target=\"_blank\">hardy-terrain-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-17:47:11] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:47:11] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:47:11] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 590ms/step - loss: 0.8894 - val_loss: 0.6605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.6528 - val_loss: 0.4356\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5448 - val_loss: 0.4266\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4965 - val_loss: 0.4851\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4866 - val_loss: 0.4124\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4691 - val_loss: 0.4236\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4580 - val_loss: 0.4095\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4475 - val_loss: 0.4024\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4463 - val_loss: 0.4038\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4346 - val_loss: 0.4879\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4461 - val_loss: 0.3993\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4273 - val_loss: 0.4109\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4664 - val_loss: 0.4303\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4501 - val_loss: 0.4777\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4330 - val_loss: 0.3904\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4439 - val_loss: 0.3919\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4058 - val_loss: 0.4388\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4429 - val_loss: 0.3898\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4066 - val_loss: 0.3875\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4122 - val_loss: 0.4055\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4063 - val_loss: 0.3878\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4099 - val_loss: 0.3920\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3897 - val_loss: 0.3923\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3987 - val_loss: 0.3913\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4096 - val_loss: 0.3959\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-17:47:50] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:47:58] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.5813WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1002s vs `on_train_batch_end` time: 0.1246s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1002s vs `on_train_batch_end` time: 0.1246s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 676ms/step - loss: 0.5795 - val_loss: 0.3912\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4567 - val_loss: 0.3947\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4434 - val_loss: 0.4136\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4191 - val_loss: 0.3878\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4158 - val_loss: 0.4236\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4022 - val_loss: 0.3811\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3669 - val_loss: 0.3753\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3908 - val_loss: 0.5027\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3880 - val_loss: 0.3699\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3468 - val_loss: 0.3804\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3566 - val_loss: 0.3788\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3286 - val_loss: 0.3964\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3215 - val_loss: 0.3687\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.2910 - val_loss: 0.3649\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.2859 - val_loss: 0.3654\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.2685 - val_loss: 0.3739\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.2690 - val_loss: 0.3674\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.2664 - val_loss: 0.3660\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.2460 - val_loss: 0.3670\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.2488 - val_loss: 0.3700\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "[2022_04_09-17:48:44] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:48:44] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:48:44] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.2896WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0950s vs `on_train_batch_end` time: 0.1270s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0950s vs `on_train_batch_end` time: 0.1270s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 687ms/step - loss: 0.2891 - val_loss: 0.3666\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>494</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  494  35\n",
       "1   75  33"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:12czo9bj) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 37167... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▄▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇</td></tr><tr><td>loss</td><td>█▅▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▅▃▃▃▃▂▃▃▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▄▂▂▂▂▄▂▂▃▄▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▄▁▁▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>13</td></tr><tr><td>best_val_loss</td><td>0.36492</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.28906</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.3666</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">hardy-terrain-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/12czo9bj\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/12czo9bj</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_174654-12czo9bj/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:12czo9bj). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/29bpsf7n\" target=\"_blank\">efficient-cloud-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-17:49:38] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:49:38] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:49:38] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 579ms/step - loss: 0.8873 - val_loss: 0.4887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.6215 - val_loss: 0.5977\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5988 - val_loss: 0.5325\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.6058 - val_loss: 0.4518\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5484 - val_loss: 0.4476\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.5268 - val_loss: 0.4996\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5930 - val_loss: 0.6074\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4993 - val_loss: 0.4885\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5286 - val_loss: 0.4983\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4594 - val_loss: 0.4046\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4868 - val_loss: 0.4018\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4481 - val_loss: 0.4248\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4432 - val_loss: 0.4008\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4415 - val_loss: 0.4095\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4261 - val_loss: 0.4274\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4287 - val_loss: 0.3987\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4414 - val_loss: 0.3968\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4286 - val_loss: 0.4237\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4245 - val_loss: 0.3957\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4295 - val_loss: 0.4030\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4283 - val_loss: 0.4006\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4303 - val_loss: 0.3931\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4329 - val_loss: 0.3946\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4226 - val_loss: 0.4094\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4169 - val_loss: 0.3967\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4278 - val_loss: 0.3940\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4116 - val_loss: 0.3967\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4211 - val_loss: 0.3917\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4256 - val_loss: 0.3927\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4120 - val_loss: 0.4062\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4187 - val_loss: 0.4111\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4200 - val_loss: 0.3942\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4218 - val_loss: 0.3933\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4178 - val_loss: 0.3929\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4092 - val_loss: 0.3931\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4236 - val_loss: 0.3933\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_09-17:50:27] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:50:36] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4318WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0967s vs `on_train_batch_end` time: 0.1258s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0967s vs `on_train_batch_end` time: 0.1258s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 718ms/step - loss: 0.4296 - val_loss: 0.3933\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4123 - val_loss: 0.4034\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4230 - val_loss: 0.4009\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4200 - val_loss: 0.3928\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4106 - val_loss: 0.3910\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4157 - val_loss: 0.3922\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4039 - val_loss: 0.3974\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4104 - val_loss: 0.3975\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4060 - val_loss: 0.3941\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.4086 - val_loss: 0.3953\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.4047 - val_loss: 0.3975\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4117 - val_loss: 0.3968\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4030 - val_loss: 0.3964\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_09-17:51:10] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:51:10] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:51:10] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4073WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0980s vs `on_train_batch_end` time: 0.1238s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0980s vs `on_train_batch_end` time: 0.1238s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 685ms/step - loss: 0.4059 - val_loss: 0.3909\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>513</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  513  16\n",
       "1   94  14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2028985507246377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:29bpsf7n) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 37490... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▄▄▄▃▄▂▃▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄█▆▃▅█▄▄▁▂▁▂▁▁▂▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.39094</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.40587</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.39094</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">efficient-cloud-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/29bpsf7n\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/29bpsf7n</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_174922-29bpsf7n/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:29bpsf7n). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/3kgedl1m\" target=\"_blank\">dulcet-capybara-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-17:52:05] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:52:05] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:52:05] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 570ms/step - loss: 1.0845 - val_loss: 0.9874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.6758 - val_loss: 0.4772\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5902 - val_loss: 0.4290\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5721 - val_loss: 0.5580\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.5690 - val_loss: 0.4551\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4999 - val_loss: 0.6714\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.5800 - val_loss: 0.4239\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4827 - val_loss: 0.4802\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4591 - val_loss: 0.4023\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4495 - val_loss: 0.4187\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4388 - val_loss: 0.4001\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4389 - val_loss: 0.4426\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4244 - val_loss: 0.3926\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4174 - val_loss: 0.4119\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4067 - val_loss: 0.3881\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4243 - val_loss: 0.4249\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4208 - val_loss: 0.3905\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4021 - val_loss: 0.3828\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3949 - val_loss: 0.3830\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4183 - val_loss: 0.4824\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4333 - val_loss: 0.3802\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4142 - val_loss: 0.3829\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4110 - val_loss: 0.3860\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4057 - val_loss: 0.4674\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4026 - val_loss: 0.3757\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4123 - val_loss: 0.3722\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3933 - val_loss: 0.4640\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4063 - val_loss: 0.3807\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4160 - val_loss: 0.3826\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3905 - val_loss: 0.3747\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3867 - val_loss: 0.3802\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3744 - val_loss: 0.3926\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3810 - val_loss: 0.3800\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3753 - val_loss: 0.3755\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_09-17:52:53] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:53:01] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3631WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0989s vs `on_train_batch_end` time: 0.1234s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0989s vs `on_train_batch_end` time: 0.1234s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 693ms/step - loss: 0.3634 - val_loss: 0.3989\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3832 - val_loss: 0.3726\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3752 - val_loss: 0.3894\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3850 - val_loss: 0.3730\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3776 - val_loss: 0.3801\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3873 - val_loss: 0.3729\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3710 - val_loss: 0.3785\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3654 - val_loss: 0.3784\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3575 - val_loss: 0.3762\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3438 - val_loss: 0.3849\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_09-17:53:28] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:53:28] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:53:28] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3696WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0959s vs `on_train_batch_end` time: 0.1236s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0959s vs `on_train_batch_end` time: 0.1236s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 710ms/step - loss: 0.3772 - val_loss: 0.3759\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>497</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  497  32\n",
       "1   82  26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3132530120481928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3kgedl1m) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 37842... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▂▃▂▂▂▂▂▂▁▂▂▁▂▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▃▂▄▂▂▂▁▂▁▁▁▂▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>25</td></tr><tr><td>best_val_loss</td><td>0.37216</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37723</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.3759</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dulcet-capybara-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/3kgedl1m\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/3kgedl1m</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_175149-3kgedl1m/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3kgedl1m). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/1nn5thbq\" target=\"_blank\">fanciful-terrain-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-17:54:23] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:54:23] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:54:23] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 560ms/step - loss: 0.9404 - val_loss: 0.9079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.6727 - val_loss: 0.4309\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5459 - val_loss: 0.4397\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5427 - val_loss: 0.4532\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4814 - val_loss: 0.4186\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4775 - val_loss: 0.4336\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4623 - val_loss: 0.4297\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4535 - val_loss: 0.3993\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4494 - val_loss: 0.3944\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4461 - val_loss: 0.4145\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4468 - val_loss: 0.3917\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4192 - val_loss: 0.4354\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4370 - val_loss: 0.3879\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4172 - val_loss: 0.4090\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4276 - val_loss: 0.4224\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4179 - val_loss: 0.3884\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4016 - val_loss: 0.3839\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4071 - val_loss: 0.3870\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4112 - val_loss: 0.3779\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4034 - val_loss: 0.3754\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4259 - val_loss: 0.5353\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4371 - val_loss: 0.3760\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4251 - val_loss: 0.4063\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4284 - val_loss: 0.3744\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3985 - val_loss: 0.3943\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3903 - val_loss: 0.3841\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4267 - val_loss: 0.3712\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4211 - val_loss: 0.6304\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.5101 - val_loss: 0.3699\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3940 - val_loss: 0.3771\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.3672 - val_loss: 0.3893\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3936 - val_loss: 0.6460\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4898 - val_loss: 0.3703\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3918 - val_loss: 0.4107\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3781 - val_loss: 0.3714\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 181ms/step - loss: 0.3564 - val_loss: 0.3855\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.3782 - val_loss: 0.3748\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_09-17:55:14] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:55:22] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3664WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0945s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0945s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 669ms/step - loss: 0.3707 - val_loss: 0.3746\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 309ms/step - loss: 0.3793 - val_loss: 0.3924\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3685 - val_loss: 0.3700\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3753 - val_loss: 0.4017\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 0.3552 - val_loss: 0.3753\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.3819 - val_loss: 0.3951\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 0.3526 - val_loss: 0.3685\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3870 - val_loss: 0.3700\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3619 - val_loss: 0.3792\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.3532 - val_loss: 0.3645\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.3356 - val_loss: 0.3859\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 309ms/step - loss: 0.3166 - val_loss: 0.3621\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 0.3132 - val_loss: 0.3723\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 0.3202 - val_loss: 0.3632\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3151 - val_loss: 0.3892\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.2973 - val_loss: 0.3725\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.2882 - val_loss: 0.3745\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3061 - val_loss: 0.3733\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 343ms/step - loss: 0.2958 - val_loss: 0.3698\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3005 - val_loss: 0.3785\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_09-17:56:07] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:56:07] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:56:07] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3271WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0972s vs `on_train_batch_end` time: 0.1252s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0972s vs `on_train_batch_end` time: 0.1252s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 687ms/step - loss: 0.3286 - val_loss: 0.3623\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>488</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  488  41\n",
       "1   74  34"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37158469945355194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1nn5thbq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 38172... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▃▃▃▃▄▄▄▅▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▃▂▂▂▂▂▂▂▃▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▂▂▁▁▁▂▂▂▁▁▁▃▁▁▁▁▄▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>11</td></tr><tr><td>best_val_loss</td><td>0.36209</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32863</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.36226</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fanciful-terrain-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/1nn5thbq\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/1nn5thbq</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_175407-1nn5thbq/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1nn5thbq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/305ts2o6\" target=\"_blank\">blooming-terrain-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-17:57:03] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:57:03] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:57:03] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 583ms/step - loss: 0.8860 - val_loss: 0.8218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.7015 - val_loss: 0.4321\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.6143 - val_loss: 0.5102\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.6983 - val_loss: 0.5746\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5711 - val_loss: 0.4482\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5011 - val_loss: 0.5322\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4948 - val_loss: 0.4104\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4657 - val_loss: 0.4103\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4718 - val_loss: 0.4719\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4672 - val_loss: 0.4059\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4611 - val_loss: 0.4112\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4519 - val_loss: 0.4146\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4553 - val_loss: 0.4040\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4490 - val_loss: 0.4078\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4455 - val_loss: 0.4159\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4469 - val_loss: 0.4026\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4416 - val_loss: 0.4086\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4412 - val_loss: 0.4087\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4416 - val_loss: 0.4031\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4433 - val_loss: 0.4187\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4359 - val_loss: 0.4082\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4291 - val_loss: 0.3978\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4404 - val_loss: 0.4001\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4297 - val_loss: 0.4110\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4337 - val_loss: 0.4084\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4349 - val_loss: 0.4014\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4293 - val_loss: 0.4002\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4313 - val_loss: 0.4008\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4248 - val_loss: 0.4031\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4347 - val_loss: 0.4058\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_09-17:57:46] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:57:55] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4720WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0937s vs `on_train_batch_end` time: 0.1264s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0937s vs `on_train_batch_end` time: 0.1264s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 698ms/step - loss: 0.4755 - val_loss: 0.3983\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4571 - val_loss: 0.3995\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4739 - val_loss: 0.4057\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.4306 - val_loss: 0.3996\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4125 - val_loss: 0.3885\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3955 - val_loss: 0.3898\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3918 - val_loss: 0.3889\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3710 - val_loss: 0.3997\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3303 - val_loss: 0.4253\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3267 - val_loss: 0.3859\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3193 - val_loss: 0.3874\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3018 - val_loss: 0.4028\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.2842 - val_loss: 0.3901\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.2678 - val_loss: 0.4405\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.2534 - val_loss: 0.4133\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.2273 - val_loss: 0.4233\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.2194 - val_loss: 0.4215\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.2115 - val_loss: 0.4301\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "[2022_04_09-17:58:36] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:58:36] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:58:37] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.2989WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0987s vs `on_train_batch_end` time: 0.1239s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0987s vs `on_train_batch_end` time: 0.1239s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 676ms/step - loss: 0.2966 - val_loss: 0.4033\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>458</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  458  71\n",
       "1   66  42"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38009049773755654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/crossval/chen_4_b.csv\"), index_col=0)\n",
    "test_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/crossval/chen_4_a.csv\"), index_col=0)\n",
    "train_data[\"seq\"] = train_data[\"heavy\"] + train_data[\"light\"]\n",
    "test_data[\"seq\"] = test_data[\"heavy\"] + test_data[\"light\"]\n",
    "\n",
    "for pat in patience:\n",
    "    for lr in learning_rate:\n",
    "        finetune_by_settings(pat, lr, \"Split 4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23db2b59-81eb-47da-a60b-31e2d4892411",
   "metadata": {},
   "source": [
    "# 5x2cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6816ec8-f1f4-4cda-a4be-256d0fdf968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_by_settings_and_data(patience, lr, project_name, train_data, test_data, dir_name):\n",
    "    wandb.init(project=project_name, entity=\"kvetab\")\n",
    "    model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)\n",
    "    training_callbacks = [\n",
    "        keras.callbacks.ReduceLROnPlateau(patience = patience[1], factor = 0.25, min_lr = 1e-07, verbose = 1),\n",
    "        keras.callbacks.EarlyStopping(patience = patience[0], restore_best_weights = True),\n",
    "        WandbCallback()\n",
    "    ]\n",
    "    epoch_num = 100\n",
    "    batch_size = 128\n",
    "    wandb.config = {\n",
    "          \"learning_rate\": lr,\n",
    "          \"epochs\": epoch_num * 2,\n",
    "          \"batch_size\": batch_size\n",
    "        }\n",
    "    print(type(lr))\n",
    "    print(lr)\n",
    "    finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data['seq'], train_data['Y'], test_data['seq'], test_data['Y'], \\\n",
    "            seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = lr, begin_with_frozen_pretrained_layers = True, \\\n",
    "            lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 512, final_lr = lr / 10, callbacks = training_callbacks)\n",
    "    \n",
    "    results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "            start_seq_len = 512, start_batch_size = 32)\n",
    "    print('Confusion matrix:')\n",
    "    display(confusion_matrix)\n",
    "    fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "    f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "    print(f1)\n",
    "    \n",
    "    mod = model_generator.create_model(seq_len = 512)\n",
    "    mod.save(path.join(DATA_DIR, f\"protein_bert/{dir_name}/2022_04_20_{patience[0]}_{patience[1]}_{lr}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1750fff9-d429-4b0a-ac05-d14f629b52bc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:lrlucoz2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 18738... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁█</td></tr><tr><td>loss</td><td>█▁</td></tr><tr><td>lr</td><td>▁▁</td></tr><tr><td>val_loss</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.55031</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>loss</td><td>0.57832</td></tr><tr><td>lr</td><td>0.01</td></tr><tr><td>val_loss</td><td>0.70689</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dry-forest-18</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/lrlucoz2\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/lrlucoz2</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_120057-lrlucoz2/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:lrlucoz2). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/1a3fxkhc\" target=\"_blank\">fragrant-energy-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-12:02:40] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:02:40] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:02:40] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 626ms/step - loss: 0.9687 - val_loss: 0.8977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.7061 - val_loss: 0.7471\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5122 - val_loss: 0.5401\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5265 - val_loss: 0.5217\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4688 - val_loss: 0.5571\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4928 - val_loss: 0.4852\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4287 - val_loss: 0.5266\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4310 - val_loss: 0.4811\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4114 - val_loss: 0.4711\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3983 - val_loss: 0.4671\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4047 - val_loss: 0.5114\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4242 - val_loss: 0.4650\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3908 - val_loss: 0.4680\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3893 - val_loss: 0.5645\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4122 - val_loss: 0.4651\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-12:03:08] Training the entire fine-tuned model...\n",
      "[2022_04_20-12:03:16] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3892WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1165s vs `on_train_batch_end` time: 0.1262s). Check your callbacks.\n",
      "6/6 [==============================] - 10s 679ms/step - loss: 0.3892 - val_loss: 0.4692\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3875 - val_loss: 0.4677\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3839 - val_loss: 0.4669\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3892 - val_loss: 0.4720\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3767 - val_loss: 0.4715\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3807 - val_loss: 0.4730\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "[2022_04_20-12:03:36] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-12:03:36] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:03:36] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3820WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0991s vs `on_train_batch_end` time: 0.1225s). Check your callbacks.\n",
      "6/6 [==============================] - 10s 673ms/step - loss: 0.3820 - val_loss: 0.4670\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>511</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>142</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  511  5\n",
       "1  142  2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.026490066225165563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "2022-04-20 12:04:04.360749: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1a3fxkhc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 18812... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█▁▁▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▃▃▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▂▂▂▁▂▁▁▁▂▁▁▃▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>11</td></tr><tr><td>best_val_loss</td><td>0.46503</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.38196</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.46701</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fragrant-energy-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/1a3fxkhc\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018a/runs/1a3fxkhc</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_120224-1a3fxkhc/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1a3fxkhc). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/3h0g9l8z\" target=\"_blank\">amber-morning-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-12:04:29] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:04:29] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:04:29] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 560ms/step - loss: 0.9079 - val_loss: 0.9602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.7536 - val_loss: 0.4968\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.5277 - val_loss: 0.4572\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4704 - val_loss: 0.5086\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4778 - val_loss: 0.4467\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4565 - val_loss: 0.4534\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4549 - val_loss: 0.4694\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4324 - val_loss: 0.4392\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4237 - val_loss: 0.4429\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4196 - val_loss: 0.4368\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4131 - val_loss: 0.5533\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4359 - val_loss: 0.4501\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4174 - val_loss: 0.4342\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3949 - val_loss: 0.4546\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3934 - val_loss: 0.4401\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3772 - val_loss: 0.4320\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3881 - val_loss: 0.4408\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3828 - val_loss: 0.4289\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3732 - val_loss: 0.4409\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3901 - val_loss: 0.4787\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3912 - val_loss: 0.4639\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-12:05:03] Training the entire fine-tuned model...\n",
      "[2022_04_20-12:05:12] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3810WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1208s vs `on_train_batch_end` time: 0.1240s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1208s vs `on_train_batch_end` time: 0.1240s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 707ms/step - loss: 0.3810 - val_loss: 0.4309\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3672 - val_loss: 0.4349\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3653 - val_loss: 0.4326\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3796 - val_loss: 0.4325\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "[2022_04_20-12:05:28] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-12:05:28] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:05:28] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3746WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0997s vs `on_train_batch_end` time: 0.1246s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0997s vs `on_train_batch_end` time: 0.1246s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 700ms/step - loss: 0.3746 - val_loss: 0.4313\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>514</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  514  18\n",
       "1  104  22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26506024096385544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3h0g9l8z) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 19022... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▁</td></tr><tr><td>loss</td><td>█▆▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████████▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▁▂▁▁▂▁▁▁▃▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>17</td></tr><tr><td>best_val_loss</td><td>0.4289</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37457</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43128</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">amber-morning-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/3h0g9l8z\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018b/runs/3h0g9l8z</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_120412-3h0g9l8z/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3h0g9l8z). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/15b7nbgc\" target=\"_blank\">comfy-glitter-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-12:06:32] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:06:32] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:06:32] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.8035WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0848s vs `on_train_batch_end` time: 0.1417s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0848s vs `on_train_batch_end` time: 0.1417s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 753ms/step - loss: 0.8007 - val_loss: 0.7307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.6311 - val_loss: 0.5429\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5301 - val_loss: 0.6089\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4932 - val_loss: 0.4979\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4809 - val_loss: 0.5137\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4933 - val_loss: 0.6499\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4855 - val_loss: 0.4767\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4506 - val_loss: 0.4797\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4396 - val_loss: 0.6069\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4659 - val_loss: 0.5105\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-12:06:54] Training the entire fine-tuned model...\n",
      "[2022_04_20-12:07:10] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4025WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0977s vs `on_train_batch_end` time: 0.1238s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0977s vs `on_train_batch_end` time: 0.1238s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 692ms/step - loss: 0.4025 - val_loss: 0.4766\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.4098 - val_loss: 0.4775\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.4118 - val_loss: 0.4860\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.4013 - val_loss: 0.4849\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_20-12:07:26] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-12:07:26] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:07:26] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4051WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0983s vs `on_train_batch_end` time: 0.1251s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0983s vs `on_train_batch_end` time: 0.1251s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 681ms/step - loss: 0.4051 - val_loss: 0.4763\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>511</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>139</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  511  5\n",
       "1  139  5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06493506493506493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:15b7nbgc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 19276... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█▁▂▃▃▁</td></tr><tr><td>loss</td><td>█▅▃▃▂▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▅▂▂▆▁▁▅▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.47627</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.40512</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.47627</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">comfy-glitter-2</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/15b7nbgc\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018a/runs/15b7nbgc</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_120607-15b7nbgc/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:15b7nbgc). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/2cw41h82\" target=\"_blank\">revived-haze-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-12:08:21] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:08:21] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:08:21] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 578ms/step - loss: 0.9865 - val_loss: 1.2914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.8147 - val_loss: 0.8073\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 220ms/step - loss: 0.6485 - val_loss: 0.7360\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.5870 - val_loss: 0.5339\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5161 - val_loss: 0.4653\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4756 - val_loss: 0.4520\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4667 - val_loss: 0.4574\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4528 - val_loss: 0.4536\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4296 - val_loss: 0.4410\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4265 - val_loss: 0.4652\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4097 - val_loss: 0.4407\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4150 - val_loss: 0.4363\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4049 - val_loss: 0.5534\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4564 - val_loss: 0.5348\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4491 - val_loss: 0.6445\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-12:08:49] Training the entire fine-tuned model...\n",
      "[2022_04_20-12:08:58] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4112WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0961s vs `on_train_batch_end` time: 0.1247s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0961s vs `on_train_batch_end` time: 0.1247s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 675ms/step - loss: 0.4112 - val_loss: 0.4429\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3993 - val_loss: 0.4362\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.4052 - val_loss: 0.4371\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3989 - val_loss: 0.4370\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3964 - val_loss: 0.4370\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_20-12:09:15] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-12:09:15] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:09:24] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4057WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1000s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1000s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 667ms/step - loss: 0.4057 - val_loss: 0.4363\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>527</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  527  5\n",
       "1  121  5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07352941176470588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2cw41h82) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 19453... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█▁▁▂▃▃▁</td></tr><tr><td>loss</td><td>█▆▄▃▂▂▂▂▁▁▁▁▁▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▂▁▁▁▁▁▁▁▁▂▂▃▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_val_loss</td><td>0.43618</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.4057</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43628</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">revived-haze-2</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/2cw41h82\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018b/runs/2cw41h82</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_120805-2cw41h82/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2cw41h82). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/31oc7joh\" target=\"_blank\">upbeat-night-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-12:10:18] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:10:18] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:10:18] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 556ms/step - loss: 1.0435 - val_loss: 0.6008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.6354 - val_loss: 0.7249\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.5555 - val_loss: 0.4956\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4959 - val_loss: 0.4898\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 180ms/step - loss: 0.4543 - val_loss: 0.5675\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 181ms/step - loss: 0.4630 - val_loss: 0.4848\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4548 - val_loss: 0.4814\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4175 - val_loss: 0.4785\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4107 - val_loss: 0.5127\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 180ms/step - loss: 0.4098 - val_loss: 0.4841\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.4075 - val_loss: 0.4985\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-12:10:40] Training the entire fine-tuned model...\n",
      "[2022_04_20-12:10:55] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4079WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0960s vs `on_train_batch_end` time: 0.1246s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0960s vs `on_train_batch_end` time: 0.1246s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 693ms/step - loss: 0.4079 - val_loss: 0.4725\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4151 - val_loss: 0.5019\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.4082 - val_loss: 0.4770\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.4068 - val_loss: 0.4740\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_20-12:11:10] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-12:11:10] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:11:11] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4051WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0956s vs `on_train_batch_end` time: 0.1252s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0956s vs `on_train_batch_end` time: 0.1252s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 678ms/step - loss: 0.4051 - val_loss: 0.4724\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>508</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>137</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  508  8\n",
       "1  137  7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0880503144654088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:31oc7joh) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 19678... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▅▅▆▇▇█▁▂▂▃▁</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▅█▂▁▄▁▁▁▂▁▂▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.4724</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.40509</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.4724</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">upbeat-night-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/31oc7joh\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018a/runs/31oc7joh</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_121001-31oc7joh/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:31oc7joh). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/2xluzegn\" target=\"_blank\">jumping-bee-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-12:12:05] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:12:05] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:12:05] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 587ms/step - loss: 0.8267 - val_loss: 0.7715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.6318 - val_loss: 0.4676\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5208 - val_loss: 0.5436\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5860 - val_loss: 0.5409\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4787 - val_loss: 0.4494\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4747 - val_loss: 0.4600\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4455 - val_loss: 0.4702\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4307 - val_loss: 0.4616\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-12:12:25] Training the entire fine-tuned model...\n",
      "[2022_04_20-12:12:43] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4631WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0990s vs `on_train_batch_end` time: 0.1245s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0990s vs `on_train_batch_end` time: 0.1245s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 717ms/step - loss: 0.4631 - val_loss: 0.4486\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.4609 - val_loss: 0.4475\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.4414 - val_loss: 0.4611\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.4343 - val_loss: 0.4455\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.4311 - val_loss: 0.4478\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.4265 - val_loss: 0.4439\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.4244 - val_loss: 0.4409\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.4088 - val_loss: 0.4437\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.4096 - val_loss: 0.4392\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3911 - val_loss: 0.4395\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3864 - val_loss: 0.4358\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3750 - val_loss: 0.4313\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3567 - val_loss: 0.4319\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3617 - val_loss: 0.4327\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3476 - val_loss: 0.4804\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_20-12:13:19] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-12:13:19] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:13:19] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3616WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0995s vs `on_train_batch_end` time: 0.1264s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0995s vs `on_train_batch_end` time: 0.1264s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 690ms/step - loss: 0.3616 - val_loss: 0.4331\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  512  20\n",
       "1  106  20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24096385542168675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2xluzegn) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 19855... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▁▁▂▃▃▃▄▅▅▅▆▇▇▇█▁</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▂▂▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▃▃▁▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>11</td></tr><tr><td>best_val_loss</td><td>0.43133</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36165</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.4331</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">jumping-bee-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/2xluzegn\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018b/runs/2xluzegn</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_121148-2xluzegn/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2xluzegn). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/51x455ek\" target=\"_blank\">youthful-resonance-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-12:14:13] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:14:14] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:14:14] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 582ms/step - loss: 0.8751 - val_loss: 0.6613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5518 - val_loss: 0.5165\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4572 - val_loss: 0.5279\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4507 - val_loss: 0.5153\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4675 - val_loss: 0.4871\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4216 - val_loss: 0.4892\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4290 - val_loss: 0.5031\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4411 - val_loss: 0.4741\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4046 - val_loss: 0.4920\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4083 - val_loss: 0.4913\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4104 - val_loss: 0.4986\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-12:14:36] Training the entire fine-tuned model...\n",
      "[2022_04_20-12:14:50] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4384WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0980s vs `on_train_batch_end` time: 0.1246s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0980s vs `on_train_batch_end` time: 0.1246s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 700ms/step - loss: 0.4384 - val_loss: 0.5535\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.4677 - val_loss: 0.5765\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.4464 - val_loss: 0.4877\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.4149 - val_loss: 0.4951\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3934 - val_loss: 0.4787\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3923 - val_loss: 0.4725\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3590 - val_loss: 0.4912\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3447 - val_loss: 0.4742\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3256 - val_loss: 0.5102\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_20-12:15:17] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-12:15:17] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:15:17] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3713WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0988s vs `on_train_batch_end` time: 0.1226s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0988s vs `on_train_batch_end` time: 0.1226s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 688ms/step - loss: 0.3713 - val_loss: 0.4683\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>507</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  507  9\n",
       "1  138  6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07547169811320754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:51x455ek) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 20072... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▅▅▆▇▇█▁▂▂▃▄▅▅▆▇▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▂▂▂▂▂▂▂▃▃▂▂▂▁▁▁▂</td></tr><tr><td>lr</td><td>███████████▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▃▂▂▂▁▂▂▂▄▅▂▂▁▁▂▁▃▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.46832</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37133</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.46832</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">youthful-resonance-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/51x455ek\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018a/runs/51x455ek</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_121357-51x455ek/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:51x455ek). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/2pvpuljv\" target=\"_blank\">autumn-puddle-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-12:16:11] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:16:11] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:16:11] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 574ms/step - loss: 0.8998 - val_loss: 0.8694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.6812 - val_loss: 0.4864\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5261 - val_loss: 0.4814\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5222 - val_loss: 0.4913\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4765 - val_loss: 0.4514\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4452 - val_loss: 0.4460\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4414 - val_loss: 0.4733\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4373 - val_loss: 0.4557\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4357 - val_loss: 0.4438\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4280 - val_loss: 0.4426\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4216 - val_loss: 0.4369\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4086 - val_loss: 0.4394\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3923 - val_loss: 0.4323\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3848 - val_loss: 0.4317\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3889 - val_loss: 0.4334\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3939 - val_loss: 0.4315\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3966 - val_loss: 0.6816\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4702 - val_loss: 0.4379\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4179 - val_loss: 0.5515\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-12:16:43] Training the entire fine-tuned model...\n",
      "[2022_04_20-12:17:09] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4452WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1013s vs `on_train_batch_end` time: 0.1224s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1013s vs `on_train_batch_end` time: 0.1224s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 660ms/step - loss: 0.4452 - val_loss: 0.5924\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.4354 - val_loss: 0.4407\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3989 - val_loss: 0.4498\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3953 - val_loss: 0.4447\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3785 - val_loss: 0.4387\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3618 - val_loss: 0.4413\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3451 - val_loss: 0.5249\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3584 - val_loss: 0.5513\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_20-12:17:32] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-12:17:32] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:17:32] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3507WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0980s vs `on_train_batch_end` time: 0.1253s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0980s vs `on_train_batch_end` time: 0.1253s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 675ms/step - loss: 0.3507 - val_loss: 0.4355\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>521</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>113</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  521  11\n",
       "1  113  13"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17333333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2pvpuljv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 20273... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██▁▁▂▂▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▅▁▃▄▁▁▁▁▁▂▃▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>15</td></tr><tr><td>best_val_loss</td><td>0.4315</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35065</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.43548</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">autumn-puddle-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/2pvpuljv\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018b/runs/2pvpuljv</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_121555-2pvpuljv/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2pvpuljv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/3srajx26\" target=\"_blank\">winter-disco-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-12:18:25] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:18:25] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:18:25] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 602ms/step - loss: 0.8659 - val_loss: 0.7050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5571 - val_loss: 0.5570\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4969 - val_loss: 0.4993\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4516 - val_loss: 0.4887\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4257 - val_loss: 0.4844\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4276 - val_loss: 0.5506\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4428 - val_loss: 0.4867\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4199 - val_loss: 0.4660\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4039 - val_loss: 0.4664\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4038 - val_loss: 0.5258\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4090 - val_loss: 0.4693\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3880 - val_loss: 0.4736\n",
      "[2022_04_20-12:18:50] Training the entire fine-tuned model...\n",
      "[2022_04_20-12:19:03] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4084WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0987s vs `on_train_batch_end` time: 0.1266s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0987s vs `on_train_batch_end` time: 0.1266s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 698ms/step - loss: 0.4084 - val_loss: 0.4689\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.4019 - val_loss: 0.4726\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3914 - val_loss: 0.4763\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3963 - val_loss: 0.4726\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.4001 - val_loss: 0.4708\n",
      "[2022_04_20-12:19:21] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-12:19:21] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:19:21] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3996WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0992s vs `on_train_batch_end` time: 0.1225s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0992s vs `on_train_batch_end` time: 0.1225s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 670ms/step - loss: 0.3996 - val_loss: 0.4687\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>512</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>142</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  512  4\n",
       "1  142  2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3srajx26) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 20510... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█▁▂▂▃▄▁</td></tr><tr><td>loss</td><td>█▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████▃▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▂▂▃▂▁▁▃▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.46603</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.39959</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.46871</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">winter-disco-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/3srajx26\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018a/runs/3srajx26</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_121810-3srajx26/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3srajx26). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/h3p9xsob\" target=\"_blank\">misty-breeze-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-12:20:14] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:20:14] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:20:15] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 610ms/step - loss: 0.9631 - val_loss: 1.4046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.8431 - val_loss: 0.7434\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.6260 - val_loss: 0.6085\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5549 - val_loss: 0.4646\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4710 - val_loss: 0.4544\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4568 - val_loss: 0.4457\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4506 - val_loss: 0.4507\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4598 - val_loss: 0.4564\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4291 - val_loss: 0.4390\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4287 - val_loss: 0.4373\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4170 - val_loss: 0.4425\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4089 - val_loss: 0.4831\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4393 - val_loss: 0.5630\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5246 - val_loss: 0.4371\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4662 - val_loss: 0.4942\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4021 - val_loss: 0.4522\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 199ms/step - loss: 0.4167 - val_loss: 0.4506\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3994 - val_loss: 0.4555\n",
      "[2022_04_20-12:20:46] Training the entire fine-tuned model...\n",
      "[2022_04_20-12:20:54] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4076WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0992s vs `on_train_batch_end` time: 0.1237s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0992s vs `on_train_batch_end` time: 0.1237s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 693ms/step - loss: 0.4076 - val_loss: 0.4333\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3939 - val_loss: 0.4322\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3964 - val_loss: 0.4322\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.4041 - val_loss: 0.4337\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3936 - val_loss: 0.4351\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.3964 - val_loss: 0.4345\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3884 - val_loss: 0.4339\n",
      "[2022_04_20-12:21:16] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-12:21:16] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:21:16] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3922WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0986s vs `on_train_batch_end` time: 0.1255s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0986s vs `on_train_batch_end` time: 0.1255s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 689ms/step - loss: 0.3922 - val_loss: 0.4321\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>526</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  526  6\n",
       "1  120  6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08695652173913043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:h3p9xsob) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 20702... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██▁▁▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▇▄▃▂▂▂▂▁▁▁▁▂▃▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.43214</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.39218</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43214</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">misty-breeze-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/h3p9xsob\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018b/runs/h3p9xsob</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_121958-h3p9xsob/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:h3p9xsob). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/24src7qg\" target=\"_blank\">rich-leaf-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-12:22:16] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:22:16] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:22:16] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 571ms/step - loss: 0.9585 - val_loss: 0.6630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5652 - val_loss: 0.6015\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5027 - val_loss: 0.5171\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4531 - val_loss: 0.5033\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4552 - val_loss: 0.5329\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4513 - val_loss: 0.4981\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4275 - val_loss: 0.4874\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4246 - val_loss: 0.5203\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4396 - val_loss: 0.6198\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4574 - val_loss: 0.4703\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4039 - val_loss: 0.4698\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4073 - val_loss: 0.5291\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4046 - val_loss: 0.4709\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3983 - val_loss: 0.4918\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3930 - val_loss: 0.4802\n",
      "[2022_04_20-12:22:44] Training the entire fine-tuned model...\n",
      "[2022_04_20-12:22:52] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3923WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1243s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1243s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 682ms/step - loss: 0.3923 - val_loss: 0.4760\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.4008 - val_loss: 0.4675\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3839 - val_loss: 0.4803\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3884 - val_loss: 0.4958\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3824 - val_loss: 0.4683\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3763 - val_loss: 0.4683\n",
      "[2022_04_20-12:23:12] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-12:23:12] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:23:19] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3964WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1211s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1211s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 832ms/step - loss: 0.3964 - val_loss: 0.4667\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>496</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  496  20\n",
       "1  126  18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1978021978021978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:24src7qg) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 20931... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█▁▁▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████▃▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▃▂▃▂▂▃▆▁▁▃▁▂▁▁▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.46674</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.39639</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.46674</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">rich-leaf-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/24src7qg\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018a/runs/24src7qg</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_122159-24src7qg/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:24src7qg). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/2obskyrp\" target=\"_blank\">good-puddle-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-12:24:13] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:24:13] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:24:13] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 777ms/step - loss: 0.9885 - val_loss: 1.0326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.6923 - val_loss: 0.6313\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.5880 - val_loss: 0.4643\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5399 - val_loss: 0.5180\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5209 - val_loss: 0.5542\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.5158 - val_loss: 0.6219\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4990 - val_loss: 0.4443\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4917 - val_loss: 0.4618\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4437 - val_loss: 0.4855\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4539 - val_loss: 0.4399\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4424 - val_loss: 0.4390\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4318 - val_loss: 0.4476\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4254 - val_loss: 0.4392\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4208 - val_loss: 0.4392\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4194 - val_loss: 0.4409\n",
      "[2022_04_20-12:24:41] Training the entire fine-tuned model...\n",
      "[2022_04_20-12:24:49] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4338WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0986s vs `on_train_batch_end` time: 0.1243s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0986s vs `on_train_batch_end` time: 0.1243s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 692ms/step - loss: 0.4338 - val_loss: 0.4405\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.4306 - val_loss: 0.4396\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.4313 - val_loss: 0.4506\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.4217 - val_loss: 0.4376\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.4222 - val_loss: 0.4372\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.4102 - val_loss: 0.4410\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.4097 - val_loss: 0.4388\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.4049 - val_loss: 0.4360\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.4100 - val_loss: 0.4502\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3931 - val_loss: 0.4368\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3910 - val_loss: 0.4387\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3948 - val_loss: 0.4425\n",
      "[2022_04_20-12:25:20] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-12:25:20] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:25:20] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4009WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0974s vs `on_train_batch_end` time: 0.1262s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0974s vs `on_train_batch_end` time: 0.1262s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 694ms/step - loss: 0.4009 - val_loss: 0.4363\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>525</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  525  7\n",
       "1  123  3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04411764705882353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2obskyrp) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 21138... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█▁▁▂▃▃▃▄▅▅▅▆▇▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▁▂▂▃▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.43605</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.4009</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43628</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">good-puddle-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/2obskyrp\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018b/runs/2obskyrp</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_122356-2obskyrp/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2obskyrp). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/2k0q1fbc\" target=\"_blank\">upbeat-snowball-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-12:26:15] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:26:15] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:26:15] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 580ms/step - loss: 0.8846 - val_loss: 1.0325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.7979 - val_loss: 0.8675\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.6093 - val_loss: 0.5815\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5209 - val_loss: 0.5364\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4992 - val_loss: 0.4966\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4336 - val_loss: 0.4862\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4217 - val_loss: 0.5281\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4326 - val_loss: 0.4948\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4301 - val_loss: 0.5242\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4163 - val_loss: 0.4712\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4106 - val_loss: 0.4710\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4132 - val_loss: 0.5042\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4047 - val_loss: 0.4687\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.3969 - val_loss: 0.4681\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3958 - val_loss: 0.4739\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4029 - val_loss: 0.4739\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3952 - val_loss: 0.4673\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3954 - val_loss: 0.4688\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3953 - val_loss: 0.4797\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3932 - val_loss: 0.4653\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3924 - val_loss: 0.4715\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3930 - val_loss: 0.4714\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3905 - val_loss: 0.4646\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4016 - val_loss: 0.4904\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3797 - val_loss: 0.4637\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3863 - val_loss: 0.4754\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3798 - val_loss: 0.4658\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3749 - val_loss: 0.4691\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3803 - val_loss: 0.4747\n",
      "[2022_04_20-12:26:57] Training the entire fine-tuned model...\n",
      "[2022_04_20-12:27:13] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3869WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1008s vs `on_train_batch_end` time: 0.1226s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1008s vs `on_train_batch_end` time: 0.1226s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 679ms/step - loss: 0.3869 - val_loss: 0.4888\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3821 - val_loss: 0.4667\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3923 - val_loss: 0.4990\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.4037 - val_loss: 0.4682\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3852 - val_loss: 0.4612\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3686 - val_loss: 0.4922\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3598 - val_loss: 0.4624\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3508 - val_loss: 0.4905\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3423 - val_loss: 0.4772\n",
      "[2022_04_20-12:27:38] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-12:27:38] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:27:40] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3691WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0996s vs `on_train_batch_end` time: 0.1246s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0996s vs `on_train_batch_end` time: 0.1246s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 693ms/step - loss: 0.3691 - val_loss: 0.4611\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>505</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  505  11\n",
       "1  132  12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1437125748502994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2k0q1fbc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 21374... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██▁▁▁▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▇▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▂▂▁▁▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.46113</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36911</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.46113</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">upbeat-snowball-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/2k0q1fbc\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018a/runs/2k0q1fbc</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_122558-2k0q1fbc/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2k0q1fbc). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/1c4zf49a\" target=\"_blank\">wild-planet-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-12:28:35] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:28:35] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:28:35] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 570ms/step - loss: 0.9609 - val_loss: 0.8160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.6889 - val_loss: 0.4835\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5645 - val_loss: 0.5271\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5758 - val_loss: 0.5799\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4981 - val_loss: 0.5018\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5009 - val_loss: 0.4529\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4798 - val_loss: 0.4863\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4472 - val_loss: 0.4499\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4459 - val_loss: 0.4556\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4416 - val_loss: 0.4468\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4479 - val_loss: 0.4432\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4414 - val_loss: 0.4515\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4292 - val_loss: 0.4418\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4358 - val_loss: 0.4560\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4334 - val_loss: 0.4526\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4277 - val_loss: 0.4412\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4154 - val_loss: 0.4521\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4217 - val_loss: 0.4426\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4162 - val_loss: 0.4418\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4099 - val_loss: 0.4441\n",
      "[2022_04_20-12:29:09] Training the entire fine-tuned model...\n",
      "[2022_04_20-12:29:33] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4255WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0992s vs `on_train_batch_end` time: 0.1220s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0992s vs `on_train_batch_end` time: 0.1220s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 681ms/step - loss: 0.4255 - val_loss: 0.4503\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.4181 - val_loss: 0.4393\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.4182 - val_loss: 0.4601\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.4049 - val_loss: 0.4360\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.4055 - val_loss: 0.4434\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.4056 - val_loss: 0.4380\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3939 - val_loss: 0.4322\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3847 - val_loss: 0.4308\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3818 - val_loss: 0.4355\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3663 - val_loss: 0.4358\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3594 - val_loss: 0.5048\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3731 - val_loss: 0.4377\n",
      "[2022_04_20-12:30:05] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-12:30:05] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:30:05] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3821WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0997s vs `on_train_batch_end` time: 0.1257s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0997s vs `on_train_batch_end` time: 0.1257s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 674ms/step - loss: 0.3821 - val_loss: 0.4296\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>524</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  524  8\n",
       "1  120  6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08571428571428572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1c4zf49a) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 21667... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▂▃▃▄▄▄▅▅▁</td></tr><tr><td>loss</td><td>█▅▃▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▃▄▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.42963</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.38205</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.42963</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">wild-planet-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/1c4zf49a\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018b/runs/1c4zf49a</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_122819-1c4zf49a/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1c4zf49a). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/sf3ln5ed\" target=\"_blank\">playful-sponge-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-12:30:58] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:30:58] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:30:58] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 583ms/step - loss: 0.8906 - val_loss: 0.9447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.6407 - val_loss: 0.8726\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5732 - val_loss: 0.6022\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.4846 - val_loss: 0.5799\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4689 - val_loss: 0.4931\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4523 - val_loss: 0.5072\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4418 - val_loss: 0.5913\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4761 - val_loss: 0.4929\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4214 - val_loss: 0.4767\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4619 - val_loss: 0.5252\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3995 - val_loss: 0.4740\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4074 - val_loss: 0.4684\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4025 - val_loss: 0.6092\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4251 - val_loss: 0.4820\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4271 - val_loss: 0.4714\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3703 - val_loss: 0.4676\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3768 - val_loss: 0.4665\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3868 - val_loss: 0.4748\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3698 - val_loss: 0.4669\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3668 - val_loss: 0.4758\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3673 - val_loss: 0.4753\n",
      "[2022_04_20-12:31:33] Training the entire fine-tuned model...\n",
      "[2022_04_20-12:31:56] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4479WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0998s vs `on_train_batch_end` time: 0.1231s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0998s vs `on_train_batch_end` time: 0.1231s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 709ms/step - loss: 0.4479 - val_loss: 0.4700\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.4411 - val_loss: 0.6408\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.4342 - val_loss: 0.4650\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3879 - val_loss: 0.4950\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3924 - val_loss: 0.4945\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3832 - val_loss: 0.4627\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3948 - val_loss: 0.5741\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3825 - val_loss: 0.4800\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3390 - val_loss: 0.5289\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3148 - val_loss: 0.4697\n",
      "[2022_04_20-12:32:24] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-12:32:24] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:32:44] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3602WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1000s vs `on_train_batch_end` time: 0.1249s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1000s vs `on_train_batch_end` time: 0.1249s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 707ms/step - loss: 0.3602 - val_loss: 0.4608\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>499</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  499  17\n",
       "1  126  18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2011173184357542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:sf3ln5ed) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 21936... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▃▃▂▂▂▂▂▂▁▁▂</td></tr><tr><td>lr</td><td>███████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▃▃▁▂▃▁▁▂▁▁▃▁▁▁▁▁▁▁▁▁▄▁▁▁▁▃▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.46079</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36021</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.46079</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">playful-sponge-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/sf3ln5ed\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018a/runs/sf3ln5ed</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_123042-sf3ln5ed/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:sf3ln5ed). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/1q2ixtyv\" target=\"_blank\">dry-totem-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-12:33:38] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:33:38] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:33:38] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 576ms/step - loss: 0.8049 - val_loss: 1.1894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.7665 - val_loss: 0.6464\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5998 - val_loss: 0.4591\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5395 - val_loss: 0.5632\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 201ms/step - loss: 0.5335 - val_loss: 0.4844\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4885 - val_loss: 0.4606\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4443 - val_loss: 0.4450\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4392 - val_loss: 0.4473\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4299 - val_loss: 0.4488\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4354 - val_loss: 0.4450\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4322 - val_loss: 0.4434\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4302 - val_loss: 0.4428\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4345 - val_loss: 0.4431\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4328 - val_loss: 0.4430\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4268 - val_loss: 0.4426\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4352 - val_loss: 0.4435\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4317 - val_loss: 0.4457\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4289 - val_loss: 0.4494\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4274 - val_loss: 0.4470\n",
      "[2022_04_20-12:34:09] Training the entire fine-tuned model...\n",
      "[2022_04_20-12:34:19] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4537WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1021s vs `on_train_batch_end` time: 0.1220s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1021s vs `on_train_batch_end` time: 0.1220s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 694ms/step - loss: 0.4537 - val_loss: 0.4593\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.4300 - val_loss: 0.4564\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.4192 - val_loss: 0.4407\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.4031 - val_loss: 0.4430\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3900 - val_loss: 0.4521\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3660 - val_loss: 0.4687\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3446 - val_loss: 0.4682\n",
      "[2022_04_20-12:34:40] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-12:34:40] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:34:40] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3848WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1004s vs `on_train_batch_end` time: 0.1243s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1004s vs `on_train_batch_end` time: 0.1243s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 704ms/step - loss: 0.3848 - val_loss: 0.4488\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>522</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>113</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  522  10\n",
       "1  113  13"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.174496644295302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1q2ixtyv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 22192... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██▁▁▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▇▅▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▁▁▂</td></tr><tr><td>lr</td><td>██████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>2</td></tr><tr><td>best_val_loss</td><td>0.44073</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.38481</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.44881</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dry-totem-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/1q2ixtyv\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018b/runs/1q2ixtyv</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_123322-1q2ixtyv/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1q2ixtyv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/2rd2so9e\" target=\"_blank\">scarlet-shape-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-12:35:37] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:35:37] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:35:37] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 565ms/step - loss: 0.8098 - val_loss: 0.7262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.5871 - val_loss: 0.5107\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.5179 - val_loss: 0.6462\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5104 - val_loss: 0.5168\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4503 - val_loss: 0.4881\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4372 - val_loss: 0.5718\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.4660 - val_loss: 0.5194\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4412 - val_loss: 0.5011\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3947 - val_loss: 0.4719\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4124 - val_loss: 0.4719\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4015 - val_loss: 0.5113\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4218 - val_loss: 0.4776\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3925 - val_loss: 0.4707\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3997 - val_loss: 0.4713\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3967 - val_loss: 0.4820\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3932 - val_loss: 0.4822\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3962 - val_loss: 0.4811\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3936 - val_loss: 0.4785\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3972 - val_loss: 0.4769\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_20-12:36:09] Training the entire fine-tuned model...\n",
      "[2022_04_20-12:36:18] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3986WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0988s vs `on_train_batch_end` time: 0.1247s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0988s vs `on_train_batch_end` time: 0.1247s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 690ms/step - loss: 0.3986 - val_loss: 0.4760\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3945 - val_loss: 0.4783\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3941 - val_loss: 0.4787\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3957 - val_loss: 0.4747\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3847 - val_loss: 0.4756\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3872 - val_loss: 0.4780\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3856 - val_loss: 0.4720\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3890 - val_loss: 0.4702\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3898 - val_loss: 0.4715\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3842 - val_loss: 0.4811\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3947 - val_loss: 0.4834\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3751 - val_loss: 0.4806\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3802 - val_loss: 0.4779\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3904 - val_loss: 0.4757\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_20-12:36:53] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-12:36:53] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:36:53] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3904WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0983s vs `on_train_batch_end` time: 0.1244s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0983s vs `on_train_batch_end` time: 0.1244s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 708ms/step - loss: 0.3904 - val_loss: 0.4703\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>508</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  508  8\n",
       "1  138  6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0759493670886076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2rd2so9e) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 22424... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██▁▁▂▂▃▃▃▄▄▅▅▅▆▆▁</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▆▂▁▄▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.47023</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.39037</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.47033</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">scarlet-shape-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/2rd2so9e\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018a/runs/2rd2so9e</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_123521-2rd2so9e/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2rd2so9e). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/go862ocn\" target=\"_blank\">hardy-dew-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-12:37:47] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:37:47] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:37:48] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 579ms/step - loss: 1.0399 - val_loss: 0.8698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.6088 - val_loss: 0.5741\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5592 - val_loss: 0.4741\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.5011 - val_loss: 0.5078\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4900 - val_loss: 0.4479\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4552 - val_loss: 0.4440\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4371 - val_loss: 0.4448\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4286 - val_loss: 0.4535\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4309 - val_loss: 0.4778\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4616 - val_loss: 0.4436\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4405 - val_loss: 0.4483\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4255 - val_loss: 0.4473\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4198 - val_loss: 0.4468\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4103 - val_loss: 0.4439\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4084 - val_loss: 0.4369\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4119 - val_loss: 0.4344\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4010 - val_loss: 0.4354\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4049 - val_loss: 0.4402\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4069 - val_loss: 0.4385\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4014 - val_loss: 0.4384\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4030 - val_loss: 0.4374\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4005 - val_loss: 0.4367\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_20-12:38:23] Training the entire fine-tuned model...\n",
      "[2022_04_20-12:38:45] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4164WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1022s vs `on_train_batch_end` time: 0.1210s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1022s vs `on_train_batch_end` time: 0.1210s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 689ms/step - loss: 0.4164 - val_loss: 0.4387\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3985 - val_loss: 0.4385\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.4103 - val_loss: 0.4356\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3920 - val_loss: 0.4347\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.4020 - val_loss: 0.4388\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3996 - val_loss: 0.4384\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.4035 - val_loss: 0.4407\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.4029 - val_loss: 0.4403\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3923 - val_loss: 0.4389\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.4015 - val_loss: 0.4367\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_20-12:39:13] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-12:39:13] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:39:13] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4026WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0992s vs `on_train_batch_end` time: 0.1264s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0992s vs `on_train_batch_end` time: 0.1264s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 801ms/step - loss: 0.4026 - val_loss: 0.4345\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>526</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  526  6\n",
       "1  120  6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08695652173913043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:go862ocn) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 22690... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▃▃▂▂▂▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>15</td></tr><tr><td>best_val_loss</td><td>0.43441</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.40265</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43455</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">hardy-dew-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/go862ocn\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018b/runs/go862ocn</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_123731-go862ocn/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:go862ocn). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/3frkujtc\" target=\"_blank\">dauntless-aardvark-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-12:40:07] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:40:07] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:40:07] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 570ms/step - loss: 1.0341 - val_loss: 0.7469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.8452 - val_loss: 1.1022\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.9456 - val_loss: 0.5594\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5854 - val_loss: 0.5433\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.5630 - val_loss: 0.4912\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4576 - val_loss: 0.5078\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4540 - val_loss: 0.4839\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4330 - val_loss: 0.4833\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4224 - val_loss: 0.4909\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4121 - val_loss: 0.4707\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3993 - val_loss: 0.4780\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3963 - val_loss: 0.4667\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4095 - val_loss: 0.4647\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4026 - val_loss: 0.5040\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3826 - val_loss: 0.5076\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4131 - val_loss: 0.6280\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4442 - val_loss: 0.4676\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3863 - val_loss: 0.4773\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3842 - val_loss: 0.4935\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_20-12:40:39] Training the entire fine-tuned model...\n",
      "[2022_04_20-12:41:06] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3906WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1014s vs `on_train_batch_end` time: 0.1209s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1014s vs `on_train_batch_end` time: 0.1209s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 713ms/step - loss: 0.3906 - val_loss: 0.4802\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3939 - val_loss: 0.4647\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3814 - val_loss: 0.4684\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3882 - val_loss: 0.4748\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3855 - val_loss: 0.4805\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3773 - val_loss: 0.4695\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3791 - val_loss: 0.4671\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3779 - val_loss: 0.4668\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_20-12:41:29] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-12:41:29] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:41:29] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3866WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0991s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0991s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 713ms/step - loss: 0.3866 - val_loss: 0.4647\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>509</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>137</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  509  7\n",
       "1  137  7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08860759493670886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3frkujtc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 22952... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██▁▁▂▂▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▆▇▃▃▂▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████▃▃▃▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄█▂▂▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.46465</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.38656</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.46465</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dauntless-aardvark-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/3frkujtc\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018a/runs/3frkujtc</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_123951-3frkujtc/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3frkujtc). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/2nkfem1b\" target=\"_blank\">sparkling-donkey-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-12:42:24] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:42:24] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:42:24] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 594ms/step - loss: 0.9025 - val_loss: 0.8737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.6771 - val_loss: 0.4769\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5536 - val_loss: 0.5493\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.5578 - val_loss: 0.5327\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4709 - val_loss: 0.4482\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4476 - val_loss: 0.4451\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4473 - val_loss: 0.5059\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4266 - val_loss: 0.4532\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4369 - val_loss: 0.4624\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4203 - val_loss: 0.4397\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4055 - val_loss: 0.4556\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4273 - val_loss: 0.4389\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4235 - val_loss: 0.4381\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4036 - val_loss: 0.4533\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4078 - val_loss: 0.4423\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4082 - val_loss: 0.4431\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4065 - val_loss: 0.4396\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4040 - val_loss: 0.4359\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3970 - val_loss: 0.4356\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3994 - val_loss: 0.4357\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3978 - val_loss: 0.4407\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4065 - val_loss: 0.4444\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4028 - val_loss: 0.4438\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4018 - val_loss: 0.4425\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4022 - val_loss: 0.4382\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_20-12:43:03] Training the entire fine-tuned model...\n",
      "[2022_04_20-12:43:22] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4099WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0965s vs `on_train_batch_end` time: 0.1244s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0965s vs `on_train_batch_end` time: 0.1244s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 678ms/step - loss: 0.4099 - val_loss: 0.4360\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.4066 - val_loss: 0.4346\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3938 - val_loss: 0.4355\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3892 - val_loss: 0.4364\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3866 - val_loss: 0.4377\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3883 - val_loss: 0.4359\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3893 - val_loss: 0.4344\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3869 - val_loss: 0.4382\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3831 - val_loss: 0.4412\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3814 - val_loss: 0.4391\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3868 - val_loss: 0.4375\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3838 - val_loss: 0.4364\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3905 - val_loss: 0.4358\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "[2022_04_20-12:43:55] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-12:43:55] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:44:09] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3881WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0961s vs `on_train_batch_end` time: 0.1279s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0961s vs `on_train_batch_end` time: 0.1279s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 697ms/step - loss: 0.3881 - val_loss: 0.4343\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>525</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  525   7\n",
       "1  116  10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13986013986013987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2nkfem1b) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 23207... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▄▄▄▅▁</td></tr><tr><td>loss</td><td>█▅▃▃▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▃▃▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.43434</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.38808</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43434</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">sparkling-donkey-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/2nkfem1b\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018b/runs/2nkfem1b</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_124207-2nkfem1b/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2nkfem1b). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/15g20lg0\" target=\"_blank\">smooth-grass-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-12:45:02] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:45:02] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:45:02] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 590ms/step - loss: 0.8353 - val_loss: 0.8340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.6687 - val_loss: 0.6634\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4995 - val_loss: 0.5111\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4890 - val_loss: 0.4897\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4386 - val_loss: 0.5110\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4319 - val_loss: 0.4788\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4131 - val_loss: 0.5321\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4389 - val_loss: 0.4938\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4232 - val_loss: 0.4742\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4005 - val_loss: 0.4740\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4173 - val_loss: 0.4750\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4153 - val_loss: 0.5476\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4083 - val_loss: 0.4696\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3992 - val_loss: 0.4806\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3841 - val_loss: 0.5003\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3867 - val_loss: 0.4910\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3701 - val_loss: 0.4705\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.3815 - val_loss: 0.4690\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3697 - val_loss: 0.4799\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3576 - val_loss: 0.4691\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3619 - val_loss: 0.4722\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3667 - val_loss: 0.4686\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3660 - val_loss: 0.4709\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3683 - val_loss: 0.4703\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3598 - val_loss: 0.4700\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3673 - val_loss: 0.4692\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3539 - val_loss: 0.4706\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3603 - val_loss: 0.4716\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_20-12:45:44] Training the entire fine-tuned model...\n",
      "[2022_04_20-12:45:53] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3688WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0977s vs `on_train_batch_end` time: 0.1250s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0977s vs `on_train_batch_end` time: 0.1250s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 672ms/step - loss: 0.3688 - val_loss: 0.4634\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3804 - val_loss: 0.4775\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3654 - val_loss: 0.4913\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3565 - val_loss: 0.4666\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3521 - val_loss: 0.4685\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3585 - val_loss: 0.4749\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3519 - val_loss: 0.4717\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_20-12:46:14] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-12:46:14] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:46:19] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3773WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0989s vs `on_train_batch_end` time: 0.1234s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0989s vs `on_train_batch_end` time: 0.1234s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 695ms/step - loss: 0.3773 - val_loss: 0.4631\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>498</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  498  18\n",
       "1  127  17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18994413407821228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:15g20lg0) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 23500... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▆▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▂▂▂▁▂▂▁▁▁▃▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.46314</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37731</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.46314</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">smooth-grass-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/15g20lg0\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018a/runs/15g20lg0</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_124446-15g20lg0/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:15g20lg0). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/3nv0jn3c\" target=\"_blank\">fresh-cosmos-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-12:47:15] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:47:15] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:47:15] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 576ms/step - loss: 0.9278 - val_loss: 0.6686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.6432 - val_loss: 0.4854\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5430 - val_loss: 0.5087\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.6142 - val_loss: 0.5061\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4819 - val_loss: 0.4825\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4743 - val_loss: 0.5225\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4600 - val_loss: 0.4448\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4382 - val_loss: 0.4453\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4287 - val_loss: 0.4427\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4318 - val_loss: 0.5054\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4369 - val_loss: 0.4384\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4093 - val_loss: 0.4652\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4509 - val_loss: 0.5010\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4264 - val_loss: 0.4386\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3925 - val_loss: 0.4391\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3927 - val_loss: 0.4357\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3973 - val_loss: 0.4399\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3930 - val_loss: 0.4447\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3882 - val_loss: 0.4350\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3854 - val_loss: 0.4568\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3847 - val_loss: 0.4331\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3827 - val_loss: 0.4329\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3774 - val_loss: 0.4354\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3730 - val_loss: 0.4359\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3774 - val_loss: 0.4318\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3750 - val_loss: 0.4386\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3751 - val_loss: 0.4309\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3831 - val_loss: 0.4305\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3719 - val_loss: 0.4441\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4089 - val_loss: 0.4315\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3783 - val_loss: 0.4374\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3897 - val_loss: 0.4299\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3721 - val_loss: 0.4404\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3797 - val_loss: 0.4446\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3794 - val_loss: 0.4307\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3798 - val_loss: 0.4299\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3747 - val_loss: 0.4302\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3709 - val_loss: 0.4308\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_20-12:48:09] Training the entire fine-tuned model...\n",
      "[2022_04_20-12:48:55] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3860WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1000s vs `on_train_batch_end` time: 0.1227s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1000s vs `on_train_batch_end` time: 0.1227s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 706ms/step - loss: 0.3860 - val_loss: 0.4306\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3767 - val_loss: 0.4318\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3658 - val_loss: 0.4369\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3583 - val_loss: 0.4336\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3466 - val_loss: 0.4359\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3420 - val_loss: 0.4274\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3547 - val_loss: 0.4276\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3442 - val_loss: 0.4308\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3290 - val_loss: 0.4259\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3422 - val_loss: 0.4286\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3370 - val_loss: 0.4289\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3436 - val_loss: 0.4270\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3295 - val_loss: 0.4278\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3309 - val_loss: 0.4306\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3275 - val_loss: 0.4336\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "[2022_04_20-12:49:31] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-12:49:31] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:49:31] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3427WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0997s vs `on_train_batch_end` time: 0.1227s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0997s vs `on_train_batch_end` time: 0.1227s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 686ms/step - loss: 0.3427 - val_loss: 0.4274\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>514</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  514  18\n",
       "1  103  23"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2754491017964072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3nv0jn3c) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 23778... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▁▂▂▂▃▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▃▄▂▁▃▁▃▁▁▁▂▂▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_val_loss</td><td>0.42592</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34269</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.42743</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fresh-cosmos-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/3nv0jn3c\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018b/runs/3nv0jn3c</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_124658-3nv0jn3c/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3nv0jn3c). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/16aqkwsz\" target=\"_blank\">stoic-grass-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-12:50:25] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:50:25] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:50:25] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 607ms/step - loss: 0.7977 - val_loss: 0.7253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.6270 - val_loss: 0.5110\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5143 - val_loss: 0.6506\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5147 - val_loss: 0.5172\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4689 - val_loss: 0.4887\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4423 - val_loss: 0.5853\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4780 - val_loss: 0.5052\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4449 - val_loss: 0.4806\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4025 - val_loss: 0.4857\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3999 - val_loss: 0.4727\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4022 - val_loss: 0.4729\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3851 - val_loss: 0.4684\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4022 - val_loss: 0.4820\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3954 - val_loss: 0.4657\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3750 - val_loss: 0.4634\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3765 - val_loss: 0.5265\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3778 - val_loss: 0.4676\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4028 - val_loss: 0.5017\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4181 - val_loss: 0.5039\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3804 - val_loss: 0.4839\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3685 - val_loss: 0.4686\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_20-12:51:00] Training the entire fine-tuned model...\n",
      "[2022_04_20-12:51:23] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4701WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0997s vs `on_train_batch_end` time: 0.1241s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0997s vs `on_train_batch_end` time: 0.1241s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 687ms/step - loss: 0.4701 - val_loss: 0.7022\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.4457 - val_loss: 0.4920\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.4090 - val_loss: 0.5154\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3742 - val_loss: 0.4656\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3657 - val_loss: 0.4710\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3412 - val_loss: 0.5310\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3634 - val_loss: 0.4914\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3235 - val_loss: 0.4890\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3101 - val_loss: 0.4795\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.2940 - val_loss: 0.5048\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_20-12:51:51] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-12:51:51] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:52:10] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3711WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1010s vs `on_train_batch_end` time: 0.1208s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1010s vs `on_train_batch_end` time: 0.1208s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 852ms/step - loss: 0.3711 - val_loss: 0.4710\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>509</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>140</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  509  7\n",
       "1  140  4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05161290322580645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:16aqkwsz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 24156... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▆▄▄▃▃▄▃▃▂▃▂▃▂▂▂▂▃▃▂▂▃▃▃▂▂▂▂▁▁▁▂</td></tr><tr><td>lr</td><td>██████████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▆▂▂▄▂▁▂▁▁▁▁▁▁▃▁▂▂▂▁▇▂▂▁▁▃▂▂▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>14</td></tr><tr><td>best_val_loss</td><td>0.46339</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37112</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.47105</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">stoic-grass-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/16aqkwsz\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018a/runs/16aqkwsz</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_125008-16aqkwsz/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:16aqkwsz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/2ioxlla6\" target=\"_blank\">rosy-valley-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-12:53:03] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:53:04] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:53:04] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 574ms/step - loss: 0.8747 - val_loss: 0.8230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.6441 - val_loss: 0.4932\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.5532 - val_loss: 0.5175\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5261 - val_loss: 0.5136\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4725 - val_loss: 0.4456\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4724 - val_loss: 0.5129\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5010 - val_loss: 0.6393\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4992 - val_loss: 0.4467\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4310 - val_loss: 0.4554\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4315 - val_loss: 0.4385\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4193 - val_loss: 0.4397\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4176 - val_loss: 0.4415\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4170 - val_loss: 0.4406\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4017 - val_loss: 0.4415\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4109 - val_loss: 0.4403\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4117 - val_loss: 0.4395\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_20-12:53:31] Training the entire fine-tuned model...\n",
      "[2022_04_20-12:53:41] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.6355WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1011s vs `on_train_batch_end` time: 0.1232s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1011s vs `on_train_batch_end` time: 0.1232s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 688ms/step - loss: 0.6355 - val_loss: 0.4500\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.4870 - val_loss: 0.4506\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.4476 - val_loss: 0.4604\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.4311 - val_loss: 0.4430\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.4011 - val_loss: 0.4729\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.4005 - val_loss: 0.4595\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3976 - val_loss: 0.4846\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3685 - val_loss: 0.4398\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3603 - val_loss: 0.4405\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3498 - val_loss: 0.4465\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3473 - val_loss: 0.4425\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3445 - val_loss: 0.4407\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3379 - val_loss: 0.4420\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3418 - val_loss: 0.4449\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "[2022_04_20-12:54:16] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-12:54:16] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:54:16] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3606WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1002s vs `on_train_batch_end` time: 0.1226s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1002s vs `on_train_batch_end` time: 0.1226s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 667ms/step - loss: 0.3606 - val_loss: 0.4416\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>513</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  513  19\n",
       "1  112  14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1761006289308176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2ioxlla6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 24422... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██▁▁▂▂▃▃▄▄▅▅▆▆▇▇▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▅▃▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▂▁▂▅▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>9</td></tr><tr><td>best_val_loss</td><td>0.43854</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36065</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.44163</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">rosy-valley-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/2ioxlla6\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018b/runs/2ioxlla6</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_125248-2ioxlla6/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2ioxlla6). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/3and948c\" target=\"_blank\">comfy-sky-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-12:55:12] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:55:12] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:55:12] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 796ms/step - loss: 0.9307 - val_loss: 0.8488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.6726 - val_loss: 0.7283\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5178 - val_loss: 0.5068\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5005 - val_loss: 0.4922\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4924 - val_loss: 0.6006\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4518 - val_loss: 0.5529\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4433 - val_loss: 0.6536\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4773 - val_loss: 0.4931\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4218 - val_loss: 0.4958\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4147 - val_loss: 0.4909\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4100 - val_loss: 0.4720\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4011 - val_loss: 0.4806\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4045 - val_loss: 0.4763\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3998 - val_loss: 0.4746\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3941 - val_loss: 0.4836\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3965 - val_loss: 0.4788\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3943 - val_loss: 0.4791\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3945 - val_loss: 0.4779\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3964 - val_loss: 0.4722\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_20-12:55:45] Training the entire fine-tuned model...\n",
      "[2022_04_20-12:56:09] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4087WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0976s vs `on_train_batch_end` time: 0.1252s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0976s vs `on_train_batch_end` time: 0.1252s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 695ms/step - loss: 0.4087 - val_loss: 0.4759\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.4078 - val_loss: 0.4861\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3984 - val_loss: 0.4853\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3976 - val_loss: 0.4796\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.4037 - val_loss: 0.4724\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.4018 - val_loss: 0.4729\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3963 - val_loss: 0.4772\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3945 - val_loss: 0.4782\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3957 - val_loss: 0.4765\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3935 - val_loss: 0.4766\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3947 - val_loss: 0.4769\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3958 - val_loss: 0.4776\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3970 - val_loss: 0.4778\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_20-12:56:42] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-12:56:42] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:56:42] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3976WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0989s vs `on_train_batch_end` time: 0.1266s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0989s vs `on_train_batch_end` time: 0.1266s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 719ms/step - loss: 0.3976 - val_loss: 0.4725\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>513</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  513  3\n",
       "1  138  6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0784313725490196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3and948c) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 24677... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██▁▁▂▂▃▃▃▄▄▅▅▅▆▁</td></tr><tr><td>loss</td><td>█▅▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▂▁▃▃▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>10</td></tr><tr><td>best_val_loss</td><td>0.47199</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.39762</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.47253</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">comfy-sky-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/3and948c\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018a/runs/3and948c</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_125455-3and948c/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3and948c). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/y1yr7o85\" target=\"_blank\">charmed-meadow-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-12:57:37] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:57:37] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:57:37] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 576ms/step - loss: 0.9532 - val_loss: 1.1846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.8581 - val_loss: 0.6154\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.6172 - val_loss: 0.4615\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.5578 - val_loss: 0.4685\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.5060 - val_loss: 0.4751\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4744 - val_loss: 0.4743\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4494 - val_loss: 0.4457\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4285 - val_loss: 0.4724\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4221 - val_loss: 0.4432\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4234 - val_loss: 0.4457\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4219 - val_loss: 0.5528\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4371 - val_loss: 0.4537\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4562 - val_loss: 0.4893\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4602 - val_loss: 0.4392\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4055 - val_loss: 0.4830\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3941 - val_loss: 0.4451\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4141 - val_loss: 0.4397\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4066 - val_loss: 0.4535\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4063 - val_loss: 0.4392\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3864 - val_loss: 0.4345\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3897 - val_loss: 0.4344\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3922 - val_loss: 0.4345\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3947 - val_loss: 0.4385\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3856 - val_loss: 0.4419\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3913 - val_loss: 0.4379\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3804 - val_loss: 0.4370\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3913 - val_loss: 0.4364\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3802 - val_loss: 0.4359\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3885 - val_loss: 0.4347\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_20-12:58:20] Training the entire fine-tuned model...\n",
      "[2022_04_20-12:58:29] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3837WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1002s vs `on_train_batch_end` time: 0.1244s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1002s vs `on_train_batch_end` time: 0.1244s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 699ms/step - loss: 0.3837 - val_loss: 0.4359\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3860 - val_loss: 0.4356\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3856 - val_loss: 0.4352\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3830 - val_loss: 0.4373\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3797 - val_loss: 0.4401\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3813 - val_loss: 0.4374\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3878 - val_loss: 0.4361\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3804 - val_loss: 0.4360\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.3801 - val_loss: 0.4361\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3755 - val_loss: 0.4357\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3838 - val_loss: 0.4354\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_20-12:58:58] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-12:58:58] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:58:58] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3897WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1003s vs `on_train_batch_end` time: 0.1240s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1003s vs `on_train_batch_end` time: 0.1240s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 664ms/step - loss: 0.3897 - val_loss: 0.4351\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>524</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  524  8\n",
       "1  118  8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11267605633802817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:y1yr7o85) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 24942... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██▁▁▁▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▇▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>20</td></tr><tr><td>best_val_loss</td><td>0.43435</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.38967</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43508</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">charmed-meadow-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/y1yr7o85\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018b/runs/y1yr7o85</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_125721-y1yr7o85/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:y1yr7o85). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/6ew06m5s\" target=\"_blank\">earnest-frog-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-12:59:57] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:59:57] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:59:57] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 577ms/step - loss: 0.9866 - val_loss: 0.7573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.6644 - val_loss: 0.6885\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5180 - val_loss: 0.4933\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4886 - val_loss: 0.4844\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4442 - val_loss: 0.4925\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4238 - val_loss: 0.4786\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4247 - val_loss: 0.4830\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4092 - val_loss: 0.4748\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4036 - val_loss: 0.5382\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4281 - val_loss: 0.4752\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3930 - val_loss: 0.4713\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4359 - val_loss: 0.5491\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4035 - val_loss: 0.4629\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3939 - val_loss: 0.4842\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4337 - val_loss: 0.5743\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4346 - val_loss: 0.4820\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3974 - val_loss: 0.4946\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4102 - val_loss: 0.5064\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4037 - val_loss: 0.5067\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3762 - val_loss: 0.4682\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3795 - val_loss: 0.4941\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_20-13:00:32] Training the entire fine-tuned model...\n",
      "[2022_04_20-13:00:39] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3806WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0982s vs `on_train_batch_end` time: 0.1265s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0982s vs `on_train_batch_end` time: 0.1265s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 819ms/step - loss: 0.3806 - val_loss: 0.4694\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3794 - val_loss: 0.4636\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3762 - val_loss: 0.4738\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3760 - val_loss: 0.4725\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3753 - val_loss: 0.4613\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3805 - val_loss: 0.4749\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3694 - val_loss: 0.4727\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3685 - val_loss: 0.4676\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3674 - val_loss: 0.4660\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3535 - val_loss: 0.4663\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3683 - val_loss: 0.4678\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3630 - val_loss: 0.4700\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3571 - val_loss: 0.4717\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_20-13:01:13] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-13:01:13] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:01:26] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3816WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0960s vs `on_train_batch_end` time: 0.1269s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0960s vs `on_train_batch_end` time: 0.1269s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 659ms/step - loss: 0.3816 - val_loss: 0.4609\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>504</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  504  12\n",
       "1  132  12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14285714285714285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:6ew06m5s) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 25257... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▅▅▅▁</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▂▂▂▁▂▂▁▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▂▂▂▁▂▁▃▁▁▃▁▂▄▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.46094</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.38161</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.46094</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">earnest-frog-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/6ew06m5s\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018a/runs/6ew06m5s</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_125936-6ew06m5s/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:6ew06m5s). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/1nstwr4g\" target=\"_blank\">mild-bird-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-13:02:20] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:02:20] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:02:20] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 560ms/step - loss: 0.9910 - val_loss: 0.8847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.6862 - val_loss: 0.5248\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.5701 - val_loss: 0.4714\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 181ms/step - loss: 0.5020 - val_loss: 0.6088\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.5253 - val_loss: 0.5389\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.5152 - val_loss: 0.5033\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4714 - val_loss: 0.4435\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4384 - val_loss: 0.4473\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4499 - val_loss: 0.5178\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4509 - val_loss: 0.4368\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4205 - val_loss: 0.4937\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4831 - val_loss: 0.6523\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4959 - val_loss: 0.4494\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4598 - val_loss: 0.4574\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4155 - val_loss: 0.4342\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3979 - val_loss: 0.4754\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4036 - val_loss: 0.4327\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4063 - val_loss: 0.4317\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3900 - val_loss: 0.4639\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4084 - val_loss: 0.4310\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3841 - val_loss: 0.4313\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3853 - val_loss: 0.4366\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3858 - val_loss: 0.4372\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3779 - val_loss: 0.4385\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3839 - val_loss: 0.4369\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3795 - val_loss: 0.4317\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3848 - val_loss: 0.4307\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3751 - val_loss: 0.4311\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3772 - val_loss: 0.4343\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3776 - val_loss: 0.4328\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3727 - val_loss: 0.4317\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3769 - val_loss: 0.4316\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3690 - val_loss: 0.4324\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3820 - val_loss: 0.4329\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3769 - val_loss: 0.4331\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_20-13:03:08] Training the entire fine-tuned model...\n",
      "[2022_04_20-13:03:16] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3874WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1005s vs `on_train_batch_end` time: 0.1224s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1005s vs `on_train_batch_end` time: 0.1224s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 685ms/step - loss: 0.3874 - val_loss: 0.4322\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3818 - val_loss: 0.4329\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3739 - val_loss: 0.4309\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.3689 - val_loss: 0.4366\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3532 - val_loss: 0.4282\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3677 - val_loss: 0.4291\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3614 - val_loss: 0.4331\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3497 - val_loss: 0.4313\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3550 - val_loss: 0.4311\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3354 - val_loss: 0.4373\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3491 - val_loss: 0.4370\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3509 - val_loss: 0.4294\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3437 - val_loss: 0.4295\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_20-13:03:50] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-13:03:50] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:03:50] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3639WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0982s vs `on_train_batch_end` time: 0.1247s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0982s vs `on_train_batch_end` time: 0.1247s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 706ms/step - loss: 0.3639 - val_loss: 0.4282\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>520</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  520  12\n",
       "1  114  12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1nstwr4g) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 25554... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▂▂▂▃▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▄▃▁▁▂▁▄▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.42815</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36389</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42815</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">mild-bird-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/1nstwr4g\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018b/runs/1nstwr4g</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_130202-1nstwr4g/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1nstwr4g). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/3f69gxkc\" target=\"_blank\">splendid-bush-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-13:04:45] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:04:45] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:04:45] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 586ms/step - loss: 0.9943 - val_loss: 0.7415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.6399 - val_loss: 0.8474\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5606 - val_loss: 0.7141\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.5764 - val_loss: 0.5882\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5173 - val_loss: 0.5563\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5246 - val_loss: 0.4809\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4355 - val_loss: 0.5027\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4162 - val_loss: 0.4715\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4172 - val_loss: 0.4712\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4029 - val_loss: 0.5158\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4151 - val_loss: 0.4784\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3967 - val_loss: 0.4649\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3983 - val_loss: 0.4770\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3930 - val_loss: 0.5027\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4019 - val_loss: 0.4783\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3858 - val_loss: 0.5136\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3805 - val_loss: 0.4631\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3970 - val_loss: 0.4606\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3780 - val_loss: 0.5194\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3841 - val_loss: 0.4654\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3750 - val_loss: 0.4632\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3640 - val_loss: 0.4672\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3648 - val_loss: 0.4714\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3661 - val_loss: 0.4695\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3677 - val_loss: 0.4698\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3663 - val_loss: 0.4659\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_20-13:05:24] Training the entire fine-tuned model...\n",
      "[2022_04_20-13:05:43] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3829WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0978s vs `on_train_batch_end` time: 0.1271s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0978s vs `on_train_batch_end` time: 0.1271s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 675ms/step - loss: 0.3829 - val_loss: 0.4618\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3756 - val_loss: 0.4616\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3718 - val_loss: 0.4705\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3649 - val_loss: 0.4699\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3487 - val_loss: 0.4676\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3476 - val_loss: 0.4849\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3532 - val_loss: 0.4753\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3316 - val_loss: 0.4693\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3366 - val_loss: 0.4705\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3432 - val_loss: 0.4696\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_20-13:06:10] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-13:06:10] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:06:10] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3633WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1264s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1264s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 683ms/step - loss: 0.3633 - val_loss: 0.4689\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>509</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>137</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  509  7\n",
       "1  137  7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08860759493670886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3f69gxkc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 25906... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▄▃▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▆█▆▃▃▁▂▁▁▂▁▁▁▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>17</td></tr><tr><td>best_val_loss</td><td>0.46061</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36327</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.46894</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">splendid-bush-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/3f69gxkc\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018a/runs/3f69gxkc</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_130427-3f69gxkc/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3f69gxkc). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/203br4sz\" target=\"_blank\">treasured-water-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-13:07:11] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:07:11] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:07:11] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 616ms/step - loss: 0.8830 - val_loss: 1.0651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.7409 - val_loss: 0.5968\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.5434 - val_loss: 0.4710\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.5244 - val_loss: 0.4892\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5045 - val_loss: 0.4794\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4825 - val_loss: 0.4524\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4398 - val_loss: 0.4608\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4358 - val_loss: 0.4435\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4428 - val_loss: 0.4457\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4324 - val_loss: 0.4420\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4049 - val_loss: 0.4499\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3999 - val_loss: 0.4407\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3947 - val_loss: 0.4572\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4082 - val_loss: 0.4406\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3842 - val_loss: 0.4738\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4296 - val_loss: 0.4345\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3868 - val_loss: 0.4362\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3773 - val_loss: 0.4346\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3824 - val_loss: 0.5108\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 200ms/step - loss: 0.3907 - val_loss: 0.4318\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3881 - val_loss: 0.4801\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4084 - val_loss: 0.4395\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3604 - val_loss: 0.4364\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3645 - val_loss: 0.4389\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3602 - val_loss: 0.4282\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3598 - val_loss: 0.4304\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3498 - val_loss: 0.4274\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3661 - val_loss: 0.4268\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3553 - val_loss: 0.4344\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3485 - val_loss: 0.4268\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3486 - val_loss: 0.4270\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3473 - val_loss: 0.4334\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3522 - val_loss: 0.4281\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3575 - val_loss: 0.4258\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3556 - val_loss: 0.4259\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3458 - val_loss: 0.4257\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3390 - val_loss: 0.4265\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3551 - val_loss: 0.4257\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3641 - val_loss: 0.4276\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3318 - val_loss: 0.4286\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3596 - val_loss: 0.4290\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3627 - val_loss: 0.4260\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3534 - val_loss: 0.4258\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3342 - val_loss: 0.4257\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3469 - val_loss: 0.4258\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3480 - val_loss: 0.4259\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_20-13:08:14] Training the entire fine-tuned model...\n",
      "[2022_04_20-13:08:22] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3411WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0977s vs `on_train_batch_end` time: 0.1240s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0977s vs `on_train_batch_end` time: 0.1240s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 689ms/step - loss: 0.3411 - val_loss: 0.4451\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3365 - val_loss: 0.4281\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3522 - val_loss: 0.4276\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3437 - val_loss: 0.4662\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3627 - val_loss: 0.4358\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3420 - val_loss: 0.4536\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3362 - val_loss: 0.4413\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3311 - val_loss: 0.4305\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3190 - val_loss: 0.4401\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3175 - val_loss: 0.4359\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3099 - val_loss: 0.4294\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_20-13:08:51] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-13:08:51] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:08:51] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3379WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1010s vs `on_train_batch_end` time: 0.1231s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1010s vs `on_train_batch_end` time: 0.1231s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 722ms/step - loss: 0.3379 - val_loss: 0.4276\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>514</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  514  18\n",
       "1  108  18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2222222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:203br4sz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 26191... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▁</td></tr><tr><td>loss</td><td>█▆▄▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▁▂▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>37</td></tr><tr><td>best_val_loss</td><td>0.42567</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33787</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.4276</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">treasured-water-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/203br4sz\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018b/runs/203br4sz</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_130648-203br4sz/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:203br4sz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/3c9o81yn\" target=\"_blank\">pleasant-darkness-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-13:09:47] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:09:47] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:09:47] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 583ms/step - loss: 0.8888 - val_loss: 0.9980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.7057 - val_loss: 0.8940\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.6124 - val_loss: 0.5212\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5470 - val_loss: 0.5256\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4861 - val_loss: 0.5593\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.5069 - val_loss: 0.5081\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4547 - val_loss: 0.5106\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4280 - val_loss: 0.4847\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3997 - val_loss: 0.4751\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4102 - val_loss: 0.4729\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4002 - val_loss: 0.5375\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4170 - val_loss: 0.4712\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3972 - val_loss: 0.4780\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3842 - val_loss: 0.4744\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3787 - val_loss: 0.4687\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3898 - val_loss: 0.4704\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4023 - val_loss: 0.5937\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4021 - val_loss: 0.4663\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3878 - val_loss: 0.4675\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3675 - val_loss: 0.5020\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3891 - val_loss: 0.4825\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3904 - val_loss: 0.4641\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3514 - val_loss: 0.4742\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3544 - val_loss: 0.4699\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3559 - val_loss: 0.4701\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3510 - val_loss: 0.5084\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3627 - val_loss: 0.4633\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3596 - val_loss: 0.4681\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3618 - val_loss: 0.4896\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3470 - val_loss: 0.4616\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3497 - val_loss: 0.4897\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3440 - val_loss: 0.4628\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3622 - val_loss: 0.4657\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3558 - val_loss: 0.4823\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3541 - val_loss: 0.4796\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3433 - val_loss: 0.4697\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3422 - val_loss: 0.4645\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3525 - val_loss: 0.4651\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_20-13:10:41] Training the entire fine-tuned model...\n",
      "[2022_04_20-13:11:27] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4721WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1006s vs `on_train_batch_end` time: 0.1226s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1006s vs `on_train_batch_end` time: 0.1226s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 690ms/step - loss: 0.4721 - val_loss: 0.4656\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.4036 - val_loss: 0.4680\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3932 - val_loss: 0.4649\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.3713 - val_loss: 0.5517\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3607 - val_loss: 0.5226\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3486 - val_loss: 0.4846\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3585 - val_loss: 0.4760\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.2911 - val_loss: 0.4800\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.2924 - val_loss: 0.5086\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.2762 - val_loss: 0.4723\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.2793 - val_loss: 0.5115\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_20-13:11:56] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-13:11:56] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:12:13] Validation set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3390WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0983s vs `on_train_batch_end` time: 0.1246s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0983s vs `on_train_batch_end` time: 0.1246s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 695ms/step - loss: 0.3390 - val_loss: 0.4748\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>508</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>134</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  508   8\n",
       "1  134  10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12345679012345678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_20_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3c9o81yn) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 26579... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▆▅▄▄▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▁▁▁▂</td></tr><tr><td>lr</td><td>█████████████████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▂▂▂▂▁▁▂▁▁▁▁▃▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>29</td></tr><tr><td>best_val_loss</td><td>0.46162</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33899</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.47478</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">pleasant-darkness-16</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018a/runs/3c9o81yn\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018a/runs/3c9o81yn</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_130929-3c9o81yn/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3c9o81yn). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/hezdn929\" target=\"_blank\">golden-cloud-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-13:13:14] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:13:14] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:13:14] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 578ms/step - loss: 0.8869 - val_loss: 0.8248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.7030 - val_loss: 0.5135\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5593 - val_loss: 0.4643\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4889 - val_loss: 0.4654\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4572 - val_loss: 0.4499\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4386 - val_loss: 0.4504\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4422 - val_loss: 0.4506\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4238 - val_loss: 0.4412\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4240 - val_loss: 0.4437\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4277 - val_loss: 0.5146\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4480 - val_loss: 0.4928\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4221 - val_loss: 0.4378\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3978 - val_loss: 0.5018\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3937 - val_loss: 0.4525\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3820 - val_loss: 0.5508\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4281 - val_loss: 0.4339\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3793 - val_loss: 0.4377\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3771 - val_loss: 0.4661\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4064 - val_loss: 0.4251\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3837 - val_loss: 0.5043\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4245 - val_loss: 0.5651\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4247 - val_loss: 0.4267\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3885 - val_loss: 0.4729\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3906 - val_loss: 0.4312\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3611 - val_loss: 0.4247\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3592 - val_loss: 0.4280\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3621 - val_loss: 0.4301\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3464 - val_loss: 0.4269\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3465 - val_loss: 0.4242\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3425 - val_loss: 0.4245\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3481 - val_loss: 0.4257\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3491 - val_loss: 0.4257\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3488 - val_loss: 0.4292\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3488 - val_loss: 0.4311\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3461 - val_loss: 0.4268\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3578 - val_loss: 0.4245\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3509 - val_loss: 0.4247\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_20-13:14:05] Training the entire fine-tuned model...\n",
      "[2022_04_20-13:14:53] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.7986WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1005s vs `on_train_batch_end` time: 0.1233s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1005s vs `on_train_batch_end` time: 0.1233s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 701ms/step - loss: 0.7986 - val_loss: 0.5990\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.4309 - val_loss: 0.4335\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.4020 - val_loss: 0.4506\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3871 - val_loss: 0.4769\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.4103 - val_loss: 0.4725\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3603 - val_loss: 0.4486\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3547 - val_loss: 0.4351\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3386 - val_loss: 0.4426\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3373 - val_loss: 0.4344\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3318 - val_loss: 0.4388\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_20-13:15:20] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-13:15:20] Training set: Filtered out 0 of 660 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:15:20] Validation set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3839WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1016s vs `on_train_batch_end` time: 0.1233s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1016s vs `on_train_batch_end` time: 0.1233s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 692ms/step - loss: 0.3839 - val_loss: 0.4355\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>519</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  519  13\n",
       "1  107  19"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24050632911392406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_20_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:hezdn929) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 26950... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇██▁▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▆▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▇▂▂▂▁▁▁▁▂</td></tr><tr><td>lr</td><td>████████████████████▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▁▁▁▁▃▂▂▁▃▁▁▁▂▃▁▂▁▁▁▁▁▁▁▁▁▁▁▄▁▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>28</td></tr><tr><td>best_val_loss</td><td>0.42421</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.38386</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.43549</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">golden-cloud-16</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2018b/runs/hezdn929\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2018b/runs/hezdn929</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_131250-hezdn929/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:hezdn929). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/3uxxqpkh\" target=\"_blank\">cool-galaxy-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-13:16:16] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:16:16] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:16:16] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 821ms/step - loss: 0.9115 - val_loss: 0.8306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.7080 - val_loss: 0.4748\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5701 - val_loss: 0.4914\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5723 - val_loss: 0.5995\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5429 - val_loss: 0.4702\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5015 - val_loss: 0.4400\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4726 - val_loss: 0.4294\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4579 - val_loss: 0.4206\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4558 - val_loss: 0.4167\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4583 - val_loss: 0.4201\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4581 - val_loss: 0.4039\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4356 - val_loss: 0.4050\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4298 - val_loss: 0.4106\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4312 - val_loss: 0.4008\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4386 - val_loss: 0.4019\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4259 - val_loss: 0.3978\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4133 - val_loss: 0.3926\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4203 - val_loss: 0.3914\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4342 - val_loss: 0.3888\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4021 - val_loss: 0.4013\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4133 - val_loss: 0.3869\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3925 - val_loss: 0.3858\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4057 - val_loss: 0.4190\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4223 - val_loss: 0.3896\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3922 - val_loss: 0.3864\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-13:16:56] Training the entire fine-tuned model...\n",
      "[2022_04_20-13:17:04] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3988WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1201s vs `on_train_batch_end` time: 0.1361s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1201s vs `on_train_batch_end` time: 0.1361s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 724ms/step - loss: 0.3988 - val_loss: 0.3865\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.4051 - val_loss: 0.3848\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.4005 - val_loss: 0.3857\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.4003 - val_loss: 0.3855\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3962 - val_loss: 0.3842\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3945 - val_loss: 0.3848\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3865 - val_loss: 0.3845\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3930 - val_loss: 0.3847\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "[2022_04_20-13:17:28] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-13:17:28] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:17:31] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3878WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1076s vs `on_train_batch_end` time: 0.1394s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1076s vs `on_train_batch_end` time: 0.1394s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 708ms/step - loss: 0.3878 - val_loss: 0.3842\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>472</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  472  10\n",
       "1   90  15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23076923076923078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3uxxqpkh) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 27292... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████████████▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▃▄▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.3842</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.38782</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.3842</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">cool-galaxy-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/3uxxqpkh\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027a/runs/3uxxqpkh</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_131558-3uxxqpkh/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3uxxqpkh). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/3nsmyy4s\" target=\"_blank\">laced-capybara-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-13:18:26] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:18:26] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:18:26] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 710ms/step - loss: 1.0069 - val_loss: 0.7121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.6606 - val_loss: 0.7166\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.6502 - val_loss: 0.5705\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4940 - val_loss: 0.5123\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4553 - val_loss: 0.5342\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4169 - val_loss: 0.4901\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3940 - val_loss: 0.5065\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3803 - val_loss: 0.4894\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3869 - val_loss: 0.4843\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3853 - val_loss: 0.5556\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3978 - val_loss: 0.4845\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3587 - val_loss: 0.5033\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-13:18:51] Training the entire fine-tuned model...\n",
      "[2022_04_20-13:19:04] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 828ms/step - loss: 0.3735 - val_loss: 0.4873\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3678 - val_loss: 0.4872\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3615 - val_loss: 0.4883\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3551 - val_loss: 0.4884\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3673 - val_loss: 0.4886\n",
      "[2022_04_20-13:19:22] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-13:19:22] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:19:22] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 1s/step - loss: 0.3623 - val_loss: 0.4873\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>561</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>161</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  561  5\n",
       "1  161  4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04597701149425287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3nsmyy4s) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 27560... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█▁▂▂▃▄▁</td></tr><tr><td>loss</td><td>█▄▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>██▄▂▃▁▂▁▁▃▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_val_loss</td><td>0.4843</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36231</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.48725</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">laced-capybara-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/3nsmyy4s\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027b/runs/3nsmyy4s</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_131809-3nsmyy4s/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3nsmyy4s). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/15ru5c6r\" target=\"_blank\">sleek-glade-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-13:20:17] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:20:17] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:20:17] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 584ms/step - loss: 0.9458 - val_loss: 0.9167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.7174 - val_loss: 0.4856\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5661 - val_loss: 0.4703\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5814 - val_loss: 0.5785\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5225 - val_loss: 0.4581\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4783 - val_loss: 0.4466\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4772 - val_loss: 0.4252\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4627 - val_loss: 0.4165\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4543 - val_loss: 0.4149\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4454 - val_loss: 0.4102\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4462 - val_loss: 0.4057\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4519 - val_loss: 0.4215\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4521 - val_loss: 0.4301\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4500 - val_loss: 0.4106\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-13:20:43] Training the entire fine-tuned model...\n",
      "[2022_04_20-13:20:55] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4442WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1069s vs `on_train_batch_end` time: 0.1397s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1069s vs `on_train_batch_end` time: 0.1397s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 715ms/step - loss: 0.4442 - val_loss: 0.4055\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.4353 - val_loss: 0.4086\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.4326 - val_loss: 0.4040\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.4291 - val_loss: 0.4045\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.4309 - val_loss: 0.4031\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.4279 - val_loss: 0.4024\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.4225 - val_loss: 0.4017\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 342ms/step - loss: 0.4239 - val_loss: 0.4029\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.4217 - val_loss: 0.4017\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 340ms/step - loss: 0.4155 - val_loss: 0.4067\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_20-13:21:23] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-13:21:23] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:21:24] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4209WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1063s vs `on_train_batch_end` time: 0.1400s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1063s vs `on_train_batch_end` time: 0.1400s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 688ms/step - loss: 0.4209 - val_loss: 0.4014\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>472</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  472  10\n",
       "1   96   9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14516129032258066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:15ru5c6r) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 27753... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▄▅▅▆▆▇▇█▁▂▂▃▃▄▄▅▅▆▁</td></tr><tr><td>loss</td><td>█▅▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.40136</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.42094</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.40136</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">sleek-glade-2</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/15ru5c6r\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027a/runs/15ru5c6r</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_131959-15ru5c6r/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:15ru5c6r). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/k2yc9evz\" target=\"_blank\">blooming-galaxy-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-13:22:18] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:22:18] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:22:18] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 725ms/step - loss: 0.8969 - val_loss: 0.5217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.5887 - val_loss: 0.7376\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.5294 - val_loss: 0.5484\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4638 - val_loss: 0.5570\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-13:22:33] Training the entire fine-tuned model...\n",
      "[2022_04_20-13:22:42] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 827ms/step - loss: 0.4539 - val_loss: 0.5225\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.4536 - val_loss: 0.5280\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.4497 - val_loss: 0.5254\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.4459 - val_loss: 0.5226\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_20-13:22:57] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-13:22:57] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:22:57] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 844ms/step - loss: 0.4541 - val_loss: 0.5224\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>566</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>165</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  566  0\n",
       "1  165  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:k2yc9evz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 27975... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▆█▁▃▆█▁</td></tr><tr><td>loss</td><td>█▃▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▁█▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.5217</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.45405</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.52242</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">blooming-galaxy-2</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/k2yc9evz\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027b/runs/k2yc9evz</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_132201-k2yc9evz/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:k2yc9evz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/r6uwl9q2\" target=\"_blank\">sweet-pine-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-13:23:52] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:23:52] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:23:52] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 575ms/step - loss: 0.8081 - val_loss: 0.7258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.6854 - val_loss: 0.4548\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5716 - val_loss: 0.4875\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.5358 - val_loss: 0.4487\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4924 - val_loss: 0.4621\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4951 - val_loss: 0.4370\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4875 - val_loss: 0.4180\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4666 - val_loss: 0.4499\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4871 - val_loss: 0.4397\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5039 - val_loss: 0.4532\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-13:24:14] Training the entire fine-tuned model...\n",
      "[2022_04_20-13:24:30] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4617WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1105s vs `on_train_batch_end` time: 0.1386s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1105s vs `on_train_batch_end` time: 0.1386s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 697ms/step - loss: 0.4617 - val_loss: 0.4301\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.4558 - val_loss: 0.4149\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.4440 - val_loss: 0.4210\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.4475 - val_loss: 0.4131\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.4375 - val_loss: 0.4210\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.4404 - val_loss: 0.4121\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.4206 - val_loss: 0.4161\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.4227 - val_loss: 0.4124\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.4128 - val_loss: 0.4138\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_20-13:24:56] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-13:24:56] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:24:57] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4223WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1075s vs `on_train_batch_end` time: 0.1391s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1075s vs `on_train_batch_end` time: 0.1391s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 692ms/step - loss: 0.4223 - val_loss: 0.4126\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>472</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  472  10\n",
       "1   98   7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11475409836065574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:r6uwl9q2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 28115... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█▁▂▃▃▄▅▆▆▇▁</td></tr><tr><td>loss</td><td>█▆▄▃▂▂▂▂▂▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▃▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.41208</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.42234</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.41258</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">sweet-pine-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/r6uwl9q2\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027a/runs/r6uwl9q2</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_132336-r6uwl9q2/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:r6uwl9q2). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/2sl1nmf3\" target=\"_blank\">efficient-sound-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-13:26:02] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:26:02] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:26:02] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 1s/step - loss: 0.7420 - val_loss: 0.6286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.5734 - val_loss: 0.7711\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.5464 - val_loss: 0.5056\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 245ms/step - loss: 0.4344 - val_loss: 0.5308\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4249 - val_loss: 0.5224\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4129 - val_loss: 0.4913\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4042 - val_loss: 0.5426\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3710 - val_loss: 0.5196\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4030 - val_loss: 0.5168\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-13:26:24] Training the entire fine-tuned model...\n",
      "[2022_04_20-13:26:32] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 833ms/step - loss: 0.3910 - val_loss: 0.4897\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3907 - val_loss: 0.4981\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3771 - val_loss: 0.5054\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3634 - val_loss: 0.4894\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3590 - val_loss: 0.5038\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3574 - val_loss: 0.4856\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3276 - val_loss: 0.4781\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3279 - val_loss: 0.4953\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3224 - val_loss: 0.4758\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3082 - val_loss: 0.5160\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3120 - val_loss: 0.4978\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3146 - val_loss: 0.5645\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_20-13:27:02] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-13:27:02] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:27:02] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 836ms/step - loss: 0.3128 - val_loss: 0.4805\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>553</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>148</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  553  13\n",
       "1  148  17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17435897435897435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2sl1nmf3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 28310... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▄▅▅▆▁▂▂▃▄▄▅▅▆▇▇█▁</td></tr><tr><td>loss</td><td>█▅▅▃▃▃▃▂▃▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▅█▂▂▂▁▃▂▂▁▂▂▁▂▁▁▁▁▂▂▃▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_val_loss</td><td>0.47575</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31281</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.48052</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">efficient-sound-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/2sl1nmf3\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027b/runs/2sl1nmf3</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_132536-2sl1nmf3/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2sl1nmf3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/1hw3mltv\" target=\"_blank\">neat-armadillo-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-13:27:57] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:27:57] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:27:57] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 605ms/step - loss: 0.9055 - val_loss: 0.8669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.7089 - val_loss: 0.4788\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5688 - val_loss: 0.4665\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5310 - val_loss: 0.5206\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5055 - val_loss: 0.4306\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4742 - val_loss: 0.4244\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4654 - val_loss: 0.4333\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4599 - val_loss: 0.4180\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4534 - val_loss: 0.4274\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4590 - val_loss: 0.4418\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4666 - val_loss: 0.4059\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4466 - val_loss: 0.4070\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4430 - val_loss: 0.4119\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4462 - val_loss: 0.3981\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4221 - val_loss: 0.3966\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4150 - val_loss: 0.4005\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.4187 - val_loss: 0.4475\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4274 - val_loss: 0.4024\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-13:28:29] Training the entire fine-tuned model...\n",
      "[2022_04_20-13:28:56] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4874WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1080s vs `on_train_batch_end` time: 0.1402s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1080s vs `on_train_batch_end` time: 0.1402s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 689ms/step - loss: 0.4874 - val_loss: 0.3962\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.4482 - val_loss: 0.4501\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.4529 - val_loss: 0.3980\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.4171 - val_loss: 0.4069\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_20-13:29:12] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-13:29:12] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:29:12] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4160WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1095s vs `on_train_batch_end` time: 0.1386s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1095s vs `on_train_batch_end` time: 0.1386s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 726ms/step - loss: 0.4160 - val_loss: 0.3984\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>477</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  477  5\n",
       "1   96  9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15126050420168066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1hw3mltv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 28518... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██▁▁▂▂▁</td></tr><tr><td>loss</td><td>█▅▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▁▂▁▁</td></tr><tr><td>lr</td><td>██████████████████▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▃▂▁▂▁▁▂▁▁▁▁▁▁▂▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.39624</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.41597</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.39841</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">neat-armadillo-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/1hw3mltv\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027a/runs/1hw3mltv</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_132740-1hw3mltv/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1hw3mltv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/7kar3am9\" target=\"_blank\">prime-snow-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-13:30:09] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:30:09] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:30:09] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 706ms/step - loss: 0.7592 - val_loss: 0.7277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.5392 - val_loss: 0.6762\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4916 - val_loss: 0.5057\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4489 - val_loss: 0.4986\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4048 - val_loss: 0.5224\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3941 - val_loss: 0.4905\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3842 - val_loss: 0.4953\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3644 - val_loss: 0.4841\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3603 - val_loss: 0.5004\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3672 - val_loss: 0.4816\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3451 - val_loss: 0.4805\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3369 - val_loss: 0.4787\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3427 - val_loss: 0.5054\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3402 - val_loss: 0.4772\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3264 - val_loss: 0.4776\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3262 - val_loss: 0.4834\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3131 - val_loss: 0.4840\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-13:30:38] Training the entire fine-tuned model...\n",
      "[2022_04_20-13:31:06] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 840ms/step - loss: 0.4832 - val_loss: 0.5859\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3697 - val_loss: 0.4824\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3711 - val_loss: 0.5070\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3516 - val_loss: 0.4789\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3428 - val_loss: 0.5049\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 371ms/step - loss: 0.3262 - val_loss: 0.4761\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3096 - val_loss: 0.4859\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.2954 - val_loss: 0.4923\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2929 - val_loss: 0.4815\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_20-13:31:32] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-13:31:32] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:31:32] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 817ms/step - loss: 0.3201 - val_loss: 0.4759\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>545</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>140</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  545  21\n",
       "1  140  25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23696682464454977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:7kar3am9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 28730... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██▁▁▂▂▃▃▄▄▅▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▄▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▂▂▂▁▂▁▂▁▁▁▂▁▁▁▁▄▁▂▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.47592</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32015</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.47592</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">prime-snow-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/7kar3am9\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027b/runs/7kar3am9</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_132950-7kar3am9/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:7kar3am9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/1c4sh75d\" target=\"_blank\">feasible-shadow-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-13:32:27] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:32:27] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:32:27] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 586ms/step - loss: 0.9509 - val_loss: 0.8417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.7401 - val_loss: 0.4630\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.5489 - val_loss: 0.4672\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5245 - val_loss: 0.4939\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4970 - val_loss: 0.4314\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4743 - val_loss: 0.4385\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4713 - val_loss: 0.4277\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4593 - val_loss: 0.4151\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4508 - val_loss: 0.4101\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4503 - val_loss: 0.4193\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4434 - val_loss: 0.4046\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4381 - val_loss: 0.4044\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4441 - val_loss: 0.4139\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4352 - val_loss: 0.4029\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4286 - val_loss: 0.4037\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4222 - val_loss: 0.3961\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4212 - val_loss: 0.3956\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4228 - val_loss: 0.3928\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4142 - val_loss: 0.3956\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4087 - val_loss: 0.3895\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4174 - val_loss: 0.3901\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4183 - val_loss: 0.3946\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3923 - val_loss: 0.3915\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4075 - val_loss: 0.3846\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3979 - val_loss: 0.3847\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3898 - val_loss: 0.3837\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3944 - val_loss: 0.3840\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3974 - val_loss: 0.3835\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3994 - val_loss: 0.3827\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3816 - val_loss: 0.3872\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3959 - val_loss: 0.3826\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3889 - val_loss: 0.3844\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3742 - val_loss: 0.3842\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3939 - val_loss: 0.3822\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3901 - val_loss: 0.3821\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3837 - val_loss: 0.3823\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3912 - val_loss: 0.3828\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3882 - val_loss: 0.3824\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3761 - val_loss: 0.3822\n",
      "[2022_04_20-13:33:20] Training the entire fine-tuned model...\n",
      "[2022_04_20-13:33:28] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3871WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1055s vs `on_train_batch_end` time: 0.1404s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1055s vs `on_train_batch_end` time: 0.1404s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 695ms/step - loss: 0.3871 - val_loss: 0.3822\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3941 - val_loss: 0.3818\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3882 - val_loss: 0.3818\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3961 - val_loss: 0.3818\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3791 - val_loss: 0.3812\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3891 - val_loss: 0.3815\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3792 - val_loss: 0.3826\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3798 - val_loss: 0.3813\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3837 - val_loss: 0.3809\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3781 - val_loss: 0.3806\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3948 - val_loss: 0.3806\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3795 - val_loss: 0.3807\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 341ms/step - loss: 0.3846 - val_loss: 0.3809\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3769 - val_loss: 0.3809\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3836 - val_loss: 0.3809\n",
      "[2022_04_20-13:34:08] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-13:34:08] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:34:08] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3754WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1078s vs `on_train_batch_end` time: 0.1403s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1078s vs `on_train_batch_end` time: 0.1403s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 689ms/step - loss: 0.3754 - val_loss: 0.3806\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>468</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  468  14\n",
       "1   86  19"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2753623188405797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1c4sh75d) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 28968... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▃▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.38058</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37537</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.38058</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">feasible-shadow-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/1c4sh75d\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027a/runs/1c4sh75d</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_133210-1c4sh75d/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1c4sh75d). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/2aazecn9\" target=\"_blank\">lemon-night-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-13:35:01] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:35:01] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:35:01] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 694ms/step - loss: 0.7973 - val_loss: 0.5465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4834 - val_loss: 0.6167\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4458 - val_loss: 0.5167\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4233 - val_loss: 0.5503\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3980 - val_loss: 0.4904\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3798 - val_loss: 0.4986\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3738 - val_loss: 0.5208\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3796 - val_loss: 0.5110\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3790 - val_loss: 0.4966\n",
      "[2022_04_20-13:35:21] Training the entire fine-tuned model...\n",
      "[2022_04_20-13:35:29] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 876ms/step - loss: 0.3852 - val_loss: 0.4899\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3819 - val_loss: 0.4928\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3838 - val_loss: 0.4965\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3826 - val_loss: 0.4974\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3877 - val_loss: 0.4973\n",
      "[2022_04_20-13:35:48] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-13:35:48] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:35:48] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 857ms/step - loss: 0.3888 - val_loss: 0.4899\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>563</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>165</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  563  3\n",
       "1  165  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2aazecn9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 29341... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▄▅▅▆▇█▁▂▃▄▅▁</td></tr><tr><td>loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████▃▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄█▂▄▁▁▃▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.48992</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.38885</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.48992</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">lemon-night-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/2aazecn9\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027b/runs/2aazecn9</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_133446-2aazecn9/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2aazecn9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/2hkslzmf\" target=\"_blank\">resilient-silence-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-13:36:44] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:36:45] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:36:45] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 572ms/step - loss: 0.8253 - val_loss: 0.5840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5468 - val_loss: 0.4659\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5379 - val_loss: 0.4429\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4907 - val_loss: 0.4434\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4850 - val_loss: 0.4353\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4757 - val_loss: 0.4212\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4635 - val_loss: 0.4157\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4574 - val_loss: 0.4077\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4463 - val_loss: 0.4083\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4434 - val_loss: 0.4073\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4491 - val_loss: 0.4386\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4452 - val_loss: 0.3970\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4592 - val_loss: 0.4293\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4427 - val_loss: 0.4443\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4433 - val_loss: 0.3933\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4719 - val_loss: 0.3903\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4437 - val_loss: 0.3877\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4248 - val_loss: 0.3981\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4247 - val_loss: 0.3896\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4103 - val_loss: 0.3889\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4009 - val_loss: 0.3827\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3993 - val_loss: 0.3830\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3945 - val_loss: 0.3823\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4010 - val_loss: 0.3830\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3907 - val_loss: 0.3826\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4042 - val_loss: 0.3828\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3877 - val_loss: 0.3834\n",
      "[2022_04_20-13:37:25] Training the entire fine-tuned model...\n",
      "[2022_04_20-13:37:42] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3988WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1066s vs `on_train_batch_end` time: 0.1407s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1066s vs `on_train_batch_end` time: 0.1407s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 703ms/step - loss: 0.3988 - val_loss: 0.3819\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3897 - val_loss: 0.3894\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3940 - val_loss: 0.3842\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3882 - val_loss: 0.3906\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3838 - val_loss: 0.3813\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3728 - val_loss: 0.3797\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3883 - val_loss: 0.3783\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3709 - val_loss: 0.3818\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3721 - val_loss: 0.3801\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3696 - val_loss: 0.3777\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 340ms/step - loss: 0.3715 - val_loss: 0.3773\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3705 - val_loss: 0.3798\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3768 - val_loss: 0.3774\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3684 - val_loss: 0.3771\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3600 - val_loss: 0.3775\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3639 - val_loss: 0.3765\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3608 - val_loss: 0.3764\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3557 - val_loss: 0.3771\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3619 - val_loss: 0.3767\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3610 - val_loss: 0.3764\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 2s 341ms/step - loss: 0.3566 - val_loss: 0.3767\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3641 - val_loss: 0.3769\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3603 - val_loss: 0.3769\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3537 - val_loss: 0.3769\n",
      "[2022_04_20-13:38:39] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-13:38:39] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:39:11] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3624WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1090s vs `on_train_batch_end` time: 0.1402s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1090s vs `on_train_batch_end` time: 0.1402s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 692ms/step - loss: 0.3624 - val_loss: 0.3766\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>461</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  461  21\n",
       "1   82  23"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3087248322147651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2hkslzmf) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 29511... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▆▇▇▇█▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▃▃▂▂▂▂▂▃▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▃▃▂▂▂▃▂▃▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>19</td></tr><tr><td>best_val_loss</td><td>0.37636</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36236</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.37658</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">resilient-silence-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/2hkslzmf\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027a/runs/2hkslzmf</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_133627-2hkslzmf/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2hkslzmf). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/s3i3plkl\" target=\"_blank\">distinctive-snowflake-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-13:40:05] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:40:05] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:40:06] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 720ms/step - loss: 0.9410 - val_loss: 0.6474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.6188 - val_loss: 0.7348\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.5951 - val_loss: 0.5099\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4953 - val_loss: 0.5731\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4690 - val_loss: 0.4948\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4252 - val_loss: 0.5415\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4287 - val_loss: 0.4879\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4178 - val_loss: 0.5545\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4076 - val_loss: 0.4854\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3677 - val_loss: 0.4995\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3648 - val_loss: 0.4827\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3522 - val_loss: 0.4962\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3441 - val_loss: 0.4795\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3511 - val_loss: 0.5128\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3367 - val_loss: 0.4789\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3447 - val_loss: 0.4844\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3538 - val_loss: 0.5274\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3313 - val_loss: 0.4796\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3213 - val_loss: 0.4870\n",
      "[2022_04_20-13:40:36] Training the entire fine-tuned model...\n",
      "[2022_04_20-13:41:03] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 834ms/step - loss: 0.3420 - val_loss: 0.4905\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3326 - val_loss: 0.4771\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3374 - val_loss: 0.4883\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3227 - val_loss: 0.4895\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3209 - val_loss: 0.4776\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3266 - val_loss: 0.4794\n",
      "[2022_04_20-13:41:23] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-13:41:23] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:41:30] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 854ms/step - loss: 0.3328 - val_loss: 0.4775\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>553</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  553  13\n",
       "1  149  16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16494845360824742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:s3i3plkl) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 29879... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██▁▁▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████▃▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▆█▂▄▁▃▁▃▁▂▁▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_val_loss</td><td>0.47714</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33283</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.47755</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">distinctive-snowflake-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/s3i3plkl\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027b/runs/s3i3plkl</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_133949-s3i3plkl/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:s3i3plkl). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/2d9g6raa\" target=\"_blank\">autumn-lion-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-13:42:23] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:42:23] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:42:23] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 560ms/step - loss: 0.8820 - val_loss: 0.5383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.6794 - val_loss: 0.5170\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5784 - val_loss: 0.5005\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5394 - val_loss: 0.4490\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5134 - val_loss: 0.4466\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4718 - val_loss: 0.4248\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4742 - val_loss: 0.4327\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4665 - val_loss: 0.4465\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4754 - val_loss: 0.4205\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4509 - val_loss: 0.4070\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4466 - val_loss: 0.4101\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4266 - val_loss: 0.4014\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4284 - val_loss: 0.4037\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4403 - val_loss: 0.3952\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4444 - val_loss: 0.4358\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4470 - val_loss: 0.3930\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4210 - val_loss: 0.3922\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4130 - val_loss: 0.3930\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4267 - val_loss: 0.4710\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4223 - val_loss: 0.3881\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4186 - val_loss: 0.3949\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4282 - val_loss: 0.4067\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4297 - val_loss: 0.4082\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4152 - val_loss: 0.3945\n",
      "[2022_04_20-13:43:00] Training the entire fine-tuned model...\n",
      "[2022_04_20-13:43:20] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4062WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1082s vs `on_train_batch_end` time: 0.1381s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1082s vs `on_train_batch_end` time: 0.1381s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 711ms/step - loss: 0.4062 - val_loss: 0.3893\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3975 - val_loss: 0.3847\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3958 - val_loss: 0.3850\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.4004 - val_loss: 0.3933\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3778 - val_loss: 0.3883\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.4018 - val_loss: 0.3819\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3849 - val_loss: 0.3899\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3874 - val_loss: 0.3811\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3878 - val_loss: 0.3821\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3690 - val_loss: 0.3842\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3818 - val_loss: 0.3812\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3732 - val_loss: 0.3807\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3806 - val_loss: 0.3806\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3676 - val_loss: 0.3809\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3584 - val_loss: 0.3814\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3710 - val_loss: 0.3813\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3570 - val_loss: 0.3812\n",
      "[2022_04_20-13:44:03] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-13:44:03] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:44:03] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3658WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1056s vs `on_train_batch_end` time: 0.1414s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1056s vs `on_train_batch_end` time: 0.1414s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 707ms/step - loss: 0.3658 - val_loss: 0.3805\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>465</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  465  17\n",
       "1   84  21"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2937062937062937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2d9g6raa) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 30113... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇██▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████████▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▆▄▄▃▃▄▃▂▂▂▂▂▃▂▂▂▅▁▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.38054</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36579</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.38054</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">autumn-lion-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/2d9g6raa\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027a/runs/2d9g6raa</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_134208-2d9g6raa/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2d9g6raa). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/r2ecgt2l\" target=\"_blank\">driven-plant-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-13:45:01] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:45:01] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:45:01] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 733ms/step - loss: 1.0130 - val_loss: 0.6973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.5907 - val_loss: 0.6776\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.6069 - val_loss: 0.5241\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4836 - val_loss: 0.5438\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.4393 - val_loss: 0.4995\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4039 - val_loss: 0.5038\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3908 - val_loss: 0.4883\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3812 - val_loss: 0.5140\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3754 - val_loss: 0.4843\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3637 - val_loss: 0.4864\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3614 - val_loss: 0.4962\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.3519 - val_loss: 0.4819\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3387 - val_loss: 0.4959\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3453 - val_loss: 0.4795\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3238 - val_loss: 0.5064\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3264 - val_loss: 0.4827\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.3288 - val_loss: 0.4773\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3195 - val_loss: 0.4945\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3267 - val_loss: 0.4871\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3125 - val_loss: 0.4778\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3219 - val_loss: 0.4822\n",
      "[2022_04_20-13:45:35] Training the entire fine-tuned model...\n",
      "[2022_04_20-13:45:42] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 839ms/step - loss: 0.3408 - val_loss: 0.4823\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3483 - val_loss: 0.5004\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3223 - val_loss: 0.4941\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3051 - val_loss: 0.4779\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3073 - val_loss: 0.4935\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3147 - val_loss: 0.4780\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2958 - val_loss: 0.4800\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3061 - val_loss: 0.4803\n",
      "[2022_04_20-13:46:06] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-13:46:06] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:46:06] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 839ms/step - loss: 0.3151 - val_loss: 0.4758\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>542</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>141</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  542  24\n",
       "1  141  24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22535211267605634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:r2ecgt2l) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 30421... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▄▄▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████████▃▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▃▃▂▂▁▂▁▁▂▁▂▁▂▁▁▂▁▁▁▁▂▂▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.47585</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31514</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.47585</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">driven-plant-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/r2ecgt2l\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027b/runs/r2ecgt2l</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_134440-r2ecgt2l/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:r2ecgt2l). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/1j65snpu\" target=\"_blank\">ethereal-salad-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-13:47:01] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:47:01] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:47:01] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 567ms/step - loss: 0.8608 - val_loss: 0.4708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5581 - val_loss: 0.5451\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5512 - val_loss: 0.4387\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5043 - val_loss: 0.4610\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5059 - val_loss: 0.4333\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4765 - val_loss: 0.4605\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4812 - val_loss: 0.4151\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4729 - val_loss: 0.4197\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4537 - val_loss: 0.4073\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4426 - val_loss: 0.4108\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4424 - val_loss: 0.4365\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4391 - val_loss: 0.4318\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4505 - val_loss: 0.4007\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4351 - val_loss: 0.3999\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4299 - val_loss: 0.3995\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4172 - val_loss: 0.3975\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4316 - val_loss: 0.3993\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4167 - val_loss: 0.3967\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4216 - val_loss: 0.3984\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4171 - val_loss: 0.3953\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4216 - val_loss: 0.3965\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4204 - val_loss: 0.3962\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4174 - val_loss: 0.3951\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4175 - val_loss: 0.3933\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4157 - val_loss: 0.3936\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4194 - val_loss: 0.3946\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4091 - val_loss: 0.3920\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4239 - val_loss: 0.4000\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4135 - val_loss: 0.3909\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4152 - val_loss: 0.3935\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4107 - val_loss: 0.3895\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4043 - val_loss: 0.3894\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4059 - val_loss: 0.3886\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4070 - val_loss: 0.3888\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4075 - val_loss: 0.3880\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4108 - val_loss: 0.3869\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3945 - val_loss: 0.3916\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3954 - val_loss: 0.3867\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4104 - val_loss: 0.3901\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4072 - val_loss: 0.3865\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4153 - val_loss: 0.3906\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4050 - val_loss: 0.3860\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3894 - val_loss: 0.3853\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3910 - val_loss: 0.3864\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3984 - val_loss: 0.3848\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3899 - val_loss: 0.3861\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3961 - val_loss: 0.3857\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3879 - val_loss: 0.3849\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3923 - val_loss: 0.3844\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3878 - val_loss: 0.3846\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3943 - val_loss: 0.3842\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3812 - val_loss: 0.3836\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3887 - val_loss: 0.3849\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3851 - val_loss: 0.3841\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3894 - val_loss: 0.3840\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3893 - val_loss: 0.3842\n",
      "[2022_04_20-13:48:14] Training the entire fine-tuned model...\n",
      "[2022_04_20-13:48:39] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5644WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1068s vs `on_train_batch_end` time: 0.1427s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1068s vs `on_train_batch_end` time: 0.1427s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 714ms/step - loss: 0.5644 - val_loss: 0.4029\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.4458 - val_loss: 0.4406\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.4378 - val_loss: 0.3933\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3941 - val_loss: 0.4335\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3963 - val_loss: 0.4300\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3672 - val_loss: 0.4013\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3433 - val_loss: 0.3950\n",
      "[2022_04_20-13:49:03] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-13:49:03] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:49:07] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3927WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1093s vs `on_train_batch_end` time: 0.1378s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1093s vs `on_train_batch_end` time: 0.1378s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 676ms/step - loss: 0.3927 - val_loss: 0.3943\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>467</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  467  15\n",
       "1   86  19"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2733812949640288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1j65snpu) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 30666... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███▁▁▁▂▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▄▂▁▁▁</td></tr><tr><td>lr</td><td>████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▅█▄▃▂▂▂▃▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>51</td></tr><tr><td>best_val_loss</td><td>0.38365</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.39267</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.39434</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">ethereal-salad-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/1j65snpu\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027a/runs/1j65snpu</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_134645-1j65snpu/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1j65snpu). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/i7mxzovq\" target=\"_blank\">sage-leaf-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-13:50:01] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:50:01] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:50:01] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 737ms/step - loss: 0.7938 - val_loss: 0.6140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.5537 - val_loss: 0.6809\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4867 - val_loss: 0.5039\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4345 - val_loss: 0.5241\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4091 - val_loss: 0.5012\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4113 - val_loss: 0.4900\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3983 - val_loss: 0.5243\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3771 - val_loss: 0.4885\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3633 - val_loss: 0.4847\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3752 - val_loss: 0.5471\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3889 - val_loss: 0.4859\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3649 - val_loss: 0.4830\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3417 - val_loss: 0.5283\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3476 - val_loss: 0.4837\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3276 - val_loss: 0.4876\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3233 - val_loss: 0.4815\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3264 - val_loss: 0.4847\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3238 - val_loss: 0.4850\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3312 - val_loss: 0.4835\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3247 - val_loss: 0.4840\n",
      "[2022_04_20-13:50:34] Training the entire fine-tuned model...\n",
      "[2022_04_20-13:50:42] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 1s/step - loss: 0.4374 - val_loss: 0.5576\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3611 - val_loss: 0.4815\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3337 - val_loss: 0.4745\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3177 - val_loss: 0.4962\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3055 - val_loss: 0.4781\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2937 - val_loss: 0.4888\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2799 - val_loss: 0.4787\n",
      "[2022_04_20-13:51:04] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-13:51:04] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:51:08] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 828ms/step - loss: 0.3145 - val_loss: 0.4754\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>552</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>152</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  552  14\n",
       "1  152  13"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13541666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:i7mxzovq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 31096... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████▃▃▃▃▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▆█▂▃▂▂▃▁▁▃▁▁▃▁▁▁▁▁▁▁▄▁▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>2</td></tr><tr><td>best_val_loss</td><td>0.47446</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31451</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.47539</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">sage-leaf-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/i7mxzovq\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027b/runs/i7mxzovq</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_134945-i7mxzovq/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:i7mxzovq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/3vs1jsij\" target=\"_blank\">generous-water-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-13:52:01] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:52:01] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:52:01] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 569ms/step - loss: 0.8573 - val_loss: 0.6048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.6805 - val_loss: 0.4686\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.6373 - val_loss: 0.5749\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5921 - val_loss: 0.5400\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5439 - val_loss: 0.4400\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4811 - val_loss: 0.4377\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4875 - val_loss: 0.5227\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4954 - val_loss: 0.4171\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4629 - val_loss: 0.4121\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4537 - val_loss: 0.4310\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4405 - val_loss: 0.4072\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4455 - val_loss: 0.4049\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4307 - val_loss: 0.4022\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4385 - val_loss: 0.4008\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4329 - val_loss: 0.4011\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4152 - val_loss: 0.3966\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4118 - val_loss: 0.4107\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4142 - val_loss: 0.3961\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4203 - val_loss: 0.3956\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4159 - val_loss: 0.4327\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4318 - val_loss: 0.3934\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4150 - val_loss: 0.4050\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4072 - val_loss: 0.3888\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3920 - val_loss: 0.3859\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3933 - val_loss: 0.3947\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4082 - val_loss: 0.3898\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3892 - val_loss: 0.3891\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3857 - val_loss: 0.3787\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3803 - val_loss: 0.3787\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3878 - val_loss: 0.3797\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3926 - val_loss: 0.3788\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3792 - val_loss: 0.3788\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3903 - val_loss: 0.3796\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3755 - val_loss: 0.3792\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_20-13:52:50] Training the entire fine-tuned model...\n",
      "[2022_04_20-13:52:58] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3898WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1083s vs `on_train_batch_end` time: 0.1375s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1083s vs `on_train_batch_end` time: 0.1375s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 687ms/step - loss: 0.3898 - val_loss: 0.3795\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3880 - val_loss: 0.3805\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3870 - val_loss: 0.3791\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3874 - val_loss: 0.3783\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3811 - val_loss: 0.3791\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3808 - val_loss: 0.3779\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3751 - val_loss: 0.3783\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3721 - val_loss: 0.3779\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3802 - val_loss: 0.3777\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3797 - val_loss: 0.3784\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3766 - val_loss: 0.3785\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3790 - val_loss: 0.3773\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 340ms/step - loss: 0.3836 - val_loss: 0.3775\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3770 - val_loss: 0.3767\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3743 - val_loss: 0.3770\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 341ms/step - loss: 0.3771 - val_loss: 0.3764\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3705 - val_loss: 0.3778\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3805 - val_loss: 0.3763\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3810 - val_loss: 0.3764\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3706 - val_loss: 0.3765\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3787 - val_loss: 0.3769\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3811 - val_loss: 0.3767\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3665 - val_loss: 0.3766\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3707 - val_loss: 0.3766\n",
      "[2022_04_20-13:53:54] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-13:53:54] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:54:26] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3721WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1055s vs `on_train_batch_end` time: 0.1410s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1055s vs `on_train_batch_end` time: 0.1410s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 699ms/step - loss: 0.3721 - val_loss: 0.3763\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>467</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  467  15\n",
       "1   85  20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2857142857142857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3vs1jsij) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 31350... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▁</td></tr><tr><td>loss</td><td>█▅▅▄▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▇▃▃▂▂▂▂▂▂▂▂▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.37625</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37215</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.37625</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">generous-water-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/3vs1jsij\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027a/runs/3vs1jsij</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_135145-3vs1jsij/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3vs1jsij). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/ivuwe3pf\" target=\"_blank\">morning-cosmos-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-13:55:22] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:55:22] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:55:22] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 718ms/step - loss: 0.9281 - val_loss: 0.5358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.5911 - val_loss: 0.7022\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.5316 - val_loss: 0.5238\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4506 - val_loss: 0.5352\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4225 - val_loss: 0.4969\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4116 - val_loss: 0.4909\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3876 - val_loss: 0.5234\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3898 - val_loss: 0.4885\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3727 - val_loss: 0.5047\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3686 - val_loss: 0.4859\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3507 - val_loss: 0.4937\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3491 - val_loss: 0.4842\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3447 - val_loss: 0.5052\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3383 - val_loss: 0.4816\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3418 - val_loss: 0.4852\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3303 - val_loss: 0.5191\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3431 - val_loss: 0.4801\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3274 - val_loss: 0.4798\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3225 - val_loss: 0.5290\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3317 - val_loss: 0.4837\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3092 - val_loss: 0.5096\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3135 - val_loss: 0.4808\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3192 - val_loss: 0.4829\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3074 - val_loss: 0.5030\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_20-13:55:58] Training the entire fine-tuned model...\n",
      "[2022_04_20-13:56:06] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 843ms/step - loss: 0.3210 - val_loss: 0.4836\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3246 - val_loss: 0.4920\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3227 - val_loss: 0.4876\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3121 - val_loss: 0.4828\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3293 - val_loss: 0.4815\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3101 - val_loss: 0.4823\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3149 - val_loss: 0.4850\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3104 - val_loss: 0.4845\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3141 - val_loss: 0.4850\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3096 - val_loss: 0.4847\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3193 - val_loss: 0.4846\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_20-13:56:36] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-13:56:36] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:56:36] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 858ms/step - loss: 0.3192 - val_loss: 0.4816\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>550</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>147</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  550  16\n",
       "1  147  18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18090452261306533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ivuwe3pf) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 31744... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▄▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃█▂▃▂▁▂▁▂▁▁▁▂▁▁▂▁▁▃▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>17</td></tr><tr><td>best_val_loss</td><td>0.47979</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31921</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.4816</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">morning-cosmos-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/ivuwe3pf\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027b/runs/ivuwe3pf</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_135504-ivuwe3pf/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ivuwe3pf). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/78nql4cl\" target=\"_blank\">divine-serenity-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-13:57:30] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:57:30] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:57:30] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 564ms/step - loss: 0.9044 - val_loss: 0.7821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.6948 - val_loss: 0.4552\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5642 - val_loss: 0.4833\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5343 - val_loss: 0.4903\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5004 - val_loss: 0.4311\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4675 - val_loss: 0.4253\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4613 - val_loss: 0.4230\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4569 - val_loss: 0.4464\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4634 - val_loss: 0.4100\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4414 - val_loss: 0.4090\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4501 - val_loss: 0.4278\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4580 - val_loss: 0.4012\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4348 - val_loss: 0.4074\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4251 - val_loss: 0.4009\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4300 - val_loss: 0.3943\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4219 - val_loss: 0.3928\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4261 - val_loss: 0.3913\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4284 - val_loss: 0.4217\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4098 - val_loss: 0.3893\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3965 - val_loss: 0.4051\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4069 - val_loss: 0.3897\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4002 - val_loss: 0.3901\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4045 - val_loss: 0.3935\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3985 - val_loss: 0.3841\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3993 - val_loss: 0.3858\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4011 - val_loss: 0.3829\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3976 - val_loss: 0.3825\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3942 - val_loss: 0.3823\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3830 - val_loss: 0.3826\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3913 - val_loss: 0.3815\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3943 - val_loss: 0.3806\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3873 - val_loss: 0.3806\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3884 - val_loss: 0.3826\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4017 - val_loss: 0.3815\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3841 - val_loss: 0.3805\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3748 - val_loss: 0.3840\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3831 - val_loss: 0.3814\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3864 - val_loss: 0.3807\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3848 - val_loss: 0.3805\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3853 - val_loss: 0.3806\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3918 - val_loss: 0.3806\n",
      "[2022_04_20-13:58:26] Training the entire fine-tuned model...\n",
      "[2022_04_20-13:59:08] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3866WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1076s vs `on_train_batch_end` time: 0.1381s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1076s vs `on_train_batch_end` time: 0.1381s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 1s/step - loss: 0.3866 - val_loss: 0.3804\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3806 - val_loss: 0.3794\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3829 - val_loss: 0.3812\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3801 - val_loss: 0.3801\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3720 - val_loss: 0.3777\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3739 - val_loss: 0.3777\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3630 - val_loss: 0.3786\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 340ms/step - loss: 0.3651 - val_loss: 0.3747\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3663 - val_loss: 0.3777\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3673 - val_loss: 0.3730\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3566 - val_loss: 0.3755\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 342ms/step - loss: 0.3493 - val_loss: 0.3726\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3657 - val_loss: 0.3782\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3425 - val_loss: 0.3748\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3336 - val_loss: 0.3788\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3359 - val_loss: 0.3764\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3331 - val_loss: 0.3750\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3238 - val_loss: 0.3750\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_20-13:59:53] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-13:59:53] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-13:59:55] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3499WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1069s vs `on_train_batch_end` time: 0.1401s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1069s vs `on_train_batch_end` time: 0.1401s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 693ms/step - loss: 0.3499 - val_loss: 0.3740\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>454</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  454  28\n",
       "1   76  29"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35802469135802467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:78nql4cl) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 32020... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▃▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>11</td></tr><tr><td>best_val_loss</td><td>0.37261</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34992</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.374</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">divine-serenity-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/78nql4cl\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027a/runs/78nql4cl</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_135713-78nql4cl/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:78nql4cl). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/k2yni66v\" target=\"_blank\">azure-frog-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-14:00:47] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:00:47] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:00:47] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 746ms/step - loss: 0.9290 - val_loss: 0.6026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.6859 - val_loss: 0.7449\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.5863 - val_loss: 0.5070\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4582 - val_loss: 0.5374\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4476 - val_loss: 0.4954\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4540 - val_loss: 0.5533\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4455 - val_loss: 0.4894\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4209 - val_loss: 0.5245\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3904 - val_loss: 0.4839\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3711 - val_loss: 0.5090\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3845 - val_loss: 0.4821\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3627 - val_loss: 0.4914\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3512 - val_loss: 0.4874\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3432 - val_loss: 0.4826\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3396 - val_loss: 0.4873\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3349 - val_loss: 0.4836\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3369 - val_loss: 0.4829\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_20-14:01:17] Training the entire fine-tuned model...\n",
      "[2022_04_20-14:01:25] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 854ms/step - loss: 0.3625 - val_loss: 0.4961\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3530 - val_loss: 0.4811\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3578 - val_loss: 0.4831\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3519 - val_loss: 0.5058\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3501 - val_loss: 0.4803\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3449 - val_loss: 0.4788\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3332 - val_loss: 0.4886\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3447 - val_loss: 0.4839\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 370ms/step - loss: 0.3341 - val_loss: 0.4771\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 372ms/step - loss: 0.3278 - val_loss: 0.4885\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 370ms/step - loss: 0.3284 - val_loss: 0.4841\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 371ms/step - loss: 0.3220 - val_loss: 0.4788\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3208 - val_loss: 0.4788\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3093 - val_loss: 0.4789\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3172 - val_loss: 0.4795\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_20-14:02:01] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-14:02:01] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:02:01] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 849ms/step - loss: 0.3348 - val_loss: 0.4775\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>551</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  551  15\n",
       "1  150  15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15384615384615385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:k2yni66v) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 32441... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄█▂▃▁▃▁▂▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_val_loss</td><td>0.47705</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33475</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.47752</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">azure-frog-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/k2yni66v\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027b/runs/k2yni66v</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_140032-k2yni66v/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:k2yni66v). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/jmntp2c5\" target=\"_blank\">still-dawn-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-14:03:08] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:03:08] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:03:08] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 582ms/step - loss: 0.8994 - val_loss: 0.4957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5627 - val_loss: 0.5230\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.5288 - val_loss: 0.4692\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.5133 - val_loss: 0.4380\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.5030 - val_loss: 0.4735\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4741 - val_loss: 0.4257\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4737 - val_loss: 0.4296\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4767 - val_loss: 0.4174\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4508 - val_loss: 0.4174\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4474 - val_loss: 0.4227\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4534 - val_loss: 0.4076\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4498 - val_loss: 0.4167\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4534 - val_loss: 0.3994\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4234 - val_loss: 0.4028\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4261 - val_loss: 0.4049\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4438 - val_loss: 0.4275\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4472 - val_loss: 0.4118\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4160 - val_loss: 0.3947\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4053 - val_loss: 0.3948\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4140 - val_loss: 0.3955\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4037 - val_loss: 0.3930\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4146 - val_loss: 0.3950\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4078 - val_loss: 0.3916\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4154 - val_loss: 0.3987\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4095 - val_loss: 0.3904\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4060 - val_loss: 0.3907\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4052 - val_loss: 0.3936\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3957 - val_loss: 0.3898\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4054 - val_loss: 0.3949\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4029 - val_loss: 0.3885\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4055 - val_loss: 0.3909\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4089 - val_loss: 0.3874\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4054 - val_loss: 0.3879\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3970 - val_loss: 0.3871\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3963 - val_loss: 0.3876\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4020 - val_loss: 0.3851\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3944 - val_loss: 0.3846\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3998 - val_loss: 0.3855\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3848 - val_loss: 0.3848\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3954 - val_loss: 0.3865\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3956 - val_loss: 0.3849\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4042 - val_loss: 0.3842\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3858 - val_loss: 0.3842\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3995 - val_loss: 0.3846\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3884 - val_loss: 0.3865\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3927 - val_loss: 0.3861\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3836 - val_loss: 0.3856\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3884 - val_loss: 0.3847\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_20-14:04:12] Training the entire fine-tuned model...\n",
      "[2022_04_20-14:04:19] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3957WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1082s vs `on_train_batch_end` time: 0.1407s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1082s vs `on_train_batch_end` time: 0.1407s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 974ms/step - loss: 0.3957 - val_loss: 0.3890\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3938 - val_loss: 0.3948\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3820 - val_loss: 0.3821\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3863 - val_loss: 0.3794\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3727 - val_loss: 0.3783\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3813 - val_loss: 0.3763\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3736 - val_loss: 0.3802\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3613 - val_loss: 0.3748\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3532 - val_loss: 0.3767\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3530 - val_loss: 0.3759\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3513 - val_loss: 0.3829\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3325 - val_loss: 0.3774\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 340ms/step - loss: 0.3312 - val_loss: 0.3764\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3373 - val_loss: 0.3787\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_20-14:04:57] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-14:04:57] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:05:06] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3482WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1066s vs `on_train_batch_end` time: 0.1413s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1066s vs `on_train_batch_end` time: 0.1413s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 693ms/step - loss: 0.3482 - val_loss: 0.3748\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>459</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  459  23\n",
       "1   81  24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3157894736842105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:jmntp2c5) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 32715... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▂▃▃</td></tr><tr><td>loss</td><td>█▄▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇█▄▆▄▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▂▂▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.37483</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34822</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.37483</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">still-dawn-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/jmntp2c5\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027a/runs/jmntp2c5</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_140251-jmntp2c5/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:jmntp2c5). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/31biy88g\" target=\"_blank\">firm-durian-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-14:06:00] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:06:00] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:06:00] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 718ms/step - loss: 1.0210 - val_loss: 0.5964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.6460 - val_loss: 0.7848\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.6671 - val_loss: 0.5405\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4989 - val_loss: 0.5258\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4464 - val_loss: 0.5167\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4256 - val_loss: 0.5025\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4260 - val_loss: 0.4989\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4035 - val_loss: 0.5069\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4013 - val_loss: 0.4843\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3937 - val_loss: 0.5575\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4121 - val_loss: 0.4814\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3522 - val_loss: 0.5241\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3732 - val_loss: 0.4798\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3400 - val_loss: 0.4892\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3425 - val_loss: 0.4872\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3318 - val_loss: 0.4780\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3351 - val_loss: 0.4839\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3333 - val_loss: 0.4769\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3362 - val_loss: 0.4802\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3257 - val_loss: 0.5059\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3366 - val_loss: 0.4772\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3292 - val_loss: 0.4841\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3179 - val_loss: 0.4887\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3119 - val_loss: 0.4765\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3208 - val_loss: 0.4798\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3109 - val_loss: 0.4925\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3088 - val_loss: 0.4779\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3099 - val_loss: 0.4779\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3071 - val_loss: 0.4791\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3019 - val_loss: 0.4813\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_20-14:06:44] Training the entire fine-tuned model...\n",
      "[2022_04_20-14:06:58] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 825ms/step - loss: 0.3167 - val_loss: 0.4755\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3082 - val_loss: 0.5072\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3082 - val_loss: 0.4795\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3212 - val_loss: 0.4963\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.2974 - val_loss: 0.4783\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3016 - val_loss: 0.4820\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2951 - val_loss: 0.4827\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_20-14:07:20] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-14:07:20] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:07:20] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 815ms/step - loss: 0.3117 - val_loss: 0.4763\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>542</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>143</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  542  24\n",
       "1  143  22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20853080568720378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:31biy88g) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 33127... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▁</td></tr><tr><td>loss</td><td>█▄▅▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄█▂▂▂▂▂▂▁▃▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.47552</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31171</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.47631</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">firm-durian-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/31biy88g\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027b/runs/31biy88g</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_140544-31biy88g/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:31biy88g). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/3vztxzo6\" target=\"_blank\">bumbling-frost-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-14:08:15] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:08:15] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:08:15] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 561ms/step - loss: 0.8851 - val_loss: 0.8542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.6888 - val_loss: 0.4664\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5818 - val_loss: 0.5032\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5587 - val_loss: 0.6460\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5181 - val_loss: 0.5166\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5604 - val_loss: 0.4345\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5014 - val_loss: 0.4595\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4716 - val_loss: 0.4340\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4823 - val_loss: 0.4300\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4729 - val_loss: 0.4297\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4617 - val_loss: 0.4206\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4637 - val_loss: 0.4207\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4617 - val_loss: 0.4200\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4508 - val_loss: 0.4172\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4526 - val_loss: 0.4173\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4589 - val_loss: 0.4153\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4502 - val_loss: 0.4126\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4512 - val_loss: 0.4141\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4463 - val_loss: 0.4125\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4521 - val_loss: 0.4119\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4439 - val_loss: 0.4090\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4418 - val_loss: 0.4090\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4434 - val_loss: 0.4089\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4374 - val_loss: 0.4058\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4351 - val_loss: 0.4078\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4388 - val_loss: 0.4039\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4364 - val_loss: 0.4069\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4358 - val_loss: 0.4034\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4338 - val_loss: 0.4013\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4443 - val_loss: 0.4060\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4306 - val_loss: 0.4008\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4319 - val_loss: 0.4017\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4250 - val_loss: 0.4007\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4226 - val_loss: 0.3984\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4267 - val_loss: 0.3995\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4198 - val_loss: 0.3969\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4177 - val_loss: 0.3968\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4196 - val_loss: 0.3951\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4210 - val_loss: 0.3959\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4204 - val_loss: 0.3953\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4179 - val_loss: 0.3933\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4124 - val_loss: 0.3941\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4132 - val_loss: 0.3935\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4201 - val_loss: 0.3927\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4306 - val_loss: 0.3917\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4216 - val_loss: 0.3941\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3988 - val_loss: 0.3911\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4050 - val_loss: 0.3920\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3998 - val_loss: 0.3920\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4074 - val_loss: 0.3902\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4039 - val_loss: 0.3891\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4102 - val_loss: 0.3886\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4003 - val_loss: 0.3899\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3981 - val_loss: 0.3879\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4061 - val_loss: 0.3894\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4033 - val_loss: 0.3881\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4009 - val_loss: 0.3879\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4003 - val_loss: 0.3886\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4025 - val_loss: 0.3900\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3961 - val_loss: 0.3868\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4012 - val_loss: 0.3867\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3977 - val_loss: 0.3899\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3946 - val_loss: 0.3879\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4031 - val_loss: 0.3863\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4007 - val_loss: 0.3864\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3948 - val_loss: 0.3905\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3921 - val_loss: 0.3872\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3989 - val_loss: 0.3865\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3924 - val_loss: 0.3862\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3971 - val_loss: 0.3863\n",
      "\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3950 - val_loss: 0.3863\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3901 - val_loss: 0.3864\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3987 - val_loss: 0.3865\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4006 - val_loss: 0.3865\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3986 - val_loss: 0.3865\n",
      "[2022_04_20-14:09:48] Training the entire fine-tuned model...\n",
      "[2022_04_20-14:10:53] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4427WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1086s vs `on_train_batch_end` time: 0.1389s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1086s vs `on_train_batch_end` time: 0.1389s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 738ms/step - loss: 0.4427 - val_loss: 0.4299\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.4538 - val_loss: 0.4234\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.4306 - val_loss: 0.3943\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.4066 - val_loss: 0.4263\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.4307 - val_loss: 0.4025\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3865 - val_loss: 0.3859\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3521 - val_loss: 0.3851\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3564 - val_loss: 0.3879\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3330 - val_loss: 0.3969\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3064 - val_loss: 0.3870\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.2961 - val_loss: 0.3807\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.2864 - val_loss: 0.3841\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.2748 - val_loss: 0.3857\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.2693 - val_loss: 0.3897\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.2543 - val_loss: 0.3891\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.2549 - val_loss: 0.3900\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.2458 - val_loss: 0.3908\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "[2022_04_20-14:11:35] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-14:11:35] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:11:40] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.2784WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1062s vs `on_train_batch_end` time: 0.1424s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1062s vs `on_train_batch_end` time: 0.1424s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 718ms/step - loss: 0.2784 - val_loss: 0.3817\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>445</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  445  37\n",
       "1   72  33"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37714285714285717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3vztxzo6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 33430... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▁▁▁</td></tr><tr><td>lr</td><td>███▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>10</td></tr><tr><td>best_val_loss</td><td>0.38069</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.2784</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.38169</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">bumbling-frost-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/3vztxzo6\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027a/runs/3vztxzo6</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_140758-3vztxzo6/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3vztxzo6). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/1qbjo6kj\" target=\"_blank\">atomic-brook-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-14:12:34] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:12:35] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:12:35] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 725ms/step - loss: 0.8957 - val_loss: 0.5624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.5346 - val_loss: 0.7146\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.5224 - val_loss: 0.5328\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4458 - val_loss: 0.5730\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4231 - val_loss: 0.4944\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3996 - val_loss: 0.5111\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3802 - val_loss: 0.4840\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3755 - val_loss: 0.4826\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3740 - val_loss: 0.5002\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3623 - val_loss: 0.4867\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3806 - val_loss: 0.4853\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.3525 - val_loss: 0.4805\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3423 - val_loss: 0.4838\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3470 - val_loss: 0.4826\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3387 - val_loss: 0.4806\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3371 - val_loss: 0.4809\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3390 - val_loss: 0.4832\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3414 - val_loss: 0.4849\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_20-14:13:06] Training the entire fine-tuned model...\n",
      "[2022_04_20-14:13:14] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 831ms/step - loss: 0.4322 - val_loss: 0.5946\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.4052 - val_loss: 0.5162\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3717 - val_loss: 0.5477\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3585 - val_loss: 0.4795\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3279 - val_loss: 0.4949\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3099 - val_loss: 0.4734\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2969 - val_loss: 0.4789\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.2795 - val_loss: 0.5034\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2784 - val_loss: 0.5105\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.2587 - val_loss: 0.4859\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.2368 - val_loss: 0.4790\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.2339 - val_loss: 0.5104\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_20-14:13:44] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-14:13:44] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:14:01] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 862ms/step - loss: 0.3074 - val_loss: 0.4753\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>554</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>148</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  554  12\n",
       "1  148  17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17525773195876287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1qbjo6kj) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 34027... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██▁▁▂▂▃▃▃▄▄▅▅▆▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▃▃▂▂▂▃▂▂▂▂▂▂▂▃▃▂▂▂▂▂▁▁▁▁▁▂</td></tr><tr><td>lr</td><td>███████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄█▃▄▂▂▁▁▂▁▁▁▁▁▁▁▁▁▅▂▃▁▂▁▁▂▂▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.47337</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30737</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.47535</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">atomic-brook-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/1qbjo6kj\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027b/runs/1qbjo6kj</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_141219-1qbjo6kj/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1qbjo6kj). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/aii75w0f\" target=\"_blank\">trim-dust-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-14:14:54] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:14:54] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:14:54] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 583ms/step - loss: 0.9181 - val_loss: 0.5691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.6007 - val_loss: 0.4705\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5227 - val_loss: 0.4565\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5032 - val_loss: 0.4521\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4822 - val_loss: 0.4327\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4727 - val_loss: 0.4222\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4643 - val_loss: 0.4311\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4608 - val_loss: 0.4130\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4525 - val_loss: 0.4083\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4421 - val_loss: 0.4083\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4390 - val_loss: 0.4020\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4346 - val_loss: 0.4023\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4371 - val_loss: 0.3990\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4272 - val_loss: 0.4036\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4278 - val_loss: 0.3967\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4375 - val_loss: 0.3934\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4585 - val_loss: 0.3941\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4311 - val_loss: 0.4423\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4147 - val_loss: 0.3926\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4082 - val_loss: 0.3856\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3996 - val_loss: 0.3847\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3946 - val_loss: 0.3840\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3856 - val_loss: 0.3841\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3978 - val_loss: 0.3843\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4099 - val_loss: 0.4011\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3963 - val_loss: 0.4319\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4021 - val_loss: 0.3872\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3827 - val_loss: 0.3859\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3728 - val_loss: 0.3818\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3892 - val_loss: 0.3808\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3696 - val_loss: 0.3828\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3819 - val_loss: 0.3799\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3695 - val_loss: 0.3806\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3806 - val_loss: 0.3797\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3806 - val_loss: 0.3799\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3765 - val_loss: 0.3809\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3711 - val_loss: 0.3795\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3925 - val_loss: 0.3806\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3896 - val_loss: 0.3795\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3671 - val_loss: 0.3794\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3714 - val_loss: 0.3789\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3726 - val_loss: 0.3806\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3818 - val_loss: 0.3784\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3731 - val_loss: 0.3790\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3879 - val_loss: 0.3779\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3835 - val_loss: 0.3797\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3739 - val_loss: 0.3776\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3902 - val_loss: 0.3794\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3947 - val_loss: 0.3774\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3799 - val_loss: 0.3787\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3706 - val_loss: 0.3778\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3708 - val_loss: 0.3786\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.3791 - val_loss: 0.3776\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.3716 - val_loss: 0.3781\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.3655 - val_loss: 0.3777\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3590 - val_loss: 0.3787\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.3676 - val_loss: 0.3781\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_20-14:16:08] Training the entire fine-tuned model...\n",
      "[2022_04_20-14:16:33] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3593WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1076s vs `on_train_batch_end` time: 0.1396s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1076s vs `on_train_batch_end` time: 0.1396s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 688ms/step - loss: 0.3593 - val_loss: 0.3775\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3685 - val_loss: 0.3772\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3649 - val_loss: 0.3774\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 332ms/step - loss: 0.3780 - val_loss: 0.3771\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3690 - val_loss: 0.3769\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3702 - val_loss: 0.3774\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3736 - val_loss: 0.3775\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3727 - val_loss: 0.3764\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3591 - val_loss: 0.3766\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3541 - val_loss: 0.3777\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3618 - val_loss: 0.3773\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3779 - val_loss: 0.3763\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3519 - val_loss: 0.3762\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3520 - val_loss: 0.3756\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3596 - val_loss: 0.3757\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3590 - val_loss: 0.3762\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 341ms/step - loss: 0.3619 - val_loss: 0.3757\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3568 - val_loss: 0.3754\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3553 - val_loss: 0.3759\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3536 - val_loss: 0.3750\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3589 - val_loss: 0.3749\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3639 - val_loss: 0.3758\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3539 - val_loss: 0.3762\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 2s 340ms/step - loss: 0.3578 - val_loss: 0.3765\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 2s 341ms/step - loss: 0.3488 - val_loss: 0.3749\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 2s 341ms/step - loss: 0.3548 - val_loss: 0.3750\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3493 - val_loss: 0.3753\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3572 - val_loss: 0.3755\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3612 - val_loss: 0.3753\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_20-14:17:39] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-14:17:39] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:18:01] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3561WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1091s vs `on_train_batch_end` time: 0.1396s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1091s vs `on_train_batch_end` time: 0.1396s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 687ms/step - loss: 0.3561 - val_loss: 0.3749\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>466</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  466  16\n",
       "1   82  23"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3194444444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:aii75w0f) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 34294... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▄▄▄▄▁</td></tr><tr><td>loss</td><td>█▃▃▂▂▂▂▂▂▂▁▂▂▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▃▂▂▂▂▃▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.37487</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3561</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.37487</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">trim-dust-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/aii75w0f\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027a/runs/aii75w0f</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_141438-aii75w0f/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:aii75w0f). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/y8bdqgv5\" target=\"_blank\">magic-frog-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-14:18:55] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:18:55] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:18:55] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 713ms/step - loss: 0.8425 - val_loss: 0.5536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.5589 - val_loss: 0.7486\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.5137 - val_loss: 0.5490\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4533 - val_loss: 0.5493\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4192 - val_loss: 0.4977\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3955 - val_loss: 0.4917\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3778 - val_loss: 0.5280\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3911 - val_loss: 0.4901\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3780 - val_loss: 0.5585\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3887 - val_loss: 0.4830\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3507 - val_loss: 0.5060\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3435 - val_loss: 0.4788\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3385 - val_loss: 0.4960\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3417 - val_loss: 0.4814\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3423 - val_loss: 0.4887\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3525 - val_loss: 0.5257\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3383 - val_loss: 0.4813\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3542 - val_loss: 0.4773\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3367 - val_loss: 0.5160\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3342 - val_loss: 0.4829\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3227 - val_loss: 0.4753\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3196 - val_loss: 0.4940\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3187 - val_loss: 0.4811\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3174 - val_loss: 0.4769\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3274 - val_loss: 0.4907\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3184 - val_loss: 0.4843\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3094 - val_loss: 0.4796\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3111 - val_loss: 0.4789\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3142 - val_loss: 0.4788\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_20-14:19:37] Training the entire fine-tuned model...\n",
      "[2022_04_20-14:19:45] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 868ms/step - loss: 0.3306 - val_loss: 0.4794\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3113 - val_loss: 0.4853\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3287 - val_loss: 0.4862\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3224 - val_loss: 0.4817\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3223 - val_loss: 0.4774\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3192 - val_loss: 0.4770\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3102 - val_loss: 0.4806\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3061 - val_loss: 0.4813\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3157 - val_loss: 0.4815\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3145 - val_loss: 0.4808\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3158 - val_loss: 0.4800\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3085 - val_loss: 0.4796\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3052 - val_loss: 0.4800\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3087 - val_loss: 0.4798\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_20-14:20:19] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-14:20:19] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:20:20] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 834ms/step - loss: 0.3183 - val_loss: 0.4773\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>551</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>148</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  551  15\n",
       "1  148  17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17258883248730963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:y8bdqgv5) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 34848... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▆▆▇▇▇▇▇██▁▁▁▂▂▃▃▃▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▄▄▃▂▂▂▂▂▂▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃█▃▃▂▁▂▁▃▁▁▂▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>20</td></tr><tr><td>best_val_loss</td><td>0.47534</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31832</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.4773</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">magic-frog-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/y8bdqgv5\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027b/runs/y8bdqgv5</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_141839-y8bdqgv5/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:y8bdqgv5). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/1cc7552p\" target=\"_blank\">winter-leaf-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-14:21:16] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:21:16] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:21:16] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 602ms/step - loss: 0.9116 - val_loss: 0.9097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.7137 - val_loss: 0.4683\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.5774 - val_loss: 0.4968\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5961 - val_loss: 0.5980\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5346 - val_loss: 0.4552\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4889 - val_loss: 0.4317\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4802 - val_loss: 0.4588\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4808 - val_loss: 0.4295\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4872 - val_loss: 0.4155\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4721 - val_loss: 0.4286\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4480 - val_loss: 0.4025\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4386 - val_loss: 0.4014\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4385 - val_loss: 0.4019\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4318 - val_loss: 0.4208\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4438 - val_loss: 0.3950\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4310 - val_loss: 0.3955\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4166 - val_loss: 0.3920\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4104 - val_loss: 0.3990\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4104 - val_loss: 0.4030\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4180 - val_loss: 0.3941\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4143 - val_loss: 0.3961\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4091 - val_loss: 0.3927\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4091 - val_loss: 0.3889\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4016 - val_loss: 0.3858\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3986 - val_loss: 0.3886\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3944 - val_loss: 0.3854\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3972 - val_loss: 0.3879\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4049 - val_loss: 0.3850\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4012 - val_loss: 0.3876\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3881 - val_loss: 0.3857\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3943 - val_loss: 0.3836\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3839 - val_loss: 0.3880\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4002 - val_loss: 0.3831\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3871 - val_loss: 0.3839\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3998 - val_loss: 0.3892\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3975 - val_loss: 0.3828\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3981 - val_loss: 0.3831\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3884 - val_loss: 0.3831\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3880 - val_loss: 0.3830\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3940 - val_loss: 0.3841\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3926 - val_loss: 0.3824\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3861 - val_loss: 0.3823\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3854 - val_loss: 0.3834\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3881 - val_loss: 0.3848\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3823 - val_loss: 0.3836\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3847 - val_loss: 0.3831\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3896 - val_loss: 0.3828\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3973 - val_loss: 0.3828\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3903 - val_loss: 0.3828\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3825 - val_loss: 0.3828\n",
      "[2022_04_20-14:22:23] Training the entire fine-tuned model...\n",
      "[2022_04_20-14:22:31] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3841WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1062s vs `on_train_batch_end` time: 0.1401s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1062s vs `on_train_batch_end` time: 0.1401s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 845ms/step - loss: 0.3841 - val_loss: 0.3868\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 340ms/step - loss: 0.3926 - val_loss: 0.3816\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3927 - val_loss: 0.3874\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 340ms/step - loss: 0.3779 - val_loss: 0.3801\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3764 - val_loss: 0.3827\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3775 - val_loss: 0.3774\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3745 - val_loss: 0.3769\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3757 - val_loss: 0.3805\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3721 - val_loss: 0.3758\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3740 - val_loss: 0.3812\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3713 - val_loss: 0.3767\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3706 - val_loss: 0.3765\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3552 - val_loss: 0.3774\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3600 - val_loss: 0.3768\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3526 - val_loss: 0.3769\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 343ms/step - loss: 0.3386 - val_loss: 0.3770\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3501 - val_loss: 0.3763\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_20-14:23:14] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-14:23:14] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:23:14] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3702WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1058s vs `on_train_batch_end` time: 0.1409s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1058s vs `on_train_batch_end` time: 0.1409s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 685ms/step - loss: 0.3702 - val_loss: 0.3759\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>464</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  464  18\n",
       "1   83  22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30344827586206896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1cc7552p) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 35178... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▆▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▄▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_val_loss</td><td>0.37576</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37017</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.37587</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">winter-leaf-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/1cc7552p\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027a/runs/1cc7552p</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_142059-1cc7552p/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1cc7552p). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/1e5wr2xk\" target=\"_blank\">dainty-brook-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-14:24:06] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:24:06] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:24:06] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 734ms/step - loss: 1.0372 - val_loss: 0.6225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.6105 - val_loss: 0.7725\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.6726 - val_loss: 0.5314\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4936 - val_loss: 0.5542\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.5134 - val_loss: 0.5139\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4301 - val_loss: 0.5074\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4035 - val_loss: 0.4957\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3825 - val_loss: 0.4885\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3635 - val_loss: 0.4994\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3678 - val_loss: 0.4816\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3592 - val_loss: 0.4899\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3540 - val_loss: 0.4894\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3561 - val_loss: 0.4793\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3531 - val_loss: 0.5395\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3700 - val_loss: 0.4885\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3463 - val_loss: 0.4888\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3396 - val_loss: 0.4979\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3284 - val_loss: 0.4887\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3160 - val_loss: 0.4772\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3221 - val_loss: 0.4787\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3212 - val_loss: 0.4867\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3264 - val_loss: 0.4898\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3215 - val_loss: 0.4778\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3161 - val_loss: 0.4788\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3161 - val_loss: 0.4804\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3181 - val_loss: 0.4831\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3256 - val_loss: 0.4834\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_20-14:24:47] Training the entire fine-tuned model...\n",
      "[2022_04_20-14:24:55] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 836ms/step - loss: 0.3218 - val_loss: 0.4811\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3279 - val_loss: 0.4782\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3195 - val_loss: 0.4959\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3174 - val_loss: 0.4762\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3099 - val_loss: 0.4802\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3122 - val_loss: 0.4774\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3032 - val_loss: 0.4791\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2961 - val_loss: 0.4869\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2990 - val_loss: 0.4805\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2956 - val_loss: 0.4784\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3029 - val_loss: 0.4821\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.2940 - val_loss: 0.4837\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_20-14:25:25] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-14:25:25] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:25:25] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 834ms/step - loss: 0.3073 - val_loss: 0.4760\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>550</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>147</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  550  16\n",
       "1  147  18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18090452261306533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1e5wr2xk) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 35627... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▄▅▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄█▂▃▂▂▁▁▂▁▁▁▁▃▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.47601</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30734</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.47601</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dainty-brook-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/1e5wr2xk\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027b/runs/1e5wr2xk</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_142350-1e5wr2xk/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1e5wr2xk). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/77tkvmmq\" target=\"_blank\">polar-cloud-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-14:26:20] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:26:20] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:26:20] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 561ms/step - loss: 0.9738 - val_loss: 0.8969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.6649 - val_loss: 0.5149\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5810 - val_loss: 0.4644\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5507 - val_loss: 0.5154\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5219 - val_loss: 0.4361\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4850 - val_loss: 0.4316\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4844 - val_loss: 0.4347\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4650 - val_loss: 0.4157\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4567 - val_loss: 0.4157\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4482 - val_loss: 0.4124\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4475 - val_loss: 0.4089\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4449 - val_loss: 0.4067\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4446 - val_loss: 0.4014\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4269 - val_loss: 0.3991\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4273 - val_loss: 0.4066\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4248 - val_loss: 0.4143\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4084 - val_loss: 0.3995\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4130 - val_loss: 0.3914\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4310 - val_loss: 0.3923\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3982 - val_loss: 0.3909\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4114 - val_loss: 0.3956\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4167 - val_loss: 0.3893\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3936 - val_loss: 0.3849\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4004 - val_loss: 0.3915\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4323 - val_loss: 0.4297\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4005 - val_loss: 0.4057\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3971 - val_loss: 0.3862\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3741 - val_loss: 0.3797\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3947 - val_loss: 0.3824\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3778 - val_loss: 0.3796\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3886 - val_loss: 0.3788\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3826 - val_loss: 0.3806\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3916 - val_loss: 0.3794\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3851 - val_loss: 0.3819\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3871 - val_loss: 0.3800\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3848 - val_loss: 0.3791\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3818 - val_loss: 0.3815\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3817 - val_loss: 0.3808\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3720 - val_loss: 0.3793\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_20-14:27:15] Training the entire fine-tuned model...\n",
      "[2022_04_20-14:27:23] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4049WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1057s vs `on_train_batch_end` time: 0.1419s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1057s vs `on_train_batch_end` time: 0.1419s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 693ms/step - loss: 0.4049 - val_loss: 0.4049\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.4039 - val_loss: 0.3787\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3891 - val_loss: 0.3798\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3739 - val_loss: 0.3753\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3705 - val_loss: 0.3764\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3622 - val_loss: 0.3813\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3580 - val_loss: 0.3755\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3429 - val_loss: 0.3837\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3398 - val_loss: 0.3758\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3280 - val_loss: 0.3752\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3255 - val_loss: 0.3754\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3203 - val_loss: 0.3781\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3354 - val_loss: 0.3769\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3220 - val_loss: 0.3755\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3177 - val_loss: 0.3755\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 343ms/step - loss: 0.3287 - val_loss: 0.3752\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3338 - val_loss: 0.3753\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3348 - val_loss: 0.3755\n",
      "[2022_04_20-14:28:06] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-14:28:06] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:28:07] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3417WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1072s vs `on_train_batch_end` time: 0.1406s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1072s vs `on_train_batch_end` time: 0.1406s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 695ms/step - loss: 0.3417 - val_loss: 0.3767\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>452</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  452  30\n",
       "1   76  29"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35365853658536583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:77tkvmmq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 35951... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>9</td></tr><tr><td>best_val_loss</td><td>0.37521</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3417</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.37674</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">polar-cloud-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/77tkvmmq\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027a/runs/77tkvmmq</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_142603-77tkvmmq/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:77tkvmmq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/11664iuj\" target=\"_blank\">deep-durian-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-14:29:01] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:29:01] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:29:01] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 776ms/step - loss: 0.8642 - val_loss: 0.5246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.5955 - val_loss: 0.7943\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.5621 - val_loss: 0.5484\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4529 - val_loss: 0.5656\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4306 - val_loss: 0.4927\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4052 - val_loss: 0.5001\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3953 - val_loss: 0.5289\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3946 - val_loss: 0.4891\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3714 - val_loss: 0.5808\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3956 - val_loss: 0.5023\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3643 - val_loss: 0.5169\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3517 - val_loss: 0.4818\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3401 - val_loss: 0.4853\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3366 - val_loss: 0.5061\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.3387 - val_loss: 0.4787\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3278 - val_loss: 0.4813\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3223 - val_loss: 0.4820\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3141 - val_loss: 0.4981\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3237 - val_loss: 0.4786\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3216 - val_loss: 0.4837\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3242 - val_loss: 0.5003\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3187 - val_loss: 0.4772\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3228 - val_loss: 0.4826\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3066 - val_loss: 0.4905\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3121 - val_loss: 0.4796\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3083 - val_loss: 0.4799\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3084 - val_loss: 0.4805\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3080 - val_loss: 0.4836\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3115 - val_loss: 0.4859\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3088 - val_loss: 0.4837\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_20-14:29:46] Training the entire fine-tuned model...\n",
      "[2022_04_20-14:29:59] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 844ms/step - loss: 0.3391 - val_loss: 0.4827\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3063 - val_loss: 0.4769\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3097 - val_loss: 0.5063\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3029 - val_loss: 0.4801\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.2952 - val_loss: 0.4893\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2793 - val_loss: 0.4870\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2752 - val_loss: 0.4856\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2678 - val_loss: 0.4873\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.2591 - val_loss: 0.4915\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2685 - val_loss: 0.4832\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_20-14:30:25] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-14:30:25] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:30:26] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 853ms/step - loss: 0.3156 - val_loss: 0.4779\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>544</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>141</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  544  22\n",
       "1  141  24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22748815165876776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:11664iuj) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 36350... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▅▅▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂</td></tr><tr><td>lr</td><td>███████████████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▂█▃▃▁▂▂▁▃▂▂▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_val_loss</td><td>0.47692</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31559</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.47789</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">deep-durian-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/11664iuj\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027b/runs/11664iuj</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_142846-11664iuj/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:11664iuj). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/1qcckos7\" target=\"_blank\">sunny-planet-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-14:31:21] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:31:21] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:31:21] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 843ms/step - loss: 0.8559 - val_loss: 1.0047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.7486 - val_loss: 0.4982\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.5596 - val_loss: 0.4548\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5402 - val_loss: 0.5848\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5624 - val_loss: 0.4393\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5281 - val_loss: 0.4761\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5303 - val_loss: 0.5347\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5080 - val_loss: 0.4324\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4734 - val_loss: 0.4111\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4887 - val_loss: 0.5025\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4710 - val_loss: 0.4146\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4532 - val_loss: 0.4067\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4371 - val_loss: 0.3999\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4376 - val_loss: 0.4058\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4677 - val_loss: 0.4111\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4352 - val_loss: 0.3958\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4192 - val_loss: 0.3969\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4170 - val_loss: 0.3936\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4074 - val_loss: 0.3996\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4174 - val_loss: 0.4077\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4113 - val_loss: 0.3954\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4016 - val_loss: 0.3874\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4119 - val_loss: 0.4012\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4158 - val_loss: 0.3873\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4023 - val_loss: 0.3890\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.4019 - val_loss: 0.3844\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3916 - val_loss: 0.3861\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3908 - val_loss: 0.3905\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3854 - val_loss: 0.3874\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4050 - val_loss: 0.4002\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4076 - val_loss: 0.3848\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3926 - val_loss: 0.3810\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3900 - val_loss: 0.3820\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3764 - val_loss: 0.3876\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3837 - val_loss: 0.3795\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3855 - val_loss: 0.3793\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3842 - val_loss: 0.3814\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3804 - val_loss: 0.3788\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3839 - val_loss: 0.3792\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3903 - val_loss: 0.3783\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3798 - val_loss: 0.3779\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3800 - val_loss: 0.3825\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3793 - val_loss: 0.3804\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3740 - val_loss: 0.3876\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3850 - val_loss: 0.3782\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3879 - val_loss: 0.3775\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3640 - val_loss: 0.3800\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3702 - val_loss: 0.3797\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3795 - val_loss: 0.3787\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3798 - val_loss: 0.3775\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3770 - val_loss: 0.3774\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3728 - val_loss: 0.3776\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3740 - val_loss: 0.3776\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3744 - val_loss: 0.3779\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3675 - val_loss: 0.3779\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3739 - val_loss: 0.3779\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3751 - val_loss: 0.3778\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 200ms/step - loss: 0.3764 - val_loss: 0.3778\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3905 - val_loss: 0.3778\n",
      "[2022_04_20-14:32:38] Training the entire fine-tuned model...\n",
      "[2022_04_20-14:32:46] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4187WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1090s vs `on_train_batch_end` time: 0.1386s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1090s vs `on_train_batch_end` time: 0.1386s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 677ms/step - loss: 0.4187 - val_loss: 0.4283\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.4101 - val_loss: 0.3812\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3771 - val_loss: 0.3774\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3683 - val_loss: 0.3801\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3752 - val_loss: 0.4173\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3489 - val_loss: 0.3750\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3322 - val_loss: 0.4134\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 340ms/step - loss: 0.3481 - val_loss: 0.4449\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3583 - val_loss: 0.4351\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3422 - val_loss: 0.3933\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.2857 - val_loss: 0.3895\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.2766 - val_loss: 0.3909\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.2653 - val_loss: 0.3850\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.2587 - val_loss: 0.3821\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_20-14:33:22] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-14:33:22] Training set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:33:33] Validation set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3227WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1108s vs `on_train_batch_end` time: 0.1361s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1108s vs `on_train_batch_end` time: 0.1361s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 708ms/step - loss: 0.3227 - val_loss: 0.3823\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>442</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  442  40\n",
       "1   69  36"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39779005524861877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_20_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1qcckos7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 36664... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▁</td></tr><tr><td>loss</td><td>█▇▄▄▄▄▃▃▃▃▃▃▃▃▂▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▁▁▂</td></tr><tr><td>lr</td><td>█████████████████▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.37497</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32269</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.38226</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">sunny-planet-16</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027a/runs/1qcckos7\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027a/runs/1qcckos7</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_143105-1qcckos7/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1qcckos7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/1t7v65tg\" target=\"_blank\">vivid-river-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-14:34:27] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:34:27] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:34:27] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 704ms/step - loss: 0.9470 - val_loss: 0.6049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.6112 - val_loss: 0.7307\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.5877 - val_loss: 0.5060\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4723 - val_loss: 0.5879\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4617 - val_loss: 0.4938\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4033 - val_loss: 0.4999\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3809 - val_loss: 0.4879\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3818 - val_loss: 0.4890\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3701 - val_loss: 0.4837\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3685 - val_loss: 0.5106\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3576 - val_loss: 0.4823\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3575 - val_loss: 0.4814\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3521 - val_loss: 0.4918\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3297 - val_loss: 0.4802\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3381 - val_loss: 0.5186\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3485 - val_loss: 0.4817\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3286 - val_loss: 0.4791\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3298 - val_loss: 0.4786\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3301 - val_loss: 0.4862\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3187 - val_loss: 0.4960\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3248 - val_loss: 0.4905\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3153 - val_loss: 0.4779\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3052 - val_loss: 0.5057\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3124 - val_loss: 0.4863\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3127 - val_loss: 0.5162\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3115 - val_loss: 0.4790\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3046 - val_loss: 0.5016\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.2920 - val_loss: 0.4817\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.2884 - val_loss: 0.4778\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.2965 - val_loss: 0.4885\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.2870 - val_loss: 0.4828\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3086 - val_loss: 0.4782\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3024 - val_loss: 0.4963\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.2975 - val_loss: 0.4868\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.2976 - val_loss: 0.4809\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.2967 - val_loss: 0.4804\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.2866 - val_loss: 0.4833\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_20-14:35:18] Training the entire fine-tuned model...\n",
      "[2022_04_20-14:36:07] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 826ms/step - loss: 0.4714 - val_loss: 0.7505\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.4194 - val_loss: 0.4804\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 372ms/step - loss: 0.3275 - val_loss: 0.5399\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3201 - val_loss: 0.4785\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3131 - val_loss: 0.5089\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.2919 - val_loss: 0.4772\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.2857 - val_loss: 0.5095\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2764 - val_loss: 0.5015\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2700 - val_loss: 0.5056\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.2794 - val_loss: 0.5992\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.2522 - val_loss: 0.4933\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.2222 - val_loss: 0.4933\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.2189 - val_loss: 0.5249\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2196 - val_loss: 0.5180\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_20-14:36:40] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-14:36:40] Training set: Filtered out 0 of 587 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:36:40] Validation set: Filtered out 0 of 731 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 1s/step - loss: 0.2771 - val_loss: 0.4778\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>534</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>135</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  534  32\n",
       "1  135  30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2643171806167401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_20_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1t7v65tg) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 37138... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇███▁▁▂▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▅▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▁▁▁▁▂</td></tr><tr><td>lr</td><td>████████████████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄▇▂▄▂▁▁▂▁▁▁▂▁▁▁▁▁▂▁▂▂▁▁▁▁▁▁▁▁█▃▁▂▂▂▂▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.47724</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.27709</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.47784</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">vivid-river-16</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2027b/runs/1t7v65tg\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2027b/runs/1t7v65tg</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_143411-1t7v65tg/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1t7v65tg). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/15uqm53g\" target=\"_blank\">gentle-shape-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-14:37:36] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:37:36] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:37:36] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 598ms/step - loss: 0.8894 - val_loss: 0.6698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.5519 - val_loss: 0.5074\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5006 - val_loss: 0.4677\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5126 - val_loss: 0.4679\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4633 - val_loss: 0.4596\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4650 - val_loss: 0.4562\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4544 - val_loss: 0.4941\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4677 - val_loss: 0.4515\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4876 - val_loss: 0.4570\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4301 - val_loss: 0.4530\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4254 - val_loss: 0.4388\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4087 - val_loss: 0.4507\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4381 - val_loss: 0.4721\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4290 - val_loss: 0.4373\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4463 - val_loss: 0.5354\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4669 - val_loss: 0.5318\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4501 - val_loss: 0.4692\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-14:38:05] Training the entire fine-tuned model...\n",
      "[2022_04_20-14:38:33] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4165WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1085s vs `on_train_batch_end` time: 0.1217s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1085s vs `on_train_batch_end` time: 0.1217s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 697ms/step - loss: 0.4165 - val_loss: 0.4342\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4138 - val_loss: 0.4372\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4080 - val_loss: 0.4400\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.4089 - val_loss: 0.4374\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "[2022_04_20-14:38:50] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-14:38:50] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:38:50] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4124WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0967s vs `on_train_batch_end` time: 0.1224s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0967s vs `on_train_batch_end` time: 0.1224s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 684ms/step - loss: 0.4132 - val_loss: 0.4341\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>534</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  534  5\n",
       "1  125  6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08450704225352113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:15uqm53g) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 37506... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██▁▁▂▂▁</td></tr><tr><td>loss</td><td>█▃▂▃▂▂▂▂▂▁▁▁▁▁▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▂▃▂▂▂▁▁▂▁▄▄▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.43412</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.4132</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43412</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">gentle-shape-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/15uqm53g\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036a/runs/15uqm53g</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_143717-15uqm53g/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:15uqm53g). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/3lczkfg3\" target=\"_blank\">whole-mountain-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-14:39:44] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:39:44] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:39:44] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 575ms/step - loss: 1.0013 - val_loss: 0.7637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.6509 - val_loss: 0.6863\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.5510 - val_loss: 0.5019\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.5506 - val_loss: 0.4886\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4576 - val_loss: 0.5805\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4742 - val_loss: 0.4815\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4319 - val_loss: 0.4960\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4142 - val_loss: 0.4579\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4078 - val_loss: 0.4792\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4170 - val_loss: 0.5520\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4668 - val_loss: 0.4923\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-14:40:08] Training the entire fine-tuned model...\n",
      "[2022_04_20-14:40:22] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4190WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1057s vs `on_train_batch_end` time: 0.1251s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1057s vs `on_train_batch_end` time: 0.1251s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 826ms/step - loss: 0.4190 - val_loss: 0.4590\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.4155 - val_loss: 0.4667\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.4041 - val_loss: 0.4688\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.4122 - val_loss: 0.4627\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "[2022_04_20-14:40:38] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-14:40:38] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:40:48] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4060WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1019s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1019s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 675ms/step - loss: 0.4060 - val_loss: 0.4589\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>509</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>136</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  509  0\n",
       "1  136  3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04225352112676056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3lczkfg3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 37712... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▅▅▆▇▇█▁▂▂▃▁</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▁▁▁▁▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▂▂▄▂▂▁▁▃▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.45791</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.40601</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.45891</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">whole-mountain-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/3lczkfg3\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036b/runs/3lczkfg3</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_143927-3lczkfg3/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3lczkfg3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/2f27rq09\" target=\"_blank\">peachy-pond-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-14:41:43] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:41:43] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:41:43] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 573ms/step - loss: 1.0113 - val_loss: 0.5984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5525 - val_loss: 0.4747\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4892 - val_loss: 0.4713\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4767 - val_loss: 0.4635\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4664 - val_loss: 0.4623\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4456 - val_loss: 0.4641\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4387 - val_loss: 0.4710\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4365 - val_loss: 0.8952\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-14:42:03] Training the entire fine-tuned model...\n",
      "[2022_04_20-14:42:11] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4502WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0978s vs `on_train_batch_end` time: 0.1211s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0978s vs `on_train_batch_end` time: 0.1211s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 832ms/step - loss: 0.4493 - val_loss: 0.4610\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4500 - val_loss: 0.4605\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4507 - val_loss: 0.4754\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.4460 - val_loss: 0.4578\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.4462 - val_loss: 0.4589\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.4363 - val_loss: 0.4588\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.4324 - val_loss: 0.4536\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.4200 - val_loss: 0.4550\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4289 - val_loss: 0.4507\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.4283 - val_loss: 0.4540\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.4162 - val_loss: 0.4512\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4195 - val_loss: 0.4474\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.4064 - val_loss: 0.4463\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.4087 - val_loss: 0.4463\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.4164 - val_loss: 0.4692\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3938 - val_loss: 0.4467\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.4029 - val_loss: 0.4463\n",
      "[2022_04_20-14:42:52] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-14:42:52] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:42:52] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4055WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0999s vs `on_train_batch_end` time: 0.1217s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0999s vs `on_train_batch_end` time: 0.1217s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 718ms/step - loss: 0.4072 - val_loss: 0.4469\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>515</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>119</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  515  24\n",
       "1  119  12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1437125748502994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2f27rq09) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 37895... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██▁</td></tr><tr><td>loss</td><td>█▃▂▂▂▂▂▁▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>13</td></tr><tr><td>best_val_loss</td><td>0.44627</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.40721</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.44692</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">peachy-pond-2</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/2f27rq09\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036a/runs/2f27rq09</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_144127-2f27rq09/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2f27rq09). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/59u2bc2i\" target=\"_blank\">flowing-thunder-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-14:43:46] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:43:46] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:43:46] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 573ms/step - loss: 0.9297 - val_loss: 0.7131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.6446 - val_loss: 0.5152\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.5309 - val_loss: 0.6007\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5091 - val_loss: 0.4906\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4375 - val_loss: 0.4916\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4555 - val_loss: 0.5282\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4391 - val_loss: 0.4622\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4098 - val_loss: 0.5048\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4097 - val_loss: 0.4535\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.3928 - val_loss: 0.4650\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3986 - val_loss: 0.5030\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4041 - val_loss: 0.4572\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-14:44:11] Training the entire fine-tuned model...\n",
      "[2022_04_20-14:44:19] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4082WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0984s vs `on_train_batch_end` time: 0.1282s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0984s vs `on_train_batch_end` time: 0.1282s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 688ms/step - loss: 0.4082 - val_loss: 0.4653\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 327ms/step - loss: 0.3962 - val_loss: 0.4541\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.4016 - val_loss: 0.4606\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3903 - val_loss: 0.4660\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3869 - val_loss: 0.4511\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3800 - val_loss: 0.4621\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3833 - val_loss: 0.4493\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 331ms/step - loss: 0.3776 - val_loss: 0.4575\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3754 - val_loss: 0.4514\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3772 - val_loss: 0.4553\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_20-14:44:47] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-14:44:47] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:45:06] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3809WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0991s vs `on_train_batch_end` time: 0.1290s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0991s vs `on_train_batch_end` time: 0.1290s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 691ms/step - loss: 0.3809 - val_loss: 0.4490\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>505</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>129</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  505   4\n",
       "1  129  10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13071895424836602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:59u2bc2i) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 38122... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█▁▂▂▃▄▄▅▅▆▇▁</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▅▂▂▃▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.44899</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3809</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.44899</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">flowing-thunder-2</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/59u2bc2i\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036b/runs/59u2bc2i</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_144329-59u2bc2i/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:59u2bc2i). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/14zjpmxe\" target=\"_blank\">deep-firefly-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-14:46:00] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:46:00] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:46:00] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 578ms/step - loss: 0.9830 - val_loss: 0.7943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.6136 - val_loss: 0.5220\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5475 - val_loss: 0.4950\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4934 - val_loss: 0.5482\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4971 - val_loss: 0.4750\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4688 - val_loss: 0.4612\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5059 - val_loss: 0.5121\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4619 - val_loss: 0.4609\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4696 - val_loss: 0.4754\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4885 - val_loss: 0.5115\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4331 - val_loss: 0.4495\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4295 - val_loss: 0.4361\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4105 - val_loss: 0.4312\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4193 - val_loss: 0.4518\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4042 - val_loss: 0.4480\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4024 - val_loss: 0.5123\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-14:46:28] Training the entire fine-tuned model...\n",
      "[2022_04_20-14:46:38] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4014WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0998s vs `on_train_batch_end` time: 0.1214s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0998s vs `on_train_batch_end` time: 0.1214s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 696ms/step - loss: 0.4015 - val_loss: 0.4752\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.4388 - val_loss: 0.4330\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.4048 - val_loss: 0.4599\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.4096 - val_loss: 0.4301\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3984 - val_loss: 0.4324\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3910 - val_loss: 0.4435\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3840 - val_loss: 0.4327\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_20-14:46:59] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-14:46:59] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:46:59] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3868WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0959s vs `on_train_batch_end` time: 0.1236s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0959s vs `on_train_batch_end` time: 0.1236s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 668ms/step - loss: 0.3875 - val_loss: 0.4291\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>525</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>119</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  525  14\n",
       "1  119  12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15286624203821655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:14zjpmxe) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 38332... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██▁▁▂▂▃▃▄▁</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▃▂▂▃▂▂▃▁▁▁▁▁▃▂▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.4291</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.38747</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.4291</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">deep-firefly-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/14zjpmxe\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036a/runs/14zjpmxe</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_144544-14zjpmxe/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:14zjpmxe). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/nwcdm2gq\" target=\"_blank\">flowing-microwave-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-14:47:55] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:47:55] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:47:55] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 551ms/step - loss: 0.8651 - val_loss: 0.7344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.5935 - val_loss: 0.5271\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5169 - val_loss: 0.5899\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.5035 - val_loss: 0.4737\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4599 - val_loss: 0.4684\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4311 - val_loss: 0.5659\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4496 - val_loss: 0.4622\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4455 - val_loss: 0.4562\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4086 - val_loss: 0.4888\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4193 - val_loss: 0.5250\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4179 - val_loss: 0.4488\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3956 - val_loss: 0.4533\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3843 - val_loss: 0.4491\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3812 - val_loss: 0.4767\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-14:48:21] Training the entire fine-tuned model...\n",
      "[2022_04_20-14:48:33] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3903WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0971s vs `on_train_batch_end` time: 0.1299s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0971s vs `on_train_batch_end` time: 0.1299s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 712ms/step - loss: 0.3903 - val_loss: 0.4485\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3934 - val_loss: 0.4788\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3795 - val_loss: 0.4480\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3733 - val_loss: 0.4836\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3857 - val_loss: 0.4475\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 328ms/step - loss: 0.3702 - val_loss: 0.4751\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3627 - val_loss: 0.4530\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3460 - val_loss: 0.4700\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_20-14:48:57] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-14:48:57] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:48:59] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3600WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0998s vs `on_train_batch_end` time: 0.1255s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0998s vs `on_train_batch_end` time: 0.1255s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 694ms/step - loss: 0.3600 - val_loss: 0.4515\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>503</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>129</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  503   6\n",
       "1  129  10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12903225806451613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:nwcdm2gq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 38548... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▄▅▅▆▆▇▇█▁▂▂▃▃▄▄▅▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▂▂▂▂▂▂▂▂▁▂▂▁▁▂▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▄▂▂▄▁▁▂▃▁▁▁▂▁▂▁▂▁▂▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>4</td></tr><tr><td>best_val_loss</td><td>0.44752</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35997</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.45152</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">flowing-microwave-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/nwcdm2gq\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036b/runs/nwcdm2gq</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_144738-nwcdm2gq/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:nwcdm2gq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/2v6c5bl6\" target=\"_blank\">hopeful-paper-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-14:49:55] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:49:55] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:49:55] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 571ms/step - loss: 0.8693 - val_loss: 0.7580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.6589 - val_loss: 0.4937\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.5386 - val_loss: 0.4747\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4812 - val_loss: 0.5398\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4999 - val_loss: 0.5288\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5036 - val_loss: 0.4586\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4894 - val_loss: 0.4522\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4350 - val_loss: 0.4630\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4484 - val_loss: 0.4484\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4410 - val_loss: 0.4599\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4547 - val_loss: 0.5192\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4507 - val_loss: 0.4440\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4413 - val_loss: 0.4787\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4395 - val_loss: 0.4831\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.4372 - val_loss: 0.4531\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-14:50:23] Training the entire fine-tuned model...\n",
      "[2022_04_20-14:50:31] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4794WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0959s vs `on_train_batch_end` time: 0.1254s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0959s vs `on_train_batch_end` time: 0.1254s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 690ms/step - loss: 0.4786 - val_loss: 0.4398\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.4483 - val_loss: 0.4931\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.4375 - val_loss: 0.4659\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.4354 - val_loss: 0.4446\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_20-14:50:48] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-14:50:48] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:50:48] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4346WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0968s vs `on_train_batch_end` time: 0.1229s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0968s vs `on_train_batch_end` time: 0.1229s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 671ms/step - loss: 0.4334 - val_loss: 0.4329\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>129</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  536  3\n",
       "1  129  2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.029411764705882353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2v6c5bl6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 38761... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█▁▁▂▃▁</td></tr><tr><td>loss</td><td>█▅▃▂▂▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▃▃▂▁▂▁▂▃▁▂▂▁▁▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.43293</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.43342</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.43293</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">hopeful-paper-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/2v6c5bl6\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036a/runs/2v6c5bl6</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_144938-2v6c5bl6/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2v6c5bl6). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/16lqp6n2\" target=\"_blank\">peachy-dawn-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-14:51:44] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:51:44] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:51:44] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 568ms/step - loss: 0.9233 - val_loss: 1.0095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.7559 - val_loss: 0.7621\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5439 - val_loss: 0.4891\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4800 - val_loss: 0.5149\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4437 - val_loss: 0.4708\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4317 - val_loss: 0.4765\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4211 - val_loss: 0.4693\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4151 - val_loss: 0.4610\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4250 - val_loss: 0.5003\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4033 - val_loss: 0.4539\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4012 - val_loss: 0.4543\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3977 - val_loss: 0.4739\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3898 - val_loss: 0.4613\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-14:52:10] Training the entire fine-tuned model...\n",
      "[2022_04_20-14:52:18] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4516WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0990s vs `on_train_batch_end` time: 0.1256s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0990s vs `on_train_batch_end` time: 0.1256s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 685ms/step - loss: 0.4516 - val_loss: 0.5722\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.4732 - val_loss: 0.4792\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.4265 - val_loss: 0.5286\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.4139 - val_loss: 0.4598\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3918 - val_loss: 0.4994\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3807 - val_loss: 0.4538\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.3700 - val_loss: 0.5511\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 328ms/step - loss: 0.3993 - val_loss: 0.4701\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3776 - val_loss: 0.4584\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_20-14:52:44] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-14:52:44] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:52:44] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3805WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1283s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1283s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 679ms/step - loss: 0.3805 - val_loss: 0.4520\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>495</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>122</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  495  14\n",
       "1  122  17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:16lqp6n2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 38967... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▅▅▆▆▇▇█▁▂▂▃▃▄▅▅▆▁</td></tr><tr><td>loss</td><td>█▆▃▂▂▂▂▂▂▁▁▁▁▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▁▂▁▁▁▁▂▁▁▁▁▃▁▂▁▂▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.45204</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.38047</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.45204</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">peachy-dawn-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/16lqp6n2\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036b/runs/16lqp6n2</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_145126-16lqp6n2/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:16lqp6n2). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/2zi2scsj\" target=\"_blank\">lunar-elevator-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-14:53:38] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:53:38] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:53:38] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 572ms/step - loss: 0.9143 - val_loss: 1.2788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.8324 - val_loss: 0.8001\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.6140 - val_loss: 0.6655\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.6117 - val_loss: 0.5250\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5285 - val_loss: 0.4804\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4497 - val_loss: 0.4864\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4720 - val_loss: 0.4814\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4558 - val_loss: 0.4550\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4281 - val_loss: 0.4555\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4411 - val_loss: 0.4454\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4216 - val_loss: 0.4479\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4235 - val_loss: 0.4563\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4918 - val_loss: 0.7620\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5955 - val_loss: 0.4443\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4820 - val_loss: 0.4917\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4309 - val_loss: 0.4974\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 199ms/step - loss: 0.4726 - val_loss: 0.4389\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4151 - val_loss: 0.4420\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4156 - val_loss: 0.4410\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4111 - val_loss: 0.4407\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4128 - val_loss: 0.4365\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4029 - val_loss: 0.4351\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4023 - val_loss: 0.4344\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4047 - val_loss: 0.4382\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4053 - val_loss: 0.4366\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4151 - val_loss: 0.4373\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3965 - val_loss: 0.4354\n",
      "[2022_04_20-14:54:20] Training the entire fine-tuned model...\n",
      "[2022_04_20-14:54:28] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4049WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0965s vs `on_train_batch_end` time: 0.1235s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0965s vs `on_train_batch_end` time: 0.1235s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 699ms/step - loss: 0.4121 - val_loss: 0.4352\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4006 - val_loss: 0.4365\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.4005 - val_loss: 0.4371\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4202 - val_loss: 0.4342\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.4078 - val_loss: 0.4322\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.4088 - val_loss: 0.4321\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.4032 - val_loss: 0.4317\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3982 - val_loss: 0.4338\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3988 - val_loss: 0.4336\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.4007 - val_loss: 0.4311\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3979 - val_loss: 0.4308\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3985 - val_loss: 0.4305\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.4089 - val_loss: 0.4308\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3991 - val_loss: 0.4318\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3944 - val_loss: 0.4300\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3891 - val_loss: 0.4298\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3998 - val_loss: 0.4313\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3974 - val_loss: 0.4312\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3977 - val_loss: 0.4304\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3996 - val_loss: 0.4307\n",
      "[2022_04_20-14:55:15] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-14:55:15] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:55:15] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3837WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0971s vs `on_train_batch_end` time: 0.1214s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0971s vs `on_train_batch_end` time: 0.1214s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 662ms/step - loss: 0.3858 - val_loss: 0.4299\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>527</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  527  12\n",
       "1  121  10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13071895424836602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2zi2scsj) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 39181... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▁</td></tr><tr><td>loss</td><td>█▇▄▄▃▂▂▂▂▁▂▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▂▁▁▁▁▁▁▄▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>15</td></tr><tr><td>best_val_loss</td><td>0.42978</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.38577</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42991</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">lunar-elevator-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/2zi2scsj\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036a/runs/2zi2scsj</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_145321-2zi2scsj/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2zi2scsj). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/3bkztepu\" target=\"_blank\">skilled-shape-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-14:56:10] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:56:10] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:56:10] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 570ms/step - loss: 0.8507 - val_loss: 0.5918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5651 - val_loss: 0.4970\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4691 - val_loss: 0.4950\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4554 - val_loss: 0.5022\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4503 - val_loss: 0.4677\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4327 - val_loss: 0.4720\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4223 - val_loss: 0.4715\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4144 - val_loss: 0.4692\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4067 - val_loss: 0.4615\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3971 - val_loss: 0.4558\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3925 - val_loss: 0.4762\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3989 - val_loss: 0.4529\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4056 - val_loss: 0.4551\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3957 - val_loss: 0.4684\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3972 - val_loss: 0.4534\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3829 - val_loss: 0.4636\n",
      "[2022_04_20-14:56:39] Training the entire fine-tuned model...\n",
      "[2022_04_20-14:56:47] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3872WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1006s vs `on_train_batch_end` time: 0.1261s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1006s vs `on_train_batch_end` time: 0.1261s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 860ms/step - loss: 0.3872 - val_loss: 0.4575\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 327ms/step - loss: 0.3976 - val_loss: 0.4612\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.3903 - val_loss: 0.4608\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3934 - val_loss: 0.4595\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3874 - val_loss: 0.4591\n",
      "[2022_04_20-14:57:06] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-14:57:06] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:57:06] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3949WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0991s vs `on_train_batch_end` time: 0.1259s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0991s vs `on_train_batch_end` time: 0.1259s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 697ms/step - loss: 0.3949 - val_loss: 0.4576\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>509</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>136</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  509  0\n",
       "1  136  3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04225352112676056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3bkztepu) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 39519... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██▁▁▂▂▃▁</td></tr><tr><td>loss</td><td>█▄▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▃▂▂▂▂▁▁▂▁▁▂▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>11</td></tr><tr><td>best_val_loss</td><td>0.45287</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.39492</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.45755</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">skilled-shape-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/3bkztepu\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036b/runs/3bkztepu</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_145553-3bkztepu/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3bkztepu). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/pkn2lmg9\" target=\"_blank\">eager-frog-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-14:57:58] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:57:58] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:57:58] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 591ms/step - loss: 0.8157 - val_loss: 0.4974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.5427 - val_loss: 0.4924\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4846 - val_loss: 0.4713\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4659 - val_loss: 0.4623\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4684 - val_loss: 0.4639\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4594 - val_loss: 0.4557\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4303 - val_loss: 0.4474\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4355 - val_loss: 0.4500\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4351 - val_loss: 0.4555\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4245 - val_loss: 0.4423\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4196 - val_loss: 0.4363\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4281 - val_loss: 0.4619\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4234 - val_loss: 0.5288\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4269 - val_loss: 0.4349\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4326 - val_loss: 0.4273\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4221 - val_loss: 0.4254\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4121 - val_loss: 0.4422\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4154 - val_loss: 0.4267\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4044 - val_loss: 0.4475\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3911 - val_loss: 0.4242\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3854 - val_loss: 0.4278\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3986 - val_loss: 0.4255\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.4048 - val_loss: 0.4262\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3998 - val_loss: 0.4239\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3746 - val_loss: 0.4242\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3915 - val_loss: 0.4244\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3834 - val_loss: 0.4232\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3801 - val_loss: 0.4229\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3872 - val_loss: 0.4232\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3826 - val_loss: 0.4246\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.3830 - val_loss: 0.4224\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3782 - val_loss: 0.4219\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 199ms/step - loss: 0.3834 - val_loss: 0.4222\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3909 - val_loss: 0.4219\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3800 - val_loss: 0.4215\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3948 - val_loss: 0.4214\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3837 - val_loss: 0.4265\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.4085 - val_loss: 0.4270\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3915 - val_loss: 0.4210\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3756 - val_loss: 0.4226\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3794 - val_loss: 0.4209\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.3747 - val_loss: 0.4211\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3763 - val_loss: 0.4210\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3896 - val_loss: 0.4238\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3871 - val_loss: 0.4229\n",
      "[2022_04_20-14:59:01] Training the entire fine-tuned model...\n",
      "[2022_04_20-14:59:09] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4052WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0977s vs `on_train_batch_end` time: 0.1236s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0977s vs `on_train_batch_end` time: 0.1236s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 681ms/step - loss: 0.4050 - val_loss: 0.4412\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3975 - val_loss: 0.4437\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.4348 - val_loss: 0.4236\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3673 - val_loss: 0.4292\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3898 - val_loss: 0.4221\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3911 - val_loss: 0.4238\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3771 - val_loss: 0.4733\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.4034 - val_loss: 0.4193\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3739 - val_loss: 0.4357\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3683 - val_loss: 0.4192\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3518 - val_loss: 0.4168\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3480 - val_loss: 0.4290\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3403 - val_loss: 0.4195\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3435 - val_loss: 0.4190\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3340 - val_loss: 0.4197\n",
      "[2022_04_20-14:59:45] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-14:59:45] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-14:59:45] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3541WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0959s vs `on_train_batch_end` time: 0.1251s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0959s vs `on_train_batch_end` time: 0.1251s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 698ms/step - loss: 0.3583 - val_loss: 0.4166\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>519</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  519  20\n",
       "1  108  23"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26436781609195403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:pkn2lmg9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 39728... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▆▆▄▄▃▃▃▂█▂▂▃▃▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▃▁▁▁▁▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.41661</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35831</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41661</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">eager-frog-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/pkn2lmg9\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036a/runs/pkn2lmg9</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_145742-pkn2lmg9/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:pkn2lmg9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/2cgf38wl\" target=\"_blank\">still-river-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-15:00:46] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:00:46] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:00:46] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 585ms/step - loss: 0.9143 - val_loss: 0.9817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.7753 - val_loss: 0.9054\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.6184 - val_loss: 0.5336\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5319 - val_loss: 0.5393\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4865 - val_loss: 0.4850\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4766 - val_loss: 0.4716\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4604 - val_loss: 0.5568\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4382 - val_loss: 0.4603\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4230 - val_loss: 0.4711\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4376 - val_loss: 0.5887\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4534 - val_loss: 0.4588\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4098 - val_loss: 0.4876\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3933 - val_loss: 0.4487\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3867 - val_loss: 0.4505\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3862 - val_loss: 0.4878\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3811 - val_loss: 0.4450\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3955 - val_loss: 0.4470\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3763 - val_loss: 0.4824\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3588 - val_loss: 0.4587\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3610 - val_loss: 0.4638\n",
      "[2022_04_20-15:01:19] Training the entire fine-tuned model...\n",
      "[2022_04_20-15:01:45] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3678WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1000s vs `on_train_batch_end` time: 0.1270s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1000s vs `on_train_batch_end` time: 0.1270s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 679ms/step - loss: 0.3678 - val_loss: 0.4587\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3730 - val_loss: 0.4461\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3717 - val_loss: 0.4637\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.3696 - val_loss: 0.4523\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3609 - val_loss: 0.4436\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3615 - val_loss: 0.4593\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3574 - val_loss: 0.4442\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3554 - val_loss: 0.4543\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3536 - val_loss: 0.4522\n",
      "[2022_04_20-15:02:11] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-15:02:11] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:02:12] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3625WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0995s vs `on_train_batch_end` time: 0.1258s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0995s vs `on_train_batch_end` time: 0.1258s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 695ms/step - loss: 0.3625 - val_loss: 0.4443\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>501</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  501   8\n",
       "1  127  12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1509433962264151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2cgf38wl) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 40138... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▂▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▆▄▃▃▃▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▃▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▂▂▂▁▂▁▁▃▁▂▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>4</td></tr><tr><td>best_val_loss</td><td>0.4436</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36248</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.4443</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">still-river-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/2cgf38wl\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036b/runs/2cgf38wl</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_150024-2cgf38wl/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2cgf38wl). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/3nzaph70\" target=\"_blank\">hopeful-flower-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-15:03:05] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:03:05] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:03:05] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 578ms/step - loss: 0.8166 - val_loss: 0.5218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5893 - val_loss: 0.4882\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4996 - val_loss: 0.5060\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.5332 - val_loss: 0.5269\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4658 - val_loss: 0.4687\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4600 - val_loss: 0.4836\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5137 - val_loss: 0.5074\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4462 - val_loss: 0.4505\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4268 - val_loss: 0.4488\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4338 - val_loss: 0.5370\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4829 - val_loss: 0.5952\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4993 - val_loss: 0.8853\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.6380 - val_loss: 0.4531\n",
      "[2022_04_20-15:03:30] Training the entire fine-tuned model...\n",
      "[2022_04_20-15:03:42] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4243WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0981s vs `on_train_batch_end` time: 0.1215s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0981s vs `on_train_batch_end` time: 0.1215s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 694ms/step - loss: 0.4233 - val_loss: 0.4451\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4422 - val_loss: 0.4464\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.4207 - val_loss: 0.4577\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.4090 - val_loss: 0.4400\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4060 - val_loss: 0.4433\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.4089 - val_loss: 0.4421\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3894 - val_loss: 0.4440\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3895 - val_loss: 0.4452\n",
      "[2022_04_20-15:04:05] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-15:04:05] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:04:05] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4043WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0986s vs `on_train_batch_end` time: 0.1208s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0986s vs `on_train_batch_end` time: 0.1208s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 698ms/step - loss: 0.4035 - val_loss: 0.4395\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>527</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  527  12\n",
       "1  127   4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05442176870748299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3nzaph70) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 40400... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▅▅▆▆▇▇█▁▂▂▃▃▄▅▅▁</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▃▂▂▂▃▃▅▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▃▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▂▂▂▂▁▂▂▁▁▃▃█▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.4395</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.40347</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.4395</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">hopeful-flower-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/3nzaph70\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036a/runs/3nzaph70</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_150249-3nzaph70/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3nzaph70). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/j7dmon0h\" target=\"_blank\">polished-meadow-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-15:05:03] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:05:03] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:05:03] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 575ms/step - loss: 0.9291 - val_loss: 0.6333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5909 - val_loss: 0.7035\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.5576 - val_loss: 0.4901\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4942 - val_loss: 0.4787\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4405 - val_loss: 0.4801\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4363 - val_loss: 0.4727\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4201 - val_loss: 0.4584\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4204 - val_loss: 0.4589\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4037 - val_loss: 0.4859\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3996 - val_loss: 0.4528\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3910 - val_loss: 0.4891\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4009 - val_loss: 0.4591\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3887 - val_loss: 0.4471\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3883 - val_loss: 0.4549\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4233 - val_loss: 0.4835\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3817 - val_loss: 0.5712\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.4181 - val_loss: 0.4450\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.4045 - val_loss: 0.4472\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3973 - val_loss: 0.5369\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3788 - val_loss: 0.4553\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3991 - val_loss: 0.4436\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3655 - val_loss: 0.4647\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3683 - val_loss: 0.4776\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.3699 - val_loss: 0.4583\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3642 - val_loss: 0.4548\n",
      "[2022_04_20-15:05:42] Training the entire fine-tuned model...\n",
      "[2022_04_20-15:05:51] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3792WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1023s vs `on_train_batch_end` time: 0.1257s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1023s vs `on_train_batch_end` time: 0.1257s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 707ms/step - loss: 0.3792 - val_loss: 0.4469\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3677 - val_loss: 0.4991\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 327ms/step - loss: 0.3586 - val_loss: 0.4459\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3774 - val_loss: 0.4700\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3695 - val_loss: 0.4593\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 327ms/step - loss: 0.3645 - val_loss: 0.4493\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3408 - val_loss: 0.4617\n",
      "[2022_04_20-15:06:13] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-15:06:13] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:06:13] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3650WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0973s vs `on_train_batch_end` time: 0.1295s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0973s vs `on_train_batch_end` time: 0.1295s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 687ms/step - loss: 0.3650 - val_loss: 0.4463\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>499</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>124</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  499  10\n",
       "1  124  15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18292682926829268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:j7dmon0h) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 40607... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▄▄▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▆█▂▂▂▂▁▁▂▁▂▁▁▁▂▄▁▁▄▁▁▂▂▁▁▁▂▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>20</td></tr><tr><td>best_val_loss</td><td>0.44356</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36502</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.44625</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">polished-meadow-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/j7dmon0h\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036b/runs/j7dmon0h</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_150446-j7dmon0h/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:j7dmon0h). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/15y0ewy0\" target=\"_blank\">stilted-tree-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-15:07:08] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:07:08] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:07:08] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 578ms/step - loss: 0.8549 - val_loss: 0.6390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.6042 - val_loss: 0.4963\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5565 - val_loss: 0.5834\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.7034 - val_loss: 0.4681\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5062 - val_loss: 0.4887\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5043 - val_loss: 0.4776\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4586 - val_loss: 0.4618\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4408 - val_loss: 0.4476\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4615 - val_loss: 0.4600\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4384 - val_loss: 0.4540\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4409 - val_loss: 0.4417\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4399 - val_loss: 0.4959\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4454 - val_loss: 0.5074\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4764 - val_loss: 0.4503\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4034 - val_loss: 0.4343\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4042 - val_loss: 0.4305\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4076 - val_loss: 0.4488\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4070 - val_loss: 0.4297\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4114 - val_loss: 0.4281\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4089 - val_loss: 0.4276\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4145 - val_loss: 0.4274\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4090 - val_loss: 0.4510\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4115 - val_loss: 0.4281\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4088 - val_loss: 0.4261\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4020 - val_loss: 0.4280\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4034 - val_loss: 0.4263\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4000 - val_loss: 0.4369\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3938 - val_loss: 0.4310\n",
      "[2022_04_20-15:07:50] Training the entire fine-tuned model...\n",
      "[2022_04_20-15:07:58] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.6016WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0962s vs `on_train_batch_end` time: 0.1242s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0962s vs `on_train_batch_end` time: 0.1242s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 695ms/step - loss: 0.5983 - val_loss: 0.4692\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.4292 - val_loss: 0.4288\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.4567 - val_loss: 0.4344\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.4153 - val_loss: 0.4451\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.4177 - val_loss: 0.4230\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3852 - val_loss: 0.4213\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3818 - val_loss: 0.4185\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3695 - val_loss: 0.4142\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3550 - val_loss: 0.4101\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3550 - val_loss: 0.4948\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3877 - val_loss: 0.4246\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3319 - val_loss: 0.4043\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.3037 - val_loss: 0.4391\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3153 - val_loss: 0.4579\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.2895 - val_loss: 0.4923\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.2777 - val_loss: 0.4476\n",
      "[2022_04_20-15:08:38] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-15:08:38] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:08:38] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3103WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0958s vs `on_train_batch_end` time: 0.1222s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0958s vs `on_train_batch_end` time: 0.1222s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 663ms/step - loss: 0.3077 - val_loss: 0.4130\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>527</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  527  12\n",
       "1  118  13"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:15y0ewy0) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 40868... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇█▁▁▂▂▂▂▃▃▃▄▄▄▄▅▁</td></tr><tr><td>loss</td><td>█▅▄▆▄▄▃▃▃▃▃▃▃▂▂▂▃▂▃▂▃▂▂▂▂▅▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▆▃▄▃▃▂▂▂▄▄▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▁▁▄▂▁▂▃▄▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>11</td></tr><tr><td>best_val_loss</td><td>0.40425</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30769</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.41298</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">stilted-tree-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/15y0ewy0\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036a/runs/15y0ewy0</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_150652-15y0ewy0/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:15y0ewy0). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/3uvl4m6k\" target=\"_blank\">kind-dawn-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-15:09:31] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:09:31] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:09:31] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 632ms/step - loss: 0.9195 - val_loss: 0.8717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.6725 - val_loss: 0.6152\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.5239 - val_loss: 0.5159\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5185 - val_loss: 0.5106\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4777 - val_loss: 0.5302\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4315 - val_loss: 0.4646\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.4129 - val_loss: 0.5123\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4249 - val_loss: 0.4549\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4100 - val_loss: 0.4684\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4060 - val_loss: 0.4494\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.3979 - val_loss: 0.4516\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3898 - val_loss: 0.4453\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3920 - val_loss: 0.4475\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3841 - val_loss: 0.4508\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3769 - val_loss: 0.4638\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3799 - val_loss: 0.4489\n",
      "[2022_04_20-15:10:01] Training the entire fine-tuned model...\n",
      "[2022_04_20-15:10:09] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5147WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1023s vs `on_train_batch_end` time: 0.1253s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1023s vs `on_train_batch_end` time: 0.1253s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 857ms/step - loss: 0.5147 - val_loss: 0.4782\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 331ms/step - loss: 0.4047 - val_loss: 0.4511\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 327ms/step - loss: 0.3945 - val_loss: 0.5340\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 327ms/step - loss: 0.3956 - val_loss: 0.4521\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3623 - val_loss: 0.4608\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 327ms/step - loss: 0.3556 - val_loss: 0.4519\n",
      "[2022_04_20-15:10:30] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-15:10:30] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:10:35] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3795WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1005s vs `on_train_batch_end` time: 0.1258s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1005s vs `on_train_batch_end` time: 0.1258s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 702ms/step - loss: 0.3795 - val_loss: 0.4513\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>506</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>134</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  506  3\n",
       "1  134  5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06802721088435375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3uvl4m6k) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 41204... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██▁▁▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▂▂▂▂▂▂▁▁▁▁▁▃▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████▃▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▂▂▁▂▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>11</td></tr><tr><td>best_val_loss</td><td>0.44533</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3795</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.45131</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">kind-dawn-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/3uvl4m6k\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036b/runs/3uvl4m6k</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_150914-3uvl4m6k/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3uvl4m6k). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/3j9xm5j6\" target=\"_blank\">desert-moon-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-15:11:31] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:11:32] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:11:32] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 588ms/step - loss: 0.9292 - val_loss: 1.0950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.7505 - val_loss: 0.6924\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.5865 - val_loss: 0.5621\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5553 - val_loss: 0.4832\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4916 - val_loss: 0.4657\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4805 - val_loss: 0.4685\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4503 - val_loss: 0.4844\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4852 - val_loss: 0.5221\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4663 - val_loss: 0.4462\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4376 - val_loss: 0.4495\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4376 - val_loss: 0.4466\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.4375 - val_loss: 0.4560\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4397 - val_loss: 0.4449\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4287 - val_loss: 0.4429\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4318 - val_loss: 0.4425\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4260 - val_loss: 0.4472\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4229 - val_loss: 0.4427\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4237 - val_loss: 0.4418\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4344 - val_loss: 0.4416\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4185 - val_loss: 0.4422\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4191 - val_loss: 0.4483\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4349 - val_loss: 0.4434\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4279 - val_loss: 0.4423\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4176 - val_loss: 0.4414\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.4298 - val_loss: 0.4408\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4268 - val_loss: 0.4405\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4267 - val_loss: 0.4404\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4196 - val_loss: 0.4403\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4150 - val_loss: 0.4406\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4214 - val_loss: 0.4416\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 199ms/step - loss: 0.4241 - val_loss: 0.4422\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.4176 - val_loss: 0.4419\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4214 - val_loss: 0.4421\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.4130 - val_loss: 0.4426\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "[2022_04_20-15:12:21] Training the entire fine-tuned model...\n",
      "[2022_04_20-15:12:29] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4239WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0968s vs `on_train_batch_end` time: 0.1217s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0968s vs `on_train_batch_end` time: 0.1217s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 693ms/step - loss: 0.4222 - val_loss: 0.4408\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.4239 - val_loss: 0.4401\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4270 - val_loss: 0.4400\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4226 - val_loss: 0.4400\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4197 - val_loss: 0.4407\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4263 - val_loss: 0.4457\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.4274 - val_loss: 0.4453\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4188 - val_loss: 0.4445\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.4171 - val_loss: 0.4428\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4201 - val_loss: 0.4425\n",
      "[2022_04_20-15:12:56] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-15:12:56] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:12:56] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4226WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0951s vs `on_train_batch_end` time: 0.1245s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0951s vs `on_train_batch_end` time: 0.1245s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 661ms/step - loss: 0.4212 - val_loss: 0.4400\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>532</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  532  7\n",
       "1  128  3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0425531914893617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3j9xm5j6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 41423... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▆▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>3</td></tr><tr><td>best_val_loss</td><td>0.43998</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.42121</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43998</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">desert-moon-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/3j9xm5j6\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036a/runs/3j9xm5j6</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_151113-3j9xm5j6/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3j9xm5j6). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/1q5tzwow\" target=\"_blank\">vibrant-dust-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-15:13:48] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:13:48] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:13:48] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 584ms/step - loss: 0.8514 - val_loss: 0.8658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.6454 - val_loss: 0.6279\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5049 - val_loss: 0.4947\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4610 - val_loss: 0.4822\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4479 - val_loss: 0.5247\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4463 - val_loss: 0.4629\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4278 - val_loss: 0.4696\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4111 - val_loss: 0.4530\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4109 - val_loss: 0.4507\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4027 - val_loss: 0.4497\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3952 - val_loss: 0.4565\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3940 - val_loss: 0.4576\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4046 - val_loss: 0.4555\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3787 - val_loss: 0.4485\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3850 - val_loss: 0.4673\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3742 - val_loss: 0.4466\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3842 - val_loss: 0.4658\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3735 - val_loss: 0.4600\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3765 - val_loss: 0.4478\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3708 - val_loss: 0.4585\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3597 - val_loss: 0.4618\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3693 - val_loss: 0.4544\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_20-15:14:23] Training the entire fine-tuned model...\n",
      "[2022_04_20-15:14:31] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3725WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0992s vs `on_train_batch_end` time: 0.1273s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0992s vs `on_train_batch_end` time: 0.1273s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 684ms/step - loss: 0.3725 - val_loss: 0.4542\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3749 - val_loss: 0.4677\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3723 - val_loss: 0.4626\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3683 - val_loss: 0.4506\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3737 - val_loss: 0.4502\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3735 - val_loss: 0.4528\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.3677 - val_loss: 0.4548\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.3623 - val_loss: 0.4536\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3639 - val_loss: 0.4526\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3673 - val_loss: 0.4513\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3669 - val_loss: 0.4506\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_20-15:15:02] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-15:15:02] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:15:02] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3612WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0996s vs `on_train_batch_end` time: 0.1283s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0996s vs `on_train_batch_end` time: 0.1283s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 686ms/step - loss: 0.3612 - val_loss: 0.4502\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>505</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>131</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  505  4\n",
       "1  131  8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10596026490066225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1q5tzwow) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 41745... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▄▁</td></tr><tr><td>loss</td><td>█▅▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>15</td></tr><tr><td>best_val_loss</td><td>0.44664</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36121</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.45016</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">vibrant-dust-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/1q5tzwow\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036b/runs/1q5tzwow</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_151332-1q5tzwow/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1q5tzwow). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/236a9i3e\" target=\"_blank\">serene-water-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-15:15:55] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:15:55] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:15:55] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 582ms/step - loss: 0.9170 - val_loss: 0.5679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.6176 - val_loss: 0.4810\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.5935 - val_loss: 0.6341\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.6358 - val_loss: 0.5464\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5095 - val_loss: 0.4820\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4633 - val_loss: 0.4792\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.4655 - val_loss: 0.4682\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4433 - val_loss: 0.4642\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4527 - val_loss: 0.4752\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4560 - val_loss: 0.4703\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4469 - val_loss: 0.4603\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4469 - val_loss: 0.4607\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4486 - val_loss: 0.4563\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4410 - val_loss: 0.4614\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4368 - val_loss: 0.4583\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4418 - val_loss: 0.4536\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4345 - val_loss: 0.4527\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4390 - val_loss: 0.4617\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4379 - val_loss: 0.4559\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4382 - val_loss: 0.4493\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4271 - val_loss: 0.4511\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4333 - val_loss: 0.4499\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4343 - val_loss: 0.4455\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4293 - val_loss: 0.4456\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4262 - val_loss: 0.4475\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.4150 - val_loss: 0.4429\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 218ms/step - loss: 0.4283 - val_loss: 0.4540\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4320 - val_loss: 0.4428\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4280 - val_loss: 0.4412\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4199 - val_loss: 0.4443\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4129 - val_loss: 0.4453\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4161 - val_loss: 0.4414\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4144 - val_loss: 0.4398\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4227 - val_loss: 0.4384\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4179 - val_loss: 0.4378\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4097 - val_loss: 0.4376\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.4138 - val_loss: 0.4376\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4136 - val_loss: 0.4388\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4112 - val_loss: 0.4376\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4067 - val_loss: 0.4374\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.4125 - val_loss: 0.4374\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4132 - val_loss: 0.4378\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4151 - val_loss: 0.4381\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4067 - val_loss: 0.4379\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4144 - val_loss: 0.4380\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4029 - val_loss: 0.4379\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4063 - val_loss: 0.4379\n",
      "[2022_04_20-15:17:00] Training the entire fine-tuned model...\n",
      "[2022_04_20-15:17:35] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4146WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1011s vs `on_train_batch_end` time: 0.1221s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1011s vs `on_train_batch_end` time: 0.1221s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 872ms/step - loss: 0.4137 - val_loss: 0.4403\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.4063 - val_loss: 0.4363\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3999 - val_loss: 0.4440\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4122 - val_loss: 0.4348\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3988 - val_loss: 0.4348\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3933 - val_loss: 0.4335\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.4015 - val_loss: 0.4313\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3948 - val_loss: 0.4321\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3985 - val_loss: 0.4408\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3847 - val_loss: 0.4354\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3957 - val_loss: 0.4305\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3894 - val_loss: 0.4289\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3835 - val_loss: 0.4346\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3792 - val_loss: 0.4339\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3827 - val_loss: 0.4294\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3880 - val_loss: 0.4292\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3815 - val_loss: 0.4284\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3855 - val_loss: 0.4278\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3839 - val_loss: 0.4277\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3844 - val_loss: 0.4289\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3806 - val_loss: 0.4302\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3768 - val_loss: 0.4301\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3755 - val_loss: 0.4300\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3883 - val_loss: 0.4303\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3771 - val_loss: 0.4301\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "[2022_04_20-15:18:31] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-15:18:31] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:18:31] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3894WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0962s vs `on_train_batch_end` time: 0.1229s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0962s vs `on_train_batch_end` time: 0.1229s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 678ms/step - loss: 0.3862 - val_loss: 0.4271\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>527</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  527  12\n",
       "1  116  15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.189873417721519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:236a9i3e) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 42037... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▃▄▄▄▅▁</td></tr><tr><td>loss</td><td>█▄▄▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▇▄▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.42714</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.38624</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42714</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">serene-water-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/236a9i3e\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036a/runs/236a9i3e</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_151539-236a9i3e/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:236a9i3e). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/2s2lea3l\" target=\"_blank\">exalted-forest-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-15:19:24] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:19:25] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:19:25] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 851ms/step - loss: 0.9272 - val_loss: 0.8370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.6498 - val_loss: 0.6938\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5360 - val_loss: 0.4866\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.5232 - val_loss: 0.4847\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4703 - val_loss: 0.5526\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4443 - val_loss: 0.5017\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4531 - val_loss: 0.4708\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4289 - val_loss: 0.4922\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4162 - val_loss: 0.4761\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4223 - val_loss: 0.4516\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4109 - val_loss: 0.4495\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3896 - val_loss: 0.4491\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3928 - val_loss: 0.4608\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3791 - val_loss: 0.4646\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3757 - val_loss: 0.4521\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3697 - val_loss: 0.4547\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3762 - val_loss: 0.4593\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3622 - val_loss: 0.4508\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_20-15:19:57] Training the entire fine-tuned model...\n",
      "[2022_04_20-15:20:05] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3848WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1004s vs `on_train_batch_end` time: 0.1278s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1004s vs `on_train_batch_end` time: 0.1278s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 687ms/step - loss: 0.3848 - val_loss: 0.4711\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3945 - val_loss: 0.4497\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3899 - val_loss: 0.4479\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3734 - val_loss: 0.4860\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3799 - val_loss: 0.4509\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3636 - val_loss: 0.4464\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3681 - val_loss: 0.4632\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3723 - val_loss: 0.4493\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3686 - val_loss: 0.4466\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3608 - val_loss: 0.4566\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3535 - val_loss: 0.4593\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.3619 - val_loss: 0.4553\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_20-15:20:36] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-15:20:36] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:20:36] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3740WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1020s vs `on_train_batch_end` time: 0.1252s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1020s vs `on_train_batch_end` time: 0.1252s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 695ms/step - loss: 0.3740 - val_loss: 0.4465\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>503</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  503   6\n",
       "1  127  12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15286624203821655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2s2lea3l) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 42510... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██▁▁▂▂▃▃▃▄▄▅▅▆▁</td></tr><tr><td>loss</td><td>█▅▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▂▂▃▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.44637</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37405</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.44647</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">exalted-forest-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/2s2lea3l\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036b/runs/2s2lea3l</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_151909-2s2lea3l/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2s2lea3l). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/qqaz03vs\" target=\"_blank\">jumping-butterfly-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-15:21:32] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:21:32] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:21:32] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 594ms/step - loss: 1.0707 - val_loss: 1.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.7254 - val_loss: 0.8584\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.7276 - val_loss: 0.8111\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.6038 - val_loss: 0.6342\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5747 - val_loss: 0.7396\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5603 - val_loss: 0.6117\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.5473 - val_loss: 0.6583\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.5151 - val_loss: 0.5065\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4666 - val_loss: 0.4475\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4649 - val_loss: 0.4433\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4578 - val_loss: 0.4938\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5075 - val_loss: 0.5906\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4996 - val_loss: 0.5151\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4934 - val_loss: 0.4344\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4291 - val_loss: 0.4498\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4123 - val_loss: 0.4340\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4256 - val_loss: 0.4331\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4143 - val_loss: 0.4487\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4132 - val_loss: 0.4324\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4289 - val_loss: 0.4381\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4137 - val_loss: 0.4515\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4164 - val_loss: 0.4342\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4096 - val_loss: 0.4368\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3984 - val_loss: 0.4376\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3963 - val_loss: 0.4351\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_20-15:22:10] Training the entire fine-tuned model...\n",
      "[2022_04_20-15:22:18] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4099WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0977s vs `on_train_batch_end` time: 0.1221s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0977s vs `on_train_batch_end` time: 0.1221s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 703ms/step - loss: 0.4110 - val_loss: 0.4306\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4253 - val_loss: 0.4319\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4026 - val_loss: 0.4288\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3937 - val_loss: 0.4268\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3961 - val_loss: 0.4283\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3959 - val_loss: 0.4267\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3875 - val_loss: 0.4267\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3917 - val_loss: 0.4272\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3760 - val_loss: 0.4314\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3751 - val_loss: 0.4279\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3740 - val_loss: 0.4277\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3701 - val_loss: 0.4280\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3620 - val_loss: 0.4286\n",
      "[2022_04_20-15:22:53] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-15:22:53] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:22:53] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3870WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0968s vs `on_train_batch_end` time: 0.1236s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0968s vs `on_train_batch_end` time: 0.1236s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 684ms/step - loss: 0.3833 - val_loss: 0.4268\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>530</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>124</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  530  9\n",
       "1  124  7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09523809523809523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:qqaz03vs) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 42801... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▄▄▄▅▁</td></tr><tr><td>loss</td><td>█▅▅▃▃▃▃▃▂▂▂▂▂▂▂▁▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▆▄▅▃▄▂▁▁▂▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>6</td></tr><tr><td>best_val_loss</td><td>0.42673</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.38333</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.42679</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">jumping-butterfly-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/qqaz03vs\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036a/runs/qqaz03vs</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_152115-qqaz03vs/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:qqaz03vs). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/29fsgs4v\" target=\"_blank\">fanciful-water-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-15:23:48] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:23:49] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:23:49] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 603ms/step - loss: 0.8752 - val_loss: 0.8713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.6505 - val_loss: 0.6522\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5175 - val_loss: 0.4860\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4788 - val_loss: 0.4921\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4696 - val_loss: 0.5723\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4698 - val_loss: 0.4854\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4307 - val_loss: 0.4626\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4217 - val_loss: 0.4597\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4214 - val_loss: 0.5258\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4185 - val_loss: 0.5097\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4269 - val_loss: 0.4783\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4171 - val_loss: 0.5056\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4233 - val_loss: 0.4785\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4054 - val_loss: 0.4547\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3783 - val_loss: 0.5052\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3943 - val_loss: 0.4578\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.3892 - val_loss: 0.4578\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3773 - val_loss: 0.4667\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3860 - val_loss: 0.4650\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3791 - val_loss: 0.4614\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_20-15:24:22] Training the entire fine-tuned model...\n",
      "[2022_04_20-15:24:30] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4029WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1027s vs `on_train_batch_end` time: 0.1277s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1027s vs `on_train_batch_end` time: 0.1277s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 709ms/step - loss: 0.4029 - val_loss: 0.4513\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3869 - val_loss: 0.4736\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3745 - val_loss: 0.4492\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.3666 - val_loss: 0.4819\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3710 - val_loss: 0.4457\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 328ms/step - loss: 0.3624 - val_loss: 0.4661\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3382 - val_loss: 0.4423\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3450 - val_loss: 0.4630\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3290 - val_loss: 0.4549\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3118 - val_loss: 0.4646\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.3257 - val_loss: 0.4644\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 328ms/step - loss: 0.3109 - val_loss: 0.4620\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.3088 - val_loss: 0.4666\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_20-15:25:03] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-15:25:03] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:25:03] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3434WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0992s vs `on_train_batch_end` time: 0.1278s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0992s vs `on_train_batch_end` time: 0.1278s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 681ms/step - loss: 0.3434 - val_loss: 0.4458\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>497</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  497  12\n",
       "1  120  19"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2235294117647059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:29fsgs4v) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 43103... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▂▃▃▄▄▄▅▅▅▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▂▃▂▁▁▂▂▂▂▂▁▂▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>6</td></tr><tr><td>best_val_loss</td><td>0.44232</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34336</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.44582</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fanciful-water-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/29fsgs4v\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036b/runs/29fsgs4v</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_152330-29fsgs4v/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:29fsgs4v). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/2nbnpwf9\" target=\"_blank\">genial-wind-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-15:26:12] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:26:12] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:26:12] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 610ms/step - loss: 0.9564 - val_loss: 0.6256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5700 - val_loss: 0.5053\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.5164 - val_loss: 0.5017\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.5239 - val_loss: 0.5154\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4770 - val_loss: 0.4566\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4386 - val_loss: 0.4702\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4652 - val_loss: 0.4464\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4303 - val_loss: 0.4591\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4425 - val_loss: 0.4540\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4386 - val_loss: 0.4538\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4166 - val_loss: 0.4561\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4332 - val_loss: 0.4485\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4196 - val_loss: 0.4393\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4271 - val_loss: 0.4376\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4128 - val_loss: 0.4386\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4154 - val_loss: 0.4366\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4140 - val_loss: 0.4357\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4263 - val_loss: 0.4688\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4142 - val_loss: 0.4358\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4340 - val_loss: 0.4331\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4204 - val_loss: 0.4664\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4265 - val_loss: 0.4351\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4004 - val_loss: 0.4395\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4016 - val_loss: 0.4326\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3996 - val_loss: 0.4305\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4156 - val_loss: 0.4331\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4054 - val_loss: 0.4299\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3963 - val_loss: 0.4308\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4119 - val_loss: 0.4323\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.4025 - val_loss: 0.4303\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.4083 - val_loss: 0.4298\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4027 - val_loss: 0.4302\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4015 - val_loss: 0.4306\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3982 - val_loss: 0.4306\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3900 - val_loss: 0.4310\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 200ms/step - loss: 0.3974 - val_loss: 0.4312\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3926 - val_loss: 0.4312\n",
      "[2022_04_20-15:27:04] Training the entire fine-tuned model...\n",
      "[2022_04_20-15:27:12] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.6315WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1206s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1206s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 694ms/step - loss: 0.6270 - val_loss: 0.4296\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.4275 - val_loss: 0.4474\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4251 - val_loss: 0.4661\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4290 - val_loss: 0.4681\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.4521 - val_loss: 0.4298\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3973 - val_loss: 0.4418\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3910 - val_loss: 0.4236\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3776 - val_loss: 0.4309\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3776 - val_loss: 0.4277\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3902 - val_loss: 0.4291\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3825 - val_loss: 0.4211\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3634 - val_loss: 0.4206\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3662 - val_loss: 0.4221\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3681 - val_loss: 0.4232\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3670 - val_loss: 0.4193\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3754 - val_loss: 0.4217\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3631 - val_loss: 0.4304\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3652 - val_loss: 0.4215\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3524 - val_loss: 0.4195\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3640 - val_loss: 0.4185\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3579 - val_loss: 0.4182\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3609 - val_loss: 0.4182\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3566 - val_loss: 0.4182\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3574 - val_loss: 0.4181\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3579 - val_loss: 0.4180\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3539 - val_loss: 0.4182\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3641 - val_loss: 0.4196\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3568 - val_loss: 0.4198\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3573 - val_loss: 0.4198\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3571 - val_loss: 0.4198\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3625 - val_loss: 0.4197\n",
      "[2022_04_20-15:28:21] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-15:28:21] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:28:21] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3634WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0967s vs `on_train_batch_end` time: 0.1240s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0967s vs `on_train_batch_end` time: 0.1240s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 696ms/step - loss: 0.3653 - val_loss: 0.4268\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>535</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  535  4\n",
       "1  126  5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07142857142857142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2nbnpwf9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 43371... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▁</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▄▃▂▂▂▂▂▂▃▂▃▂▁▁▁▁▁▁▁▁▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>24</td></tr><tr><td>best_val_loss</td><td>0.41799</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36527</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.42676</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">genial-wind-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/2nbnpwf9\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036a/runs/2nbnpwf9</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_152556-2nbnpwf9/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2nbnpwf9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/2by7k3xr\" target=\"_blank\">cool-firefly-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-15:29:36] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:29:36] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:29:36] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 577ms/step - loss: 0.8905 - val_loss: 0.8931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.6540 - val_loss: 0.6430\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5404 - val_loss: 0.5369\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5235 - val_loss: 0.4954\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4630 - val_loss: 0.4939\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4340 - val_loss: 0.4637\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4140 - val_loss: 0.4600\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 200ms/step - loss: 0.4078 - val_loss: 0.4905\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4362 - val_loss: 0.4721\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4145 - val_loss: 0.4553\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4260 - val_loss: 0.5016\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.4107 - val_loss: 0.5107\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4046 - val_loss: 0.4488\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3813 - val_loss: 0.4510\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3733 - val_loss: 0.4464\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3732 - val_loss: 0.5205\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3855 - val_loss: 0.4659\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3685 - val_loss: 0.4568\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 199ms/step - loss: 0.3624 - val_loss: 0.4586\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3536 - val_loss: 0.4480\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3661 - val_loss: 0.4614\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_20-15:30:11] Training the entire fine-tuned model...\n",
      "[2022_04_20-15:30:19] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4671WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0980s vs `on_train_batch_end` time: 0.1301s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0980s vs `on_train_batch_end` time: 0.1301s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 703ms/step - loss: 0.4671 - val_loss: 0.6318\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.4564 - val_loss: 0.4970\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.4192 - val_loss: 0.5189\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 327ms/step - loss: 0.3819 - val_loss: 0.4495\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3661 - val_loss: 0.4891\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 328ms/step - loss: 0.3692 - val_loss: 0.4511\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 327ms/step - loss: 0.3370 - val_loss: 0.4532\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3448 - val_loss: 0.4626\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3311 - val_loss: 0.5021\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.3323 - val_loss: 0.4619\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_20-15:30:47] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-15:30:47] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:31:06] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3793WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1007s vs `on_train_batch_end` time: 0.1250s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1007s vs `on_train_batch_end` time: 0.1250s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 695ms/step - loss: 0.3793 - val_loss: 0.4492\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>497</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>129</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  497  12\n",
       "1  129  10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12422360248447205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2by7k3xr) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 43816... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▃▃▂▂▁▁▁▁▁▁▂</td></tr><tr><td>lr</td><td>██████████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▂▂▁▁▂▁▁▂▂▁▁▁▂▁▁▁▁▁▄▂▂▁▂▁▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>14</td></tr><tr><td>best_val_loss</td><td>0.44638</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37926</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.44922</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">cool-firefly-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/2by7k3xr\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036b/runs/2by7k3xr</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_152919-2by7k3xr/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2by7k3xr). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/3pna38r7\" target=\"_blank\">vocal-wind-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-15:32:02] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:32:02] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:32:02] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 609ms/step - loss: 0.9289 - val_loss: 1.4949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.8525 - val_loss: 0.9755\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.8816 - val_loss: 0.8375\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.7103 - val_loss: 0.7640\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.8160 - val_loss: 0.5401\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.6810 - val_loss: 0.5963\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.6803 - val_loss: 0.4545\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4993 - val_loss: 0.4646\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5135 - val_loss: 0.4547\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4407 - val_loss: 0.4628\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4395 - val_loss: 0.4433\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4237 - val_loss: 0.4775\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4303 - val_loss: 0.4370\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4279 - val_loss: 0.4324\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4235 - val_loss: 0.4473\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4126 - val_loss: 0.4283\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3999 - val_loss: 0.4424\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4099 - val_loss: 0.4323\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4116 - val_loss: 0.4460\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3948 - val_loss: 0.4491\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4187 - val_loss: 0.4254\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3963 - val_loss: 0.4401\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3927 - val_loss: 0.4274\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3999 - val_loss: 0.4330\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4058 - val_loss: 0.4260\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3842 - val_loss: 0.4275\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3914 - val_loss: 0.4263\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3938 - val_loss: 0.4257\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3853 - val_loss: 0.4254\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_20-15:32:44] Training the entire fine-tuned model...\n",
      "[2022_04_20-15:32:59] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3914WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0971s vs `on_train_batch_end` time: 0.1237s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0971s vs `on_train_batch_end` time: 0.1237s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 1s/step - loss: 0.3933 - val_loss: 0.4281\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3892 - val_loss: 0.4278\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3974 - val_loss: 0.4255\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3904 - val_loss: 0.4255\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3877 - val_loss: 0.4300\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3932 - val_loss: 0.4333\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3830 - val_loss: 0.4264\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3881 - val_loss: 0.4256\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3859 - val_loss: 0.4255\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3813 - val_loss: 0.4260\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3864 - val_loss: 0.4272\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3812 - val_loss: 0.4273\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3879 - val_loss: 0.4273\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3903 - val_loss: 0.4271\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3832 - val_loss: 0.4271\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.4020 - val_loss: 0.4272\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3931 - val_loss: 0.4271\n",
      "[2022_04_20-15:33:41] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-15:33:41] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:33:46] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3919WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0975s vs `on_train_batch_end` time: 0.1240s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0975s vs `on_train_batch_end` time: 0.1240s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 697ms/step - loss: 0.3922 - val_loss: 0.4254\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>523</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>115</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  523  16\n",
       "1  115  16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19631901840490798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3pna38r7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 44091... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▅▅▅▅▅▆▆▇▇▇▇██▁▁▁▂▂▃▃▃▃▄▄▄▅▅▁</td></tr><tr><td>loss</td><td>█▇▇▅▇▅▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>20</td></tr><tr><td>best_val_loss</td><td>0.42535</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.39224</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42536</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">vocal-wind-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/3pna38r7\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036a/runs/3pna38r7</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_153144-3pna38r7/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3pna38r7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/12w0ufhy\" target=\"_blank\">rural-microwave-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-15:34:40] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:34:40] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:34:40] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 586ms/step - loss: 0.8166 - val_loss: 0.7816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.5994 - val_loss: 0.5655\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4963 - val_loss: 0.5253\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4782 - val_loss: 0.4807\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4556 - val_loss: 0.4988\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4555 - val_loss: 0.6424\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4956 - val_loss: 0.4601\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4282 - val_loss: 0.4580\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4025 - val_loss: 0.4689\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3917 - val_loss: 0.4510\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.4013 - val_loss: 0.4496\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4072 - val_loss: 0.5895\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4236 - val_loss: 0.4430\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.4265 - val_loss: 0.4470\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 0.3844 - val_loss: 0.4662\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3870 - val_loss: 0.4782\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3635 - val_loss: 0.4451\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.3687 - val_loss: 0.4644\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3618 - val_loss: 0.4553\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3585 - val_loss: 0.4487\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 200ms/step - loss: 0.3658 - val_loss: 0.4574\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_20-15:35:14] Training the entire fine-tuned model...\n",
      "[2022_04_20-15:35:37] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3866WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1000s vs `on_train_batch_end` time: 0.1270s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1000s vs `on_train_batch_end` time: 0.1270s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 688ms/step - loss: 0.3866 - val_loss: 0.4481\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3670 - val_loss: 0.4548\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3734 - val_loss: 0.4511\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3871 - val_loss: 0.4498\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 327ms/step - loss: 0.3839 - val_loss: 0.4489\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3759 - val_loss: 0.4494\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3733 - val_loss: 0.4504\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3680 - val_loss: 0.4513\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 329ms/step - loss: 0.3661 - val_loss: 0.4518\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_20-15:36:02] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-15:36:02] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:36:03] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3733WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1001s vs `on_train_batch_end` time: 0.1273s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1001s vs `on_train_batch_end` time: 0.1273s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 670ms/step - loss: 0.3733 - val_loss: 0.4483\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>504</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  504  5\n",
       "1  132  7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09271523178807947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:12w0ufhy) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 44424... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▅▃▃▂▂▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▅▁▁▂▁▁▄▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>12</td></tr><tr><td>best_val_loss</td><td>0.44303</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37327</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.44831</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">rural-microwave-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/12w0ufhy\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036b/runs/12w0ufhy</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_153424-12w0ufhy/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:12w0ufhy). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/q4vcaaem\" target=\"_blank\">drawn-dust-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-15:36:57] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:36:57] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:36:57] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 576ms/step - loss: 1.0222 - val_loss: 1.1520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.7405 - val_loss: 0.8577\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.7188 - val_loss: 0.8790\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.6588 - val_loss: 0.6681\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5796 - val_loss: 0.7511\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.5925 - val_loss: 0.5820\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5304 - val_loss: 0.5167\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4528 - val_loss: 0.4474\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4606 - val_loss: 0.4795\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4964 - val_loss: 0.4721\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4446 - val_loss: 0.4540\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4724 - val_loss: 0.4786\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4572 - val_loss: 0.4397\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4280 - val_loss: 0.4749\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4164 - val_loss: 0.4429\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4335 - val_loss: 0.4389\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4232 - val_loss: 0.4563\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4217 - val_loss: 0.4390\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4445 - val_loss: 0.4564\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4269 - val_loss: 0.4565\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4383 - val_loss: 0.4520\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4169 - val_loss: 0.4352\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4160 - val_loss: 0.4368\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4161 - val_loss: 0.4360\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4149 - val_loss: 0.4347\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4168 - val_loss: 0.4420\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4077 - val_loss: 0.4424\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4096 - val_loss: 0.4363\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4026 - val_loss: 0.4354\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4063 - val_loss: 0.4361\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4201 - val_loss: 0.4358\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4154 - val_loss: 0.4344\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4020 - val_loss: 0.4341\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4025 - val_loss: 0.4341\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3988 - val_loss: 0.4343\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4055 - val_loss: 0.4346\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4097 - val_loss: 0.4350\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4074 - val_loss: 0.4351\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4158 - val_loss: 0.4351\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4138 - val_loss: 0.4354\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4099 - val_loss: 0.4357\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "[2022_04_20-15:37:53] Training the entire fine-tuned model...\n",
      "[2022_04_20-15:38:36] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4119WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1002s vs `on_train_batch_end` time: 0.1192s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1002s vs `on_train_batch_end` time: 0.1192s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 684ms/step - loss: 0.4091 - val_loss: 0.4374\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.4011 - val_loss: 0.4319\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4152 - val_loss: 0.4335\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.4044 - val_loss: 0.4307\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3940 - val_loss: 0.4313\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.4076 - val_loss: 0.4313\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3937 - val_loss: 0.4292\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4012 - val_loss: 0.4327\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3887 - val_loss: 0.4277\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3895 - val_loss: 0.4454\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3857 - val_loss: 0.4283\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3968 - val_loss: 0.4307\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3853 - val_loss: 0.4426\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3770 - val_loss: 0.4377\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3871 - val_loss: 0.4297\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3781 - val_loss: 0.4296\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3817 - val_loss: 0.4313\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_20-15:39:17] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-15:39:17] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:39:17] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3758WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0973s vs `on_train_batch_end` time: 0.1234s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0973s vs `on_train_batch_end` time: 0.1234s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 687ms/step - loss: 0.3786 - val_loss: 0.4278\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>525</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  525  14\n",
       "1  120  11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14102564102564102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:q4vcaaem) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 44680... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▅▅▃▃▂▂▂▂▂▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▅▄▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_val_loss</td><td>0.42773</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37861</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42777</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">drawn-dust-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/q4vcaaem\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036a/runs/q4vcaaem</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_153641-q4vcaaem/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:q4vcaaem). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/2lq9dg86\" target=\"_blank\">fiery-paper-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-15:41:38] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:41:38] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:41:38] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 852ms/step - loss: 0.8134 - val_loss: 0.6231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5407 - val_loss: 0.4936\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4861 - val_loss: 0.5830\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4750 - val_loss: 0.4828\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4570 - val_loss: 0.4713\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4286 - val_loss: 0.4842\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4214 - val_loss: 0.5444\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4438 - val_loss: 0.6177\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5205 - val_loss: 0.6103\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4505 - val_loss: 0.4536\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4103 - val_loss: 0.4600\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3895 - val_loss: 0.4828\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3869 - val_loss: 0.4512\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4039 - val_loss: 0.4629\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4053 - val_loss: 0.4615\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3878 - val_loss: 0.4538\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3924 - val_loss: 0.4769\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3866 - val_loss: 0.4627\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3881 - val_loss: 0.4525\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3865 - val_loss: 0.4545\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3885 - val_loss: 0.4558\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_20-15:42:13] Training the entire fine-tuned model...\n",
      "[2022_04_20-15:42:21] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3873WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1012s vs `on_train_batch_end` time: 0.1237s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1012s vs `on_train_batch_end` time: 0.1237s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 693ms/step - loss: 0.3873 - val_loss: 0.4691\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.3870 - val_loss: 0.4547\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3854 - val_loss: 0.4590\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3802 - val_loss: 0.4525\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3789 - val_loss: 0.4597\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3816 - val_loss: 0.4469\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3763 - val_loss: 0.4562\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 0.3765 - val_loss: 0.4518\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3663 - val_loss: 0.4481\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 327ms/step - loss: 0.3634 - val_loss: 0.4622\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3674 - val_loss: 0.4646\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3592 - val_loss: 0.4493\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.3619 - val_loss: 0.4455\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 327ms/step - loss: 0.3505 - val_loss: 0.4482\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3422 - val_loss: 0.4603\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.3498 - val_loss: 0.4583\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3443 - val_loss: 0.4496\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3497 - val_loss: 0.4496\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.3439 - val_loss: 0.4511\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3454 - val_loss: 0.4509\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.3530 - val_loss: 0.4521\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "[2022_04_20-15:43:09] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-15:43:09] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:43:49] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3546WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0984s vs `on_train_batch_end` time: 0.1279s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0984s vs `on_train_batch_end` time: 0.1279s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 701ms/step - loss: 0.3546 - val_loss: 0.4490\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>502</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>129</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  502   7\n",
       "1  129  10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1282051282051282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2lq9dg86) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 45094... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▂▃▃▄▄▅▅▅▆▆▆▇▇▇█▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▂▂▃▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▆▂▂▃▅█▇▁▂▂▁▂▁▂▂▁▁▁▂▁▂▁▂▁▁▁▂▂▁▁▁▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>12</td></tr><tr><td>best_val_loss</td><td>0.44548</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35459</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.44899</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fiery-paper-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/2lq9dg86\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036b/runs/2lq9dg86</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_153955-2lq9dg86/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2lq9dg86). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/1n7v5cqs\" target=\"_blank\">absurd-field-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-15:44:44] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:44:44] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:44:44] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 599ms/step - loss: 0.8645 - val_loss: 0.7605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.7089 - val_loss: 0.4853\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.6029 - val_loss: 0.5429\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.5564 - val_loss: 0.5926\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5294 - val_loss: 0.5388\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.5048 - val_loss: 0.5314\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4704 - val_loss: 0.4533\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4504 - val_loss: 0.4516\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4320 - val_loss: 0.4667\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4496 - val_loss: 0.4496\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4473 - val_loss: 0.4485\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4470 - val_loss: 0.4500\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4380 - val_loss: 0.4479\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4333 - val_loss: 0.4675\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4433 - val_loss: 0.4463\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4369 - val_loss: 0.4433\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4318 - val_loss: 0.4429\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4333 - val_loss: 0.4438\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4362 - val_loss: 0.4421\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4313 - val_loss: 0.4450\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4368 - val_loss: 0.4454\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4320 - val_loss: 0.4411\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4360 - val_loss: 0.4402\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4320 - val_loss: 0.4620\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4165 - val_loss: 0.4459\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4207 - val_loss: 0.4472\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4366 - val_loss: 0.4420\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4119 - val_loss: 0.4367\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4098 - val_loss: 0.4361\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4178 - val_loss: 0.4362\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4167 - val_loss: 0.4364\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4158 - val_loss: 0.4366\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4161 - val_loss: 0.4364\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4132 - val_loss: 0.4360\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4105 - val_loss: 0.4359\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4172 - val_loss: 0.4361\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4114 - val_loss: 0.4359\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4125 - val_loss: 0.4356\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4198 - val_loss: 0.4354\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4158 - val_loss: 0.4353\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4170 - val_loss: 0.4354\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.4094 - val_loss: 0.4360\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4192 - val_loss: 0.4360\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4158 - val_loss: 0.4359\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4155 - val_loss: 0.4361\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4088 - val_loss: 0.4360\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4102 - val_loss: 0.4360\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4221 - val_loss: 0.4361\n",
      "[2022_04_20-15:45:48] Training the entire fine-tuned model...\n",
      "[2022_04_20-15:46:22] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4247WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0983s vs `on_train_batch_end` time: 0.1229s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0983s vs `on_train_batch_end` time: 0.1229s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 687ms/step - loss: 0.4211 - val_loss: 0.4602\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.4417 - val_loss: 0.4467\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.4094 - val_loss: 0.4326\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4084 - val_loss: 0.4326\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3934 - val_loss: 0.4355\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3799 - val_loss: 0.4425\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3833 - val_loss: 0.4322\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3756 - val_loss: 0.4318\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3740 - val_loss: 0.4338\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3645 - val_loss: 0.4579\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3599 - val_loss: 0.4361\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3611 - val_loss: 0.5060\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3925 - val_loss: 0.4445\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3371 - val_loss: 0.4433\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3538 - val_loss: 0.4299\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3150 - val_loss: 0.4394\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3207 - val_loss: 0.4347\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3155 - val_loss: 0.4364\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3040 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3134 - val_loss: 0.4465\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3150 - val_loss: 0.4417\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3097 - val_loss: 0.4406\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3012 - val_loss: 0.4409\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "[2022_04_20-15:47:15] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-15:47:15] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:47:15] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3252WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0964s vs `on_train_batch_end` time: 0.1249s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0964s vs `on_train_batch_end` time: 0.1249s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 694ms/step - loss: 0.3230 - val_loss: 0.4295\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>515</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  515  24\n",
       "1  109  22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24858757062146894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1n7v5cqs) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 45495... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▄▄▄▁</td></tr><tr><td>loss</td><td>█▆▄▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▄▃▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.42951</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32298</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.42951</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">absurd-field-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/1n7v5cqs\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036a/runs/1n7v5cqs</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_154427-1n7v5cqs/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1n7v5cqs). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/yyb4dksm\" target=\"_blank\">wobbly-thunder-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-15:48:22] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:48:22] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:48:22] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 864ms/step - loss: 0.9918 - val_loss: 0.7657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.6734 - val_loss: 0.7244\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5515 - val_loss: 0.4837\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4896 - val_loss: 0.4822\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4564 - val_loss: 0.4836\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4288 - val_loss: 0.4853\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4201 - val_loss: 0.4605\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4040 - val_loss: 0.5075\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4090 - val_loss: 0.4626\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4359 - val_loss: 0.4543\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4140 - val_loss: 0.5252\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4083 - val_loss: 0.4492\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4085 - val_loss: 0.4591\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4007 - val_loss: 0.5093\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3944 - val_loss: 0.4867\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4023 - val_loss: 0.4533\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3780 - val_loss: 0.4762\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3750 - val_loss: 0.4629\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.3733 - val_loss: 0.4458\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3590 - val_loss: 0.4617\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3697 - val_loss: 0.4506\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3715 - val_loss: 0.4474\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3615 - val_loss: 0.4764\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3654 - val_loss: 0.4626\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3597 - val_loss: 0.4513\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3701 - val_loss: 0.4514\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3619 - val_loss: 0.4547\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_20-15:49:04] Training the entire fine-tuned model...\n",
      "[2022_04_20-15:49:19] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3769WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0981s vs `on_train_batch_end` time: 0.1286s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0981s vs `on_train_batch_end` time: 0.1286s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 690ms/step - loss: 0.3769 - val_loss: 0.4465\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.3650 - val_loss: 0.4850\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.3709 - val_loss: 0.4513\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3604 - val_loss: 0.4830\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3481 - val_loss: 0.4448\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3452 - val_loss: 0.4621\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3425 - val_loss: 0.4466\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 329ms/step - loss: 0.3398 - val_loss: 0.4766\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3477 - val_loss: 0.4432\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3377 - val_loss: 0.4605\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 327ms/step - loss: 0.3215 - val_loss: 0.4591\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.3203 - val_loss: 0.4480\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.3131 - val_loss: 0.4963\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 327ms/step - loss: 0.3130 - val_loss: 0.4669\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.3081 - val_loss: 0.4513\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.3076 - val_loss: 0.4767\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3002 - val_loss: 0.4734\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_20-15:50:00] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-15:50:00] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:50:00] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3340WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0989s vs `on_train_batch_end` time: 0.1274s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0989s vs `on_train_batch_end` time: 0.1274s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 682ms/step - loss: 0.3340 - val_loss: 0.4421\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>483</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  483  26\n",
       "1  112  27"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:yyb4dksm) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 45964... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▂▂▂▂▁▂▁▃▁▁▂▂▁▂▁▁▁▁▂▁▁▁▁▂▁▂▁▁▁▂▁▁▁▂▂▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.44211</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33401</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.44211</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">wobbly-thunder-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/yyb4dksm\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036b/runs/yyb4dksm</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_154754-yyb4dksm/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:yyb4dksm). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/3qa40ouw\" target=\"_blank\">ruby-star-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-15:50:54] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:50:54] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:50:54] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 592ms/step - loss: 0.9074 - val_loss: 1.7136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 1.0239 - val_loss: 0.9554\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.7637 - val_loss: 0.6394\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5386 - val_loss: 0.5101\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4712 - val_loss: 0.5385\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4722 - val_loss: 0.4905\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4582 - val_loss: 0.5067\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4566 - val_loss: 0.4593\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4434 - val_loss: 0.4651\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4289 - val_loss: 0.4473\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 199ms/step - loss: 0.4327 - val_loss: 0.4968\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4552 - val_loss: 0.4674\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4345 - val_loss: 0.4937\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4241 - val_loss: 0.4457\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4202 - val_loss: 0.4391\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4162 - val_loss: 0.4347\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4262 - val_loss: 0.4913\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4043 - val_loss: 0.4363\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4009 - val_loss: 0.4314\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4022 - val_loss: 0.4345\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4012 - val_loss: 0.4562\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3989 - val_loss: 0.4461\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4288 - val_loss: 0.4973\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4291 - val_loss: 0.4398\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4137 - val_loss: 0.4594\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4011 - val_loss: 0.4262\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3993 - val_loss: 0.4247\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3931 - val_loss: 0.4422\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3793 - val_loss: 0.4244\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3797 - val_loss: 0.4219\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3810 - val_loss: 0.4215\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3872 - val_loss: 0.4224\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3822 - val_loss: 0.4192\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3854 - val_loss: 0.4186\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3809 - val_loss: 0.4230\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3776 - val_loss: 0.4248\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4017 - val_loss: 0.4288\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3790 - val_loss: 0.4257\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3668 - val_loss: 0.4224\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3776 - val_loss: 0.4223\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3673 - val_loss: 0.4251\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3825 - val_loss: 0.4222\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_20-15:51:51] Training the entire fine-tuned model...\n",
      "[2022_04_20-15:52:32] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.6224WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0965s vs `on_train_batch_end` time: 0.1232s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0965s vs `on_train_batch_end` time: 0.1232s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 1s/step - loss: 0.6231 - val_loss: 0.4517\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.5319 - val_loss: 0.4634\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.4310 - val_loss: 0.4617\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.4090 - val_loss: 0.4337\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3904 - val_loss: 0.4388\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4065 - val_loss: 0.4215\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3623 - val_loss: 0.4197\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3661 - val_loss: 0.4777\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.4209 - val_loss: 0.4683\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3803 - val_loss: 0.4257\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 321ms/step - loss: 0.3488 - val_loss: 0.4247\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3296 - val_loss: 0.4217\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3317 - val_loss: 0.4186\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 322ms/step - loss: 0.3153 - val_loss: 0.4231\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3297 - val_loss: 0.4163\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.3123 - val_loss: 0.4169\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3071 - val_loss: 0.4174\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.2959 - val_loss: 0.4272\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3095 - val_loss: 0.4200\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.2952 - val_loss: 0.4204\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 2s 320ms/step - loss: 0.2973 - val_loss: 0.4228\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3036 - val_loss: 0.4264\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.2884 - val_loss: 0.4209\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "[2022_04_20-15:53:25] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-15:53:25] Training set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:53:59] Validation set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3114WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0984s vs `on_train_batch_end` time: 0.1202s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0984s vs `on_train_batch_end` time: 0.1202s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 684ms/step - loss: 0.3087 - val_loss: 0.4165\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>520</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  520  19\n",
       "1  109  22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2558139534883721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_20_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3qa40ouw) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 46293... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▂▂▂▂▃▃▃▃▄▄▄▅▁</td></tr><tr><td>loss</td><td>▇█▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>14</td></tr><tr><td>best_val_loss</td><td>0.41631</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30873</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.41651</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">ruby-star-16</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036a/runs/3qa40ouw\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036a/runs/3qa40ouw</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_155037-3qa40ouw/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3qa40ouw). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/1owbqh3a\" target=\"_blank\">graceful-silence-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-15:54:54] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:54:54] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:54:54] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 601ms/step - loss: 0.9324 - val_loss: 1.1899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.8411 - val_loss: 0.8684\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.6219 - val_loss: 0.5894\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4968 - val_loss: 0.6118\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.5089 - val_loss: 0.4698\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4419 - val_loss: 0.4756\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4324 - val_loss: 0.4724\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4140 - val_loss: 0.4584\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4103 - val_loss: 0.4981\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4090 - val_loss: 0.4576\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3968 - val_loss: 0.4482\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4083 - val_loss: 0.4680\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.3840 - val_loss: 0.4468\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3879 - val_loss: 0.4467\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3813 - val_loss: 0.4476\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3841 - val_loss: 0.5002\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3880 - val_loss: 0.4972\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3848 - val_loss: 0.4665\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3714 - val_loss: 0.4461\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3646 - val_loss: 0.4651\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3698 - val_loss: 0.4504\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3670 - val_loss: 0.4526\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3759 - val_loss: 0.4720\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3715 - val_loss: 0.4551\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3547 - val_loss: 0.4532\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3643 - val_loss: 0.4502\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3588 - val_loss: 0.4470\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_20-15:55:34] Training the entire fine-tuned model...\n",
      "[2022_04_20-15:55:44] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4765WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1008s vs `on_train_batch_end` time: 0.1252s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1008s vs `on_train_batch_end` time: 0.1252s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 699ms/step - loss: 0.4765 - val_loss: 0.5764\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 329ms/step - loss: 0.4033 - val_loss: 0.4657\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3864 - val_loss: 0.4837\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 328ms/step - loss: 0.3683 - val_loss: 0.4724\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 329ms/step - loss: 0.3665 - val_loss: 0.4539\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 329ms/step - loss: 0.3467 - val_loss: 0.4491\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 328ms/step - loss: 0.3427 - val_loss: 0.4907\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.3164 - val_loss: 0.4759\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3134 - val_loss: 0.4664\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.3034 - val_loss: 0.4809\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.2753 - val_loss: 0.4752\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.2623 - val_loss: 0.4903\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.2428 - val_loss: 0.4852\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 329ms/step - loss: 0.2259 - val_loss: 0.4954\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_20-15:56:19] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-15:56:19] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:56:31] Validation set: Filtered out 0 of 648 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3353WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0972s vs `on_train_batch_end` time: 0.1314s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0972s vs `on_train_batch_end` time: 0.1314s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 900ms/step - loss: 0.3353 - val_loss: 0.4525\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>492</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  492  17\n",
       "1  121  18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20689655172413793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_20_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1owbqh3a) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 46732... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▃▄▄▄▁</td></tr><tr><td>loss</td><td>█▇▅▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▂▂▂▂▂▂▂▂▁▁▁▂</td></tr><tr><td>lr</td><td>█████████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▂▃▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>18</td></tr><tr><td>best_val_loss</td><td>0.44615</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33534</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.4525</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">graceful-silence-16</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2036b/runs/1owbqh3a\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2036b/runs/1owbqh3a</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_155438-1owbqh3a/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1owbqh3a). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/269pr7ya\" target=\"_blank\">super-snowball-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-15:57:25] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:57:25] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:57:25] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 733ms/step - loss: 0.7905 - val_loss: 0.7774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.6305 - val_loss: 0.6207\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.5376 - val_loss: 0.4868\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4819 - val_loss: 0.4794\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4397 - val_loss: 0.4763\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4367 - val_loss: 0.4676\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4333 - val_loss: 0.4864\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4183 - val_loss: 0.4624\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3990 - val_loss: 0.4779\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4117 - val_loss: 0.4543\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3877 - val_loss: 0.4527\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3991 - val_loss: 0.4563\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3726 - val_loss: 0.4493\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 245ms/step - loss: 0.3848 - val_loss: 0.4611\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3891 - val_loss: 0.4647\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3846 - val_loss: 0.4518\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-15:57:55] Training the entire fine-tuned model...\n",
      "[2022_04_20-15:58:03] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 866ms/step - loss: 0.3792 - val_loss: 0.4501\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3726 - val_loss: 0.4491\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3697 - val_loss: 0.4491\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3621 - val_loss: 0.4491\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3711 - val_loss: 0.4491\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "[2022_04_20-15:58:21] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-15:58:21] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:58:30] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 832ms/step - loss: 0.3667 - val_loss: 0.4491\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>547</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>142</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  547  16\n",
       "1  142  11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12222222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:269pr7ya) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 47039... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██▁▁▂▂▃▁</td></tr><tr><td>loss</td><td>█▅▄▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.44905</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36673</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.44905</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">super-snowball-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/269pr7ya\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042a/runs/269pr7ya</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_155709-269pr7ya/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:269pr7ya). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/1919gc53\" target=\"_blank\">robust-plant-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-15:59:24] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:59:24] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-15:59:24] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 886ms/step - loss: 0.8871 - val_loss: 0.8010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.6387 - val_loss: 0.4852\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5156 - val_loss: 0.4998\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.5459 - val_loss: 0.5436\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4832 - val_loss: 0.4693\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4615 - val_loss: 0.4461\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4416 - val_loss: 0.4430\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4445 - val_loss: 0.4429\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4336 - val_loss: 0.4492\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4337 - val_loss: 0.4411\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4288 - val_loss: 0.4591\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4279 - val_loss: 0.4514\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4343 - val_loss: 0.4463\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-15:59:50] Training the entire fine-tuned model...\n",
      "[2022_04_20-15:59:58] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4150WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1146s vs `on_train_batch_end` time: 0.1341s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1146s vs `on_train_batch_end` time: 0.1341s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 687ms/step - loss: 0.4150 - val_loss: 0.4344\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.4273 - val_loss: 0.4343\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 331ms/step - loss: 0.4210 - val_loss: 0.4342\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 332ms/step - loss: 0.4230 - val_loss: 0.4337\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.4141 - val_loss: 0.4341\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 331ms/step - loss: 0.4105 - val_loss: 0.4333\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.4054 - val_loss: 0.4330\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 332ms/step - loss: 0.4083 - val_loss: 0.4327\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.4169 - val_loss: 0.4326\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.4149 - val_loss: 0.4326\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.4155 - val_loss: 0.4322\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 332ms/step - loss: 0.4027 - val_loss: 0.4319\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.4092 - val_loss: 0.4316\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.4035 - val_loss: 0.4312\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.4026 - val_loss: 0.4309\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.4071 - val_loss: 0.4308\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.4053 - val_loss: 0.4308\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.4066 - val_loss: 0.4299\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3989 - val_loss: 0.4297\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.4102 - val_loss: 0.4296\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3998 - val_loss: 0.4291\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3962 - val_loss: 0.4288\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.4078 - val_loss: 0.4286\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.4033 - val_loss: 0.4285\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3985 - val_loss: 0.4282\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3996 - val_loss: 0.4279\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3895 - val_loss: 0.4277\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3893 - val_loss: 0.4278\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3966 - val_loss: 0.4271\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3955 - val_loss: 0.4271\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3828 - val_loss: 0.4268\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3776 - val_loss: 0.4266\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3799 - val_loss: 0.4271\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3865 - val_loss: 0.4262\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3804 - val_loss: 0.4258\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3877 - val_loss: 0.4259\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3807 - val_loss: 0.4256\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3793 - val_loss: 0.4260\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3749 - val_loss: 0.4254\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3795 - val_loss: 0.4254\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3751 - val_loss: 0.4254\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3808 - val_loss: 0.4253\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3741 - val_loss: 0.4252\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3833 - val_loss: 0.4253\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3699 - val_loss: 0.4254\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3758 - val_loss: 0.4255\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_20-16:01:38] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-16:01:38] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:01:38] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3793WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1044s vs `on_train_batch_end` time: 0.1387s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1044s vs `on_train_batch_end` time: 0.1387s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 676ms/step - loss: 0.3793 - val_loss: 0.4253\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>474</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  474  11\n",
       "1   98  19"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2585034013605442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1919gc53) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 47247... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁</td></tr><tr><td>loss</td><td>█▅▃▃▂▂▂▂▂▂▂▂▂▁▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▃▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>42</td></tr><tr><td>best_val_loss</td><td>0.42525</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37934</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42527</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">robust-plant-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/1919gc53\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042b/runs/1919gc53</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_155907-1919gc53/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1919gc53). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/ytkdnshl\" target=\"_blank\">exalted-aardvark-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-16:02:34] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:02:34] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:02:35] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 732ms/step - loss: 0.9826 - val_loss: 0.5696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 246ms/step - loss: 0.6420 - val_loss: 0.7225\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.7106 - val_loss: 0.5048\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.5755 - val_loss: 0.5292\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.5662 - val_loss: 0.5358\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4966 - val_loss: 0.4703\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4475 - val_loss: 0.4773\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4147 - val_loss: 0.4682\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4137 - val_loss: 0.4687\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3975 - val_loss: 0.4579\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4029 - val_loss: 0.4543\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3902 - val_loss: 0.4693\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4081 - val_loss: 0.4602\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3878 - val_loss: 0.4588\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-16:03:00] Training the entire fine-tuned model...\n",
      "[2022_04_20-16:03:11] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 818ms/step - loss: 0.3944 - val_loss: 0.4550\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3889 - val_loss: 0.4539\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3842 - val_loss: 0.4621\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3904 - val_loss: 0.4533\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3734 - val_loss: 0.4529\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3687 - val_loss: 0.4544\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3710 - val_loss: 0.4523\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3781 - val_loss: 0.4523\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3539 - val_loss: 0.4565\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3616 - val_loss: 0.4538\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3555 - val_loss: 0.4542\n",
      "[2022_04_20-16:03:39] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-16:03:39] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:03:39] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 870ms/step - loss: 0.3601 - val_loss: 0.4525\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>137</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  536  27\n",
       "1  137  16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16326530612244897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ytkdnshl) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 47670... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▄▅▅▆▆▇▇█▁▂▂▃▃▄▄▅▅▆▆▁</td></tr><tr><td>loss</td><td>█▄▅▃▃▃▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄█▂▃▃▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.45233</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36014</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.45249</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">exalted-aardvark-2</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/ytkdnshl\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042a/runs/ytkdnshl</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_160217-ytkdnshl/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ytkdnshl). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/10v1iwh0\" target=\"_blank\">wobbly-surf-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-16:04:35] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:04:35] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:04:35] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 573ms/step - loss: 0.8662 - val_loss: 0.6601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5666 - val_loss: 0.4684\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5132 - val_loss: 0.4715\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4736 - val_loss: 0.4546\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4607 - val_loss: 0.4536\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4452 - val_loss: 0.4540\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4583 - val_loss: 0.4429\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4274 - val_loss: 0.4489\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4293 - val_loss: 0.4346\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4129 - val_loss: 0.4843\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4288 - val_loss: 0.4286\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4070 - val_loss: 0.4276\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4056 - val_loss: 0.4589\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4105 - val_loss: 0.4267\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3990 - val_loss: 0.4564\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3928 - val_loss: 0.4322\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4078 - val_loss: 0.4563\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-16:05:04] Training the entire fine-tuned model...\n",
      "[2022_04_20-16:05:33] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3828WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1048s vs `on_train_batch_end` time: 0.1374s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1048s vs `on_train_batch_end` time: 0.1374s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 705ms/step - loss: 0.3828 - val_loss: 0.4288\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 332ms/step - loss: 0.3817 - val_loss: 0.4230\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 330ms/step - loss: 0.3810 - val_loss: 0.4222\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 332ms/step - loss: 0.3869 - val_loss: 0.4230\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3767 - val_loss: 0.4220\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3807 - val_loss: 0.4208\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 332ms/step - loss: 0.3836 - val_loss: 0.4198\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3810 - val_loss: 0.4193\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3714 - val_loss: 0.4208\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 332ms/step - loss: 0.3685 - val_loss: 0.4201\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 332ms/step - loss: 0.3681 - val_loss: 0.4191\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 332ms/step - loss: 0.3627 - val_loss: 0.4184\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3556 - val_loss: 0.4180\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3581 - val_loss: 0.4197\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3490 - val_loss: 0.4190\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3464 - val_loss: 0.4206\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_20-16:06:13] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-16:06:13] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:06:20] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3628WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1069s vs `on_train_batch_end` time: 0.1357s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1069s vs `on_train_batch_end` time: 0.1357s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 700ms/step - loss: 0.3628 - val_loss: 0.4182\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>474</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  474  11\n",
       "1   97  20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2702702702702703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:10v1iwh0) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 47890... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇█▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▃▂▂▂▂▂▁▃▁▁▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>12</td></tr><tr><td>best_val_loss</td><td>0.418</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36277</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41818</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">wobbly-surf-2</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/10v1iwh0\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042b/runs/10v1iwh0</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_160418-10v1iwh0/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:10v1iwh0). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/23ynm45r\" target=\"_blank\">proud-shadow-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-16:07:15] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:07:15] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:07:15] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 727ms/step - loss: 0.9484 - val_loss: 0.5094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.6784 - val_loss: 0.7895\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.6748 - val_loss: 0.5041\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.5263 - val_loss: 0.5655\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.5165 - val_loss: 0.4857\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4725 - val_loss: 0.5247\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4774 - val_loss: 0.4861\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4302 - val_loss: 0.4891\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-16:07:36] Training the entire fine-tuned model...\n",
      "[2022_04_20-16:07:43] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 826ms/step - loss: 0.4339 - val_loss: 0.5022\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.4296 - val_loss: 0.4842\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.4267 - val_loss: 0.4732\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.4229 - val_loss: 0.4770\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.4093 - val_loss: 0.4715\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.4028 - val_loss: 0.4728\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3906 - val_loss: 0.4673\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3822 - val_loss: 0.4782\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3663 - val_loss: 0.4777\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3615 - val_loss: 0.5092\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_20-16:08:10] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-16:08:10] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:08:10] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 815ms/step - loss: 0.3839 - val_loss: 0.4679\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>543</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>142</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  543  20\n",
       "1  142  11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11956521739130435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:23ynm45r) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 48170... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▁▂▃▃▄▅▆▆▇█▁</td></tr><tr><td>loss</td><td>█▅▅▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▂█▂▃▁▂▁▁▂▁▁▁▁▁▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>6</td></tr><tr><td>best_val_loss</td><td>0.4673</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.38393</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.46787</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">proud-shadow-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/23ynm45r\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042a/runs/23ynm45r</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_160658-23ynm45r/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:23ynm45r). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/1r58n9lw\" target=\"_blank\">upbeat-fire-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-16:09:04] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:09:04] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:09:04] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 581ms/step - loss: 0.8538 - val_loss: 0.7651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.6167 - val_loss: 0.4785\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5104 - val_loss: 0.5056\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5259 - val_loss: 0.4797\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4810 - val_loss: 0.4519\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4452 - val_loss: 0.4642\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4679 - val_loss: 0.4433\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4436 - val_loss: 0.4631\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4434 - val_loss: 0.4365\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4287 - val_loss: 0.4535\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4362 - val_loss: 0.4317\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4175 - val_loss: 0.4282\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4196 - val_loss: 0.4752\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4254 - val_loss: 0.4269\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4176 - val_loss: 0.4250\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4142 - val_loss: 0.4542\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4156 - val_loss: 0.4265\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3959 - val_loss: 0.4202\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3849 - val_loss: 0.4223\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3863 - val_loss: 0.5173\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4166 - val_loss: 0.4179\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3773 - val_loss: 0.4492\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4089 - val_loss: 0.4178\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3825 - val_loss: 0.4693\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3983 - val_loss: 0.4235\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3786 - val_loss: 0.4173\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3759 - val_loss: 0.4145\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3601 - val_loss: 0.4141\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3624 - val_loss: 0.4140\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3695 - val_loss: 0.4144\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3687 - val_loss: 0.4139\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3645 - val_loss: 0.4169\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3650 - val_loss: 0.4132\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3630 - val_loss: 0.4129\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3480 - val_loss: 0.4130\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3612 - val_loss: 0.4134\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3611 - val_loss: 0.4130\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_20-16:09:55] Training the entire fine-tuned model...\n",
      "[2022_04_20-16:10:05] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3850WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1080s vs `on_train_batch_end` time: 0.1359s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1080s vs `on_train_batch_end` time: 0.1359s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 697ms/step - loss: 0.3850 - val_loss: 0.4450\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3870 - val_loss: 0.4224\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3642 - val_loss: 0.4207\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3588 - val_loss: 0.4119\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 332ms/step - loss: 0.3552 - val_loss: 0.4125\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3519 - val_loss: 0.4118\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3404 - val_loss: 0.4082\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 332ms/step - loss: 0.3184 - val_loss: 0.4183\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3224 - val_loss: 0.4126\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3183 - val_loss: 0.4242\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_20-16:10:33] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-16:10:33] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:10:33] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3430WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1064s vs `on_train_batch_end` time: 0.1368s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1064s vs `on_train_batch_end` time: 0.1368s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 704ms/step - loss: 0.3430 - val_loss: 0.4088\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>466</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  466  19\n",
       "1   94  23"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2893081761006289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1r58n9lw) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 48364... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇██▁▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████████▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▃▂▂▂▂▂▂▁▂▁▁▂▁▁▃▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>6</td></tr><tr><td>best_val_loss</td><td>0.40821</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34302</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.40876</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">upbeat-fire-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/1r58n9lw\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042b/runs/1r58n9lw</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_160848-1r58n9lw/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1r58n9lw). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/p7f987b9\" target=\"_blank\">toasty-violet-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-16:11:31] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:11:31] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:11:31] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 731ms/step - loss: 0.7769 - val_loss: 1.0285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.7165 - val_loss: 0.8506\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.5950 - val_loss: 0.6636\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.5588 - val_loss: 0.6023\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4781 - val_loss: 0.5125\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4673 - val_loss: 0.4941\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4277 - val_loss: 0.4694\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.4158 - val_loss: 0.4643\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3961 - val_loss: 0.4658\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3945 - val_loss: 0.4575\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3951 - val_loss: 0.4625\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3888 - val_loss: 0.4566\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3918 - val_loss: 0.4634\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3727 - val_loss: 0.4569\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3872 - val_loss: 0.4527\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3623 - val_loss: 0.4736\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3688 - val_loss: 0.4505\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3619 - val_loss: 0.4497\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3506 - val_loss: 0.4564\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3671 - val_loss: 0.4486\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3480 - val_loss: 0.4472\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3571 - val_loss: 0.4615\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3517 - val_loss: 0.4780\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3601 - val_loss: 0.4604\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-16:12:09] Training the entire fine-tuned model...\n",
      "[2022_04_20-16:12:17] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 831ms/step - loss: 0.5066 - val_loss: 0.4492\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.4104 - val_loss: 0.4872\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3818 - val_loss: 0.4522\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3708 - val_loss: 0.4495\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_20-16:12:32] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-16:12:32] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:12:32] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 871ms/step - loss: 0.3622 - val_loss: 0.4453\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>561</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>147</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  561  2\n",
       "1  147  6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07453416149068323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:p7f987b9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 48709... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇██▁▁▂▂▁</td></tr><tr><td>loss</td><td>█▇▅▄▃▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▄▂▂▁▁</td></tr><tr><td>lr</td><td>████████████████████████▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.4453</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36215</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.4453</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">toasty-violet-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/p7f987b9\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042a/runs/p7f987b9</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_161112-p7f987b9/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:p7f987b9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/336xpz1s\" target=\"_blank\">generous-bush-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-16:13:29] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:13:29] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:13:29] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 579ms/step - loss: 0.8618 - val_loss: 0.6175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.6363 - val_loss: 0.4808\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 223ms/step - loss: 0.5887 - val_loss: 0.6220\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.5418 - val_loss: 0.6129\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5507 - val_loss: 0.4663\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4895 - val_loss: 0.4647\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4565 - val_loss: 0.4690\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4388 - val_loss: 0.4427\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4262 - val_loss: 0.4414\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4238 - val_loss: 0.4349\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4253 - val_loss: 0.4469\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4263 - val_loss: 0.4394\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4333 - val_loss: 0.4265\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4236 - val_loss: 0.4615\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4108 - val_loss: 0.4251\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4039 - val_loss: 0.4318\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3880 - val_loss: 0.4312\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3869 - val_loss: 0.4192\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3934 - val_loss: 0.4279\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3859 - val_loss: 0.4322\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4015 - val_loss: 0.4246\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-16:14:03] Training the entire fine-tuned model...\n",
      "[2022_04_20-16:14:26] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5278WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1061s vs `on_train_batch_end` time: 0.1366s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1061s vs `on_train_batch_end` time: 0.1366s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 687ms/step - loss: 0.5278 - val_loss: 0.5696\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.4486 - val_loss: 0.4924\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 331ms/step - loss: 0.4296 - val_loss: 0.4305\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3917 - val_loss: 0.4148\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 332ms/step - loss: 0.3846 - val_loss: 0.4255\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3837 - val_loss: 0.4711\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3837 - val_loss: 0.4762\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_20-16:14:48] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-16:14:48] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:14:48] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3677WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1049s vs `on_train_batch_end` time: 0.1362s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1049s vs `on_train_batch_end` time: 0.1362s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 700ms/step - loss: 0.3677 - val_loss: 0.4145\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>474</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  474  11\n",
       "1  105  12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17142857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:336xpz1s) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 48950... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▃▄▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▃▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████████▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃██▃▃▃▂▂▂▂▂▁▃▁▂▂▁▁▂▁▆▄▂▁▁▃▃▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.41454</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36774</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.41454</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">generous-bush-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/336xpz1s\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042b/runs/336xpz1s</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_161311-336xpz1s/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:336xpz1s). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/pte0w4jw\" target=\"_blank\">swift-waterfall-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-16:15:47] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:15:47] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:15:47] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 717ms/step - loss: 0.8620 - val_loss: 0.5538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 229ms/step - loss: 0.6152 - val_loss: 0.7212\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 229ms/step - loss: 0.5549 - val_loss: 0.5743\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 229ms/step - loss: 0.5093 - val_loss: 0.5242\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4638 - val_loss: 0.4767\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4713 - val_loss: 0.4695\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4313 - val_loss: 0.4733\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4255 - val_loss: 0.4676\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4105 - val_loss: 0.4913\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4129 - val_loss: 0.4734\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3988 - val_loss: 0.4756\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4051 - val_loss: 0.4508\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3948 - val_loss: 0.4512\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3817 - val_loss: 0.4600\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3874 - val_loss: 0.4498\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3853 - val_loss: 0.4499\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3862 - val_loss: 0.4504\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3785 - val_loss: 0.4499\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3736 - val_loss: 0.4494\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3803 - val_loss: 0.4493\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3834 - val_loss: 0.4494\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3773 - val_loss: 0.4493\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3859 - val_loss: 0.4495\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3726 - val_loss: 0.4494\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3680 - val_loss: 0.4495\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3724 - val_loss: 0.4495\n",
      "[2022_04_20-16:16:26] Training the entire fine-tuned model...\n",
      "[2022_04_20-16:16:34] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 834ms/step - loss: 0.3741 - val_loss: 0.4492\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3708 - val_loss: 0.4503\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3682 - val_loss: 0.4492\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3646 - val_loss: 0.4488\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3730 - val_loss: 0.4488\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3740 - val_loss: 0.4487\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3653 - val_loss: 0.4489\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3655 - val_loss: 0.4487\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3599 - val_loss: 0.4486\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3706 - val_loss: 0.4489\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3610 - val_loss: 0.4487\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3634 - val_loss: 0.4486\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3606 - val_loss: 0.4487\n",
      "[2022_04_20-16:17:08] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-16:17:08] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:17:22] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 854ms/step - loss: 0.3577 - val_loss: 0.4486\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>542</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>140</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  542  21\n",
       "1  140  13"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13903743315508021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:pte0w4jw) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 357... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▄▄▄▄▁</td></tr><tr><td>loss</td><td>█▅▄▃▂▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄█▄▃▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_val_loss</td><td>0.44858</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35774</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.44865</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">swift-waterfall-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/pte0w4jw\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042a/runs/pte0w4jw</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_161531-pte0w4jw/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:pte0w4jw). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/3to7pqwn\" target=\"_blank\">major-disco-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-16:18:16] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:18:16] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:18:16] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 861ms/step - loss: 0.8991 - val_loss: 0.8625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.6414 - val_loss: 0.4919\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5198 - val_loss: 0.4986\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5179 - val_loss: 0.5356\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4991 - val_loss: 0.4513\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4469 - val_loss: 0.4546\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4502 - val_loss: 0.4879\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4614 - val_loss: 0.4414\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4310 - val_loss: 0.4390\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4267 - val_loss: 0.4335\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4251 - val_loss: 0.4325\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4193 - val_loss: 0.4323\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3985 - val_loss: 0.4300\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4006 - val_loss: 0.4276\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4048 - val_loss: 0.4286\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3948 - val_loss: 0.4232\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3831 - val_loss: 0.4417\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4053 - val_loss: 0.4189\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3932 - val_loss: 0.4292\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3963 - val_loss: 0.4390\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4052 - val_loss: 0.4218\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3728 - val_loss: 0.4180\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3666 - val_loss: 0.4175\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3729 - val_loss: 0.4163\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3797 - val_loss: 0.4146\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3707 - val_loss: 0.4140\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3752 - val_loss: 0.4133\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3697 - val_loss: 0.4128\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3666 - val_loss: 0.4132\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3751 - val_loss: 0.4119\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3743 - val_loss: 0.4129\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3731 - val_loss: 0.4125\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3551 - val_loss: 0.4120\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3571 - val_loss: 0.4115\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3574 - val_loss: 0.4124\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3609 - val_loss: 0.4123\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3703 - val_loss: 0.4123\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3657 - val_loss: 0.4119\n",
      "[2022_04_20-16:19:10] Training the entire fine-tuned model...\n",
      "[2022_04_20-16:19:18] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3599WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1075s vs `on_train_batch_end` time: 0.1355s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1075s vs `on_train_batch_end` time: 0.1355s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 714ms/step - loss: 0.3599 - val_loss: 0.4122\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3584 - val_loss: 0.4116\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3591 - val_loss: 0.4124\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3533 - val_loss: 0.4117\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3650 - val_loss: 0.4113\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3648 - val_loss: 0.4116\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3504 - val_loss: 0.4128\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3620 - val_loss: 0.4121\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3464 - val_loss: 0.4114\n",
      "[2022_04_20-16:19:44] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-16:19:44] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:19:44] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3558WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1072s vs `on_train_batch_end` time: 0.1351s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1072s vs `on_train_batch_end` time: 0.1351s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 694ms/step - loss: 0.3558 - val_loss: 0.4113\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>470</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  470  15\n",
       "1   94  23"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2967741935483871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3to7pqwn) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 655... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇██▁▁▁▂▂▂▂▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>4</td></tr><tr><td>best_val_loss</td><td>0.41133</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3558</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41133</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">major-disco-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/3to7pqwn\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042b/runs/3to7pqwn</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_161759-3to7pqwn/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3to7pqwn). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/2la2emof\" target=\"_blank\">devout-durian-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-16:20:39] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:20:40] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:20:40] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 722ms/step - loss: 1.0365 - val_loss: 0.5546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.6565 - val_loss: 0.6425\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.5489 - val_loss: 0.5211\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4883 - val_loss: 0.5189\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4680 - val_loss: 0.4771\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4511 - val_loss: 0.4719\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4239 - val_loss: 0.4663\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4161 - val_loss: 0.4635\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4085 - val_loss: 0.4588\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4041 - val_loss: 0.4570\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3894 - val_loss: 0.4544\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3886 - val_loss: 0.4515\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3837 - val_loss: 0.4536\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3823 - val_loss: 0.4478\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3773 - val_loss: 0.4490\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3736 - val_loss: 0.4476\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3661 - val_loss: 0.4492\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3758 - val_loss: 0.4721\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3785 - val_loss: 0.5063\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3725 - val_loss: 0.4469\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3736 - val_loss: 0.4505\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3593 - val_loss: 0.4638\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3560 - val_loss: 0.4444\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 245ms/step - loss: 0.3563 - val_loss: 0.4450\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3480 - val_loss: 0.4456\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3581 - val_loss: 0.4432\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3548 - val_loss: 0.4431\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3524 - val_loss: 0.4432\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3488 - val_loss: 0.4458\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3443 - val_loss: 0.4434\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3490 - val_loss: 0.4434\n",
      "[2022_04_20-16:21:24] Training the entire fine-tuned model...\n",
      "[2022_04_20-16:21:32] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 848ms/step - loss: 0.3630 - val_loss: 0.4433\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3512 - val_loss: 0.4451\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3384 - val_loss: 0.4434\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3340 - val_loss: 0.4439\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3482 - val_loss: 0.4433\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3376 - val_loss: 0.4433\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3388 - val_loss: 0.4434\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3283 - val_loss: 0.4436\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3317 - val_loss: 0.4435\n",
      "[2022_04_20-16:21:56] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-16:21:56] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:21:56] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 853ms/step - loss: 0.3326 - val_loss: 0.4434\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>528</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  528  35\n",
       "1  126  27"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25116279069767444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2la2emof) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 1058... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▅█▄▄▂▂▂▂▂▁▁▁▁▁▁▁▁▂▃▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>26</td></tr><tr><td>best_val_loss</td><td>0.44311</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33259</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.44335</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">devout-durian-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/2la2emof\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042a/runs/2la2emof</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_162023-2la2emof/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2la2emof). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/iyaecl2n\" target=\"_blank\">eager-haze-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-16:22:54] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:22:54] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:22:54] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 600ms/step - loss: 0.9120 - val_loss: 0.7936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.6529 - val_loss: 0.4936\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5083 - val_loss: 0.5011\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5394 - val_loss: 0.5338\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5119 - val_loss: 0.4503\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4794 - val_loss: 0.4862\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4736 - val_loss: 0.4760\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4489 - val_loss: 0.4407\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4395 - val_loss: 0.4781\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4465 - val_loss: 0.4704\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4352 - val_loss: 0.4341\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4050 - val_loss: 0.4330\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4100 - val_loss: 0.4300\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4028 - val_loss: 0.4269\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4040 - val_loss: 0.4246\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3991 - val_loss: 0.4287\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3878 - val_loss: 0.4296\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3933 - val_loss: 0.4404\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3992 - val_loss: 0.4232\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3839 - val_loss: 0.4207\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3883 - val_loss: 0.4217\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3858 - val_loss: 0.4208\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3817 - val_loss: 0.4191\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3831 - val_loss: 0.4198\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3758 - val_loss: 0.4204\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3826 - val_loss: 0.4183\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3826 - val_loss: 0.4181\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3737 - val_loss: 0.4190\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3733 - val_loss: 0.4175\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3744 - val_loss: 0.4182\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3699 - val_loss: 0.4165\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3700 - val_loss: 0.4176\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3714 - val_loss: 0.4159\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3747 - val_loss: 0.4162\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3828 - val_loss: 0.4155\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3856 - val_loss: 0.4157\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3739 - val_loss: 0.4249\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3622 - val_loss: 0.4174\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3543 - val_loss: 0.4148\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3697 - val_loss: 0.4166\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3714 - val_loss: 0.4164\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3627 - val_loss: 0.4145\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3641 - val_loss: 0.4145\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3748 - val_loss: 0.4146\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3657 - val_loss: 0.4145\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3644 - val_loss: 0.4145\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3648 - val_loss: 0.4145\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3653 - val_loss: 0.4145\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3508 - val_loss: 0.4145\n",
      "[2022_04_20-16:23:59] Training the entire fine-tuned model...\n",
      "[2022_04_20-16:24:32] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3680WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1078s vs `on_train_batch_end` time: 0.1359s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1078s vs `on_train_batch_end` time: 0.1359s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 710ms/step - loss: 0.3680 - val_loss: 0.4167\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 332ms/step - loss: 0.3688 - val_loss: 0.4162\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3574 - val_loss: 0.4204\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3577 - val_loss: 0.4157\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 332ms/step - loss: 0.3524 - val_loss: 0.4267\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3600 - val_loss: 0.4161\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3478 - val_loss: 0.4282\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3501 - val_loss: 0.4210\n",
      "[2022_04_20-16:24:58] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-16:24:58] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:24:58] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3559WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1063s vs `on_train_batch_end` time: 0.1377s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1063s vs `on_train_batch_end` time: 0.1377s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 704ms/step - loss: 0.3559 - val_loss: 0.4152\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>459</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  459  26\n",
       "1   93  24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2874251497005988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:iyaecl2n) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 1405... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███▁▁▁▂▂▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▂▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▃▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>44</td></tr><tr><td>best_val_loss</td><td>0.41446</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35594</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41521</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">eager-haze-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/iyaecl2n\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042b/runs/iyaecl2n</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_162236-iyaecl2n/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:iyaecl2n). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/1hrlnqa4\" target=\"_blank\">worthy-snowflake-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-16:25:56] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:25:56] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:25:56] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 743ms/step - loss: 0.9353 - val_loss: 0.5408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.6229 - val_loss: 0.7246\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.5620 - val_loss: 0.5948\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.5177 - val_loss: 0.5591\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4756 - val_loss: 0.4792\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4476 - val_loss: 0.4874\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4349 - val_loss: 0.4863\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4395 - val_loss: 0.4826\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4326 - val_loss: 0.4760\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4294 - val_loss: 0.4736\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4198 - val_loss: 0.4732\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4274 - val_loss: 0.4731\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4261 - val_loss: 0.4742\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4267 - val_loss: 0.4743\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4213 - val_loss: 0.4730\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4259 - val_loss: 0.4717\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4231 - val_loss: 0.4715\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4157 - val_loss: 0.4710\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4226 - val_loss: 0.4708\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4218 - val_loss: 0.4711\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4196 - val_loss: 0.4729\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4164 - val_loss: 0.4706\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4179 - val_loss: 0.4691\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4268 - val_loss: 0.4688\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4237 - val_loss: 0.4687\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4166 - val_loss: 0.4687\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4134 - val_loss: 0.4678\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4149 - val_loss: 0.4679\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4173 - val_loss: 0.4669\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4161 - val_loss: 0.4669\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4138 - val_loss: 0.4670\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4092 - val_loss: 0.4660\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4099 - val_loss: 0.4658\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4116 - val_loss: 0.4653\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4083 - val_loss: 0.4663\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4036 - val_loss: 0.4662\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4109 - val_loss: 0.4646\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4134 - val_loss: 0.4641\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4055 - val_loss: 0.4645\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4107 - val_loss: 0.4648\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4063 - val_loss: 0.4631\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4065 - val_loss: 0.4629\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4098 - val_loss: 0.4623\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4010 - val_loss: 0.4632\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3995 - val_loss: 0.4627\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4087 - val_loss: 0.4619\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4077 - val_loss: 0.4616\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4017 - val_loss: 0.4611\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4017 - val_loss: 0.4604\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4033 - val_loss: 0.4612\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3981 - val_loss: 0.4600\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4084 - val_loss: 0.4610\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3897 - val_loss: 0.4594\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3979 - val_loss: 0.4590\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3943 - val_loss: 0.4601\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3973 - val_loss: 0.4594\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3909 - val_loss: 0.4582\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3968 - val_loss: 0.4582\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3936 - val_loss: 0.4598\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3939 - val_loss: 0.4587\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3946 - val_loss: 0.4582\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3919 - val_loss: 0.4577\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3967 - val_loss: 0.4575\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3974 - val_loss: 0.4575\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3939 - val_loss: 0.4576\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3932 - val_loss: 0.4577\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3945 - val_loss: 0.4577\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3909 - val_loss: 0.4577\n",
      "[2022_04_20-16:27:22] Training the entire fine-tuned model...\n",
      "[2022_04_20-16:27:31] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 833ms/step - loss: 0.3938 - val_loss: 0.4710\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3984 - val_loss: 0.4687\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3973 - val_loss: 0.4569\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3854 - val_loss: 0.4566\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3760 - val_loss: 0.4616\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3667 - val_loss: 0.4554\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3596 - val_loss: 0.4571\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3613 - val_loss: 0.4604\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3554 - val_loss: 0.4591\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3320 - val_loss: 0.4612\n",
      "[2022_04_20-16:27:58] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-16:27:58] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:27:58] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 1s/step - loss: 0.3604 - val_loss: 0.4541\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>538</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  538  25\n",
       "1  138  15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15544041450777202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1hrlnqa4) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 1874... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███▁▁▁▂▂▁</td></tr><tr><td>loss</td><td>█▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>lr</td><td>██▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▅█▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.45413</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36038</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.45413</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">worthy-snowflake-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/1hrlnqa4\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042a/runs/1hrlnqa4</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_162535-1hrlnqa4/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1hrlnqa4). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/2bykdfw1\" target=\"_blank\">fanciful-hill-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-16:29:12] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:29:12] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:29:12] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 574ms/step - loss: 0.9227 - val_loss: 0.8817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.6830 - val_loss: 0.5125\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5579 - val_loss: 0.5030\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4950 - val_loss: 0.5221\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4838 - val_loss: 0.4631\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4547 - val_loss: 0.4472\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4486 - val_loss: 0.4533\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4336 - val_loss: 0.4408\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4336 - val_loss: 0.4426\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4240 - val_loss: 0.4352\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4182 - val_loss: 0.4391\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4173 - val_loss: 0.4449\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4419 - val_loss: 0.4383\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4092 - val_loss: 0.4392\n",
      "[2022_04_20-16:29:40] Training the entire fine-tuned model...\n",
      "[2022_04_20-16:29:48] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4152WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1041s vs `on_train_batch_end` time: 0.1391s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1041s vs `on_train_batch_end` time: 0.1391s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 685ms/step - loss: 0.4152 - val_loss: 0.4380\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.4181 - val_loss: 0.4425\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.4164 - val_loss: 0.4397\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 331ms/step - loss: 0.4004 - val_loss: 0.4381\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.4014 - val_loss: 0.4284\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 332ms/step - loss: 0.3899 - val_loss: 0.4291\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3935 - val_loss: 0.4275\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.4027 - val_loss: 0.4271\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3795 - val_loss: 0.4267\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3907 - val_loss: 0.4269\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3889 - val_loss: 0.4263\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3814 - val_loss: 0.4276\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3863 - val_loss: 0.4253\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3840 - val_loss: 0.4258\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3798 - val_loss: 0.4243\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3758 - val_loss: 0.4243\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 332ms/step - loss: 0.3748 - val_loss: 0.4246\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3669 - val_loss: 0.4240\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3722 - val_loss: 0.4243\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3626 - val_loss: 0.4247\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3783 - val_loss: 0.4237\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 3s 497ms/step - loss: 0.3630 - val_loss: 0.4230\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3598 - val_loss: 0.4260\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3524 - val_loss: 0.4227\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3431 - val_loss: 0.4260\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3474 - val_loss: 0.4239\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3453 - val_loss: 0.4268\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3421 - val_loss: 0.4260\n",
      "[2022_04_20-16:30:52] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-16:30:52] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:30:52] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3499WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1057s vs `on_train_batch_end` time: 0.1380s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1057s vs `on_train_batch_end` time: 0.1380s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 690ms/step - loss: 0.3499 - val_loss: 0.4227\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>466</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  466  19\n",
       "1   93  24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2bykdfw1) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2394... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇▇██▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>23</td></tr><tr><td>best_val_loss</td><td>0.42266</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34994</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.42274</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fanciful-hill-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/2bykdfw1\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042b/runs/2bykdfw1</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_162855-2bykdfw1/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2bykdfw1). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/h4hp59j8\" target=\"_blank\">bumbling-pond-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-16:31:45] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:31:46] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:31:46] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 717ms/step - loss: 0.9711 - val_loss: 0.5365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.6050 - val_loss: 0.6357\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.5314 - val_loss: 0.5044\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4627 - val_loss: 0.5044\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4645 - val_loss: 0.4868\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4580 - val_loss: 0.4794\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4363 - val_loss: 0.4953\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4110 - val_loss: 0.5051\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4298 - val_loss: 0.5068\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4156 - val_loss: 0.4594\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4076 - val_loss: 0.4641\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.4023 - val_loss: 0.4677\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3982 - val_loss: 0.4601\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3919 - val_loss: 0.4573\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3891 - val_loss: 0.4574\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3910 - val_loss: 0.4569\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3888 - val_loss: 0.4572\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3901 - val_loss: 0.4589\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3923 - val_loss: 0.4582\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3892 - val_loss: 0.4576\n",
      "[2022_04_20-16:32:18] Training the entire fine-tuned model...\n",
      "[2022_04_20-16:32:44] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 839ms/step - loss: 0.5794 - val_loss: 0.4583\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.4379 - val_loss: 0.4843\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.4124 - val_loss: 0.4682\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3941 - val_loss: 0.4873\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3968 - val_loss: 0.4638\n",
      "[2022_04_20-16:33:01] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-16:33:01] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:33:10] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 865ms/step - loss: 0.3999 - val_loss: 0.4577\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>562</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  562  1\n",
       "1  150  3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03821656050955414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:h4hp59j8) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2719... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▃▂▁▁▁▁</td></tr><tr><td>lr</td><td>█████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄█▃▃▂▂▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>15</td></tr><tr><td>best_val_loss</td><td>0.45695</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.39989</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.45774</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">bumbling-pond-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/h4hp59j8\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042a/runs/h4hp59j8</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_163129-h4hp59j8/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:h4hp59j8). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/3oq8aoxw\" target=\"_blank\">eternal-blaze-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-16:34:05] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:34:05] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:34:05] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 824ms/step - loss: 0.8776 - val_loss: 0.7572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.6274 - val_loss: 0.4756\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5159 - val_loss: 0.5291\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5321 - val_loss: 0.5169\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4859 - val_loss: 0.4527\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4514 - val_loss: 0.4515\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4377 - val_loss: 0.4580\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4377 - val_loss: 0.4395\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4300 - val_loss: 0.4594\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4368 - val_loss: 0.4343\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4187 - val_loss: 0.4305\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4021 - val_loss: 0.4305\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4108 - val_loss: 0.4268\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3948 - val_loss: 0.4258\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3933 - val_loss: 0.4308\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4089 - val_loss: 0.4227\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3897 - val_loss: 0.4192\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3939 - val_loss: 0.4463\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4306 - val_loss: 0.4498\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4031 - val_loss: 0.4811\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4463 - val_loss: 0.4175\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4057 - val_loss: 0.4249\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3782 - val_loss: 0.4407\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3636 - val_loss: 0.4224\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3926 - val_loss: 0.4204\n",
      "[2022_04_20-16:34:44] Training the entire fine-tuned model...\n",
      "[2022_04_20-16:35:02] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.6204WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1053s vs `on_train_batch_end` time: 0.1375s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1053s vs `on_train_batch_end` time: 0.1375s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 694ms/step - loss: 0.6204 - val_loss: 0.4986\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 332ms/step - loss: 0.4751 - val_loss: 0.4244\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.4262 - val_loss: 0.4156\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 332ms/step - loss: 0.4031 - val_loss: 0.4193\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3961 - val_loss: 0.4135\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 332ms/step - loss: 0.3782 - val_loss: 0.4143\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3790 - val_loss: 0.4146\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3545 - val_loss: 0.4216\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3305 - val_loss: 0.4164\n",
      "[2022_04_20-16:35:28] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-16:35:28] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:35:28] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3857WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1044s vs `on_train_batch_end` time: 0.1376s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1044s vs `on_train_batch_end` time: 0.1376s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 699ms/step - loss: 0.3857 - val_loss: 0.4130\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>477</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  477   8\n",
       "1  105  12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17518248175182483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3oq8aoxw) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2944... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▃▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▅▃▂▂▂▂▂▁▁▂</td></tr><tr><td>lr</td><td>████████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▂▂▂▁▁▂▁▁▃▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.41299</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.38572</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.41299</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">eternal-blaze-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/3oq8aoxw\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042b/runs/3oq8aoxw</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_163348-3oq8aoxw/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3oq8aoxw). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/gfekx1ye\" target=\"_blank\">zany-capybara-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-16:36:24] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:36:24] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:36:24] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 722ms/step - loss: 0.8860 - val_loss: 0.5593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.6420 - val_loss: 0.8306\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.6397 - val_loss: 0.6041\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.5434 - val_loss: 0.6002\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 227ms/step - loss: 0.5187 - val_loss: 0.4945\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4430 - val_loss: 0.5003\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4539 - val_loss: 0.4751\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4451 - val_loss: 0.4970\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4383 - val_loss: 0.4726\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4329 - val_loss: 0.4758\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4329 - val_loss: 0.4762\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4237 - val_loss: 0.4695\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4207 - val_loss: 0.4677\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4174 - val_loss: 0.4679\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4127 - val_loss: 0.4658\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4116 - val_loss: 0.4644\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4151 - val_loss: 0.4633\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4060 - val_loss: 0.4639\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4025 - val_loss: 0.4617\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4134 - val_loss: 0.4602\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4004 - val_loss: 0.4655\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4072 - val_loss: 0.4591\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3998 - val_loss: 0.4605\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3952 - val_loss: 0.4579\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3917 - val_loss: 0.4580\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3954 - val_loss: 0.4565\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3862 - val_loss: 0.4573\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3847 - val_loss: 0.4550\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3911 - val_loss: 0.4540\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3928 - val_loss: 0.4576\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3919 - val_loss: 0.4532\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 248ms/step - loss: 0.3950 - val_loss: 0.4525\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3836 - val_loss: 0.4521\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3781 - val_loss: 0.4514\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3792 - val_loss: 0.4511\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3660 - val_loss: 0.4515\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3857 - val_loss: 0.4509\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3708 - val_loss: 0.4505\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3758 - val_loss: 0.4526\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3728 - val_loss: 0.4546\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3693 - val_loss: 0.4491\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3723 - val_loss: 0.4485\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3732 - val_loss: 0.4502\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3659 - val_loss: 0.4496\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3655 - val_loss: 0.4480\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3702 - val_loss: 0.4502\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3647 - val_loss: 0.4523\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3650 - val_loss: 0.4543\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3687 - val_loss: 0.4498\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3515 - val_loss: 0.4469\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3562 - val_loss: 0.4475\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3621 - val_loss: 0.4468\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3654 - val_loss: 0.4474\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3609 - val_loss: 0.4472\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3567 - val_loss: 0.4467\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3589 - val_loss: 0.4467\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3539 - val_loss: 0.4467\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3601 - val_loss: 0.4466\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3557 - val_loss: 0.4467\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3601 - val_loss: 0.4466\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3519 - val_loss: 0.4466\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 1s 225ms/step - loss: 0.3486 - val_loss: 0.4466\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 1s 228ms/step - loss: 0.3606 - val_loss: 0.4466\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 1s 229ms/step - loss: 0.3586 - val_loss: 0.4466\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 1s 226ms/step - loss: 0.3529 - val_loss: 0.4466\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 1s 226ms/step - loss: 0.3554 - val_loss: 0.4466\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 1s 225ms/step - loss: 0.3564 - val_loss: 0.4466\n",
      "[2022_04_20-16:37:48] Training the entire fine-tuned model...\n",
      "[2022_04_20-16:37:55] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 828ms/step - loss: 0.3626 - val_loss: 0.4465\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3579 - val_loss: 0.4466\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3566 - val_loss: 0.4464\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3557 - val_loss: 0.4466\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3541 - val_loss: 0.4466\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3533 - val_loss: 0.4465\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3466 - val_loss: 0.4464\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3539 - val_loss: 0.4464\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3530 - val_loss: 0.4464\n",
      "[2022_04_20-16:38:19] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-16:38:19] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:38:19] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 853ms/step - loss: 0.3545 - val_loss: 0.4464\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>543</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>139</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  543  20\n",
       "1  139  14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1497326203208556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:gfekx1ye) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 3230... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇███▁▁▂▂▁</td></tr><tr><td>loss</td><td>█▅▄▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃█▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>2</td></tr><tr><td>best_val_loss</td><td>0.44639</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35447</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.44639</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">zany-capybara-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/gfekx1ye\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042a/runs/gfekx1ye</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_163607-gfekx1ye/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:gfekx1ye). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/wkrv5zd5\" target=\"_blank\">lucky-dew-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-16:39:18] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:39:18] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:39:18] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 575ms/step - loss: 0.8687 - val_loss: 0.9615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.6976 - val_loss: 0.5419\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5374 - val_loss: 0.4856\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5256 - val_loss: 0.5073\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4944 - val_loss: 0.4506\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4516 - val_loss: 0.4560\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4551 - val_loss: 0.4636\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4369 - val_loss: 0.4404\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4331 - val_loss: 0.4439\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4263 - val_loss: 0.4346\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4193 - val_loss: 0.4320\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4140 - val_loss: 0.4294\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4000 - val_loss: 0.4280\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4059 - val_loss: 0.4312\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4071 - val_loss: 0.4242\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4108 - val_loss: 0.4487\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3987 - val_loss: 0.4199\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4137 - val_loss: 0.4356\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3815 - val_loss: 0.4201\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3989 - val_loss: 0.4168\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3795 - val_loss: 0.4272\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3774 - val_loss: 0.4193\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3894 - val_loss: 0.4208\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3836 - val_loss: 0.4174\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3731 - val_loss: 0.4197\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3707 - val_loss: 0.4155\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3759 - val_loss: 0.4190\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3680 - val_loss: 0.4142\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3703 - val_loss: 0.4143\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3614 - val_loss: 0.4139\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3716 - val_loss: 0.4136\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3858 - val_loss: 0.4174\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3517 - val_loss: 0.4174\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3655 - val_loss: 0.4187\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3679 - val_loss: 0.4137\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3699 - val_loss: 0.4128\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3688 - val_loss: 0.4129\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3575 - val_loss: 0.4135\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3494 - val_loss: 0.4136\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3575 - val_loss: 0.4134\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3723 - val_loss: 0.4133\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3665 - val_loss: 0.4131\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_20-16:40:14] Training the entire fine-tuned model...\n",
      "[2022_04_20-16:40:22] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3593WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1048s vs `on_train_batch_end` time: 0.1362s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1048s vs `on_train_batch_end` time: 0.1362s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 686ms/step - loss: 0.3593 - val_loss: 0.4136\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3631 - val_loss: 0.4127\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3506 - val_loss: 0.4127\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3561 - val_loss: 0.4129\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3565 - val_loss: 0.4134\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3557 - val_loss: 0.4131\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3618 - val_loss: 0.4131\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3625 - val_loss: 0.4129\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3531 - val_loss: 0.4129\n",
      "[2022_04_20-16:40:48] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-16:40:48] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:40:48] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3543WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1047s vs `on_train_batch_end` time: 0.1374s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1047s vs `on_train_batch_end` time: 0.1374s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 698ms/step - loss: 0.3543 - val_loss: 0.4128\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>473</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  473  12\n",
       "1   98  19"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25675675675675674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:wkrv5zd5) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 3711... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▁</td></tr><tr><td>loss</td><td>█▆▄▃▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>2</td></tr><tr><td>best_val_loss</td><td>0.41273</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35426</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41276</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">lucky-dew-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/wkrv5zd5\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042b/runs/wkrv5zd5</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_163900-wkrv5zd5/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:wkrv5zd5). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/30wamhg4\" target=\"_blank\">avid-thunder-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-16:41:44] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:41:44] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:41:44] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 722ms/step - loss: 0.9485 - val_loss: 0.5278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.5812 - val_loss: 0.6504\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.5247 - val_loss: 0.5805\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.5023 - val_loss: 0.5271\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4592 - val_loss: 0.4828\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4314 - val_loss: 0.4930\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4308 - val_loss: 0.4644\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4073 - val_loss: 0.4593\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4056 - val_loss: 0.4577\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3956 - val_loss: 0.4550\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3918 - val_loss: 0.4546\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3802 - val_loss: 0.4511\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3880 - val_loss: 0.4550\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3906 - val_loss: 0.4503\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3887 - val_loss: 0.4670\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3783 - val_loss: 0.4502\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3857 - val_loss: 0.4813\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3853 - val_loss: 0.4511\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3561 - val_loss: 0.4577\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3571 - val_loss: 0.4492\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3537 - val_loss: 0.4473\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3520 - val_loss: 0.4473\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3493 - val_loss: 0.4472\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3535 - val_loss: 0.4468\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3452 - val_loss: 0.4487\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3501 - val_loss: 0.4509\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3556 - val_loss: 0.4473\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3510 - val_loss: 0.4476\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3574 - val_loss: 0.4464\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3449 - val_loss: 0.4462\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3487 - val_loss: 0.4463\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3382 - val_loss: 0.4463\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3420 - val_loss: 0.4463\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3534 - val_loss: 0.4461\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3464 - val_loss: 0.4461\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3386 - val_loss: 0.4460\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3459 - val_loss: 0.4461\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3498 - val_loss: 0.4460\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3409 - val_loss: 0.4460\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3421 - val_loss: 0.4460\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3432 - val_loss: 0.4460\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3337 - val_loss: 0.4460\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3417 - val_loss: 0.4460\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3446 - val_loss: 0.4460\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3406 - val_loss: 0.4460\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3367 - val_loss: 0.4460\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3498 - val_loss: 0.4460\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3419 - val_loss: 0.4460\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3510 - val_loss: 0.4460\n",
      "[2022_04_20-16:42:48] Training the entire fine-tuned model...\n",
      "[2022_04_20-16:42:56] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 854ms/step - loss: 0.3571 - val_loss: 0.4546\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3553 - val_loss: 0.4554\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3452 - val_loss: 0.4493\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3380 - val_loss: 0.4456\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3405 - val_loss: 0.4465\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3232 - val_loss: 0.4477\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3219 - val_loss: 0.4493\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3215 - val_loss: 0.4496\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3189 - val_loss: 0.4501\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3235 - val_loss: 0.4506\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_20-16:43:24] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-16:43:24] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:43:24] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 862ms/step - loss: 0.3170 - val_loss: 0.4457\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>531</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>129</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  531  32\n",
       "1  129  24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22966507177033493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:30wamhg4) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 4089... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄█▄▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>3</td></tr><tr><td>best_val_loss</td><td>0.44556</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31701</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.44572</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">avid-thunder-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/30wamhg4\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042a/runs/30wamhg4</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_164127-30wamhg4/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:30wamhg4). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/2i3hgtxa\" target=\"_blank\">ethereal-aardvark-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-16:44:38] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:44:38] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:44:38] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 842ms/step - loss: 0.8980 - val_loss: 0.7712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.6521 - val_loss: 0.4890\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5381 - val_loss: 0.5445\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5812 - val_loss: 0.6320\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5430 - val_loss: 0.5118\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4926 - val_loss: 0.4594\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4716 - val_loss: 0.4590\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4510 - val_loss: 0.4623\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4498 - val_loss: 0.4523\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4414 - val_loss: 0.4493\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4441 - val_loss: 0.4471\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4420 - val_loss: 0.4486\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4379 - val_loss: 0.4448\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4328 - val_loss: 0.4439\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4312 - val_loss: 0.4451\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 199ms/step - loss: 0.4345 - val_loss: 0.4421\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4282 - val_loss: 0.4410\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4246 - val_loss: 0.4398\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4241 - val_loss: 0.4400\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4203 - val_loss: 0.4383\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4153 - val_loss: 0.4378\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4201 - val_loss: 0.4400\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4237 - val_loss: 0.4358\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4164 - val_loss: 0.4348\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4140 - val_loss: 0.4345\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4145 - val_loss: 0.4339\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4118 - val_loss: 0.4345\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4080 - val_loss: 0.4321\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4076 - val_loss: 0.4323\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4006 - val_loss: 0.4298\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4088 - val_loss: 0.4291\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4058 - val_loss: 0.4307\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4024 - val_loss: 0.4276\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3970 - val_loss: 0.4277\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4039 - val_loss: 0.4264\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3974 - val_loss: 0.4259\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3972 - val_loss: 0.4252\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3942 - val_loss: 0.4257\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3959 - val_loss: 0.4254\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3901 - val_loss: 0.4248\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3920 - val_loss: 0.4240\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3981 - val_loss: 0.4237\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3861 - val_loss: 0.4230\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3952 - val_loss: 0.4228\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3878 - val_loss: 0.4220\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3904 - val_loss: 0.4219\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3953 - val_loss: 0.4221\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3887 - val_loss: 0.4212\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3863 - val_loss: 0.4211\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3781 - val_loss: 0.4196\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3877 - val_loss: 0.4193\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3811 - val_loss: 0.4193\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3730 - val_loss: 0.4188\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3745 - val_loss: 0.4188\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3734 - val_loss: 0.4185\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3801 - val_loss: 0.4181\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3742 - val_loss: 0.4177\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3683 - val_loss: 0.4173\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3747 - val_loss: 0.4171\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3694 - val_loss: 0.4197\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3886 - val_loss: 0.4163\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3744 - val_loss: 0.4164\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3791 - val_loss: 0.4162\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3761 - val_loss: 0.4166\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3819 - val_loss: 0.4183\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3697 - val_loss: 0.4158\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3646 - val_loss: 0.4161\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3657 - val_loss: 0.4156\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3662 - val_loss: 0.4162\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3641 - val_loss: 0.4154\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3651 - val_loss: 0.4152\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3709 - val_loss: 0.4152\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3696 - val_loss: 0.4158\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3674 - val_loss: 0.4151\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3650 - val_loss: 0.4154\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3606 - val_loss: 0.4149\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3650 - val_loss: 0.4160\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3667 - val_loss: 0.4149\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3614 - val_loss: 0.4153\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3648 - val_loss: 0.4151\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3650 - val_loss: 0.4148\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3689 - val_loss: 0.4146\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3652 - val_loss: 0.4146\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3645 - val_loss: 0.4146\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3694 - val_loss: 0.4146\n",
      "\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3627 - val_loss: 0.4146\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3595 - val_loss: 0.4146\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3632 - val_loss: 0.4146\n",
      "\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3546 - val_loss: 0.4146\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3635 - val_loss: 0.4146\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3646 - val_loss: 0.4146\n",
      "\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3531 - val_loss: 0.4146\n",
      "[2022_04_20-16:46:33] Training the entire fine-tuned model...\n",
      "[2022_04_20-16:47:16] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3650WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1060s vs `on_train_batch_end` time: 0.1350s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1060s vs `on_train_batch_end` time: 0.1350s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 710ms/step - loss: 0.3650 - val_loss: 0.4168\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 330ms/step - loss: 0.3661 - val_loss: 0.4188\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 332ms/step - loss: 0.3552 - val_loss: 0.4115\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 329ms/step - loss: 0.3584 - val_loss: 0.4139\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 331ms/step - loss: 0.3542 - val_loss: 0.4125\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 331ms/step - loss: 0.3487 - val_loss: 0.4142\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 330ms/step - loss: 0.3452 - val_loss: 0.4124\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 331ms/step - loss: 0.3412 - val_loss: 0.4129\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 332ms/step - loss: 0.3497 - val_loss: 0.4128\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_20-16:47:42] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-16:47:42] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:47:43] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3659WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1064s vs `on_train_batch_end` time: 0.1368s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1064s vs `on_train_batch_end` time: 0.1368s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 693ms/step - loss: 0.3659 - val_loss: 0.4117\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>473</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  473  12\n",
       "1   95  22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2913907284768212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2i3hgtxa) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 4486... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███▁▁▁▁</td></tr><tr><td>loss</td><td>█▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>2</td></tr><tr><td>best_val_loss</td><td>0.41154</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36586</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41167</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">ethereal-aardvark-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/2i3hgtxa\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042b/runs/2i3hgtxa</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_164423-2i3hgtxa/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2i3hgtxa). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/2hognaf5\" target=\"_blank\">olive-dust-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-16:48:38] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:48:38] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:48:38] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 717ms/step - loss: 0.9116 - val_loss: 0.5108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.6126 - val_loss: 0.6953\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.5567 - val_loss: 0.5643\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.4971 - val_loss: 0.5396\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4555 - val_loss: 0.4787\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4453 - val_loss: 0.4871\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4436 - val_loss: 0.4811\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4334 - val_loss: 0.4844\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4317 - val_loss: 0.4773\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4290 - val_loss: 0.4733\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4267 - val_loss: 0.4728\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4280 - val_loss: 0.4724\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4314 - val_loss: 0.4730\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4316 - val_loss: 0.4731\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4270 - val_loss: 0.4724\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.4205 - val_loss: 0.4720\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4252 - val_loss: 0.4716\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4284 - val_loss: 0.4713\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4233 - val_loss: 0.4713\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4284 - val_loss: 0.4714\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4266 - val_loss: 0.4712\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4289 - val_loss: 0.4714\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4233 - val_loss: 0.4710\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4235 - val_loss: 0.4706\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4260 - val_loss: 0.4704\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4198 - val_loss: 0.4703\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4246 - val_loss: 0.4703\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4263 - val_loss: 0.4706\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4197 - val_loss: 0.4707\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4197 - val_loss: 0.4706\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4203 - val_loss: 0.4706\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4241 - val_loss: 0.4706\n",
      "[2022_04_20-16:49:23] Training the entire fine-tuned model...\n",
      "[2022_04_20-16:49:31] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 861ms/step - loss: 0.4207 - val_loss: 0.4701\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.4249 - val_loss: 0.4668\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.4134 - val_loss: 0.4643\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.4040 - val_loss: 0.4643\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.4051 - val_loss: 0.4616\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3894 - val_loss: 0.4641\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3739 - val_loss: 0.4646\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3686 - val_loss: 0.4672\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3542 - val_loss: 0.4679\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.3488 - val_loss: 0.4701\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3545 - val_loss: 0.4725\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_20-16:50:00] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-16:50:00] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:50:00] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 858ms/step - loss: 0.3877 - val_loss: 0.4619\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>559</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>147</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  559  4\n",
       "1  147  6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0736196319018405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2hognaf5) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 5109... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▄▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▂█▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>4</td></tr><tr><td>best_val_loss</td><td>0.46157</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.38774</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.46189</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">olive-dust-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/2hognaf5\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042a/runs/2hognaf5</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_164821-2hognaf5/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2hognaf5). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/2lq81kfj\" target=\"_blank\">lunar-field-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-16:51:14] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:51:14] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:51:14] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 584ms/step - loss: 0.8769 - val_loss: 0.8062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.7492 - val_loss: 0.5432\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5625 - val_loss: 0.4933\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5369 - val_loss: 0.5140\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4918 - val_loss: 0.4900\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4608 - val_loss: 0.5079\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4737 - val_loss: 0.4424\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4489 - val_loss: 0.4692\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4436 - val_loss: 0.5291\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4569 - val_loss: 0.4354\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4278 - val_loss: 0.4858\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4561 - val_loss: 0.4831\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4383 - val_loss: 0.4278\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4195 - val_loss: 0.4271\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4132 - val_loss: 0.4476\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4126 - val_loss: 0.4250\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4008 - val_loss: 0.4419\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4005 - val_loss: 0.4309\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3810 - val_loss: 0.4190\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4002 - val_loss: 0.4330\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4000 - val_loss: 0.4404\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4108 - val_loss: 0.4542\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3872 - val_loss: 0.4219\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3888 - val_loss: 0.4150\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3977 - val_loss: 0.4219\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3875 - val_loss: 0.4240\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3603 - val_loss: 0.4390\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3912 - val_loss: 0.4259\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3737 - val_loss: 0.4141\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3732 - val_loss: 0.4199\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3745 - val_loss: 0.4143\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3658 - val_loss: 0.4162\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3688 - val_loss: 0.4165\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3737 - val_loss: 0.4161\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3797 - val_loss: 0.4148\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_20-16:52:03] Training the entire fine-tuned model...\n",
      "[2022_04_20-16:52:11] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3819WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1055s vs `on_train_batch_end` time: 0.1384s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1055s vs `on_train_batch_end` time: 0.1384s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 695ms/step - loss: 0.3819 - val_loss: 0.4140\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 332ms/step - loss: 0.3721 - val_loss: 0.4210\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3732 - val_loss: 0.4169\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3630 - val_loss: 0.4168\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3682 - val_loss: 0.4156\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3447 - val_loss: 0.4140\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3543 - val_loss: 0.4135\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3541 - val_loss: 0.4139\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3384 - val_loss: 0.4146\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3386 - val_loss: 0.4135\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3443 - val_loss: 0.4136\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3350 - val_loss: 0.4142\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3430 - val_loss: 0.4148\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3382 - val_loss: 0.4144\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3469 - val_loss: 0.4141\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3397 - val_loss: 0.4138\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "[2022_04_20-16:52:51] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-16:52:51] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:52:51] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3483WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1023s vs `on_train_batch_end` time: 0.1379s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1023s vs `on_train_batch_end` time: 0.1379s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 679ms/step - loss: 0.3483 - val_loss: 0.4137\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>462</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  462  23\n",
       "1   92  25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30303030303030304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2lq81kfj) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 5435... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▁▂▂▂▃▃▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▆▄▄▃▃▂▃▂▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▃▃▂▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>9</td></tr><tr><td>best_val_loss</td><td>0.41349</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34833</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.41365</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">lunar-field-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/2lq81kfj\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042b/runs/2lq81kfj</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_165057-2lq81kfj/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2lq81kfj). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/5tjf5f0t\" target=\"_blank\">autumn-glitter-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-16:53:53] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:53:54] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:53:54] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 741ms/step - loss: 0.9781 - val_loss: 0.5189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.5937 - val_loss: 0.6646\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.5577 - val_loss: 0.5740\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.5061 - val_loss: 0.5621\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4741 - val_loss: 0.4838\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4539 - val_loss: 0.5013\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4562 - val_loss: 0.4909\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4431 - val_loss: 0.4808\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4276 - val_loss: 0.4774\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4322 - val_loss: 0.4748\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4274 - val_loss: 0.4793\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4262 - val_loss: 0.4729\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4192 - val_loss: 0.4713\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4208 - val_loss: 0.4694\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.4234 - val_loss: 0.4697\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4139 - val_loss: 0.4751\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4113 - val_loss: 0.4641\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4118 - val_loss: 0.4631\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4002 - val_loss: 0.4644\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3981 - val_loss: 0.4613\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4017 - val_loss: 0.4611\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3991 - val_loss: 0.4602\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3974 - val_loss: 0.4589\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3945 - val_loss: 0.4583\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3905 - val_loss: 0.4573\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3870 - val_loss: 0.4569\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3886 - val_loss: 0.4571\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3900 - val_loss: 0.4559\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3873 - val_loss: 0.4554\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3845 - val_loss: 0.4575\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3777 - val_loss: 0.4548\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3810 - val_loss: 0.4531\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3835 - val_loss: 0.4527\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3843 - val_loss: 0.4528\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3737 - val_loss: 0.4563\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3728 - val_loss: 0.4517\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3755 - val_loss: 0.4510\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3712 - val_loss: 0.4513\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3713 - val_loss: 0.4507\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3691 - val_loss: 0.4499\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3696 - val_loss: 0.4491\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3701 - val_loss: 0.4486\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3665 - val_loss: 0.4485\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3684 - val_loss: 0.4480\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3667 - val_loss: 0.4476\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3607 - val_loss: 0.4495\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3638 - val_loss: 0.4473\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3571 - val_loss: 0.4476\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3615 - val_loss: 0.4486\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3613 - val_loss: 0.4468\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3500 - val_loss: 0.4476\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3583 - val_loss: 0.4474\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3689 - val_loss: 0.4500\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3529 - val_loss: 0.4457\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3471 - val_loss: 0.4466\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3466 - val_loss: 0.4458\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3505 - val_loss: 0.4465\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3616 - val_loss: 0.4464\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3602 - val_loss: 0.4460\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3537 - val_loss: 0.4453\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3553 - val_loss: 0.4452\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3530 - val_loss: 0.4452\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3477 - val_loss: 0.4453\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3511 - val_loss: 0.4453\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3531 - val_loss: 0.4453\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3516 - val_loss: 0.4453\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3504 - val_loss: 0.4453\n",
      "[2022_04_20-16:55:17] Training the entire fine-tuned model...\n",
      "[2022_04_20-16:55:25] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 830ms/step - loss: 0.4989 - val_loss: 0.5647\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.4319 - val_loss: 0.5065\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3959 - val_loss: 0.4665\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3660 - val_loss: 0.4466\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3634 - val_loss: 0.4453\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3412 - val_loss: 0.4421\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3327 - val_loss: 0.4479\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3204 - val_loss: 0.4467\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3137 - val_loss: 0.4529\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.2943 - val_loss: 0.4539\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.2850 - val_loss: 0.4544\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.2864 - val_loss: 0.4602\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_20-16:55:55] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-16:55:55] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:55:55] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 833ms/step - loss: 0.3316 - val_loss: 0.4426\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>544</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  544  19\n",
       "1  138  15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16042780748663102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:5tjf5f0t) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 5803... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>lr</td><td>██▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▅█▃▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.44211</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33159</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.44255</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">autumn-glitter-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/5tjf5f0t\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042a/runs/5tjf5f0t</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_165337-5tjf5f0t/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:5tjf5f0t). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/28eksio6\" target=\"_blank\">divine-bird-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-16:57:06] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:57:06] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:57:06] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 594ms/step - loss: 0.9019 - val_loss: 0.6625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.6123 - val_loss: 0.4718\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5232 - val_loss: 0.5224\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5112 - val_loss: 0.4892\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4690 - val_loss: 0.4551\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4604 - val_loss: 0.4514\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4519 - val_loss: 0.4479\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4440 - val_loss: 0.5195\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4552 - val_loss: 0.4468\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4279 - val_loss: 0.4601\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4346 - val_loss: 0.4679\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4178 - val_loss: 0.4301\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4026 - val_loss: 0.4346\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3907 - val_loss: 0.4260\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4023 - val_loss: 0.4286\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3975 - val_loss: 0.4249\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3857 - val_loss: 0.4222\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3833 - val_loss: 0.4302\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3953 - val_loss: 0.4494\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4008 - val_loss: 0.4194\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3683 - val_loss: 0.4193\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3744 - val_loss: 0.4213\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3655 - val_loss: 0.4308\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.3733 - val_loss: 0.4195\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3599 - val_loss: 0.4133\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3638 - val_loss: 0.4133\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3536 - val_loss: 0.4152\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3783 - val_loss: 0.4133\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3646 - val_loss: 0.4134\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3613 - val_loss: 0.4140\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3579 - val_loss: 0.4158\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_20-16:57:51] Training the entire fine-tuned model...\n",
      "[2022_04_20-16:57:59] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.6314WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1055s vs `on_train_batch_end` time: 0.1361s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1055s vs `on_train_batch_end` time: 0.1361s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 706ms/step - loss: 0.6314 - val_loss: 0.5117\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 332ms/step - loss: 0.5028 - val_loss: 0.5253\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.4388 - val_loss: 0.4348\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.4283 - val_loss: 0.4162\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 332ms/step - loss: 0.3995 - val_loss: 0.4193\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3876 - val_loss: 0.4123\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3549 - val_loss: 0.4124\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3380 - val_loss: 0.4147\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3186 - val_loss: 0.4166\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 332ms/step - loss: 0.2867 - val_loss: 0.4185\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.2956 - val_loss: 0.4166\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 332ms/step - loss: 0.2972 - val_loss: 0.4267\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_20-16:58:32] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-16:58:32] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:58:32] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3471WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1046s vs `on_train_batch_end` time: 0.1368s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1046s vs `on_train_batch_end` time: 0.1368s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 709ms/step - loss: 0.3471 - val_loss: 0.4102\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>472</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  472  13\n",
       "1  103  14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19444444444444445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:28eksio6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 6303... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▅▃▃▂▂▂▂▁▁▁▂</td></tr><tr><td>lr</td><td>██████████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▄▃▂▂▂▄▂▂▂▂▁▂▁▁▂▂▁▁▂▁▁▁▁▁▁▁▁▄▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.41015</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34709</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.41015</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">divine-bird-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/28eksio6\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042b/runs/28eksio6</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_165651-28eksio6/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:28eksio6). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/cy9uc7a5\" target=\"_blank\">faithful-frog-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-16:59:28] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:59:28] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-16:59:28] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 748ms/step - loss: 1.0492 - val_loss: 0.5192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.6391 - val_loss: 0.6860\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.6113 - val_loss: 0.5145\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.5218 - val_loss: 0.5907\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.5286 - val_loss: 0.4865\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4566 - val_loss: 0.4987\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.4480 - val_loss: 0.4716\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4222 - val_loss: 0.4906\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4152 - val_loss: 0.4697\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4150 - val_loss: 0.4582\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3959 - val_loss: 0.4634\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4003 - val_loss: 0.4543\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3917 - val_loss: 0.4528\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3762 - val_loss: 0.4505\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3910 - val_loss: 0.4599\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3705 - val_loss: 0.4483\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3666 - val_loss: 0.4539\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3617 - val_loss: 0.4617\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3665 - val_loss: 0.4767\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3907 - val_loss: 0.4523\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3633 - val_loss: 0.4554\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3533 - val_loss: 0.4508\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3526 - val_loss: 0.4485\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3482 - val_loss: 0.4485\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_20-17:00:05] Training the entire fine-tuned model...\n",
      "[2022_04_20-17:00:27] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 836ms/step - loss: 0.3631 - val_loss: 0.4493\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3642 - val_loss: 0.4484\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3676 - val_loss: 0.4485\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3719 - val_loss: 0.4485\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3672 - val_loss: 0.4486\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3706 - val_loss: 0.4488\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.3604 - val_loss: 0.4489\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3720 - val_loss: 0.4487\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3728 - val_loss: 0.4486\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3670 - val_loss: 0.4486\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_20-17:00:53] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-17:00:53] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:01:14] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 849ms/step - loss: 0.3707 - val_loss: 0.4483\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>544</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>139</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  544  19\n",
       "1  139  14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15053763440860216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:cy9uc7a5) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 6622... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃█▃▅▂▂▂▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>15</td></tr><tr><td>best_val_loss</td><td>0.44833</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37065</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.44835</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">faithful-frog-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/cy9uc7a5\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042a/runs/cy9uc7a5</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_165910-cy9uc7a5/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:cy9uc7a5). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/ftt3q21g\" target=\"_blank\">vital-night-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-17:02:08] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:02:08] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:02:08] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 591ms/step - loss: 0.8410 - val_loss: 0.4918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5916 - val_loss: 0.5749\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5583 - val_loss: 0.4647\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4893 - val_loss: 0.5090\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4982 - val_loss: 0.5714\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5167 - val_loss: 0.4882\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4801 - val_loss: 0.4742\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4640 - val_loss: 0.4411\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4271 - val_loss: 0.4452\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.4282 - val_loss: 0.4393\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4321 - val_loss: 0.4386\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4238 - val_loss: 0.4395\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4218 - val_loss: 0.4372\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4176 - val_loss: 0.4360\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4207 - val_loss: 0.4371\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4201 - val_loss: 0.4348\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4201 - val_loss: 0.4381\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4184 - val_loss: 0.4333\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4147 - val_loss: 0.4323\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4100 - val_loss: 0.4319\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4073 - val_loss: 0.4309\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4077 - val_loss: 0.4297\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4165 - val_loss: 0.4290\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4047 - val_loss: 0.4284\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3956 - val_loss: 0.4298\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4081 - val_loss: 0.4271\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4113 - val_loss: 0.4280\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3892 - val_loss: 0.4303\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4084 - val_loss: 0.4273\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3989 - val_loss: 0.4247\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4030 - val_loss: 0.4245\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3873 - val_loss: 0.4245\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4000 - val_loss: 0.4237\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3970 - val_loss: 0.4239\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3925 - val_loss: 0.4232\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3912 - val_loss: 0.4227\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3961 - val_loss: 0.4217\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3893 - val_loss: 0.4221\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3973 - val_loss: 0.4203\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3939 - val_loss: 0.4202\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3896 - val_loss: 0.4197\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3955 - val_loss: 0.4203\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3816 - val_loss: 0.4212\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3817 - val_loss: 0.4188\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3797 - val_loss: 0.4183\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3807 - val_loss: 0.4180\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3761 - val_loss: 0.4179\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3848 - val_loss: 0.4178\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3723 - val_loss: 0.4176\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3740 - val_loss: 0.4168\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3740 - val_loss: 0.4172\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3767 - val_loss: 0.4165\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3753 - val_loss: 0.4161\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3718 - val_loss: 0.4161\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3802 - val_loss: 0.4170\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3692 - val_loss: 0.4163\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3726 - val_loss: 0.4157\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3678 - val_loss: 0.4197\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3626 - val_loss: 0.4165\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3604 - val_loss: 0.4154\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3672 - val_loss: 0.4148\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3728 - val_loss: 0.4155\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3667 - val_loss: 0.4146\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3524 - val_loss: 0.4153\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3629 - val_loss: 0.4146\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3659 - val_loss: 0.4170\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3678 - val_loss: 0.4145\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3550 - val_loss: 0.4136\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3597 - val_loss: 0.4161\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3581 - val_loss: 0.4151\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3531 - val_loss: 0.4135\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3684 - val_loss: 0.4135\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3516 - val_loss: 0.4141\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3555 - val_loss: 0.4143\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3600 - val_loss: 0.4133\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3569 - val_loss: 0.4132\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3608 - val_loss: 0.4134\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3628 - val_loss: 0.4132\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3650 - val_loss: 0.4130\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3594 - val_loss: 0.4135\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3538 - val_loss: 0.4129\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3469 - val_loss: 0.4135\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3569 - val_loss: 0.4130\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3574 - val_loss: 0.4129\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3542 - val_loss: 0.4128\n",
      "\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3477 - val_loss: 0.4127\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3526 - val_loss: 0.4127\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3586 - val_loss: 0.4126\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3615 - val_loss: 0.4125\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3553 - val_loss: 0.4126\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3420 - val_loss: 0.4125\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3533 - val_loss: 0.4126\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3561 - val_loss: 0.4126\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3604 - val_loss: 0.4126\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3517 - val_loss: 0.4126\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3594 - val_loss: 0.4126\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3579 - val_loss: 0.4126\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3550 - val_loss: 0.4126\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3492 - val_loss: 0.4126\n",
      "[2022_04_20-17:04:10] Training the entire fine-tuned model...\n",
      "[2022_04_20-17:04:18] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3607WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1072s vs `on_train_batch_end` time: 0.1365s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1072s vs `on_train_batch_end` time: 0.1365s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 719ms/step - loss: 0.3607 - val_loss: 0.4127\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3618 - val_loss: 0.4143\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3545 - val_loss: 0.4128\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3535 - val_loss: 0.4127\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3487 - val_loss: 0.4131\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3524 - val_loss: 0.4132\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3551 - val_loss: 0.4133\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3506 - val_loss: 0.4131\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3538 - val_loss: 0.4130\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3564 - val_loss: 0.4130\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3436 - val_loss: 0.4129\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3513 - val_loss: 0.4130\n",
      "[2022_04_20-17:04:50] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-17:04:50] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:04:50] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3485WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1046s vs `on_train_batch_end` time: 0.1365s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1046s vs `on_train_batch_end` time: 0.1365s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 672ms/step - loss: 0.3485 - val_loss: 0.4127\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>474</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  474  11\n",
       "1   98  19"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2585034013605442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ftt3q21g) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 6930... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███▁▁▁▂▁</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆█▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>90</td></tr><tr><td>best_val_loss</td><td>0.41252</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34846</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41275</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">vital-night-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/ftt3q21g\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042b/runs/ftt3q21g</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_170150-ftt3q21g/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ftt3q21g). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/2qvstilz\" target=\"_blank\">apricot-wave-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-17:05:45] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:05:45] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:05:46] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 725ms/step - loss: 0.9992 - val_loss: 0.5090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.6505 - val_loss: 0.7747\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.6746 - val_loss: 0.4964\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.5165 - val_loss: 0.5402\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4917 - val_loss: 0.4883\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4502 - val_loss: 0.4935\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4256 - val_loss: 0.4710\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4124 - val_loss: 0.4679\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4084 - val_loss: 0.4610\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3970 - val_loss: 0.4584\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3910 - val_loss: 0.4549\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3911 - val_loss: 0.4546\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3919 - val_loss: 0.4515\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3880 - val_loss: 0.4799\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.3976 - val_loss: 0.4506\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3866 - val_loss: 0.4594\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3827 - val_loss: 0.4484\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3665 - val_loss: 0.4635\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3677 - val_loss: 0.4721\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3638 - val_loss: 0.4576\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3617 - val_loss: 0.4441\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3469 - val_loss: 0.4560\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 226ms/step - loss: 0.3543 - val_loss: 0.4448\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3495 - val_loss: 0.4448\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3307 - val_loss: 0.4497\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3386 - val_loss: 0.4442\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3362 - val_loss: 0.4439\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3263 - val_loss: 0.4450\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3383 - val_loss: 0.4437\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3446 - val_loss: 0.4434\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3367 - val_loss: 0.4432\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3370 - val_loss: 0.4431\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3433 - val_loss: 0.4444\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3320 - val_loss: 0.4433\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3300 - val_loss: 0.4440\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3313 - val_loss: 0.4432\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3261 - val_loss: 0.4430\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3279 - val_loss: 0.4430\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3310 - val_loss: 0.4428\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3277 - val_loss: 0.4434\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3328 - val_loss: 0.4430\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3321 - val_loss: 0.4427\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3393 - val_loss: 0.4432\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3333 - val_loss: 0.4432\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3257 - val_loss: 0.4429\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3318 - val_loss: 0.4427\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3298 - val_loss: 0.4427\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3285 - val_loss: 0.4428\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3423 - val_loss: 0.4430\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3279 - val_loss: 0.4428\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3385 - val_loss: 0.4428\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3427 - val_loss: 0.4428\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3331 - val_loss: 0.4427\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3378 - val_loss: 0.4427\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3222 - val_loss: 0.4427\n",
      "[2022_04_20-17:06:56] Training the entire fine-tuned model...\n",
      "[2022_04_20-17:07:04] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 857ms/step - loss: 0.3489 - val_loss: 0.4499\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3375 - val_loss: 0.4502\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3268 - val_loss: 0.4466\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3246 - val_loss: 0.4420\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3078 - val_loss: 0.4438\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3043 - val_loss: 0.4467\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.2939 - val_loss: 0.4505\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.2924 - val_loss: 0.4514\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3034 - val_loss: 0.4524\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3016 - val_loss: 0.4532\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.2737 - val_loss: 0.4543\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.2862 - val_loss: 0.4554\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_20-17:07:34] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-17:07:34] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:07:34] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 848ms/step - loss: 0.3127 - val_loss: 0.4417\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>533</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  533  30\n",
       "1  126  27"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2571428571428571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2qvstilz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 7602... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▁</td></tr><tr><td>loss</td><td>█▅▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▂█▃▂▂▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.4417</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31268</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.4417</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">apricot-wave-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/2qvstilz\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042a/runs/2qvstilz</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_170529-2qvstilz/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2qvstilz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/2fxy73w0\" target=\"_blank\">honest-lion-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-17:08:29] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:08:29] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:08:29] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 582ms/step - loss: 0.8994 - val_loss: 0.8195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.6476 - val_loss: 0.4834\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5356 - val_loss: 0.5288\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5270 - val_loss: 0.4782\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4702 - val_loss: 0.4491\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4548 - val_loss: 0.4709\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4618 - val_loss: 0.4463\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4300 - val_loss: 0.4437\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4302 - val_loss: 0.4373\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4212 - val_loss: 0.4359\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4140 - val_loss: 0.4319\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4173 - val_loss: 0.4637\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4185 - val_loss: 0.4273\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4091 - val_loss: 0.4284\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3979 - val_loss: 0.4550\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4129 - val_loss: 0.4436\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4146 - val_loss: 0.4679\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4032 - val_loss: 0.4387\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3971 - val_loss: 0.4185\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3940 - val_loss: 0.4183\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3918 - val_loss: 0.4256\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3853 - val_loss: 0.4232\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3850 - val_loss: 0.4189\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3825 - val_loss: 0.4175\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3688 - val_loss: 0.4178\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3864 - val_loss: 0.4172\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3771 - val_loss: 0.4172\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3794 - val_loss: 0.4178\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3813 - val_loss: 0.4175\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3792 - val_loss: 0.4174\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3729 - val_loss: 0.4166\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3702 - val_loss: 0.4166\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3757 - val_loss: 0.4165\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3675 - val_loss: 0.4165\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3695 - val_loss: 0.4163\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3798 - val_loss: 0.4168\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3752 - val_loss: 0.4173\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3733 - val_loss: 0.4163\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3695 - val_loss: 0.4161\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3757 - val_loss: 0.4159\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3711 - val_loss: 0.4159\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3674 - val_loss: 0.4164\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3805 - val_loss: 0.4158\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3746 - val_loss: 0.4159\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3615 - val_loss: 0.4156\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3664 - val_loss: 0.4156\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3610 - val_loss: 0.4157\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3735 - val_loss: 0.4159\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3770 - val_loss: 0.4157\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3722 - val_loss: 0.4157\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3679 - val_loss: 0.4156\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3663 - val_loss: 0.4156\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3680 - val_loss: 0.4156\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3652 - val_loss: 0.4156\n",
      "[2022_04_20-17:09:39] Training the entire fine-tuned model...\n",
      "[2022_04_20-17:09:47] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3778WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1054s vs `on_train_batch_end` time: 0.1387s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1054s vs `on_train_batch_end` time: 0.1387s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 702ms/step - loss: 0.3778 - val_loss: 0.4176\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3659 - val_loss: 0.4171\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3602 - val_loss: 0.4168\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3591 - val_loss: 0.4155\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3549 - val_loss: 0.4157\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3452 - val_loss: 0.4152\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3475 - val_loss: 0.4159\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3308 - val_loss: 0.4171\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3313 - val_loss: 0.4228\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3271 - val_loss: 0.4250\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3239 - val_loss: 0.4225\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3172 - val_loss: 0.4203\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3084 - val_loss: 0.4218\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3131 - val_loss: 0.4210\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_20-17:10:23] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-17:10:23] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:10:23] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3491WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1080s vs `on_train_batch_end` time: 0.1350s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1080s vs `on_train_batch_end` time: 0.1350s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 697ms/step - loss: 0.3491 - val_loss: 0.4149\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>459</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  459  26\n",
       "1   91  26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3076923076923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2fxy73w0) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 8049... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇██▁▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.41489</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34905</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41489</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">honest-lion-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/2fxy73w0\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042b/runs/2fxy73w0</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_170813-2fxy73w0/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2fxy73w0). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/2vvm2uft\" target=\"_blank\">sunny-sponge-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-17:11:18] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:11:18] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:11:18] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 718ms/step - loss: 0.9829 - val_loss: 0.5479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.6753 - val_loss: 0.6993\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.5813 - val_loss: 0.5237\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4987 - val_loss: 0.5161\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4498 - val_loss: 0.4885\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4464 - val_loss: 0.5205\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4316 - val_loss: 0.4753\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4139 - val_loss: 0.4790\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4125 - val_loss: 0.4620\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3962 - val_loss: 0.4647\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3937 - val_loss: 0.4574\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3903 - val_loss: 0.4650\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3836 - val_loss: 0.4543\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3702 - val_loss: 0.4536\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3683 - val_loss: 0.4537\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3749 - val_loss: 0.4754\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3750 - val_loss: 0.4977\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3844 - val_loss: 0.4704\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3607 - val_loss: 0.4560\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3603 - val_loss: 0.4581\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3540 - val_loss: 0.4510\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3553 - val_loss: 0.4508\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3575 - val_loss: 0.4539\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3596 - val_loss: 0.4495\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3640 - val_loss: 0.4493\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3564 - val_loss: 0.4550\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3516 - val_loss: 0.4490\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3520 - val_loss: 0.4495\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3420 - val_loss: 0.4528\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3434 - val_loss: 0.4496\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3561 - val_loss: 0.4486\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3387 - val_loss: 0.4532\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3524 - val_loss: 0.4479\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3563 - val_loss: 0.4475\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3536 - val_loss: 0.4512\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3430 - val_loss: 0.4483\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3459 - val_loss: 0.4470\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3417 - val_loss: 0.4518\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3350 - val_loss: 0.4498\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3417 - val_loss: 0.4475\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.3356 - val_loss: 0.4470\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3318 - val_loss: 0.4474\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3396 - val_loss: 0.4471\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3426 - val_loss: 0.4476\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3543 - val_loss: 0.4472\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_20-17:12:17] Training the entire fine-tuned model...\n",
      "[2022_04_20-17:12:25] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 1s/step - loss: 0.3549 - val_loss: 0.4514\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3299 - val_loss: 0.4488\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3482 - val_loss: 0.4462\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3417 - val_loss: 0.4453\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3259 - val_loss: 0.4526\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3357 - val_loss: 0.4495\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3113 - val_loss: 0.4543\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.3175 - val_loss: 0.4550\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3063 - val_loss: 0.4527\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.2963 - val_loss: 0.4546\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.2991 - val_loss: 0.4569\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.2994 - val_loss: 0.4581\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_20-17:12:57] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-17:12:57] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:12:57] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 855ms/step - loss: 0.3250 - val_loss: 0.4442\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>528</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  528  35\n",
       "1  126  27"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25116279069767444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2vvm2uft) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 8500... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄█▃▂▃▂▁▁▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.44424</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32504</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.44424</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">sunny-sponge-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/2vvm2uft\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042a/runs/2vvm2uft</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_171102-2vvm2uft/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2vvm2uft). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/h8ki7ep2\" target=\"_blank\">neat-universe-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-17:14:07] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:14:07] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:14:07] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 597ms/step - loss: 0.8750 - val_loss: 0.7413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.5817 - val_loss: 0.5093\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5034 - val_loss: 0.4739\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4847 - val_loss: 0.4608\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4562 - val_loss: 0.4493\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4433 - val_loss: 0.4451\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4356 - val_loss: 0.4530\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4398 - val_loss: 0.4395\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4338 - val_loss: 0.4529\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4526 - val_loss: 0.4585\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4458 - val_loss: 0.4384\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4258 - val_loss: 0.4458\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4200 - val_loss: 0.4281\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4051 - val_loss: 0.4249\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3981 - val_loss: 0.4236\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3958 - val_loss: 0.4220\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3849 - val_loss: 0.4258\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3887 - val_loss: 0.4340\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3835 - val_loss: 0.4301\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.3913 - val_loss: 0.4184\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3964 - val_loss: 0.4230\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3994 - val_loss: 0.4228\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3717 - val_loss: 0.4498\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3859 - val_loss: 0.4493\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3799 - val_loss: 0.4199\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3759 - val_loss: 0.4177\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3778 - val_loss: 0.4206\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3709 - val_loss: 0.4166\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3640 - val_loss: 0.4216\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3536 - val_loss: 0.4169\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3541 - val_loss: 0.4175\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3597 - val_loss: 0.4155\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3619 - val_loss: 0.4155\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3612 - val_loss: 0.4152\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3643 - val_loss: 0.4148\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3563 - val_loss: 0.4146\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3447 - val_loss: 0.4153\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3552 - val_loss: 0.4142\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3616 - val_loss: 0.4157\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3550 - val_loss: 0.4140\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3507 - val_loss: 0.4139\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3538 - val_loss: 0.4167\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3519 - val_loss: 0.4140\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3530 - val_loss: 0.4138\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3449 - val_loss: 0.4142\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3594 - val_loss: 0.4135\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3578 - val_loss: 0.4142\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3581 - val_loss: 0.4136\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3507 - val_loss: 0.4144\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3487 - val_loss: 0.4135\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3422 - val_loss: 0.4132\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3375 - val_loss: 0.4134\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3407 - val_loss: 0.4132\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3536 - val_loss: 0.4132\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3400 - val_loss: 0.4130\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3572 - val_loss: 0.4135\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3533 - val_loss: 0.4151\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3463 - val_loss: 0.4129\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3514 - val_loss: 0.4127\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3457 - val_loss: 0.4140\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3591 - val_loss: 0.4128\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3402 - val_loss: 0.4124\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3457 - val_loss: 0.4125\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3404 - val_loss: 0.4126\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3451 - val_loss: 0.4124\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3540 - val_loss: 0.4131\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3500 - val_loss: 0.4131\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3292 - val_loss: 0.4127\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3439 - val_loss: 0.4126\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3484 - val_loss: 0.4126\n",
      "\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_20-17:15:35] Training the entire fine-tuned model...\n",
      "[2022_04_20-17:15:46] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3771WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1053s vs `on_train_batch_end` time: 0.1357s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1053s vs `on_train_batch_end` time: 0.1357s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 688ms/step - loss: 0.3771 - val_loss: 0.4366\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3561 - val_loss: 0.4162\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3510 - val_loss: 0.4193\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3294 - val_loss: 0.4171\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3412 - val_loss: 0.4193\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3346 - val_loss: 0.4151\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3206 - val_loss: 0.4156\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 332ms/step - loss: 0.3214 - val_loss: 0.4181\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3145 - val_loss: 0.4160\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3107 - val_loss: 0.4238\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3047 - val_loss: 0.4149\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.2950 - val_loss: 0.4166\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3040 - val_loss: 0.4189\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3023 - val_loss: 0.4172\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.2884 - val_loss: 0.4172\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.2973 - val_loss: 0.4176\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.2905 - val_loss: 0.4188\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.2872 - val_loss: 0.4207\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.2966 - val_loss: 0.4197\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "[2022_04_20-17:16:32] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-17:16:32] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:16:32] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3137WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1073s vs `on_train_batch_end` time: 0.1358s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1073s vs `on_train_batch_end` time: 0.1358s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 691ms/step - loss: 0.3137 - val_loss: 0.4145\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>456</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  456  29\n",
       "1   89  28"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3218390804597701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:h8ki7ep2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 8890... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇██▁▁▂▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>61</td></tr><tr><td>best_val_loss</td><td>0.41237</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3137</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.41446</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">neat-universe-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/h8ki7ep2\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042b/runs/h8ki7ep2</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_171349-h8ki7ep2/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:h8ki7ep2). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/w9jy1dy0\" target=\"_blank\">astral-pyramid-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-17:17:26] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:17:26] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:17:27] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 717ms/step - loss: 0.8923 - val_loss: 0.6022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.5508 - val_loss: 0.6682\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.5170 - val_loss: 0.5251\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4944 - val_loss: 0.4950\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4485 - val_loss: 0.4892\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4470 - val_loss: 0.4875\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4313 - val_loss: 0.4881\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4107 - val_loss: 0.4678\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4071 - val_loss: 0.4584\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4007 - val_loss: 0.4778\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3994 - val_loss: 0.4930\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3999 - val_loss: 0.4734\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3912 - val_loss: 0.4520\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3821 - val_loss: 0.4570\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3756 - val_loss: 0.4491\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3668 - val_loss: 0.4895\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3961 - val_loss: 0.4482\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3576 - val_loss: 0.4469\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3568 - val_loss: 0.4505\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3432 - val_loss: 0.4613\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.3639 - val_loss: 0.4449\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3476 - val_loss: 0.4496\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3334 - val_loss: 0.4537\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3488 - val_loss: 0.4478\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3454 - val_loss: 0.4523\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3487 - val_loss: 0.4440\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3383 - val_loss: 0.4441\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3347 - val_loss: 0.4445\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3284 - val_loss: 0.4450\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3198 - val_loss: 0.4446\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3252 - val_loss: 0.4438\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3390 - val_loss: 0.4437\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.3317 - val_loss: 0.4436\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3205 - val_loss: 0.4445\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3436 - val_loss: 0.4441\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3185 - val_loss: 0.4436\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3310 - val_loss: 0.4436\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3351 - val_loss: 0.4437\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3227 - val_loss: 0.4437\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3234 - val_loss: 0.4437\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3273 - val_loss: 0.4437\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_20-17:18:22] Training the entire fine-tuned model...\n",
      "[2022_04_20-17:19:06] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 832ms/step - loss: 0.7443 - val_loss: 0.9463\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.6021 - val_loss: 0.4519\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3919 - val_loss: 0.4528\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3684 - val_loss: 0.4564\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3481 - val_loss: 0.4511\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3577 - val_loss: 0.4516\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3410 - val_loss: 0.4451\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3149 - val_loss: 0.4424\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3136 - val_loss: 0.4438\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3233 - val_loss: 0.4754\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.2877 - val_loss: 0.4557\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.2862 - val_loss: 0.4531\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.2492 - val_loss: 0.4458\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.2413 - val_loss: 0.4503\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.2405 - val_loss: 0.4577\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.2413 - val_loss: 0.4626\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_20-17:19:43] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-17:19:43] Training set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:19:43] Validation set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 836ms/step - loss: 0.3181 - val_loss: 0.4474\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>557</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>139</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  557   6\n",
       "1  139  14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16184971098265896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_20_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:w9jy1dy0) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9506... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▅▃▂▂▂▂▂▁▁▁▂</td></tr><tr><td>lr</td><td>██████████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▆█▄▂▂▂▁▃▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.44236</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31807</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.44735</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">astral-pyramid-16</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%2042a/runs/w9jy1dy0\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%2042a/runs/w9jy1dy0</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_171710-w9jy1dy0/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:w9jy1dy0). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%2042b/runs/29tbv6ih\" target=\"_blank\">ruby-universe-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-17:20:46] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:20:46] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:20:46] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 570ms/step - loss: 0.7736 - val_loss: 0.4834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5238 - val_loss: 0.5694\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5116 - val_loss: 0.4649\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5007 - val_loss: 0.5270\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4852 - val_loss: 0.4866\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4811 - val_loss: 0.4817\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4784 - val_loss: 0.4408\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4483 - val_loss: 0.4374\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4300 - val_loss: 0.4359\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4126 - val_loss: 0.4324\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4091 - val_loss: 0.4357\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.4106 - val_loss: 0.4278\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4041 - val_loss: 0.4507\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4269 - val_loss: 0.4269\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4111 - val_loss: 0.4413\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3981 - val_loss: 0.4313\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3932 - val_loss: 0.4460\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4091 - val_loss: 0.4348\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3963 - val_loss: 0.4148\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3781 - val_loss: 0.4163\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3721 - val_loss: 0.4195\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3755 - val_loss: 0.4155\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3669 - val_loss: 0.4156\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3692 - val_loss: 0.4140\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3708 - val_loss: 0.4135\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3765 - val_loss: 0.4137\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3807 - val_loss: 0.4134\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3818 - val_loss: 0.4142\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3718 - val_loss: 0.4136\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3704 - val_loss: 0.4135\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3775 - val_loss: 0.4133\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3721 - val_loss: 0.4133\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3649 - val_loss: 0.4133\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3725 - val_loss: 0.4133\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3576 - val_loss: 0.4133\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3732 - val_loss: 0.4133\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3691 - val_loss: 0.4132\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3810 - val_loss: 0.4132\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3725 - val_loss: 0.4132\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3638 - val_loss: 0.4132\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3703 - val_loss: 0.4132\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3703 - val_loss: 0.4132\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3744 - val_loss: 0.4132\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3770 - val_loss: 0.4132\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3642 - val_loss: 0.4132\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3653 - val_loss: 0.4132\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3581 - val_loss: 0.4132\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3741 - val_loss: 0.4132\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3675 - val_loss: 0.4132\n",
      "[2022_04_20-17:21:52] Training the entire fine-tuned model...\n",
      "[2022_04_20-17:22:25] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.7679WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1082s vs `on_train_batch_end` time: 0.1366s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1082s vs `on_train_batch_end` time: 0.1366s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 708ms/step - loss: 0.7679 - val_loss: 0.4848\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 331ms/step - loss: 0.4648 - val_loss: 0.4214\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 331ms/step - loss: 0.4463 - val_loss: 0.4116\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.4306 - val_loss: 0.4140\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 332ms/step - loss: 0.3969 - val_loss: 0.4100\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3827 - val_loss: 0.4094\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3623 - val_loss: 0.4091\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3585 - val_loss: 0.4111\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3404 - val_loss: 0.4166\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 332ms/step - loss: 0.3260 - val_loss: 0.4202\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3073 - val_loss: 0.4166\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 331ms/step - loss: 0.3015 - val_loss: 0.4181\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3032 - val_loss: 0.4146\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.2805 - val_loss: 0.4158\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.2809 - val_loss: 0.4259\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_20-17:23:03] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-17:23:03] Training set: Filtered out 0 of 716 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:23:03] Validation set: Filtered out 0 of 602 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3453WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1050s vs `on_train_batch_end` time: 0.1353s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1050s vs `on_train_batch_end` time: 0.1353s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 709ms/step - loss: 0.3453 - val_loss: 0.4096\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>455</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  455  30\n",
       "1   93  24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2807017543859649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_20_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "for seed in [18, 27, 36, 42]: # 4\n",
    "    train = pd.read_csv(path.join(DATA_DIR, f\"chen/deduplicated/crossval/chen_{seed}_a.csv\"), index_col=0)\n",
    "    test = pd.read_csv(path.join(DATA_DIR, f\"chen/deduplicated/crossval/chen_{seed}_b.csv\"), index_col=0)\n",
    "    train[\"seq\"] = train[\"heavy\"] + train[\"light\"]\n",
    "    test[\"seq\"] = test[\"heavy\"] + test[\"light\"]\n",
    "    for pat in patience:\n",
    "        for lr in learning_rate:\n",
    "            finetune_by_settings_and_data(pat, lr, f\"Split old {seed}a\", train, test, f\"{seed}a\")\n",
    "            finetune_by_settings_and_data(pat, lr, f\"Split old {seed}b\", test, train, f\"{seed}b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb331fe-614c-4e08-a9cb-128e4ba50983",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:f11hjn5w) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10340... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>loss</td><td>█▄▄▃▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▄▂▃▂▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>14</td></tr><tr><td>best_val_loss</td><td>0.44195</td></tr><tr><td>epoch</td><td>15</td></tr><tr><td>loss</td><td>0.37977</td></tr><tr><td>lr</td><td>0.01</td></tr><tr><td>val_loss</td><td>0.44224</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">sunny-dream-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/f11hjn5w\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204a/runs/f11hjn5w</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_172341-f11hjn5w/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:f11hjn5w). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/6s3jak81\" target=\"_blank\">wobbly-violet-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-17:48:41] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:48:41] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:48:41] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 749ms/step - loss: 0.9210 - val_loss: 0.6540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.6000 - val_loss: 0.5865\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4941 - val_loss: 0.5380\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4860 - val_loss: 0.5283\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4663 - val_loss: 0.4732\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4538 - val_loss: 0.4617\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4351 - val_loss: 0.4628\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4311 - val_loss: 0.4632\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4203 - val_loss: 0.4686\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-17:49:02] Training the entire fine-tuned model...\n",
      "[2022_04_20-17:49:18] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 849ms/step - loss: 0.4316 - val_loss: 0.4591\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.4272 - val_loss: 0.4581\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 371ms/step - loss: 0.4300 - val_loss: 0.4579\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 372ms/step - loss: 0.4230 - val_loss: 0.4576\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.4269 - val_loss: 0.4574\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.4231 - val_loss: 0.4572\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.4166 - val_loss: 0.4569\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 372ms/step - loss: 0.4287 - val_loss: 0.4568\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.4227 - val_loss: 0.4567\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.4167 - val_loss: 0.4561\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.4218 - val_loss: 0.4555\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.4242 - val_loss: 0.4552\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 372ms/step - loss: 0.4201 - val_loss: 0.4549\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.4223 - val_loss: 0.4546\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.4156 - val_loss: 0.4543\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.4144 - val_loss: 0.4540\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.4134 - val_loss: 0.4537\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.4180 - val_loss: 0.4535\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.4148 - val_loss: 0.4532\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.4130 - val_loss: 0.4528\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.4085 - val_loss: 0.4524\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.4081 - val_loss: 0.4520\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.4104 - val_loss: 0.4517\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.4049 - val_loss: 0.4514\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.4035 - val_loss: 0.4511\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.4060 - val_loss: 0.4507\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.4064 - val_loss: 0.4501\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.4019 - val_loss: 0.4501\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.4049 - val_loss: 0.4495\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.4006 - val_loss: 0.4491\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3979 - val_loss: 0.4487\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3991 - val_loss: 0.4483\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3900 - val_loss: 0.4479\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.4081 - val_loss: 0.4475\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3964 - val_loss: 0.4471\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3963 - val_loss: 0.4466\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.4034 - val_loss: 0.4462\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3927 - val_loss: 0.4458\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3914 - val_loss: 0.4454\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3899 - val_loss: 0.4450\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3900 - val_loss: 0.4446\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3904 - val_loss: 0.4442\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3877 - val_loss: 0.4439\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3891 - val_loss: 0.4435\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3826 - val_loss: 0.4434\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3917 - val_loss: 0.4430\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3821 - val_loss: 0.4424\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3805 - val_loss: 0.4426\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3794 - val_loss: 0.4418\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3742 - val_loss: 0.4412\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3800 - val_loss: 0.4407\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3747 - val_loss: 0.4401\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3778 - val_loss: 0.4401\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3797 - val_loss: 0.4395\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3747 - val_loss: 0.4390\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3711 - val_loss: 0.4389\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3656 - val_loss: 0.4386\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3666 - val_loss: 0.4381\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3711 - val_loss: 0.4377\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3653 - val_loss: 0.4373\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3608 - val_loss: 0.4369\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3708 - val_loss: 0.4365\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3630 - val_loss: 0.4366\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3587 - val_loss: 0.4366\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3638 - val_loss: 0.4354\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3489 - val_loss: 0.4357\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3463 - val_loss: 0.4349\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3536 - val_loss: 0.4359\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3437 - val_loss: 0.4345\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3355 - val_loss: 0.4346\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3380 - val_loss: 0.4344\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3374 - val_loss: 0.4343\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3423 - val_loss: 0.4340\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3279 - val_loss: 0.4347\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3290 - val_loss: 0.4340\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3269 - val_loss: 0.4350\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3432 - val_loss: 0.4347\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3370 - val_loss: 0.4337\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3286 - val_loss: 0.4336\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3260 - val_loss: 0.4335\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3317 - val_loss: 0.4335\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3350 - val_loss: 0.4335\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3350 - val_loss: 0.4335\n",
      "[2022_04_20-17:51:58] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-17:51:58] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:51:58] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 894ms/step - loss: 0.3330 - val_loss: 0.4335\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>556</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  556  26\n",
       "1  118  33"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3142857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:6s3jak81) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 12095... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███▁</td></tr><tr><td>loss</td><td>█▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.43349</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.333</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43349</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">wobbly-violet-2</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/6s3jak81\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204a/runs/6s3jak81</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_174822-6s3jak81/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:6s3jak81). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204b/runs/3ckwf0yz\" target=\"_blank\">pretty-brook-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-17:52:53] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:52:54] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:52:54] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 575ms/step - loss: 0.8240 - val_loss: 0.7397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.6559 - val_loss: 0.4860\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5051 - val_loss: 0.5249\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4861 - val_loss: 0.4664\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4562 - val_loss: 0.4778\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4446 - val_loss: 0.4606\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4208 - val_loss: 0.4591\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4290 - val_loss: 0.4670\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4313 - val_loss: 0.4566\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4247 - val_loss: 0.4437\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4201 - val_loss: 0.4569\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4140 - val_loss: 0.4821\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4304 - val_loss: 0.4488\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-17:53:18] Training the entire fine-tuned model...\n",
      "[2022_04_20-17:53:28] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4078WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1215s vs `on_train_batch_end` time: 0.1370s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1215s vs `on_train_batch_end` time: 0.1370s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 705ms/step - loss: 0.4078 - val_loss: 0.4440\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.4065 - val_loss: 0.4435\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.4058 - val_loss: 0.4434\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.4026 - val_loss: 0.4435\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.4002 - val_loss: 0.4431\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 341ms/step - loss: 0.4004 - val_loss: 0.4430\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.4029 - val_loss: 0.4435\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.4013 - val_loss: 0.4430\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3869 - val_loss: 0.4426\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3981 - val_loss: 0.4426\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3954 - val_loss: 0.4424\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3967 - val_loss: 0.4423\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3933 - val_loss: 0.4429\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3984 - val_loss: 0.4421\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3979 - val_loss: 0.4420\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3827 - val_loss: 0.4419\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3928 - val_loss: 0.4419\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3900 - val_loss: 0.4419\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3906 - val_loss: 0.4420\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 341ms/step - loss: 0.3947 - val_loss: 0.4420\n",
      "[2022_04_20-17:54:16] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-17:54:16] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:54:56] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3913WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1097s vs `on_train_batch_end` time: 0.1388s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1097s vs `on_train_batch_end` time: 0.1388s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 691ms/step - loss: 0.3913 - val_loss: 0.4419\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>458</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  458   8\n",
       "1  104  15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2112676056338028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_20_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_20_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3ckwf0yz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 12734... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██▁</td></tr><tr><td>loss</td><td>█▅▃▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▃▂▂▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>16</td></tr><tr><td>best_val_loss</td><td>0.44185</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.39129</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.44187</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">pretty-brook-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204b/runs/3ckwf0yz\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204b/runs/3ckwf0yz</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_175236-3ckwf0yz/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3ckwf0yz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/2bvvoxj9\" target=\"_blank\">silvery-surf-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-17:55:49] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:55:49] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:55:49] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 735ms/step - loss: 0.9270 - val_loss: 0.5640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.7213 - val_loss: 0.7977\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.6776 - val_loss: 0.5362\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.5315 - val_loss: 0.5613\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4988 - val_loss: 0.4862\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4637 - val_loss: 0.5118\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4603 - val_loss: 0.4827\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4440 - val_loss: 0.5128\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4521 - val_loss: 0.4892\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.4323 - val_loss: 0.4738\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4085 - val_loss: 0.4574\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3952 - val_loss: 0.4766\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4020 - val_loss: 0.4545\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3901 - val_loss: 0.4691\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4025 - val_loss: 0.4417\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4113 - val_loss: 0.4450\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4036 - val_loss: 0.4677\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3913 - val_loss: 0.4405\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3656 - val_loss: 0.4402\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3611 - val_loss: 0.4348\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3602 - val_loss: 0.4364\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3610 - val_loss: 0.4443\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.3748 - val_loss: 0.4379\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-17:56:26] Training the entire fine-tuned model...\n",
      "[2022_04_20-17:56:35] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 847ms/step - loss: 0.3622 - val_loss: 0.4322\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3535 - val_loss: 0.4316\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3608 - val_loss: 0.4313\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3477 - val_loss: 0.4306\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3419 - val_loss: 0.4293\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3307 - val_loss: 0.4298\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3327 - val_loss: 0.4374\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3357 - val_loss: 0.4308\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_20-17:56:57] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-17:56:57] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:56:57] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 824ms/step - loss: 0.3277 - val_loss: 0.4293\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>572</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>136</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  572  10\n",
       "1  136  15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17045454545454544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2bvvoxj9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 13018... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▆▅▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████████▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄█▃▄▂▃▂▃▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.42931</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32768</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42931</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">silvery-surf-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/2bvvoxj9\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204a/runs/2bvvoxj9</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_175533-2bvvoxj9/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2bvvoxj9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204b/runs/32tki33z\" target=\"_blank\">fearless-universe-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-17:57:55] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:57:55] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:57:55] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 582ms/step - loss: 0.8427 - val_loss: 0.6905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.6591 - val_loss: 0.5008\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5545 - val_loss: 0.5439\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.5387 - val_loss: 0.4796\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4840 - val_loss: 0.4621\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4570 - val_loss: 0.4932\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4396 - val_loss: 0.4887\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4374 - val_loss: 0.4811\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-17:58:14] Training the entire fine-tuned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_20-17:58:32] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4448WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1076s vs `on_train_batch_end` time: 0.1408s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1076s vs `on_train_batch_end` time: 0.1408s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 1s/step - loss: 0.4448 - val_loss: 0.4581\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.4400 - val_loss: 0.4555\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.4340 - val_loss: 0.4542\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.4237 - val_loss: 0.4527\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.4190 - val_loss: 0.4513\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.4141 - val_loss: 0.4507\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.4102 - val_loss: 0.4506\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.4024 - val_loss: 0.4528\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3988 - val_loss: 0.4543\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3921 - val_loss: 0.4546\n",
      "[2022_04_20-17:59:01] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-17:59:01] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-17:59:01] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4039WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1112s vs `on_train_batch_end` time: 0.1368s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1112s vs `on_train_batch_end` time: 0.1368s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 700ms/step - loss: 0.4039 - val_loss: 0.4505\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>459</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  459   7\n",
       "1  109  10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14705882352941177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>562</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  562  20\n",
       "1  123  28"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2814070351758794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2jifvdfj) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 13484... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>lr</td><td>████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃█▄▄▂▃▃▂▂▂▂▂▂▂▂▂▂▁▁▃▁▁▂▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>17</td></tr><tr><td>best_val_loss</td><td>0.44135</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34083</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.44196</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">floral-dragon-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/2jifvdfj\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204a/runs/2jifvdfj</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_175938-2jifvdfj/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2jifvdfj). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204b/runs/lf270y8d\" target=\"_blank\">solar-butterfly-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-18:02:29] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:02:29] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:02:29] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 587ms/step - loss: 0.8751 - val_loss: 0.8627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.6846 - val_loss: 0.5454\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5373 - val_loss: 0.5378\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5363 - val_loss: 0.5688\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5051 - val_loss: 0.4941\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4487 - val_loss: 0.4555\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4328 - val_loss: 0.4579\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4310 - val_loss: 0.4758\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4353 - val_loss: 0.4480\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4153 - val_loss: 0.4454\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4135 - val_loss: 0.4487\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3993 - val_loss: 0.4530\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4044 - val_loss: 0.4378\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3929 - val_loss: 0.4520\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3968 - val_loss: 0.4381\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3873 - val_loss: 0.4347\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3764 - val_loss: 0.4506\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3919 - val_loss: 0.4324\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3730 - val_loss: 0.4873\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3883 - val_loss: 0.4389\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3756 - val_loss: 0.4378\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-18:03:04] Training the entire fine-tuned model...\n",
      "[2022_04_20-18:03:27] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3743WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1059s vs `on_train_batch_end` time: 0.1406s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1059s vs `on_train_batch_end` time: 0.1406s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 709ms/step - loss: 0.3743 - val_loss: 0.4529\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3921 - val_loss: 0.4388\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3778 - val_loss: 0.4344\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3683 - val_loss: 0.4416\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3498 - val_loss: 0.4356\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3456 - val_loss: 0.4467\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_20-18:03:47] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-18:03:47] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:03:47] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3574WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1066s vs `on_train_batch_end` time: 0.1402s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1066s vs `on_train_batch_end` time: 0.1402s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 707ms/step - loss: 0.3574 - val_loss: 0.4338\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>448</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  448  18\n",
       "1   93  26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31901840490797545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_20_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_20_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:lf270y8d) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 13726... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▅▄▄▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████████▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▃▂▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>17</td></tr><tr><td>best_val_loss</td><td>0.43236</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35737</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.4338</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">solar-butterfly-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204b/runs/lf270y8d\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204b/runs/lf270y8d</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_180213-lf270y8d/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:lf270y8d). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/2iq62ynp\" target=\"_blank\">jolly-hill-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-18:04:42] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:04:42] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:04:42] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 711ms/step - loss: 0.9003 - val_loss: 0.7613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.6327 - val_loss: 0.7062\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.5476 - val_loss: 0.6185\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.5255 - val_loss: 0.5622\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4896 - val_loss: 0.4764\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4653 - val_loss: 0.4641\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4323 - val_loss: 0.4545\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 246ms/step - loss: 0.4153 - val_loss: 0.4519\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4104 - val_loss: 0.4550\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4140 - val_loss: 0.4528\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3982 - val_loss: 0.4465\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3857 - val_loss: 0.4462\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3951 - val_loss: 0.4484\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3992 - val_loss: 0.4516\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3866 - val_loss: 0.4391\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3793 - val_loss: 0.4424\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3725 - val_loss: 0.4408\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3565 - val_loss: 0.4402\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_20-18:05:13] Training the entire fine-tuned model...\n",
      "[2022_04_20-18:05:39] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 837ms/step - loss: 0.4464 - val_loss: 0.6533\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.4767 - val_loss: 0.5547\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 372ms/step - loss: 0.4260 - val_loss: 0.4918\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 372ms/step - loss: 0.3697 - val_loss: 0.4797\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 370ms/step - loss: 0.3870 - val_loss: 0.4608\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 371ms/step - loss: 0.3690 - val_loss: 0.4486\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3403 - val_loss: 0.4447\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3433 - val_loss: 0.4524\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3210 - val_loss: 0.4630\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3121 - val_loss: 0.4979\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_20-18:06:05] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-18:06:05] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:06:05] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 833ms/step - loss: 0.3356 - val_loss: 0.4357\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>576</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  576   6\n",
       "1  138  13"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15294117647058825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2iq62ynp) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 13964... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██▁▁▂▂▃▃▃▄▄▅▁</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▅▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▆▄▂▂▂▁▁▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.43574</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33561</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.43574</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">jolly-hill-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/2iq62ynp\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204a/runs/2iq62ynp</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_180426-2iq62ynp/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2iq62ynp). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204b/runs/20gye42r\" target=\"_blank\">sleek-bee-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-18:06:58] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:06:58] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:06:58] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 552ms/step - loss: 0.8473 - val_loss: 0.5550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.6553 - val_loss: 0.4803\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5635 - val_loss: 0.6463\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5368 - val_loss: 0.6038\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5142 - val_loss: 0.4802\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4465 - val_loss: 0.4590\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4421 - val_loss: 0.4571\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4369 - val_loss: 0.4502\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4151 - val_loss: 0.4556\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4094 - val_loss: 0.4475\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4209 - val_loss: 0.4694\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4191 - val_loss: 0.4459\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4124 - val_loss: 0.4543\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4154 - val_loss: 0.4406\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4019 - val_loss: 0.4398\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3971 - val_loss: 0.4388\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3915 - val_loss: 0.4333\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3790 - val_loss: 0.4408\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4035 - val_loss: 0.4317\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3837 - val_loss: 0.4360\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3811 - val_loss: 0.4309\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3694 - val_loss: 0.4351\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3691 - val_loss: 0.4299\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3768 - val_loss: 0.4336\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3586 - val_loss: 0.4298\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3602 - val_loss: 0.4334\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3521 - val_loss: 0.4271\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3667 - val_loss: 0.4287\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3596 - val_loss: 0.4282\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3555 - val_loss: 0.4279\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_20-18:07:42] Training the entire fine-tuned model...\n",
      "[2022_04_20-18:07:57] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5503WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1097s vs `on_train_batch_end` time: 0.1390s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1097s vs `on_train_batch_end` time: 0.1390s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 720ms/step - loss: 0.5503 - val_loss: 0.6508\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.5047 - val_loss: 0.4359\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.4089 - val_loss: 0.4432\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 341ms/step - loss: 0.4044 - val_loss: 0.4350\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3755 - val_loss: 0.4268\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3583 - val_loss: 0.4227\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3677 - val_loss: 0.4324\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3608 - val_loss: 0.4221\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 341ms/step - loss: 0.3357 - val_loss: 0.4503\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 340ms/step - loss: 0.3538 - val_loss: 0.4611\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3096 - val_loss: 0.4288\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_20-18:08:27] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-18:08:27] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:08:44] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3356WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1098s vs `on_train_batch_end` time: 0.1379s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1098s vs `on_train_batch_end` time: 0.1379s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 683ms/step - loss: 0.3356 - val_loss: 0.4221\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>439</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  439  27\n",
       "1   87  32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3595505617977528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_20_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_20_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:20gye42r) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 14206... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▄▃▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▅▃█▇▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁█▁▂▁▁▁▁▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.42211</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33559</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.42214</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">sleek-bee-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204b/runs/20gye42r\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204b/runs/20gye42r</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_180643-20gye42r/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:20gye42r). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/bkno7htc\" target=\"_blank\">ethereal-totem-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-18:09:37] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:09:37] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:09:37] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 700ms/step - loss: 0.9043 - val_loss: 0.5515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.6290 - val_loss: 0.7251\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.5629 - val_loss: 0.6465\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.5414 - val_loss: 0.6104\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.5443 - val_loss: 0.4923\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4640 - val_loss: 0.5164\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4756 - val_loss: 0.4693\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4485 - val_loss: 0.4865\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4445 - val_loss: 0.4651\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4380 - val_loss: 0.4653\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4342 - val_loss: 0.4667\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4339 - val_loss: 0.4635\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4341 - val_loss: 0.4608\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4295 - val_loss: 0.4596\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4201 - val_loss: 0.4580\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4178 - val_loss: 0.4569\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4223 - val_loss: 0.4561\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4288 - val_loss: 0.4571\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4148 - val_loss: 0.4640\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4085 - val_loss: 0.4539\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4090 - val_loss: 0.4527\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4134 - val_loss: 0.4513\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4076 - val_loss: 0.4504\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3995 - val_loss: 0.4507\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4050 - val_loss: 0.4488\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4057 - val_loss: 0.4486\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4006 - val_loss: 0.4481\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3961 - val_loss: 0.4473\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3821 - val_loss: 0.4470\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3969 - val_loss: 0.4467\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3956 - val_loss: 0.4453\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3905 - val_loss: 0.4464\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3927 - val_loss: 0.4453\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3843 - val_loss: 0.4438\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.3804 - val_loss: 0.4435\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3824 - val_loss: 0.4435\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3853 - val_loss: 0.4434\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3906 - val_loss: 0.4425\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3788 - val_loss: 0.4445\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3730 - val_loss: 0.4412\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3811 - val_loss: 0.4415\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3804 - val_loss: 0.4440\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3761 - val_loss: 0.4412\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3778 - val_loss: 0.4400\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3677 - val_loss: 0.4395\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3711 - val_loss: 0.4397\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3748 - val_loss: 0.4398\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3738 - val_loss: 0.4392\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3668 - val_loss: 0.4392\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3682 - val_loss: 0.4389\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3611 - val_loss: 0.4393\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3615 - val_loss: 0.4387\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3668 - val_loss: 0.4392\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3604 - val_loss: 0.4389\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3666 - val_loss: 0.4384\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3665 - val_loss: 0.4384\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.3569 - val_loss: 0.4382\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3677 - val_loss: 0.4384\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3724 - val_loss: 0.4380\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3654 - val_loss: 0.4382\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3595 - val_loss: 0.4378\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3625 - val_loss: 0.4378\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3707 - val_loss: 0.4377\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3517 - val_loss: 0.4375\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3629 - val_loss: 0.4374\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3687 - val_loss: 0.4373\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3676 - val_loss: 0.4373\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3632 - val_loss: 0.4371\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3615 - val_loss: 0.4371\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3622 - val_loss: 0.4373\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3485 - val_loss: 0.4371\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3639 - val_loss: 0.4370\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3684 - val_loss: 0.4368\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 1s 247ms/step - loss: 0.3718 - val_loss: 0.4367\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3626 - val_loss: 0.4367\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3593 - val_loss: 0.4367\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3511 - val_loss: 0.4367\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3583 - val_loss: 0.4366\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3542 - val_loss: 0.4366\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3629 - val_loss: 0.4366\n",
      "\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3664 - val_loss: 0.4366\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3620 - val_loss: 0.4366\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3615 - val_loss: 0.4366\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3496 - val_loss: 0.4366\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3638 - val_loss: 0.4366\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3680 - val_loss: 0.4366\n",
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3558 - val_loss: 0.4366\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3548 - val_loss: 0.4366\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3609 - val_loss: 0.4366\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3609 - val_loss: 0.4366\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3568 - val_loss: 0.4366\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.3620 - val_loss: 0.4366\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3547 - val_loss: 0.4366\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3631 - val_loss: 0.4366\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3595 - val_loss: 0.4366\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3510 - val_loss: 0.4366\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3571 - val_loss: 0.4366\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3605 - val_loss: 0.4366\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3549 - val_loss: 0.4366\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3600 - val_loss: 0.4366\n",
      "[2022_04_20-18:11:39] Training the entire fine-tuned model...\n",
      "[2022_04_20-18:11:49] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 873ms/step - loss: 0.3574 - val_loss: 0.4364\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3553 - val_loss: 0.4363\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3698 - val_loss: 0.4359\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3570 - val_loss: 0.4355\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3556 - val_loss: 0.4351\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3563 - val_loss: 0.4350\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3557 - val_loss: 0.4345\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3546 - val_loss: 0.4346\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3483 - val_loss: 0.4346\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3482 - val_loss: 0.4337\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3572 - val_loss: 0.4334\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3498 - val_loss: 0.4333\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3493 - val_loss: 0.4330\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3439 - val_loss: 0.4328\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3557 - val_loss: 0.4325\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3495 - val_loss: 0.4324\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3451 - val_loss: 0.4324\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3430 - val_loss: 0.4323\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3492 - val_loss: 0.4315\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3428 - val_loss: 0.4317\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3434 - val_loss: 0.4311\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3275 - val_loss: 0.4310\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3444 - val_loss: 0.4311\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3393 - val_loss: 0.4311\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3342 - val_loss: 0.4307\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3409 - val_loss: 0.4300\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3344 - val_loss: 0.4298\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3468 - val_loss: 0.4304\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3470 - val_loss: 0.4304\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3329 - val_loss: 0.4294\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3346 - val_loss: 0.4293\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3379 - val_loss: 0.4293\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3300 - val_loss: 0.4291\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3292 - val_loss: 0.4293\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3308 - val_loss: 0.4295\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3309 - val_loss: 0.4291\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3352 - val_loss: 0.4291\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3284 - val_loss: 0.4289\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3286 - val_loss: 0.4289\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3218 - val_loss: 0.4287\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3291 - val_loss: 0.4288\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3315 - val_loss: 0.4288\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3245 - val_loss: 0.4288\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3259 - val_loss: 0.4288\n",
      "[2022_04_20-18:13:17] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-18:13:17] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:13:17] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 11s 842ms/step - loss: 0.3250 - val_loss: 0.4287\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>569</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>134</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  569  13\n",
       "1  134  17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1878453038674033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:bkno7htc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 14526... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▃▄▄</td></tr><tr><td>loss</td><td>█▆▄▄▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁</td></tr><tr><td>lr</td><td>██▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.42872</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32502</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42872</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">ethereal-totem-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/bkno7htc\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204a/runs/bkno7htc</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_180921-bkno7htc/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:bkno7htc). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204b/runs/1bbdh4jm\" target=\"_blank\">silver-monkey-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-18:14:12] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:14:12] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:14:12] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 596ms/step - loss: 0.8249 - val_loss: 0.6787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.6901 - val_loss: 0.4884\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5806 - val_loss: 0.6286\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5408 - val_loss: 0.5616\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4812 - val_loss: 0.4972\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4581 - val_loss: 0.4623\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4403 - val_loss: 0.4613\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4354 - val_loss: 0.4603\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4354 - val_loss: 0.4566\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4296 - val_loss: 0.4554\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4336 - val_loss: 0.4554\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4376 - val_loss: 0.4588\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4292 - val_loss: 0.4578\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4283 - val_loss: 0.4533\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4243 - val_loss: 0.4524\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4204 - val_loss: 0.4533\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4237 - val_loss: 0.4512\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4259 - val_loss: 0.4515\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4272 - val_loss: 0.4507\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4191 - val_loss: 0.4507\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4145 - val_loss: 0.4505\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4168 - val_loss: 0.4499\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4154 - val_loss: 0.4497\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4140 - val_loss: 0.4494\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4206 - val_loss: 0.4492\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4131 - val_loss: 0.4489\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4191 - val_loss: 0.4486\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4204 - val_loss: 0.4484\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4199 - val_loss: 0.4484\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4139 - val_loss: 0.4486\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4183 - val_loss: 0.4479\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4166 - val_loss: 0.4480\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4140 - val_loss: 0.4474\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4082 - val_loss: 0.4472\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4134 - val_loss: 0.4472\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4101 - val_loss: 0.4465\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4136 - val_loss: 0.4463\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4170 - val_loss: 0.4461\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4113 - val_loss: 0.4464\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4153 - val_loss: 0.4457\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4113 - val_loss: 0.4456\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4127 - val_loss: 0.4462\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4068 - val_loss: 0.4452\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4103 - val_loss: 0.4456\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4092 - val_loss: 0.4448\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4084 - val_loss: 0.4444\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4160 - val_loss: 0.4445\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4094 - val_loss: 0.4443\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4044 - val_loss: 0.4440\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4106 - val_loss: 0.4448\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4028 - val_loss: 0.4432\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4014 - val_loss: 0.4430\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4030 - val_loss: 0.4428\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4021 - val_loss: 0.4426\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4035 - val_loss: 0.4431\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4042 - val_loss: 0.4423\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4046 - val_loss: 0.4421\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4015 - val_loss: 0.4418\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4027 - val_loss: 0.4429\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3989 - val_loss: 0.4415\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3958 - val_loss: 0.4418\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4022 - val_loss: 0.4410\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4032 - val_loss: 0.4408\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3959 - val_loss: 0.4406\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3941 - val_loss: 0.4404\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4029 - val_loss: 0.4402\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3956 - val_loss: 0.4403\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3986 - val_loss: 0.4400\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3989 - val_loss: 0.4397\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3969 - val_loss: 0.4395\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3990 - val_loss: 0.4398\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4015 - val_loss: 0.4393\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3973 - val_loss: 0.4390\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3930 - val_loss: 0.4390\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3877 - val_loss: 0.4386\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3975 - val_loss: 0.4386\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3887 - val_loss: 0.4383\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3910 - val_loss: 0.4381\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3971 - val_loss: 0.4380\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3895 - val_loss: 0.4379\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3908 - val_loss: 0.4379\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3900 - val_loss: 0.4374\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3943 - val_loss: 0.4374\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3974 - val_loss: 0.4375\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3921 - val_loss: 0.4371\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3916 - val_loss: 0.4369\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3884 - val_loss: 0.4368\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3928 - val_loss: 0.4370\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3903 - val_loss: 0.4367\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3875 - val_loss: 0.4366\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3948 - val_loss: 0.4362\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3869 - val_loss: 0.4364\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3866 - val_loss: 0.4366\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3863 - val_loss: 0.4357\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3947 - val_loss: 0.4359\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3871 - val_loss: 0.4354\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3812 - val_loss: 0.4360\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3837 - val_loss: 0.4355\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3938 - val_loss: 0.4358\n",
      "\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3927 - val_loss: 0.4353\n",
      "[2022_04_20-18:16:17] Training the entire fine-tuned model...\n",
      "[2022_04_20-18:16:24] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3838WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1070s vs `on_train_batch_end` time: 0.1427s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1070s vs `on_train_batch_end` time: 0.1427s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 711ms/step - loss: 0.3838 - val_loss: 0.4347\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3777 - val_loss: 0.4347\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3850 - val_loss: 0.4345\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3759 - val_loss: 0.4352\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3836 - val_loss: 0.4346\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3856 - val_loss: 0.4343\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3830 - val_loss: 0.4348\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3840 - val_loss: 0.4346\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 342ms/step - loss: 0.3748 - val_loss: 0.4344\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3809 - val_loss: 0.4344\n",
      "[2022_04_20-18:16:54] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-18:16:54] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:16:54] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3820WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1072s vs `on_train_batch_end` time: 0.1410s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1072s vs `on_train_batch_end` time: 0.1410s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 690ms/step - loss: 0.3820 - val_loss: 0.4343\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>460</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  460   6\n",
       "1  103  16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22695035460992907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_20_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_20_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1bbdh4jm) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 15371... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███▁▁▁▁</td></tr><tr><td>loss</td><td>█▄▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.43432</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.38201</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43432</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">silver-monkey-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204b/runs/1bbdh4jm\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204b/runs/1bbdh4jm</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_181356-1bbdh4jm/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1bbdh4jm). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/2vcy0kef\" target=\"_blank\">fearless-breeze-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-18:17:48] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:17:48] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:17:48] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 726ms/step - loss: 0.8773 - val_loss: 0.6502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.5896 - val_loss: 0.6047\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4943 - val_loss: 0.5098\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.5030 - val_loss: 0.4722\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4535 - val_loss: 0.4866\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4575 - val_loss: 0.4842\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4372 - val_loss: 0.4828\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4283 - val_loss: 0.4539\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4191 - val_loss: 0.4543\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4212 - val_loss: 0.4604\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4100 - val_loss: 0.4520\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4154 - val_loss: 0.4515\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4041 - val_loss: 0.4541\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4010 - val_loss: 0.4498\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4037 - val_loss: 0.4491\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4059 - val_loss: 0.4520\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4005 - val_loss: 0.4490\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3935 - val_loss: 0.4472\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3937 - val_loss: 0.4495\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3914 - val_loss: 0.4460\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4028 - val_loss: 0.4466\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3889 - val_loss: 0.4504\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3778 - val_loss: 0.4448\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3918 - val_loss: 0.4437\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4019 - val_loss: 0.4461\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3806 - val_loss: 0.4503\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3788 - val_loss: 0.4468\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3847 - val_loss: 0.4431\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3828 - val_loss: 0.4428\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3839 - val_loss: 0.4437\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3830 - val_loss: 0.4419\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3805 - val_loss: 0.4421\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3828 - val_loss: 0.4420\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3770 - val_loss: 0.4414\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3750 - val_loss: 0.4419\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3735 - val_loss: 0.4414\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3807 - val_loss: 0.4414\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3820 - val_loss: 0.4413\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3667 - val_loss: 0.4412\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3710 - val_loss: 0.4410\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3712 - val_loss: 0.4409\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3739 - val_loss: 0.4409\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3713 - val_loss: 0.4409\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3726 - val_loss: 0.4409\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3765 - val_loss: 0.4408\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3715 - val_loss: 0.4408\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3851 - val_loss: 0.4408\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3675 - val_loss: 0.4408\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3656 - val_loss: 0.4408\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3798 - val_loss: 0.4408\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3787 - val_loss: 0.4408\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3764 - val_loss: 0.4408\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3725 - val_loss: 0.4408\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3757 - val_loss: 0.4408\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3757 - val_loss: 0.4408\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3729 - val_loss: 0.4408\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3734 - val_loss: 0.4408\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3631 - val_loss: 0.4408\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3714 - val_loss: 0.4408\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3757 - val_loss: 0.4408\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3771 - val_loss: 0.4408\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3721 - val_loss: 0.4408\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3696 - val_loss: 0.4408\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3800 - val_loss: 0.4408\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3700 - val_loss: 0.4408\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3717 - val_loss: 0.4408\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3665 - val_loss: 0.4408\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3868 - val_loss: 0.4408\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3719 - val_loss: 0.4408\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3669 - val_loss: 0.4408\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3684 - val_loss: 0.4408\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3683 - val_loss: 0.4408\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3733 - val_loss: 0.4408\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3774 - val_loss: 0.4408\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3745 - val_loss: 0.4408\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3738 - val_loss: 0.4408\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3710 - val_loss: 0.4408\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3783 - val_loss: 0.4408\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3731 - val_loss: 0.4408\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3750 - val_loss: 0.4408\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3704 - val_loss: 0.4408\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3713 - val_loss: 0.4408\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3689 - val_loss: 0.4408\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3664 - val_loss: 0.4408\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3709 - val_loss: 0.4408\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3766 - val_loss: 0.4408\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3661 - val_loss: 0.4408\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3796 - val_loss: 0.4408\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3713 - val_loss: 0.4408\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3728 - val_loss: 0.4408\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3646 - val_loss: 0.4408\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3801 - val_loss: 0.4408\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3664 - val_loss: 0.4408\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3707 - val_loss: 0.4408\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3733 - val_loss: 0.4408\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3740 - val_loss: 0.4408\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3691 - val_loss: 0.4408\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3704 - val_loss: 0.4408\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3746 - val_loss: 0.4408\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3774 - val_loss: 0.4408\n",
      "[2022_04_20-18:20:18] Training the entire fine-tuned model...\n",
      "[2022_04_20-18:20:28] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 829ms/step - loss: 0.3807 - val_loss: 0.4398\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3782 - val_loss: 0.4405\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3752 - val_loss: 0.4409\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3684 - val_loss: 0.4416\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3662 - val_loss: 0.4388\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3608 - val_loss: 0.4371\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3608 - val_loss: 0.4382\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3554 - val_loss: 0.4370\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3546 - val_loss: 0.4364\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3529 - val_loss: 0.4367\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3498 - val_loss: 0.4359\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3526 - val_loss: 0.4352\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3514 - val_loss: 0.4350\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3508 - val_loss: 0.4347\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3563 - val_loss: 0.4344\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3543 - val_loss: 0.4337\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3546 - val_loss: 0.4334\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3516 - val_loss: 0.4331\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3453 - val_loss: 0.4330\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3468 - val_loss: 0.4327\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3447 - val_loss: 0.4325\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3453 - val_loss: 0.4324\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3532 - val_loss: 0.4322\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3357 - val_loss: 0.4321\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3429 - val_loss: 0.4318\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3421 - val_loss: 0.4319\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3350 - val_loss: 0.4314\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3359 - val_loss: 0.4312\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3326 - val_loss: 0.4309\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3308 - val_loss: 0.4325\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3464 - val_loss: 0.4315\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3384 - val_loss: 0.4304\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3313 - val_loss: 0.4303\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3244 - val_loss: 0.4298\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3355 - val_loss: 0.4306\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3263 - val_loss: 0.4302\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3221 - val_loss: 0.4287\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3204 - val_loss: 0.4282\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3170 - val_loss: 0.4279\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3259 - val_loss: 0.4287\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3214 - val_loss: 0.4289\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3231 - val_loss: 0.4278\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3109 - val_loss: 0.4281\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3186 - val_loss: 0.4277\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3040 - val_loss: 0.4275\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3074 - val_loss: 0.4284\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.2985 - val_loss: 0.4285\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3084 - val_loss: 0.4269\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3017 - val_loss: 0.4281\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3138 - val_loss: 0.4268\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 2s 394ms/step - loss: 0.3049 - val_loss: 0.4275\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3081 - val_loss: 0.4277\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2912 - val_loss: 0.4278\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.2975 - val_loss: 0.4273\n",
      "[2022_04_20-18:22:14] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-18:22:14] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:22:14] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 850ms/step - loss: 0.2940 - val_loss: 0.4267\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>567</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>129</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  567  15\n",
       "1  129  22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23404255319148937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2vcy0kef) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 16036... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▄▄▄▅▅</td></tr><tr><td>loss</td><td>█▆▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>lr</td><td>██▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.42665</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.29397</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42665</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fearless-breeze-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/2vcy0kef\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204a/runs/2vcy0kef</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_181730-2vcy0kef/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2vcy0kef). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204b/runs/3mgcn3z5\" target=\"_blank\">treasured-waterfall-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-18:23:53] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:23:53] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:23:53] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 578ms/step - loss: 0.8793 - val_loss: 0.8221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.6797 - val_loss: 0.5246\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.5388 - val_loss: 0.5454\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5118 - val_loss: 0.5230\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4909 - val_loss: 0.4769\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4429 - val_loss: 0.4566\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4359 - val_loss: 0.4585\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4240 - val_loss: 0.4504\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4193 - val_loss: 0.4537\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4160 - val_loss: 0.4437\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4157 - val_loss: 0.4418\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3994 - val_loss: 0.4389\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4059 - val_loss: 0.4475\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3871 - val_loss: 0.4481\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3935 - val_loss: 0.4399\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3877 - val_loss: 0.4349\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3865 - val_loss: 0.4341\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3850 - val_loss: 0.4343\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 0.3854 - val_loss: 0.4361\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3791 - val_loss: 0.4342\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3854 - val_loss: 0.4333\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3783 - val_loss: 0.4339\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3734 - val_loss: 0.4330\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3851 - val_loss: 0.4330\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3764 - val_loss: 0.4329\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3831 - val_loss: 0.4329\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3851 - val_loss: 0.4335\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3777 - val_loss: 0.4328\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3756 - val_loss: 0.4327\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3816 - val_loss: 0.4328\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3797 - val_loss: 0.4329\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3786 - val_loss: 0.4327\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3803 - val_loss: 0.4327\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3815 - val_loss: 0.4327\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3833 - val_loss: 0.4327\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3822 - val_loss: 0.4327\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3777 - val_loss: 0.4327\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3806 - val_loss: 0.4327\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3813 - val_loss: 0.4327\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3772 - val_loss: 0.4327\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3834 - val_loss: 0.4327\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3842 - val_loss: 0.4327\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3807 - val_loss: 0.4327\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3882 - val_loss: 0.4327\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3811 - val_loss: 0.4327\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3801 - val_loss: 0.4327\n",
      "[2022_04_20-18:24:56] Training the entire fine-tuned model...\n",
      "[2022_04_20-18:25:04] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3744WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1085s vs `on_train_batch_end` time: 0.1403s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1085s vs `on_train_batch_end` time: 0.1403s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 706ms/step - loss: 0.3744 - val_loss: 0.4435\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3750 - val_loss: 0.4393\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3725 - val_loss: 0.4348\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3812 - val_loss: 0.4320\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3746 - val_loss: 0.4319\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3736 - val_loss: 0.4318\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 341ms/step - loss: 0.3655 - val_loss: 0.4326\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3542 - val_loss: 0.4345\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3533 - val_loss: 0.4353\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3637 - val_loss: 0.4352\n",
      "[2022_04_20-18:25:32] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-18:25:32] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:25:32] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3609WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1119s vs `on_train_batch_end` time: 0.1389s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1119s vs `on_train_batch_end` time: 0.1389s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 694ms/step - loss: 0.3609 - val_loss: 0.4319\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>452</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  452  14\n",
       "1   95  24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3057324840764331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_20_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_20_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3mgcn3z5) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 16951... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▁</td></tr><tr><td>loss</td><td>█▅▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.43177</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3609</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43192</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">treasured-waterfall-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204b/runs/3mgcn3z5\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204b/runs/3mgcn3z5</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_182333-3mgcn3z5/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3mgcn3z5). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/3k22tv6t\" target=\"_blank\">dry-wave-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-18:26:27] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:26:28] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:26:28] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 718ms/step - loss: 0.9220 - val_loss: 0.6724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.6703 - val_loss: 0.7696\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.6203 - val_loss: 0.6283\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.5386 - val_loss: 0.5634\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4905 - val_loss: 0.5263\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4533 - val_loss: 0.5387\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4848 - val_loss: 0.4810\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.4221 - val_loss: 0.5184\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 228ms/step - loss: 0.4472 - val_loss: 0.5041\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.4396 - val_loss: 0.4784\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.4237 - val_loss: 0.4468\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 230ms/step - loss: 0.4042 - val_loss: 0.4437\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.3905 - val_loss: 0.4424\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 226ms/step - loss: 0.3814 - val_loss: 0.4429\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.3819 - val_loss: 0.4440\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.3745 - val_loss: 0.4472\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 229ms/step - loss: 0.3825 - val_loss: 0.4396\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 228ms/step - loss: 0.3701 - val_loss: 0.4449\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 229ms/step - loss: 0.3670 - val_loss: 0.4414\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.3758 - val_loss: 0.4395\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 225ms/step - loss: 0.3669 - val_loss: 0.4387\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 225ms/step - loss: 0.3689 - val_loss: 0.4387\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 227ms/step - loss: 0.3638 - val_loss: 0.4386\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 226ms/step - loss: 0.3643 - val_loss: 0.4384\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 229ms/step - loss: 0.3719 - val_loss: 0.4384\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 226ms/step - loss: 0.3628 - val_loss: 0.4390\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 228ms/step - loss: 0.3628 - val_loss: 0.4387\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.3602 - val_loss: 0.4384\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 227ms/step - loss: 0.3597 - val_loss: 0.4382\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 225ms/step - loss: 0.3664 - val_loss: 0.4381\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 229ms/step - loss: 0.3643 - val_loss: 0.4381\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.3614 - val_loss: 0.4380\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.3666 - val_loss: 0.4381\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 227ms/step - loss: 0.3618 - val_loss: 0.4380\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 228ms/step - loss: 0.3682 - val_loss: 0.4380\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 226ms/step - loss: 0.3667 - val_loss: 0.4380\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 229ms/step - loss: 0.3666 - val_loss: 0.4380\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.3725 - val_loss: 0.4380\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 225ms/step - loss: 0.3565 - val_loss: 0.4380\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3700 - val_loss: 0.4380\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3637 - val_loss: 0.4380\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3721 - val_loss: 0.4380\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3661 - val_loss: 0.4380\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3551 - val_loss: 0.4380\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3666 - val_loss: 0.4380\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3783 - val_loss: 0.4380\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3633 - val_loss: 0.4380\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3652 - val_loss: 0.4380\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3628 - val_loss: 0.4380\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3569 - val_loss: 0.4380\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3713 - val_loss: 0.4380\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3708 - val_loss: 0.4380\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3689 - val_loss: 0.4380\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3630 - val_loss: 0.4380\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3654 - val_loss: 0.4380\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3732 - val_loss: 0.4380\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3638 - val_loss: 0.4380\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3649 - val_loss: 0.4380\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3572 - val_loss: 0.4380\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3651 - val_loss: 0.4380\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3616 - val_loss: 0.4380\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3639 - val_loss: 0.4380\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3706 - val_loss: 0.4380\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3623 - val_loss: 0.4380\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3613 - val_loss: 0.4380\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3660 - val_loss: 0.4380\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3715 - val_loss: 0.4380\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3674 - val_loss: 0.4380\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3629 - val_loss: 0.4380\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3615 - val_loss: 0.4380\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3665 - val_loss: 0.4380\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3622 - val_loss: 0.4380\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3625 - val_loss: 0.4380\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3672 - val_loss: 0.4380\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3571 - val_loss: 0.4380\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3639 - val_loss: 0.4380\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3689 - val_loss: 0.4380\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3755 - val_loss: 0.4380\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3754 - val_loss: 0.4380\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3668 - val_loss: 0.4380\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.3684 - val_loss: 0.4380\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3606 - val_loss: 0.4380\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3647 - val_loss: 0.4380\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3680 - val_loss: 0.4380\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3632 - val_loss: 0.4380\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3608 - val_loss: 0.4380\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3636 - val_loss: 0.4380\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3713 - val_loss: 0.4380\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3675 - val_loss: 0.4380\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3621 - val_loss: 0.4380\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 1s 245ms/step - loss: 0.3631 - val_loss: 0.4380\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3670 - val_loss: 0.4380\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3652 - val_loss: 0.4380\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3623 - val_loss: 0.4380\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3679 - val_loss: 0.4380\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3727 - val_loss: 0.4380\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3722 - val_loss: 0.4380\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3726 - val_loss: 0.4380\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3632 - val_loss: 0.4380\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3573 - val_loss: 0.4380\n",
      "[2022_04_20-18:28:58] Training the entire fine-tuned model...\n",
      "[2022_04_20-18:29:06] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 840ms/step - loss: 0.3611 - val_loss: 0.4375\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3585 - val_loss: 0.4418\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3472 - val_loss: 0.4426\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 372ms/step - loss: 0.3609 - val_loss: 0.4332\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 372ms/step - loss: 0.3389 - val_loss: 0.4300\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3301 - val_loss: 0.4295\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3236 - val_loss: 0.4319\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3111 - val_loss: 0.4358\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3079 - val_loss: 0.4353\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3020 - val_loss: 0.4307\n",
      "[2022_04_20-18:29:32] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-18:29:32] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:29:32] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 829ms/step - loss: 0.3297 - val_loss: 0.4298\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>574</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>136</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  574   8\n",
       "1  136  15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1724137931034483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3k22tv6t) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 17332... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███▁▁▁▁</td></tr><tr><td>loss</td><td>█▅▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>lr</td><td>██████▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▄▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.42946</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32975</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.42979</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dry-wave-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/3k22tv6t\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204a/runs/3k22tv6t</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_182610-3k22tv6t/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3k22tv6t). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204b/runs/2dga7e0e\" target=\"_blank\">cool-sea-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-18:30:30] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:30:31] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:30:31] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 571ms/step - loss: 0.9331 - val_loss: 0.9219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.7319 - val_loss: 0.6336\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5568 - val_loss: 0.4729\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4950 - val_loss: 0.4791\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4688 - val_loss: 0.4809\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4490 - val_loss: 0.4548\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4509 - val_loss: 0.4727\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4365 - val_loss: 0.4545\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4457 - val_loss: 0.4922\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4431 - val_loss: 0.5363\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4551 - val_loss: 0.4415\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4167 - val_loss: 0.4560\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4194 - val_loss: 0.4399\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4079 - val_loss: 0.4398\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3937 - val_loss: 0.4363\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3897 - val_loss: 0.4381\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3978 - val_loss: 0.4343\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3876 - val_loss: 0.4332\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3774 - val_loss: 0.4362\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3860 - val_loss: 0.4422\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3730 - val_loss: 0.4303\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3700 - val_loss: 0.4314\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3644 - val_loss: 0.4307\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3665 - val_loss: 0.4573\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3724 - val_loss: 0.4386\n",
      "[2022_04_20-18:31:08] Training the entire fine-tuned model...\n",
      "[2022_04_20-18:31:28] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3968WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1075s vs `on_train_batch_end` time: 0.1406s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1075s vs `on_train_batch_end` time: 0.1406s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 1s/step - loss: 0.3968 - val_loss: 0.4410\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3720 - val_loss: 0.4317\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3679 - val_loss: 0.4320\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3596 - val_loss: 0.4306\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3535 - val_loss: 0.4338\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3388 - val_loss: 0.4404\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3496 - val_loss: 0.4500\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3437 - val_loss: 0.4400\n",
      "[2022_04_20-18:31:53] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-18:31:53] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:31:53] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3496WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1089s vs `on_train_batch_end` time: 0.1383s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1089s vs `on_train_batch_end` time: 0.1383s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 687ms/step - loss: 0.3496 - val_loss: 0.4313\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>457</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  457   9\n",
       "1   94  25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32679738562091504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_20_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_20_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2dga7e0e) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 18009... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▆▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████████████▃▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▂▂▁▂▁▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>20</td></tr><tr><td>best_val_loss</td><td>0.43035</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34956</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.43125</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">cool-sea-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204b/runs/2dga7e0e\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204b/runs/2dga7e0e</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_183012-2dga7e0e/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2dga7e0e). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/mwxv3vqc\" target=\"_blank\">glorious-totem-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-18:32:52] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:32:52] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:32:52] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 775ms/step - loss: 0.9740 - val_loss: 0.5321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.6769 - val_loss: 0.7702\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.6401 - val_loss: 0.5767\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.5492 - val_loss: 0.5615\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.5163 - val_loss: 0.4789\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4610 - val_loss: 0.4895\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4606 - val_loss: 0.4643\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4654 - val_loss: 0.4852\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4618 - val_loss: 0.4638\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4437 - val_loss: 0.4610\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4333 - val_loss: 0.4640\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4380 - val_loss: 0.4582\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4317 - val_loss: 0.4574\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.4290 - val_loss: 0.4568\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4232 - val_loss: 0.4554\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4263 - val_loss: 0.4545\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4283 - val_loss: 0.4568\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4242 - val_loss: 0.4528\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4156 - val_loss: 0.4525\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4148 - val_loss: 0.4529\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4132 - val_loss: 0.4503\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4005 - val_loss: 0.4497\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4098 - val_loss: 0.4486\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4001 - val_loss: 0.4485\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4076 - val_loss: 0.4469\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4033 - val_loss: 0.4473\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4008 - val_loss: 0.4518\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4108 - val_loss: 0.4492\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4018 - val_loss: 0.4457\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4002 - val_loss: 0.4462\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4033 - val_loss: 0.4468\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3917 - val_loss: 0.4448\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.3946 - val_loss: 0.4445\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3928 - val_loss: 0.4446\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3986 - val_loss: 0.4449\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3947 - val_loss: 0.4441\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3863 - val_loss: 0.4441\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3915 - val_loss: 0.4442\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3893 - val_loss: 0.4439\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3967 - val_loss: 0.4437\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3967 - val_loss: 0.4438\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3906 - val_loss: 0.4434\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3958 - val_loss: 0.4433\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3874 - val_loss: 0.4431\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3932 - val_loss: 0.4429\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3832 - val_loss: 0.4428\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3862 - val_loss: 0.4426\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3887 - val_loss: 0.4425\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3892 - val_loss: 0.4424\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3895 - val_loss: 0.4423\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3814 - val_loss: 0.4422\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3874 - val_loss: 0.4420\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3872 - val_loss: 0.4425\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3891 - val_loss: 0.4416\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3801 - val_loss: 0.4416\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3815 - val_loss: 0.4415\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3860 - val_loss: 0.4414\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3801 - val_loss: 0.4413\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3850 - val_loss: 0.4414\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3772 - val_loss: 0.4411\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3835 - val_loss: 0.4414\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3840 - val_loss: 0.4407\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3829 - val_loss: 0.4408\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3853 - val_loss: 0.4406\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3866 - val_loss: 0.4406\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3834 - val_loss: 0.4401\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3744 - val_loss: 0.4402\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3759 - val_loss: 0.4398\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3705 - val_loss: 0.4401\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3761 - val_loss: 0.4396\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3747 - val_loss: 0.4396\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3770 - val_loss: 0.4393\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3759 - val_loss: 0.4392\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3690 - val_loss: 0.4392\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3746 - val_loss: 0.4392\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3677 - val_loss: 0.4390\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3724 - val_loss: 0.4394\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3757 - val_loss: 0.4388\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3738 - val_loss: 0.4396\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3782 - val_loss: 0.4391\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3687 - val_loss: 0.4386\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3717 - val_loss: 0.4385\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3724 - val_loss: 0.4383\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3732 - val_loss: 0.4381\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 1s 258ms/step - loss: 0.3694 - val_loss: 0.4392\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3785 - val_loss: 0.4382\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3676 - val_loss: 0.4380\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3656 - val_loss: 0.4378\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3723 - val_loss: 0.4377\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3735 - val_loss: 0.4378\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3722 - val_loss: 0.4394\n",
      "\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3695 - val_loss: 0.4387\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3735 - val_loss: 0.4377\n",
      "[2022_04_20-18:34:46] Training the entire fine-tuned model...\n",
      "[2022_04_20-18:35:32] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 824ms/step - loss: 0.4824 - val_loss: 0.4434\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 371ms/step - loss: 0.4215 - val_loss: 0.4459\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 372ms/step - loss: 0.3789 - val_loss: 0.4442\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 372ms/step - loss: 0.3749 - val_loss: 0.4440\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3531 - val_loss: 0.4392\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3532 - val_loss: 0.4426\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3531 - val_loss: 0.4364\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3493 - val_loss: 0.4356\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3468 - val_loss: 0.4345\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3365 - val_loss: 0.4336\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3355 - val_loss: 0.4344\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3312 - val_loss: 0.4321\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3228 - val_loss: 0.4328\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3070 - val_loss: 0.4315\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3173 - val_loss: 0.4328\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3067 - val_loss: 0.4328\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2938 - val_loss: 0.4313\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.2819 - val_loss: 0.4345\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2713 - val_loss: 0.4355\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.2673 - val_loss: 0.4361\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2553 - val_loss: 0.4359\n",
      "[2022_04_20-18:36:18] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-18:36:18] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:36:19] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 843ms/step - loss: 0.2860 - val_loss: 0.4327\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>557</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>124</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  557  25\n",
       "1  124  27"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2660098522167488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:mwxv3vqc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 18305... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▁▁▁</td></tr><tr><td>lr</td><td>██▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▆█▄▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>16</td></tr><tr><td>best_val_loss</td><td>0.43126</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.28599</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.43266</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">glorious-totem-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/mwxv3vqc\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204a/runs/mwxv3vqc</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_183232-mwxv3vqc/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:mwxv3vqc). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204b/runs/17yhmqaz\" target=\"_blank\">dauntless-river-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-18:37:14] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:37:14] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:37:14] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 616ms/step - loss: 0.9170 - val_loss: 0.9439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.6929 - val_loss: 0.6421\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5884 - val_loss: 0.4901\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.5361 - val_loss: 0.4876\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4896 - val_loss: 0.5109\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4746 - val_loss: 0.5335\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4769 - val_loss: 0.4585\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4383 - val_loss: 0.4657\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4438 - val_loss: 0.4763\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4303 - val_loss: 0.4455\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4202 - val_loss: 0.4520\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4195 - val_loss: 0.4502\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4106 - val_loss: 0.4785\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4044 - val_loss: 0.4455\n",
      "[2022_04_20-18:37:41] Training the entire fine-tuned model...\n",
      "[2022_04_20-18:37:52] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4592WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1070s vs `on_train_batch_end` time: 0.1391s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1070s vs `on_train_batch_end` time: 0.1391s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 704ms/step - loss: 0.4592 - val_loss: 0.4547\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.4157 - val_loss: 0.4436\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.4061 - val_loss: 0.4426\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3880 - val_loss: 0.4812\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3825 - val_loss: 0.4452\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3558 - val_loss: 0.4487\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3279 - val_loss: 0.4504\n",
      "[2022_04_20-18:38:14] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-18:38:14] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:38:18] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3874WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1067s vs `on_train_batch_end` time: 0.1406s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1067s vs `on_train_batch_end` time: 0.1406s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 695ms/step - loss: 0.3874 - val_loss: 0.4396\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>460</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  460   6\n",
       "1  108  11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16176470588235295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_20_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_20_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:17yhmqaz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 18994... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▄▅▅▆▆▇▇█▁▂▂▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▂▂▂▂▂▂▂▃▂▂▂▂▁▁▂</td></tr><tr><td>lr</td><td>█████████████▃▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▂▂▂▁▁▂▁▁▁▂▁▁▁▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.43956</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.38739</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.43956</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dauntless-river-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204b/runs/17yhmqaz\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204b/runs/17yhmqaz</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_183656-17yhmqaz/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:17yhmqaz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/3dczxsxs\" target=\"_blank\">tough-armadillo-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-18:39:12] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:39:12] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:39:12] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 697ms/step - loss: 0.7600 - val_loss: 0.6928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.5511 - val_loss: 0.5230\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4888 - val_loss: 0.4813\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4620 - val_loss: 0.4895\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4566 - val_loss: 0.4601\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4231 - val_loss: 0.4637\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4327 - val_loss: 0.4633\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4154 - val_loss: 0.4494\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4111 - val_loss: 0.4464\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3888 - val_loss: 0.4472\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3828 - val_loss: 0.4595\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3909 - val_loss: 0.4424\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3756 - val_loss: 0.4894\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4283 - val_loss: 0.4423\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3758 - val_loss: 0.4826\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.4046 - val_loss: 0.4369\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3738 - val_loss: 0.4430\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3765 - val_loss: 0.4487\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3703 - val_loss: 0.4383\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3578 - val_loss: 0.4369\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3632 - val_loss: 0.4354\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3615 - val_loss: 0.4361\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3502 - val_loss: 0.4351\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3585 - val_loss: 0.4351\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3501 - val_loss: 0.4349\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3573 - val_loss: 0.4349\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3576 - val_loss: 0.4351\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3533 - val_loss: 0.4349\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3544 - val_loss: 0.4348\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3579 - val_loss: 0.4349\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3496 - val_loss: 0.4347\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3502 - val_loss: 0.4347\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3534 - val_loss: 0.4346\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3498 - val_loss: 0.4346\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3610 - val_loss: 0.4346\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3508 - val_loss: 0.4346\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3532 - val_loss: 0.4346\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3489 - val_loss: 0.4346\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3486 - val_loss: 0.4346\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3541 - val_loss: 0.4346\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3511 - val_loss: 0.4346\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3572 - val_loss: 0.4346\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3590 - val_loss: 0.4346\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3569 - val_loss: 0.4346\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3565 - val_loss: 0.4346\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3573 - val_loss: 0.4346\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3445 - val_loss: 0.4346\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3529 - val_loss: 0.4346\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3520 - val_loss: 0.4346\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3497 - val_loss: 0.4346\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3544 - val_loss: 0.4346\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3503 - val_loss: 0.4346\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3530 - val_loss: 0.4346\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3635 - val_loss: 0.4346\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3447 - val_loss: 0.4346\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3597 - val_loss: 0.4346\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3756 - val_loss: 0.4346\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3570 - val_loss: 0.4346\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3533 - val_loss: 0.4346\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3492 - val_loss: 0.4346\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3505 - val_loss: 0.4346\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3532 - val_loss: 0.4346\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3509 - val_loss: 0.4346\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3509 - val_loss: 0.4346\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3583 - val_loss: 0.4346\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3586 - val_loss: 0.4346\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3474 - val_loss: 0.4346\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3568 - val_loss: 0.4346\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3579 - val_loss: 0.4346\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3627 - val_loss: 0.4346\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3594 - val_loss: 0.4346\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3558 - val_loss: 0.4346\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3558 - val_loss: 0.4346\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3525 - val_loss: 0.4346\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3589 - val_loss: 0.4346\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3551 - val_loss: 0.4346\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3592 - val_loss: 0.4346\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3471 - val_loss: 0.4346\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3645 - val_loss: 0.4346\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3479 - val_loss: 0.4346\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3543 - val_loss: 0.4346\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3563 - val_loss: 0.4346\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3510 - val_loss: 0.4346\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3496 - val_loss: 0.4346\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3660 - val_loss: 0.4346\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3634 - val_loss: 0.4346\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3643 - val_loss: 0.4346\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3563 - val_loss: 0.4346\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3606 - val_loss: 0.4346\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3534 - val_loss: 0.4346\n",
      "[2022_04_20-18:41:01] Training the entire fine-tuned model...\n",
      "[2022_04_20-18:41:09] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 830ms/step - loss: 0.3557 - val_loss: 0.4346\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3529 - val_loss: 0.4345\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3511 - val_loss: 0.4344\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3581 - val_loss: 0.4341\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3560 - val_loss: 0.4338\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3529 - val_loss: 0.4337\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3447 - val_loss: 0.4336\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3474 - val_loss: 0.4332\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3557 - val_loss: 0.4328\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3521 - val_loss: 0.4328\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3539 - val_loss: 0.4330\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3399 - val_loss: 0.4328\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3571 - val_loss: 0.4326\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3439 - val_loss: 0.4324\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3389 - val_loss: 0.4322\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3553 - val_loss: 0.4322\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3489 - val_loss: 0.4321\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3416 - val_loss: 0.4321\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3497 - val_loss: 0.4321\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3430 - val_loss: 0.4320\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3417 - val_loss: 0.4319\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3437 - val_loss: 0.4319\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3425 - val_loss: 0.4318\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3400 - val_loss: 0.4317\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3441 - val_loss: 0.4317\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3353 - val_loss: 0.4317\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3500 - val_loss: 0.4316\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3467 - val_loss: 0.4316\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3415 - val_loss: 0.4315\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3516 - val_loss: 0.4315\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3575 - val_loss: 0.4314\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3395 - val_loss: 0.4314\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3386 - val_loss: 0.4313\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3389 - val_loss: 0.4313\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3439 - val_loss: 0.4312\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3328 - val_loss: 0.4311\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3353 - val_loss: 0.4310\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3418 - val_loss: 0.4310\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3427 - val_loss: 0.4309\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3404 - val_loss: 0.4308\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3442 - val_loss: 0.4308\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3426 - val_loss: 0.4307\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3329 - val_loss: 0.4307\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3389 - val_loss: 0.4306\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3431 - val_loss: 0.4306\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3500 - val_loss: 0.4306\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3501 - val_loss: 0.4306\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3258 - val_loss: 0.4306\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3452 - val_loss: 0.4306\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3329 - val_loss: 0.4305\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3392 - val_loss: 0.4305\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3399 - val_loss: 0.4305\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3360 - val_loss: 0.4305\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3418 - val_loss: 0.4305\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3403 - val_loss: 0.4305\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3476 - val_loss: 0.4305\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3505 - val_loss: 0.4305\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3373 - val_loss: 0.4305\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3357 - val_loss: 0.4305\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3352 - val_loss: 0.4305\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3300 - val_loss: 0.4305\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3368 - val_loss: 0.4305\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3396 - val_loss: 0.4305\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3522 - val_loss: 0.4305\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3305 - val_loss: 0.4305\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3527 - val_loss: 0.4305\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3426 - val_loss: 0.4305\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3364 - val_loss: 0.4305\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3372 - val_loss: 0.4305\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3268 - val_loss: 0.4305\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3415 - val_loss: 0.4305\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3437 - val_loss: 0.4305\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3412 - val_loss: 0.4305\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3384 - val_loss: 0.4305\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3286 - val_loss: 0.4305\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3290 - val_loss: 0.4305\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3387 - val_loss: 0.4305\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3506 - val_loss: 0.4305\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3420 - val_loss: 0.4305\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.3471 - val_loss: 0.4304\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3286 - val_loss: 0.4304\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3378 - val_loss: 0.4304\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3287 - val_loss: 0.4304\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3352 - val_loss: 0.4304\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3362 - val_loss: 0.4304\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3248 - val_loss: 0.4304\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3315 - val_loss: 0.4304\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3404 - val_loss: 0.4304\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3177 - val_loss: 0.4304\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3379 - val_loss: 0.4304\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3300 - val_loss: 0.4304\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3370 - val_loss: 0.4304\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3303 - val_loss: 0.4304\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3372 - val_loss: 0.4304\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3364 - val_loss: 0.4304\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3452 - val_loss: 0.4304\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3503 - val_loss: 0.4304\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3427 - val_loss: 0.4304\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3431 - val_loss: 0.4304\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3404 - val_loss: 0.4304\n",
      "[2022_04_20-18:44:37] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-18:44:37] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:44:37] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 862ms/step - loss: 0.3313 - val_loss: 0.4304\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>134</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  570  12\n",
       "1  134  17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18888888888888888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3dczxsxs) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 19202... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▁▁▂▂▂▃▃▃▄▄▄▅▅▆▆▆▇▇▇██</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▂▁▂▁▂▁▁▂▁▁▁▁</td></tr><tr><td>lr</td><td>███▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.4304</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33134</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.4304</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">tough-armadillo-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/3dczxsxs\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204a/runs/3dczxsxs</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_183855-3dczxsxs/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3dczxsxs). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204b/runs/68w2s7vy\" target=\"_blank\">resilient-plant-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-18:45:35] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:45:35] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:45:35] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 563ms/step - loss: 1.0494 - val_loss: 0.5799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.6602 - val_loss: 0.7477\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.6049 - val_loss: 0.6440\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5435 - val_loss: 0.5322\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4956 - val_loss: 0.4636\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4884 - val_loss: 0.4842\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4486 - val_loss: 0.4818\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4456 - val_loss: 0.4531\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4280 - val_loss: 0.4537\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4241 - val_loss: 0.4504\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4226 - val_loss: 0.4484\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4142 - val_loss: 0.4474\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4052 - val_loss: 0.4394\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3987 - val_loss: 0.4600\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4056 - val_loss: 0.4633\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4004 - val_loss: 0.4371\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3950 - val_loss: 0.4559\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3843 - val_loss: 0.4652\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3839 - val_loss: 0.4367\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3771 - val_loss: 0.4425\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3836 - val_loss: 0.4322\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3865 - val_loss: 0.4488\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3828 - val_loss: 0.4400\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3595 - val_loss: 0.4356\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3705 - val_loss: 0.4335\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3648 - val_loss: 0.4293\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3710 - val_loss: 0.4284\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3598 - val_loss: 0.4293\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3602 - val_loss: 0.4289\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3707 - val_loss: 0.4302\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3644 - val_loss: 0.4280\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3720 - val_loss: 0.4278\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3536 - val_loss: 0.4278\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3698 - val_loss: 0.4287\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3552 - val_loss: 0.4278\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3645 - val_loss: 0.4278\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3576 - val_loss: 0.4278\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3630 - val_loss: 0.4277\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3736 - val_loss: 0.4277\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3609 - val_loss: 0.4277\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3698 - val_loss: 0.4277\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3592 - val_loss: 0.4277\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3635 - val_loss: 0.4277\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3613 - val_loss: 0.4277\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3628 - val_loss: 0.4277\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3592 - val_loss: 0.4277\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3528 - val_loss: 0.4277\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "[2022_04_20-18:46:37] Training the entire fine-tuned model...\n",
      "[2022_04_20-18:47:13] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3646WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1082s vs `on_train_batch_end` time: 0.1410s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1082s vs `on_train_batch_end` time: 0.1410s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 703ms/step - loss: 0.3646 - val_loss: 0.4276\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3632 - val_loss: 0.4277\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3772 - val_loss: 0.4282\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3588 - val_loss: 0.4279\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3561 - val_loss: 0.4280\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3580 - val_loss: 0.4281\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3573 - val_loss: 0.4282\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_20-18:47:35] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-18:47:35] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:47:39] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3600WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1079s vs `on_train_batch_end` time: 0.1392s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1079s vs `on_train_batch_end` time: 0.1392s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 681ms/step - loss: 0.3600 - val_loss: 0.4276\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>447</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  447  19\n",
       "1   92  27"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32727272727272727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_20_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_20_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:68w2s7vy) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 20305... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇███▁▁▁▂▂▁</td></tr><tr><td>loss</td><td>█▄▄▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄█▆▂▂▂▂▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.42759</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35996</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42765</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">resilient-plant-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204b/runs/68w2s7vy\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204b/runs/68w2s7vy</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_184518-68w2s7vy/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:68w2s7vy). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/gjkeup9a\" target=\"_blank\">rare-vortex-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-18:48:34] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:48:34] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:48:34] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 728ms/step - loss: 0.8852 - val_loss: 0.9780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.7602 - val_loss: 0.7371\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.5456 - val_loss: 0.6546\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.5510 - val_loss: 0.6090\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.5098 - val_loss: 0.5119\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4659 - val_loss: 0.4947\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4494 - val_loss: 0.4639\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4225 - val_loss: 0.4659\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4233 - val_loss: 0.4502\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4179 - val_loss: 0.4489\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4193 - val_loss: 0.4686\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4062 - val_loss: 0.4539\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4069 - val_loss: 0.4819\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 230ms/step - loss: 0.4172 - val_loss: 0.4427\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.3894 - val_loss: 0.4589\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3913 - val_loss: 0.4441\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3782 - val_loss: 0.4449\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3843 - val_loss: 0.4413\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3799 - val_loss: 0.4398\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3926 - val_loss: 0.4400\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3880 - val_loss: 0.4398\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3819 - val_loss: 0.4396\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3752 - val_loss: 0.4395\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3778 - val_loss: 0.4401\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3787 - val_loss: 0.4397\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 245ms/step - loss: 0.3728 - val_loss: 0.4395\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3818 - val_loss: 0.4394\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3740 - val_loss: 0.4393\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3877 - val_loss: 0.4393\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3743 - val_loss: 0.4392\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3751 - val_loss: 0.4392\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3761 - val_loss: 0.4392\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3694 - val_loss: 0.4392\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3884 - val_loss: 0.4392\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3778 - val_loss: 0.4392\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3746 - val_loss: 0.4392\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.3844 - val_loss: 0.4392\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3829 - val_loss: 0.4392\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3778 - val_loss: 0.4392\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3809 - val_loss: 0.4392\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3677 - val_loss: 0.4392\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3781 - val_loss: 0.4392\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3726 - val_loss: 0.4392\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3800 - val_loss: 0.4392\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3697 - val_loss: 0.4392\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3774 - val_loss: 0.4392\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3792 - val_loss: 0.4392\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3833 - val_loss: 0.4392\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3755 - val_loss: 0.4392\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3770 - val_loss: 0.4392\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3887 - val_loss: 0.4392\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3790 - val_loss: 0.4392\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3777 - val_loss: 0.4392\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3788 - val_loss: 0.4392\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3798 - val_loss: 0.4392\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3793 - val_loss: 0.4392\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3764 - val_loss: 0.4392\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3783 - val_loss: 0.4392\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3762 - val_loss: 0.4392\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3783 - val_loss: 0.4392\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.3731 - val_loss: 0.4392\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3744 - val_loss: 0.4392\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3742 - val_loss: 0.4392\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3853 - val_loss: 0.4392\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3828 - val_loss: 0.4392\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3809 - val_loss: 0.4392\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3754 - val_loss: 0.4392\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3742 - val_loss: 0.4392\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3847 - val_loss: 0.4392\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3789 - val_loss: 0.4392\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3736 - val_loss: 0.4392\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3802 - val_loss: 0.4392\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.3800 - val_loss: 0.4392\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3794 - val_loss: 0.4392\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3749 - val_loss: 0.4392\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3803 - val_loss: 0.4392\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3741 - val_loss: 0.4392\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3723 - val_loss: 0.4392\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3734 - val_loss: 0.4392\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3761 - val_loss: 0.4392\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3744 - val_loss: 0.4392\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3816 - val_loss: 0.4392\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3733 - val_loss: 0.4392\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3670 - val_loss: 0.4392\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3773 - val_loss: 0.4392\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3818 - val_loss: 0.4392\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3727 - val_loss: 0.4392\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3756 - val_loss: 0.4392\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3735 - val_loss: 0.4392\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3781 - val_loss: 0.4392\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3724 - val_loss: 0.4392\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3806 - val_loss: 0.4392\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.3786 - val_loss: 0.4392\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3790 - val_loss: 0.4392\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3811 - val_loss: 0.4392\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3815 - val_loss: 0.4392\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3740 - val_loss: 0.4392\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3755 - val_loss: 0.4392\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3834 - val_loss: 0.4392\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3735 - val_loss: 0.4392\n",
      "[2022_04_20-18:50:35] Training the entire fine-tuned model...\n",
      "[2022_04_20-18:50:45] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 851ms/step - loss: 0.3866 - val_loss: 0.4391\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3816 - val_loss: 0.4391\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3732 - val_loss: 0.4384\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3693 - val_loss: 0.4364\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3639 - val_loss: 0.4378\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3677 - val_loss: 0.4340\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 372ms/step - loss: 0.3627 - val_loss: 0.4333\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3607 - val_loss: 0.4319\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3709 - val_loss: 0.4318\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3502 - val_loss: 0.4308\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3498 - val_loss: 0.4295\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3422 - val_loss: 0.4301\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3327 - val_loss: 0.4319\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3336 - val_loss: 0.4333\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3330 - val_loss: 0.4309\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3439 - val_loss: 0.4285\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3299 - val_loss: 0.4290\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3293 - val_loss: 0.4284\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3417 - val_loss: 0.4292\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3303 - val_loss: 0.4285\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3342 - val_loss: 0.4285\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3258 - val_loss: 0.4284\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3328 - val_loss: 0.4284\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.3312 - val_loss: 0.4283\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3337 - val_loss: 0.4283\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3350 - val_loss: 0.4283\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3337 - val_loss: 0.4283\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3339 - val_loss: 0.4283\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3295 - val_loss: 0.4283\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3268 - val_loss: 0.4283\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3220 - val_loss: 0.4283\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3219 - val_loss: 0.4283\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3233 - val_loss: 0.4283\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3269 - val_loss: 0.4283\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3224 - val_loss: 0.4283\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3238 - val_loss: 0.4283\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3249 - val_loss: 0.4283\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3308 - val_loss: 0.4283\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3310 - val_loss: 0.4282\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.3218 - val_loss: 0.4282\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3302 - val_loss: 0.4282\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3297 - val_loss: 0.4282\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3194 - val_loss: 0.4282\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3281 - val_loss: 0.4283\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3201 - val_loss: 0.4283\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3194 - val_loss: 0.4283\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3322 - val_loss: 0.4283\n",
      "[2022_04_20-18:52:19] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-18:52:19] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:53:13] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 843ms/step - loss: 0.3287 - val_loss: 0.4282\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>568</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  568  14\n",
       "1  125  26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27225130890052357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:gjkeup9a) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 20688... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▄▄▄▁</td></tr><tr><td>loss</td><td>█▅▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.42817</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32873</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42817</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">rare-vortex-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/gjkeup9a\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204a/runs/gjkeup9a</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_184818-gjkeup9a/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:gjkeup9a). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204b/runs/1ya9wxdu\" target=\"_blank\">kind-sponge-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-18:54:07] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:54:07] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:54:07] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 581ms/step - loss: 0.8406 - val_loss: 0.6693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.6560 - val_loss: 0.4927\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5300 - val_loss: 0.5362\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5110 - val_loss: 0.5056\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4753 - val_loss: 0.4601\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4646 - val_loss: 0.4920\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.4405 - val_loss: 0.4905\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4444 - val_loss: 0.4571\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4259 - val_loss: 0.4603\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4223 - val_loss: 0.4528\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4105 - val_loss: 0.4488\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4076 - val_loss: 0.4398\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4011 - val_loss: 0.4377\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3949 - val_loss: 0.4365\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3863 - val_loss: 0.4395\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3883 - val_loss: 0.4388\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3849 - val_loss: 0.4538\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4005 - val_loss: 0.4360\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3920 - val_loss: 0.4321\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3818 - val_loss: 0.4329\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3811 - val_loss: 0.4344\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3775 - val_loss: 0.4321\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3791 - val_loss: 0.4323\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3757 - val_loss: 0.4314\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3716 - val_loss: 0.4329\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3778 - val_loss: 0.4310\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3707 - val_loss: 0.4311\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3812 - val_loss: 0.4308\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3724 - val_loss: 0.4311\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3754 - val_loss: 0.4308\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3762 - val_loss: 0.4307\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3742 - val_loss: 0.4306\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3724 - val_loss: 0.4305\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3687 - val_loss: 0.4304\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3722 - val_loss: 0.4305\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3696 - val_loss: 0.4307\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3784 - val_loss: 0.4307\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3783 - val_loss: 0.4305\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3743 - val_loss: 0.4306\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3777 - val_loss: 0.4305\n",
      "[2022_04_20-18:55:02] Training the entire fine-tuned model...\n",
      "[2022_04_20-18:55:47] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3839WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1074s vs `on_train_batch_end` time: 0.1393s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1074s vs `on_train_batch_end` time: 0.1393s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 721ms/step - loss: 0.3839 - val_loss: 0.4322\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3813 - val_loss: 0.4333\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3744 - val_loss: 0.4292\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 0.3716 - val_loss: 0.4348\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3704 - val_loss: 0.4298\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 340ms/step - loss: 0.3654 - val_loss: 0.4300\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.3646 - val_loss: 0.4288\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3620 - val_loss: 0.4289\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3616 - val_loss: 0.4291\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 341ms/step - loss: 0.3587 - val_loss: 0.4292\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3595 - val_loss: 0.4292\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3618 - val_loss: 0.4296\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.3535 - val_loss: 0.4298\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "[2022_04_20-18:56:21] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-18:56:21] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:56:21] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3625WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1077s vs `on_train_batch_end` time: 0.1401s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1077s vs `on_train_batch_end` time: 0.1401s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 688ms/step - loss: 0.3625 - val_loss: 0.4288\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>450</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  450  16\n",
       "1   93  26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32298136645962733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_20_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_20_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1ya9wxdu) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 21549... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██▁▁▂▂▂▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▄▂▃▃▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.4288</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3625</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.4288</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">kind-sponge-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204b/runs/1ya9wxdu\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204b/runs/1ya9wxdu</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_185350-1ya9wxdu/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1ya9wxdu). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/2o5qnxoj\" target=\"_blank\">astral-yogurt-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-18:57:13] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:57:13] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-18:57:13] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 703ms/step - loss: 0.8340 - val_loss: 0.8092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.6365 - val_loss: 0.7659\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.6038 - val_loss: 0.6201\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.5186 - val_loss: 0.5507\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.5022 - val_loss: 0.4701\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4567 - val_loss: 0.4661\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4519 - val_loss: 0.4674\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4301 - val_loss: 0.4539\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4095 - val_loss: 0.4623\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4164 - val_loss: 0.4473\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4028 - val_loss: 0.4487\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4006 - val_loss: 0.4925\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4085 - val_loss: 0.4597\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3954 - val_loss: 0.4465\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3896 - val_loss: 0.4488\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3773 - val_loss: 0.4479\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3843 - val_loss: 0.4420\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3680 - val_loss: 0.4494\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3708 - val_loss: 0.4419\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3938 - val_loss: 0.4447\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3743 - val_loss: 0.4406\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3755 - val_loss: 0.4431\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3794 - val_loss: 0.4416\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3703 - val_loss: 0.4402\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3721 - val_loss: 0.4402\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3741 - val_loss: 0.4400\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3739 - val_loss: 0.4399\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3694 - val_loss: 0.4399\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3705 - val_loss: 0.4396\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3689 - val_loss: 0.4395\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3633 - val_loss: 0.4394\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3654 - val_loss: 0.4393\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3610 - val_loss: 0.4392\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3679 - val_loss: 0.4393\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3643 - val_loss: 0.4392\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3635 - val_loss: 0.4389\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3705 - val_loss: 0.4388\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3714 - val_loss: 0.4389\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3631 - val_loss: 0.4390\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3702 - val_loss: 0.4388\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3626 - val_loss: 0.4387\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3764 - val_loss: 0.4386\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3554 - val_loss: 0.4385\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3708 - val_loss: 0.4385\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3598 - val_loss: 0.4385\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3563 - val_loss: 0.4384\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3633 - val_loss: 0.4384\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3612 - val_loss: 0.4384\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3666 - val_loss: 0.4384\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3662 - val_loss: 0.4384\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3726 - val_loss: 0.4384\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3705 - val_loss: 0.4384\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3646 - val_loss: 0.4384\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3643 - val_loss: 0.4384\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3738 - val_loss: 0.4384\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3631 - val_loss: 0.4384\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3718 - val_loss: 0.4384\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3612 - val_loss: 0.4384\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3689 - val_loss: 0.4384\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3592 - val_loss: 0.4384\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3738 - val_loss: 0.4384\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3642 - val_loss: 0.4384\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3639 - val_loss: 0.4384\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3659 - val_loss: 0.4384\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3650 - val_loss: 0.4384\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3662 - val_loss: 0.4384\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3643 - val_loss: 0.4384\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3636 - val_loss: 0.4384\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3635 - val_loss: 0.4384\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3610 - val_loss: 0.4384\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3648 - val_loss: 0.4384\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3622 - val_loss: 0.4384\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3585 - val_loss: 0.4384\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3610 - val_loss: 0.4384\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3658 - val_loss: 0.4384\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3602 - val_loss: 0.4384\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3632 - val_loss: 0.4384\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.3664 - val_loss: 0.4384\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3731 - val_loss: 0.4384\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3793 - val_loss: 0.4384\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3616 - val_loss: 0.4384\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3689 - val_loss: 0.4384\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3696 - val_loss: 0.4384\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3695 - val_loss: 0.4384\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.3626 - val_loss: 0.4384\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3766 - val_loss: 0.4384\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3662 - val_loss: 0.4384\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3632 - val_loss: 0.4384\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3672 - val_loss: 0.4384\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3675 - val_loss: 0.4384\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3648 - val_loss: 0.4384\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3762 - val_loss: 0.4384\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3684 - val_loss: 0.4384\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3656 - val_loss: 0.4384\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3626 - val_loss: 0.4384\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3694 - val_loss: 0.4384\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3605 - val_loss: 0.4384\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3756 - val_loss: 0.4384\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.3687 - val_loss: 0.4384\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3621 - val_loss: 0.4384\n",
      "[2022_04_20-18:59:45] Training the entire fine-tuned model...\n",
      "[2022_04_20-18:59:53] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 841ms/step - loss: 0.3740 - val_loss: 0.4613\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3712 - val_loss: 0.4469\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3647 - val_loss: 0.4404\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3548 - val_loss: 0.4337\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 372ms/step - loss: 0.3473 - val_loss: 0.4309\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 372ms/step - loss: 0.3546 - val_loss: 0.4304\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 371ms/step - loss: 0.3446 - val_loss: 0.4314\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3409 - val_loss: 0.4389\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3289 - val_loss: 0.4310\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3210 - val_loss: 0.4313\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3206 - val_loss: 0.4300\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3101 - val_loss: 0.4329\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3097 - val_loss: 0.4285\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3142 - val_loss: 0.4286\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3206 - val_loss: 0.4308\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3025 - val_loss: 0.4286\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3093 - val_loss: 0.4280\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3135 - val_loss: 0.4278\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3074 - val_loss: 0.4277\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3042 - val_loss: 0.4275\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2951 - val_loss: 0.4273\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3021 - val_loss: 0.4273\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3092 - val_loss: 0.4274\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2984 - val_loss: 0.4272\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3021 - val_loss: 0.4268\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2992 - val_loss: 0.4267\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.2961 - val_loss: 0.4266\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3021 - val_loss: 0.4269\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2924 - val_loss: 0.4269\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3001 - val_loss: 0.4265\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2953 - val_loss: 0.4265\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.2998 - val_loss: 0.4265\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3001 - val_loss: 0.4265\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.2931 - val_loss: 0.4265\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3001 - val_loss: 0.4265\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.2935 - val_loss: 0.4265\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3065 - val_loss: 0.4265\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.2920 - val_loss: 0.4265\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.2957 - val_loss: 0.4265\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2923 - val_loss: 0.4265\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.2988 - val_loss: 0.4265\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2961 - val_loss: 0.4265\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2848 - val_loss: 0.4265\n",
      "[2022_04_20-19:01:19] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-19:01:19] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:01:19] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 846ms/step - loss: 0.2982 - val_loss: 0.4265\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>563</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  563  19\n",
       "1  123  28"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2828282828282828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2o5qnxoj) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 21928... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▆▄▃▃▃▃▃▃▃▃▃▃▂▃▂▂▂▃▂▂▃▃▃▂▃▃▂▃▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.42645</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.29825</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.42645</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">astral-yogurt-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/2o5qnxoj\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204a/runs/2o5qnxoj</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_185658-2o5qnxoj/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2o5qnxoj). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204b/runs/xc3ucq9l\" target=\"_blank\">easy-deluge-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-19:02:17] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:02:17] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:02:17] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 576ms/step - loss: 0.9104 - val_loss: 0.8021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.6126 - val_loss: 0.5443\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5002 - val_loss: 0.4846\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4749 - val_loss: 0.4866\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4535 - val_loss: 0.4650\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4364 - val_loss: 0.4593\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4292 - val_loss: 0.4532\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4295 - val_loss: 0.4496\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4143 - val_loss: 0.4551\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4034 - val_loss: 0.4494\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4055 - val_loss: 0.4419\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4041 - val_loss: 0.4391\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4079 - val_loss: 0.4406\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3960 - val_loss: 0.4426\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3928 - val_loss: 0.4350\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3782 - val_loss: 0.4375\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3895 - val_loss: 0.4337\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3818 - val_loss: 0.4334\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3928 - val_loss: 0.4472\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3986 - val_loss: 0.4893\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3810 - val_loss: 0.4320\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3770 - val_loss: 0.4330\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3738 - val_loss: 0.4329\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3773 - val_loss: 0.4903\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3907 - val_loss: 0.4453\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3702 - val_loss: 0.4321\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3807 - val_loss: 0.4287\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3564 - val_loss: 0.4288\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3606 - val_loss: 0.4292\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3585 - val_loss: 0.4284\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3700 - val_loss: 0.4282\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3568 - val_loss: 0.4280\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3559 - val_loss: 0.4277\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3613 - val_loss: 0.4277\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3570 - val_loss: 0.4274\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3666 - val_loss: 0.4272\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3543 - val_loss: 0.4290\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3549 - val_loss: 0.4276\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3509 - val_loss: 0.4270\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3580 - val_loss: 0.4268\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3559 - val_loss: 0.4268\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3609 - val_loss: 0.4271\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3600 - val_loss: 0.4278\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3605 - val_loss: 0.4268\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3548 - val_loss: 0.4268\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3466 - val_loss: 0.4298\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3476 - val_loss: 0.4281\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3525 - val_loss: 0.4269\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3536 - val_loss: 0.4265\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3487 - val_loss: 0.4265\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3601 - val_loss: 0.4266\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3450 - val_loss: 0.4265\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3458 - val_loss: 0.4265\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3475 - val_loss: 0.4265\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3531 - val_loss: 0.4266\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3510 - val_loss: 0.4266\n",
      "[2022_04_20-19:03:30] Training the entire fine-tuned model...\n",
      "[2022_04_20-19:03:55] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3542WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1069s vs `on_train_batch_end` time: 0.1414s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1069s vs `on_train_batch_end` time: 0.1414s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 720ms/step - loss: 0.3542 - val_loss: 0.4433\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3555 - val_loss: 0.4271\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3604 - val_loss: 0.4258\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3467 - val_loss: 0.4259\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3527 - val_loss: 0.4275\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3299 - val_loss: 0.4322\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 343ms/step - loss: 0.3260 - val_loss: 0.4344\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3262 - val_loss: 0.4342\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3084 - val_loss: 0.4362\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_20-19:04:21] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-19:04:21] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:04:21] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3507WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1108s vs `on_train_batch_end` time: 0.1384s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1108s vs `on_train_batch_end` time: 0.1384s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 710ms/step - loss: 0.3507 - val_loss: 0.4253\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>441</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  441  25\n",
       "1   91  28"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32558139534883723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_20_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_20_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:xc3ucq9l) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 22783... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.42534</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35071</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.42534</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">easy-deluge-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204b/runs/xc3ucq9l\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204b/runs/xc3ucq9l</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_190159-xc3ucq9l/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:xc3ucq9l). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/3ok9qnw6\" target=\"_blank\">laced-music-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-19:05:20] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:05:20] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:05:20] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 730ms/step - loss: 0.8653 - val_loss: 0.6471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.5580 - val_loss: 0.5919\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4994 - val_loss: 0.5056\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4519 - val_loss: 0.5011\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4584 - val_loss: 0.4602\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4500 - val_loss: 0.4643\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4383 - val_loss: 0.4853\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4330 - val_loss: 0.4484\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4166 - val_loss: 0.4543\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4359 - val_loss: 0.4491\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4174 - val_loss: 0.4505\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3850 - val_loss: 0.4412\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3865 - val_loss: 0.4435\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3908 - val_loss: 0.4439\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3841 - val_loss: 0.4401\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3885 - val_loss: 0.4397\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3933 - val_loss: 0.4454\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3936 - val_loss: 0.4426\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3772 - val_loss: 0.4391\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3726 - val_loss: 0.4422\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3741 - val_loss: 0.4416\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3744 - val_loss: 0.4380\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3818 - val_loss: 0.4376\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3697 - val_loss: 0.4374\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3767 - val_loss: 0.4381\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3616 - val_loss: 0.4377\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3689 - val_loss: 0.4357\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3672 - val_loss: 0.4367\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3654 - val_loss: 0.4352\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3583 - val_loss: 0.4347\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3602 - val_loss: 0.4343\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3567 - val_loss: 0.4345\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3709 - val_loss: 0.4366\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3606 - val_loss: 0.4352\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3647 - val_loss: 0.4348\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3553 - val_loss: 0.4343\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3630 - val_loss: 0.4343\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3644 - val_loss: 0.4342\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3538 - val_loss: 0.4341\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3565 - val_loss: 0.4339\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3517 - val_loss: 0.4335\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3585 - val_loss: 0.4334\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3513 - val_loss: 0.4334\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3507 - val_loss: 0.4333\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3543 - val_loss: 0.4334\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 245ms/step - loss: 0.3517 - val_loss: 0.4333\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3518 - val_loss: 0.4333\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3627 - val_loss: 0.4333\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3508 - val_loss: 0.4333\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3607 - val_loss: 0.4333\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3531 - val_loss: 0.4333\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3540 - val_loss: 0.4333\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3462 - val_loss: 0.4333\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3515 - val_loss: 0.4333\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3563 - val_loss: 0.4333\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3464 - val_loss: 0.4333\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3598 - val_loss: 0.4333\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3553 - val_loss: 0.4333\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3622 - val_loss: 0.4333\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3664 - val_loss: 0.4333\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3618 - val_loss: 0.4333\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3540 - val_loss: 0.4333\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3562 - val_loss: 0.4333\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3502 - val_loss: 0.4333\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3603 - val_loss: 0.4333\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3610 - val_loss: 0.4333\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3576 - val_loss: 0.4333\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3460 - val_loss: 0.4333\n",
      "[2022_04_20-19:06:45] Training the entire fine-tuned model...\n",
      "[2022_04_20-19:06:53] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 865ms/step - loss: 0.3748 - val_loss: 0.4502\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3688 - val_loss: 0.4346\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3446 - val_loss: 0.4558\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3453 - val_loss: 0.4283\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3343 - val_loss: 0.4300\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3032 - val_loss: 0.4267\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2745 - val_loss: 0.4387\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2666 - val_loss: 0.4329\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.2415 - val_loss: 0.4358\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.1908 - val_loss: 0.4671\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2046 - val_loss: 0.4475\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.1770 - val_loss: 0.4747\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_20-19:07:25] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-19:07:25] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:07:25] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 838ms/step - loss: 0.2770 - val_loss: 0.4248\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>555</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>117</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  555  27\n",
       "1  117  34"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32075471698113206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3ok9qnw6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 23223... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▁▂</td></tr><tr><td>lr</td><td>██████▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▃▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.42484</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.27703</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.42484</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">laced-music-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/3ok9qnw6\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204a/runs/3ok9qnw6</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_190502-3ok9qnw6/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3ok9qnw6). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204b/runs/28f6ssgg\" target=\"_blank\">decent-tree-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-19:08:20] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:08:20] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:08:20] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 555ms/step - loss: 1.0549 - val_loss: 0.6782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.7921 - val_loss: 0.9338\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.7694 - val_loss: 0.7161\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.6000 - val_loss: 0.5567\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5051 - val_loss: 0.4856\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4660 - val_loss: 0.4605\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4357 - val_loss: 0.4567\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4253 - val_loss: 0.4545\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4224 - val_loss: 0.4559\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4143 - val_loss: 0.4523\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4210 - val_loss: 0.4545\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4169 - val_loss: 0.4478\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4168 - val_loss: 0.4446\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4042 - val_loss: 0.4464\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4059 - val_loss: 0.4589\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4011 - val_loss: 0.4388\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3857 - val_loss: 0.4360\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3892 - val_loss: 0.4351\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3832 - val_loss: 0.4439\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3768 - val_loss: 0.4414\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3873 - val_loss: 0.4483\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3879 - val_loss: 0.4336\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3886 - val_loss: 0.4407\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3840 - val_loss: 0.4358\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3782 - val_loss: 0.4400\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3801 - val_loss: 0.4340\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3768 - val_loss: 0.4328\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3726 - val_loss: 0.4328\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3704 - val_loss: 0.4329\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3761 - val_loss: 0.4328\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3757 - val_loss: 0.4328\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3707 - val_loss: 0.4329\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3747 - val_loss: 0.4328\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3796 - val_loss: 0.4328\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3751 - val_loss: 0.4328\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3747 - val_loss: 0.4328\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3681 - val_loss: 0.4327\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3761 - val_loss: 0.4327\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3645 - val_loss: 0.4327\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3721 - val_loss: 0.4327\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3667 - val_loss: 0.4327\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3696 - val_loss: 0.4327\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3705 - val_loss: 0.4327\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3641 - val_loss: 0.4327\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3772 - val_loss: 0.4327\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3682 - val_loss: 0.4327\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3765 - val_loss: 0.4327\n",
      "[2022_04_20-19:09:22] Training the entire fine-tuned model...\n",
      "[2022_04_20-19:09:31] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5700WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1102s vs `on_train_batch_end` time: 0.1408s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1102s vs `on_train_batch_end` time: 0.1408s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 711ms/step - loss: 0.5700 - val_loss: 0.5645\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.4657 - val_loss: 0.5359\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.4088 - val_loss: 0.4759\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3881 - val_loss: 0.4509\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3730 - val_loss: 0.4283\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3720 - val_loss: 0.4242\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3498 - val_loss: 0.4348\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3723 - val_loss: 0.4267\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3583 - val_loss: 0.4510\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3356 - val_loss: 0.4309\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 340ms/step - loss: 0.3377 - val_loss: 0.4235\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3047 - val_loss: 0.4256\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3094 - val_loss: 0.4242\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 340ms/step - loss: 0.3015 - val_loss: 0.4254\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3094 - val_loss: 0.4255\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.2952 - val_loss: 0.4260\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.2907 - val_loss: 0.4267\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "[2022_04_20-19:10:14] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-19:10:14] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:10:18] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3211WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1072s vs `on_train_batch_end` time: 0.1423s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1072s vs `on_train_batch_end` time: 0.1423s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 697ms/step - loss: 0.3211 - val_loss: 0.4231\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>433</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  433  33\n",
       "1   84  35"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37433155080213903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_20_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_20_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:28f6ssgg) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 23726... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▆▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▄▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.42314</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32113</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.42314</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">decent-tree-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204b/runs/28f6ssgg\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204b/runs/28f6ssgg</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_190803-28f6ssgg/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:28f6ssgg). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/s7bsyqzx\" target=\"_blank\">rose-haze-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-19:11:13] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:11:13] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:11:13] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 732ms/step - loss: 0.8152 - val_loss: 0.9439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.6647 - val_loss: 0.8182\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.6771 - val_loss: 0.5579\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.5429 - val_loss: 0.5596\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.4893 - val_loss: 0.4866\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4774 - val_loss: 0.4879\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.4697 - val_loss: 0.4618\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4615 - val_loss: 0.4544\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4611 - val_loss: 0.4925\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4645 - val_loss: 0.4816\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4485 - val_loss: 0.4767\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4153 - val_loss: 0.4493\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3894 - val_loss: 0.4452\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3960 - val_loss: 0.4497\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3910 - val_loss: 0.4497\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3765 - val_loss: 0.4624\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3924 - val_loss: 0.4415\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3792 - val_loss: 0.4383\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3717 - val_loss: 0.4386\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3620 - val_loss: 0.4375\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3567 - val_loss: 0.4498\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3684 - val_loss: 0.4379\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3588 - val_loss: 0.4342\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3517 - val_loss: 0.4496\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3539 - val_loss: 0.4337\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3519 - val_loss: 0.4427\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3552 - val_loss: 0.4531\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3501 - val_loss: 0.4330\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3536 - val_loss: 0.4308\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3416 - val_loss: 0.4539\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3521 - val_loss: 0.4447\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3347 - val_loss: 0.4415\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.3298 - val_loss: 0.4395\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3321 - val_loss: 0.4294\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3316 - val_loss: 0.4324\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3254 - val_loss: 0.4290\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3326 - val_loss: 0.4303\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3271 - val_loss: 0.4312\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3226 - val_loss: 0.4283\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3220 - val_loss: 0.4295\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3261 - val_loss: 0.4282\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3275 - val_loss: 0.4278\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3313 - val_loss: 0.4276\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3198 - val_loss: 0.4273\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3226 - val_loss: 0.4271\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3160 - val_loss: 0.4282\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3150 - val_loss: 0.4270\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3193 - val_loss: 0.4270\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3163 - val_loss: 0.4272\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3062 - val_loss: 0.4272\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3189 - val_loss: 0.4267\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3149 - val_loss: 0.4267\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3263 - val_loss: 0.4271\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3154 - val_loss: 0.4266\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3170 - val_loss: 0.4267\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3174 - val_loss: 0.4267\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3026 - val_loss: 0.4264\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3215 - val_loss: 0.4266\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3157 - val_loss: 0.4268\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3190 - val_loss: 0.4263\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3062 - val_loss: 0.4262\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3114 - val_loss: 0.4262\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3098 - val_loss: 0.4264\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3069 - val_loss: 0.4265\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3198 - val_loss: 0.4261\n",
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3026 - val_loss: 0.4261\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3127 - val_loss: 0.4260\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3137 - val_loss: 0.4260\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3093 - val_loss: 0.4260\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3158 - val_loss: 0.4260\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3032 - val_loss: 0.4260\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3160 - val_loss: 0.4260\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3028 - val_loss: 0.4260\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3084 - val_loss: 0.4260\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3229 - val_loss: 0.4260\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3068 - val_loss: 0.4260\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3209 - val_loss: 0.4260\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3148 - val_loss: 0.4260\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.3222 - val_loss: 0.4260\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3115 - val_loss: 0.4260\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3205 - val_loss: 0.4260\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3196 - val_loss: 0.4260\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 1s 245ms/step - loss: 0.3120 - val_loss: 0.4260\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3160 - val_loss: 0.4260\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3124 - val_loss: 0.4260\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3008 - val_loss: 0.4260\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.3077 - val_loss: 0.4260\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3205 - val_loss: 0.4260\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3083 - val_loss: 0.4260\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3144 - val_loss: 0.4260\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3089 - val_loss: 0.4260\n",
      "\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3092 - val_loss: 0.4260\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3211 - val_loss: 0.4260\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3132 - val_loss: 0.4260\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3205 - val_loss: 0.4260\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3108 - val_loss: 0.4260\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3128 - val_loss: 0.4260\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3148 - val_loss: 0.4260\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3035 - val_loss: 0.4260\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3215 - val_loss: 0.4260\n",
      "[2022_04_20-19:13:45] Training the entire fine-tuned model...\n",
      "[2022_04_20-19:13:52] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 851ms/step - loss: 0.3238 - val_loss: 0.4262\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3111 - val_loss: 0.4257\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3106 - val_loss: 0.4254\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3016 - val_loss: 0.4261\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3189 - val_loss: 0.4253\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.3092 - val_loss: 0.4255\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3046 - val_loss: 0.4250\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3034 - val_loss: 0.4250\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3164 - val_loss: 0.4250\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3101 - val_loss: 0.4248\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3034 - val_loss: 0.4258\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3066 - val_loss: 0.4256\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2984 - val_loss: 0.4245\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3150 - val_loss: 0.4264\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2915 - val_loss: 0.4242\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.2907 - val_loss: 0.4243\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.2987 - val_loss: 0.4240\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3092 - val_loss: 0.4241\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2921 - val_loss: 0.4240\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2935 - val_loss: 0.4242\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2946 - val_loss: 0.4244\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3020 - val_loss: 0.4244\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2955 - val_loss: 0.4244\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2957 - val_loss: 0.4244\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.2991 - val_loss: 0.4244\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2941 - val_loss: 0.4244\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2951 - val_loss: 0.4244\n",
      "[2022_04_20-19:14:49] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-19:14:49] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:14:49] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 830ms/step - loss: 0.3001 - val_loss: 0.4240\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>564</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  564  18\n",
       "1  125  26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:s7bsyqzx) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 24172... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▆▄▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.42399</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30009</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42399</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">rose-haze-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/s7bsyqzx\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204a/runs/s7bsyqzx</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_191056-s7bsyqzx/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:s7bsyqzx). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204b/runs/2k7p4e6b\" target=\"_blank\">eternal-donkey-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-19:15:44] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:15:44] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:15:44] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 607ms/step - loss: 0.9534 - val_loss: 1.0543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.8290 - val_loss: 0.8907\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.6702 - val_loss: 0.6797\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5620 - val_loss: 0.5349\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4803 - val_loss: 0.4665\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4604 - val_loss: 0.4631\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4448 - val_loss: 0.4685\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4377 - val_loss: 0.4518\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4203 - val_loss: 0.4492\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4120 - val_loss: 0.4450\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4187 - val_loss: 0.4425\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4105 - val_loss: 0.4422\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4098 - val_loss: 0.4388\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3914 - val_loss: 0.4380\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3935 - val_loss: 0.4356\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3996 - val_loss: 0.4356\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3900 - val_loss: 0.4336\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3862 - val_loss: 0.4343\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3865 - val_loss: 0.4323\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3872 - val_loss: 0.4424\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3919 - val_loss: 0.4445\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4093 - val_loss: 0.4565\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3947 - val_loss: 0.4406\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.3728 - val_loss: 0.4297\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.3816 - val_loss: 0.4306\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3733 - val_loss: 0.4299\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3800 - val_loss: 0.4301\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3617 - val_loss: 0.4286\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3739 - val_loss: 0.4307\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3726 - val_loss: 0.4284\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3602 - val_loss: 0.4277\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3627 - val_loss: 0.4278\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3610 - val_loss: 0.4272\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3689 - val_loss: 0.4278\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3619 - val_loss: 0.4274\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3688 - val_loss: 0.4275\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3546 - val_loss: 0.4271\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3682 - val_loss: 0.4276\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3724 - val_loss: 0.4268\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3597 - val_loss: 0.4267\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3597 - val_loss: 0.4266\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3589 - val_loss: 0.4278\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3567 - val_loss: 0.4270\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3579 - val_loss: 0.4268\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3693 - val_loss: 0.4264\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3600 - val_loss: 0.4264\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3608 - val_loss: 0.4268\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3673 - val_loss: 0.4263\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3572 - val_loss: 0.4262\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3582 - val_loss: 0.4262\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3624 - val_loss: 0.4262\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3582 - val_loss: 0.4265\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3641 - val_loss: 0.4261\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3593 - val_loss: 0.4261\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3692 - val_loss: 0.4264\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3584 - val_loss: 0.4263\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3490 - val_loss: 0.4263\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3643 - val_loss: 0.4263\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3638 - val_loss: 0.4262\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3596 - val_loss: 0.4260\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3606 - val_loss: 0.4259\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3472 - val_loss: 0.4259\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3579 - val_loss: 0.4259\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3631 - val_loss: 0.4259\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3666 - val_loss: 0.4260\n",
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3564 - val_loss: 0.4260\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3607 - val_loss: 0.4260\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3553 - val_loss: 0.4260\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3650 - val_loss: 0.4260\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3506 - val_loss: 0.4260\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3524 - val_loss: 0.4260\n",
      "[2022_04_20-19:17:12] Training the entire fine-tuned model...\n",
      "[2022_04_20-19:17:20] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3551WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1068s vs `on_train_batch_end` time: 0.1402s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1068s vs `on_train_batch_end` time: 0.1402s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 690ms/step - loss: 0.3551 - val_loss: 0.4262\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3609 - val_loss: 0.4261\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3648 - val_loss: 0.4262\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3491 - val_loss: 0.4259\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 340ms/step - loss: 0.3588 - val_loss: 0.4266\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 342ms/step - loss: 0.3626 - val_loss: 0.4260\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 340ms/step - loss: 0.3589 - val_loss: 0.4262\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3493 - val_loss: 0.4266\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3510 - val_loss: 0.4263\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3514 - val_loss: 0.4262\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3525 - val_loss: 0.4263\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 341ms/step - loss: 0.3575 - val_loss: 0.4264\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_20-19:17:52] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-19:17:52] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:17:52] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3570WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1121s vs `on_train_batch_end` time: 0.1391s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1121s vs `on_train_batch_end` time: 0.1391s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 712ms/step - loss: 0.3570 - val_loss: 0.4259\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>451</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  451  15\n",
       "1   92  27"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33540372670807456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_20_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_20_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2k7p4e6b) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 24928... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇██▁▁▁▂▂▁</td></tr><tr><td>loss</td><td>█▅▃▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>62</td></tr><tr><td>best_val_loss</td><td>0.42586</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35696</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42589</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">eternal-donkey-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204b/runs/2k7p4e6b\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204b/runs/2k7p4e6b</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_191527-2k7p4e6b/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2k7p4e6b). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/3r1qb6lk\" target=\"_blank\">deep-cherry-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-19:18:51] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:18:51] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:18:51] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 720ms/step - loss: 0.9728 - val_loss: 0.5144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.6595 - val_loss: 0.7010\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.5974 - val_loss: 0.5572\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.5067 - val_loss: 0.5457\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4893 - val_loss: 0.4990\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4685 - val_loss: 0.5034\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4355 - val_loss: 0.5053\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4548 - val_loss: 0.4797\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4202 - val_loss: 0.4718\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4354 - val_loss: 0.4497\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4083 - val_loss: 0.4600\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4145 - val_loss: 0.4462\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4013 - val_loss: 0.4476\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3855 - val_loss: 0.4424\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3841 - val_loss: 0.4410\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3991 - val_loss: 0.4792\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4173 - val_loss: 0.5947\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4515 - val_loss: 0.5216\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4132 - val_loss: 0.4418\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3664 - val_loss: 0.4469\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3636 - val_loss: 0.4359\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3528 - val_loss: 0.4380\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3705 - val_loss: 0.4355\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3707 - val_loss: 0.4368\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3607 - val_loss: 0.4362\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3465 - val_loss: 0.4353\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3536 - val_loss: 0.4349\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3585 - val_loss: 0.4355\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3580 - val_loss: 0.4352\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3610 - val_loss: 0.4378\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3441 - val_loss: 0.4347\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3543 - val_loss: 0.4359\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3483 - val_loss: 0.4347\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3542 - val_loss: 0.4333\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3506 - val_loss: 0.4330\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 246ms/step - loss: 0.3423 - val_loss: 0.4324\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3460 - val_loss: 0.4338\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3569 - val_loss: 0.4325\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.3496 - val_loss: 0.4332\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3450 - val_loss: 0.4330\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3473 - val_loss: 0.4328\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3401 - val_loss: 0.4314\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3484 - val_loss: 0.4313\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3374 - val_loss: 0.4313\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3462 - val_loss: 0.4313\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3464 - val_loss: 0.4312\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3444 - val_loss: 0.4312\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3371 - val_loss: 0.4312\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3401 - val_loss: 0.4313\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3384 - val_loss: 0.4313\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3423 - val_loss: 0.4313\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3426 - val_loss: 0.4312\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3422 - val_loss: 0.4312\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3425 - val_loss: 0.4312\n",
      "[2022_04_20-19:20:01] Training the entire fine-tuned model...\n",
      "[2022_04_20-19:20:09] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 866ms/step - loss: 0.3317 - val_loss: 0.4302\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3451 - val_loss: 0.4301\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3342 - val_loss: 0.4320\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3241 - val_loss: 0.4304\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3334 - val_loss: 0.4294\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.3311 - val_loss: 0.4255\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3231 - val_loss: 0.4258\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3314 - val_loss: 0.4245\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3192 - val_loss: 0.4259\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3227 - val_loss: 0.4257\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.3174 - val_loss: 0.4277\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3065 - val_loss: 0.4243\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3020 - val_loss: 0.4245\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.2968 - val_loss: 0.4263\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.2939 - val_loss: 0.4239\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3014 - val_loss: 0.4243\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.2998 - val_loss: 0.4244\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.2840 - val_loss: 0.4264\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2866 - val_loss: 0.4274\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2663 - val_loss: 0.4270\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.2798 - val_loss: 0.4263\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.2757 - val_loss: 0.4261\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.2737 - val_loss: 0.4271\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_20-19:21:00] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-19:21:00] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:21:00] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 836ms/step - loss: 0.2963 - val_loss: 0.4239\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>560</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  560  22\n",
       "1  126  25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25252525252525254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3r1qb6lk) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 25470... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃█▄▃▂▂▂▁▂▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>14</td></tr><tr><td>best_val_loss</td><td>0.42389</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.29626</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42391</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">deep-cherry-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/3r1qb6lk\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204a/runs/3r1qb6lk</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_191830-3r1qb6lk/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3r1qb6lk). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204b/runs/3bhy18go\" target=\"_blank\">vocal-dragon-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_20-19:22:31] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:22:31] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:22:31] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 586ms/step - loss: 0.8084 - val_loss: 0.7572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.6651 - val_loss: 0.4840\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5288 - val_loss: 0.5858\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5242 - val_loss: 0.4966\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4643 - val_loss: 0.4611\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4409 - val_loss: 0.4662\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4274 - val_loss: 0.4557\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4205 - val_loss: 0.4560\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4311 - val_loss: 0.4815\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4278 - val_loss: 0.4439\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4106 - val_loss: 0.4486\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4016 - val_loss: 0.4405\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4043 - val_loss: 0.4370\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4128 - val_loss: 0.4513\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4124 - val_loss: 0.4592\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3875 - val_loss: 0.4500\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3992 - val_loss: 0.4399\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3764 - val_loss: 0.4319\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3815 - val_loss: 0.4314\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3689 - val_loss: 0.4329\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3735 - val_loss: 0.4321\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3883 - val_loss: 0.4313\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3869 - val_loss: 0.4318\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3754 - val_loss: 0.4304\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3708 - val_loss: 0.4314\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3755 - val_loss: 0.4296\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3698 - val_loss: 0.4294\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3690 - val_loss: 0.4314\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3713 - val_loss: 0.4295\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3668 - val_loss: 0.4300\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3656 - val_loss: 0.4282\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3709 - val_loss: 0.4295\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3686 - val_loss: 0.4276\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3723 - val_loss: 0.4288\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3675 - val_loss: 0.4297\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3666 - val_loss: 0.4307\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3524 - val_loss: 0.4278\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3632 - val_loss: 0.4295\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3668 - val_loss: 0.4274\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3706 - val_loss: 0.4293\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3571 - val_loss: 0.4268\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3621 - val_loss: 0.4265\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3694 - val_loss: 0.4265\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3640 - val_loss: 0.4268\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3659 - val_loss: 0.4263\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3591 - val_loss: 0.4264\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3654 - val_loss: 0.4270\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3535 - val_loss: 0.4265\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3643 - val_loss: 0.4263\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3622 - val_loss: 0.4264\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3586 - val_loss: 0.4264\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3613 - val_loss: 0.4263\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3685 - val_loss: 0.4264\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3634 - val_loss: 0.4264\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3520 - val_loss: 0.4264\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3651 - val_loss: 0.4264\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3627 - val_loss: 0.4264\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "[2022_04_20-19:23:44] Training the entire fine-tuned model...\n",
      "[2022_04_20-19:23:54] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3713WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1068s vs `on_train_batch_end` time: 0.1399s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1068s vs `on_train_batch_end` time: 0.1399s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 710ms/step - loss: 0.3713 - val_loss: 0.4420\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3655 - val_loss: 0.4364\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.3500 - val_loss: 0.4316\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3566 - val_loss: 0.4286\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3449 - val_loss: 0.4311\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3461 - val_loss: 0.4319\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3443 - val_loss: 0.4301\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3388 - val_loss: 0.4307\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 336ms/step - loss: 0.3408 - val_loss: 0.4296\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3401 - val_loss: 0.4292\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3439 - val_loss: 0.4297\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3350 - val_loss: 0.4300\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_20-19:24:26] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-19:24:26] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:24:40] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3601WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1090s vs `on_train_batch_end` time: 0.1372s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1090s vs `on_train_batch_end` time: 0.1372s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 694ms/step - loss: 0.3601 - val_loss: 0.4281\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>438</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  438  28\n",
       "1   91  28"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_20_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_20_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3bhy18go) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 25968... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███▁▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▆▄▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>48</td></tr><tr><td>best_val_loss</td><td>0.42632</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36014</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42813</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">vocal-dragon-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204b/runs/3bhy18go\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204b/runs/3bhy18go</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_192215-3bhy18go/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3bhy18go). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/2czq6ief\" target=\"_blank\">driven-wood-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-19:25:34] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:25:34] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:25:34] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 782ms/step - loss: 0.9938 - val_loss: 0.4993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.7151 - val_loss: 0.6984\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.6289 - val_loss: 0.5187\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.5240 - val_loss: 0.5319\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.5132 - val_loss: 0.4768\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4600 - val_loss: 0.4925\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.4507 - val_loss: 0.4734\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4413 - val_loss: 0.4964\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4435 - val_loss: 0.4709\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4144 - val_loss: 0.4894\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4472 - val_loss: 0.4484\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4042 - val_loss: 0.4462\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4020 - val_loss: 0.4445\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3850 - val_loss: 0.4427\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3769 - val_loss: 0.4419\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3795 - val_loss: 0.4400\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 245ms/step - loss: 0.3831 - val_loss: 0.4384\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3730 - val_loss: 0.4483\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3746 - val_loss: 0.4465\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3621 - val_loss: 0.4365\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3710 - val_loss: 0.4691\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3629 - val_loss: 0.4693\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3751 - val_loss: 0.4432\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3675 - val_loss: 0.4485\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3672 - val_loss: 0.4402\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3415 - val_loss: 0.4431\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3486 - val_loss: 0.4311\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3467 - val_loss: 0.4358\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3452 - val_loss: 0.4313\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3514 - val_loss: 0.4320\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3416 - val_loss: 0.4324\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3413 - val_loss: 0.4307\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3431 - val_loss: 0.4305\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3495 - val_loss: 0.4307\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3420 - val_loss: 0.4305\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3371 - val_loss: 0.4305\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3451 - val_loss: 0.4304\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3503 - val_loss: 0.4305\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3343 - val_loss: 0.4303\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3371 - val_loss: 0.4303\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3316 - val_loss: 0.4302\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.3365 - val_loss: 0.4305\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3379 - val_loss: 0.4308\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3387 - val_loss: 0.4306\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3431 - val_loss: 0.4306\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3322 - val_loss: 0.4303\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3335 - val_loss: 0.4302\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3373 - val_loss: 0.4301\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3411 - val_loss: 0.4301\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 245ms/step - loss: 0.3463 - val_loss: 0.4302\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3435 - val_loss: 0.4301\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3471 - val_loss: 0.4301\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3395 - val_loss: 0.4301\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.3324 - val_loss: 0.4301\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3266 - val_loss: 0.4301\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3421 - val_loss: 0.4302\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3352 - val_loss: 0.4302\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3413 - val_loss: 0.4302\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3468 - val_loss: 0.4301\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "[2022_04_20-19:26:51] Training the entire fine-tuned model...\n",
      "[2022_04_20-19:27:14] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 831ms/step - loss: 0.3500 - val_loss: 0.4332\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3375 - val_loss: 0.4279\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3383 - val_loss: 0.4278\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3523 - val_loss: 0.4267\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3205 - val_loss: 0.4237\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3279 - val_loss: 0.4242\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.2998 - val_loss: 0.4230\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3085 - val_loss: 0.4261\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2888 - val_loss: 0.4215\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.2990 - val_loss: 0.4217\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2773 - val_loss: 0.4221\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2745 - val_loss: 0.4274\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.2745 - val_loss: 0.4251\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.2671 - val_loss: 0.4216\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2620 - val_loss: 0.4213\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2507 - val_loss: 0.4214\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.2467 - val_loss: 0.4219\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2428 - val_loss: 0.4230\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2527 - val_loss: 0.4243\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2487 - val_loss: 0.4238\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2438 - val_loss: 0.4222\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.2434 - val_loss: 0.4224\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2395 - val_loss: 0.4222\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "[2022_04_20-19:28:03] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-19:28:03] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:28:03] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 11s 862ms/step - loss: 0.2548 - val_loss: 0.4208\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>551</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>119</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  551  31\n",
       "1  119  32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29906542056074764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2czq6ief) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 26430... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇█▅▅▅▃▃▃▂▃▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.42083</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.25479</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.42083</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">driven-wood-16</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/2czq6ief\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204a/runs/2czq6ief</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_192518-2czq6ief/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2czq6ief). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204b/runs/339s7cg3\" target=\"_blank\">fallen-butterfly-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_20-19:29:06] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:29:06] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:29:06] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 587ms/step - loss: 0.7819 - val_loss: 0.5938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.5556 - val_loss: 0.4859\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4968 - val_loss: 0.5026\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4730 - val_loss: 0.4707\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4545 - val_loss: 0.4558\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4469 - val_loss: 0.4533\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4242 - val_loss: 0.4839\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4370 - val_loss: 0.4562\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4283 - val_loss: 0.5540\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4702 - val_loss: 0.4954\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4266 - val_loss: 0.4716\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4222 - val_loss: 0.4454\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4020 - val_loss: 0.4388\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.4003 - val_loss: 0.4402\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3967 - val_loss: 0.4392\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4066 - val_loss: 0.4374\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3937 - val_loss: 0.4371\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3975 - val_loss: 0.4367\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3996 - val_loss: 0.4374\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 0.3859 - val_loss: 0.4364\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3947 - val_loss: 0.4351\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3931 - val_loss: 0.4350\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3815 - val_loss: 0.4359\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3971 - val_loss: 0.4354\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3845 - val_loss: 0.4378\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3852 - val_loss: 0.4372\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3969 - val_loss: 0.4351\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3833 - val_loss: 0.4344\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3837 - val_loss: 0.4355\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3845 - val_loss: 0.4334\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3894 - val_loss: 0.4335\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3843 - val_loss: 0.4332\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3865 - val_loss: 0.4335\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.3797 - val_loss: 0.4333\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3799 - val_loss: 0.4330\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3867 - val_loss: 0.4330\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3806 - val_loss: 0.4337\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3815 - val_loss: 0.4326\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3788 - val_loss: 0.4324\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3803 - val_loss: 0.4323\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3832 - val_loss: 0.4323\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3758 - val_loss: 0.4326\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3758 - val_loss: 0.4320\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3820 - val_loss: 0.4320\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3833 - val_loss: 0.4329\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 197ms/step - loss: 0.3793 - val_loss: 0.4319\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3869 - val_loss: 0.4319\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3755 - val_loss: 0.4318\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3686 - val_loss: 0.4321\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3747 - val_loss: 0.4317\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3756 - val_loss: 0.4314\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3746 - val_loss: 0.4314\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3829 - val_loss: 0.4319\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3659 - val_loss: 0.4313\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3825 - val_loss: 0.4313\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3760 - val_loss: 0.4321\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3802 - val_loss: 0.4313\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3753 - val_loss: 0.4313\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3712 - val_loss: 0.4312\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3728 - val_loss: 0.4312\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3722 - val_loss: 0.4312\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 0.3661 - val_loss: 0.4312\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3717 - val_loss: 0.4312\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3815 - val_loss: 0.4312\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3732 - val_loss: 0.4313\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3725 - val_loss: 0.4312\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3766 - val_loss: 0.4313\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.3696 - val_loss: 0.4313\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3766 - val_loss: 0.4312\n",
      "[2022_04_20-19:30:33] Training the entire fine-tuned model...\n",
      "[2022_04_20-19:30:41] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3817WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1055s vs `on_train_batch_end` time: 0.1415s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1055s vs `on_train_batch_end` time: 0.1415s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 685ms/step - loss: 0.3817 - val_loss: 0.4309\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 341ms/step - loss: 0.3673 - val_loss: 0.4304\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3707 - val_loss: 0.4311\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 341ms/step - loss: 0.3657 - val_loss: 0.4402\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3646 - val_loss: 0.4315\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3547 - val_loss: 0.4310\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 341ms/step - loss: 0.3489 - val_loss: 0.4318\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.3565 - val_loss: 0.4311\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 338ms/step - loss: 0.3451 - val_loss: 0.4326\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 341ms/step - loss: 0.3539 - val_loss: 0.4325\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_20-19:31:09] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-19:31:09] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:31:09] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3680WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1095s vs `on_train_batch_end` time: 0.1397s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1095s vs `on_train_batch_end` time: 0.1397s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 718ms/step - loss: 0.3680 - val_loss: 0.4304\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>454</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  454  12\n",
       "1   95  24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3096774193548387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_20_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_20_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:339s7cg3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 26958... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███▁▁▂▂▁</td></tr><tr><td>loss</td><td>█▃▃▂▂▂▂▂▂▂▂▁▁▂▁▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▃▆▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_val_loss</td><td>0.43038</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36803</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.43039</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fallen-butterfly-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204b/runs/339s7cg3\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204b/runs/339s7cg3</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_192842-339s7cg3/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:339s7cg3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/38i0g0o3\" target=\"_blank\">grateful-valley-17</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-19:32:05] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:32:05] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:32:05] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 742ms/step - loss: 0.8491 - val_loss: 1.0534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.6742 - val_loss: 0.7875\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.6286 - val_loss: 0.6512\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.5244 - val_loss: 0.5790\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.4920 - val_loss: 0.5304\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4769 - val_loss: 0.5134\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4409 - val_loss: 0.4927\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4298 - val_loss: 0.4877\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4291 - val_loss: 0.4568\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.4005 - val_loss: 0.4632\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4213 - val_loss: 0.4462\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4213 - val_loss: 0.4502\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4010 - val_loss: 0.4563\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.4000 - val_loss: 0.4405\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3924 - val_loss: 0.4434\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3895 - val_loss: 0.4788\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3926 - val_loss: 0.4732\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3770 - val_loss: 0.4411\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3694 - val_loss: 0.4385\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3622 - val_loss: 0.4377\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3668 - val_loss: 0.4375\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3576 - val_loss: 0.4372\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3546 - val_loss: 0.4367\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3533 - val_loss: 0.4361\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3624 - val_loss: 0.4374\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3661 - val_loss: 0.4368\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3558 - val_loss: 0.4351\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3459 - val_loss: 0.4351\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3573 - val_loss: 0.4348\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3529 - val_loss: 0.4369\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3529 - val_loss: 0.4367\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3504 - val_loss: 0.4341\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3491 - val_loss: 0.4361\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3536 - val_loss: 0.4363\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3566 - val_loss: 0.4332\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3502 - val_loss: 0.4400\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3663 - val_loss: 0.4358\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3565 - val_loss: 0.4327\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3465 - val_loss: 0.4325\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3368 - val_loss: 0.4311\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3449 - val_loss: 0.4316\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3417 - val_loss: 0.4375\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3437 - val_loss: 0.4308\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3508 - val_loss: 0.4313\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3338 - val_loss: 0.4383\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3353 - val_loss: 0.4299\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3347 - val_loss: 0.4303\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3397 - val_loss: 0.4300\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3361 - val_loss: 0.4297\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3375 - val_loss: 0.4291\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3352 - val_loss: 0.4293\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3445 - val_loss: 0.4314\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3276 - val_loss: 0.4289\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3301 - val_loss: 0.4288\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3411 - val_loss: 0.4287\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3294 - val_loss: 0.4310\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3231 - val_loss: 0.4340\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3392 - val_loss: 0.4281\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3321 - val_loss: 0.4276\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3301 - val_loss: 0.4313\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3352 - val_loss: 0.4294\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3294 - val_loss: 0.4275\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3258 - val_loss: 0.4275\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3264 - val_loss: 0.4271\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3202 - val_loss: 0.4273\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3298 - val_loss: 0.4270\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3324 - val_loss: 0.4269\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3184 - val_loss: 0.4269\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3282 - val_loss: 0.4268\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3255 - val_loss: 0.4269\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3298 - val_loss: 0.4267\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3316 - val_loss: 0.4267\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3244 - val_loss: 0.4268\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3250 - val_loss: 0.4266\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3198 - val_loss: 0.4264\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3218 - val_loss: 0.4264\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3344 - val_loss: 0.4264\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3298 - val_loss: 0.4264\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3151 - val_loss: 0.4264\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3246 - val_loss: 0.4264\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3283 - val_loss: 0.4264\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3232 - val_loss: 0.4264\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3273 - val_loss: 0.4264\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3380 - val_loss: 0.4264\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3234 - val_loss: 0.4264\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3208 - val_loss: 0.4264\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3209 - val_loss: 0.4264\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3230 - val_loss: 0.4264\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.3311 - val_loss: 0.4264\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3279 - val_loss: 0.4264\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3213 - val_loss: 0.4264\n",
      "\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3220 - val_loss: 0.4264\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3222 - val_loss: 0.4264\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3270 - val_loss: 0.4264\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3148 - val_loss: 0.4264\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3204 - val_loss: 0.4264\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.3301 - val_loss: 0.4264\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3216 - val_loss: 0.4264\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3275 - val_loss: 0.4264\n",
      "\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3379 - val_loss: 0.4264\n",
      "[2022_04_20-19:34:35] Training the entire fine-tuned model...\n",
      "[2022_04_20-19:34:43] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 816ms/step - loss: 0.6057 - val_loss: 0.6043\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.4360 - val_loss: 0.4699\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.4149 - val_loss: 0.4430\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 372ms/step - loss: 0.3658 - val_loss: 0.4538\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.3639 - val_loss: 0.4300\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3226 - val_loss: 0.4409\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.3313 - val_loss: 0.4267\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.3050 - val_loss: 0.4300\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.2866 - val_loss: 0.4252\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2637 - val_loss: 0.4252\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2597 - val_loss: 0.4264\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.2340 - val_loss: 0.4347\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2230 - val_loss: 0.5144\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2131 - val_loss: 0.4698\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.2196 - val_loss: 0.4425\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.1937 - val_loss: 0.4467\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.1739 - val_loss: 0.4761\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_20-19:35:22] Training on final epochs of sequence length 512...\n",
      "[2022_04_20-19:35:22] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:35:22] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 834ms/step - loss: 0.2761 - val_loss: 0.4270\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>133</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  570  12\n",
       "1  133  18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19889502762430938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_20_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:38i0g0o3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 27470... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▃▃▃▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▁▁▂</td></tr><tr><td>lr</td><td>██████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_val_loss</td><td>0.42515</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.27613</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.42701</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">grateful-valley-17</strong>: <a href=\"https://wandb.ai/kvetab/Split%20old%204a/runs/38i0g0o3\" target=\"_blank\">https://wandb.ai/kvetab/Split%20old%204a/runs/38i0g0o3</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_193148-38i0g0o3/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:38i0g0o3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%20old%204b/runs/vqmsoi05\" target=\"_blank\">pleasant-wood-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%20old%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_20-19:36:25] Training set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:36:25] Validation set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:36:25] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 578ms/step - loss: 0.7732 - val_loss: 0.7297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.5885 - val_loss: 0.4778\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4999 - val_loss: 0.5877\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.5343 - val_loss: 0.4718\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4667 - val_loss: 0.4706\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4398 - val_loss: 0.4743\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4284 - val_loss: 0.4524\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4206 - val_loss: 0.4500\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4116 - val_loss: 0.4453\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4102 - val_loss: 0.4432\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3955 - val_loss: 0.4607\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 0.4289 - val_loss: 0.4414\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4226 - val_loss: 0.5073\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4315 - val_loss: 0.4476\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4134 - val_loss: 0.4423\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3826 - val_loss: 0.4386\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3847 - val_loss: 0.4386\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3745 - val_loss: 0.4317\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.3684 - val_loss: 0.4384\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3874"
     ]
    }
   ],
   "source": [
    "for seed in [4]: # 4\n",
    "    train = pd.read_csv(path.join(DATA_DIR, f\"chen/deduplicated/crossval/chen_{seed}_a.csv\"), index_col=0)\n",
    "    test = pd.read_csv(path.join(DATA_DIR, f\"chen/deduplicated/crossval/chen_{seed}_b.csv\"), index_col=0)\n",
    "    train[\"seq\"] = train[\"heavy\"] + train[\"light\"]\n",
    "    test[\"seq\"] = test[\"heavy\"] + test[\"light\"]\n",
    "    for pat in patience:\n",
    "        for lr in learning_rate:\n",
    "            finetune_by_settings_and_data(pat, lr, f\"Split old {seed}a\", train, test, f\"{seed}a\")\n",
    "            finetune_by_settings_and_data(pat, lr, f\"Split old {seed}b\", test, train, f\"{seed}b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec00e839-cf6f-400c-9cba-3fc6e08f52b4",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56644fd0-ccff-434a-87a3-6a053d7f1416",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e816cc74-29d5-4c85-9fcc-1db0f5042900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_20-17:24:46] Test set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n"
     ]
    }
   ],
   "source": [
    "seq_len = 512\n",
    "encoded_test_set = encode_dataset(test_data[\"seq\"], test_data[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'Test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ebf1fb3-80bd-423c-a6af-2c51d82675ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X, test_Y, test_sample_weigths = encoded_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76d7099d-ada5-4464-8337-e64f1eaa7fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antibody_ID</th>\n",
       "      <th>heavy</th>\n",
       "      <th>light</th>\n",
       "      <th>CDR_length</th>\n",
       "      <th>PSH</th>\n",
       "      <th>PPC</th>\n",
       "      <th>PNC</th>\n",
       "      <th>SFvCSP</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abagovomab</td>\n",
       "      <td>QVKLQESGAELARPGASVKLSCKASGYTFTNYWMQWVKQRPGQGLD...</td>\n",
       "      <td>DIELTQSPASLSASVGETVTITCQASENIYSYLAWHQQKQGKSPQL...</td>\n",
       "      <td>46</td>\n",
       "      <td>129.7603</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abituzumab</td>\n",
       "      <td>QVQLQQSGGELAKPGASVKVSCKASGYTFSSFWMHWVRQAPGQGLE...</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCRASQDISNYLAWYQQKPGKAPKL...</td>\n",
       "      <td>45</td>\n",
       "      <td>115.9106</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0421</td>\n",
       "      <td>-3.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abrilumab</td>\n",
       "      <td>QVQLVQSGAEVKKPGASVKVSCKVSGYTLSDLSIHWVRQAPGKGLE...</td>\n",
       "      <td>DIQMTQSPSSVSASVGDRVTITCRASQGISSWLAWYQQKPGKAPKL...</td>\n",
       "      <td>45</td>\n",
       "      <td>109.6995</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8965</td>\n",
       "      <td>-4.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Actoxumab</td>\n",
       "      <td>QVQLVESGGGVVQPGRSLRLSCAASGFSFSNYGMHWVRQAPGKGLE...</td>\n",
       "      <td>DIQMTQSPSSVSASVGDRVTITCRASQGISSWLAWYQHKPGKAPKL...</td>\n",
       "      <td>49</td>\n",
       "      <td>112.6290</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.1247</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adalimumab</td>\n",
       "      <td>EVQLVESGGGLVQPGRSLRLSCAASGFTFDDYAMHWVRQAPGKGLE...</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCRASQGIRNYLAWYQQKPGKAPKL...</td>\n",
       "      <td>48</td>\n",
       "      <td>111.2512</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>1.1364</td>\n",
       "      <td>-19.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Antibody_ID                                              heavy  \\\n",
       "0  Abagovomab  QVKLQESGAELARPGASVKLSCKASGYTFTNYWMQWVKQRPGQGLD...   \n",
       "1  Abituzumab  QVQLQQSGGELAKPGASVKVSCKASGYTFSSFWMHWVRQAPGQGLE...   \n",
       "2   Abrilumab  QVQLVQSGAEVKKPGASVKVSCKVSGYTLSDLSIHWVRQAPGKGLE...   \n",
       "3   Actoxumab  QVQLVESGGGVVQPGRSLRLSCAASGFSFSNYGMHWVRQAPGKGLE...   \n",
       "4  Adalimumab  EVQLVESGGGLVQPGRSLRLSCAASGFTFDDYAMHWVRQAPGKGLE...   \n",
       "\n",
       "                                               light  CDR_length       PSH  \\\n",
       "0  DIELTQSPASLSASVGETVTITCQASENIYSYLAWHQQKQGKSPQL...          46  129.7603   \n",
       "1  DIQMTQSPSSLSASVGDRVTITCRASQDISNYLAWYQQKPGKAPKL...          45  115.9106   \n",
       "2  DIQMTQSPSSVSASVGDRVTITCRASQGISSWLAWYQQKPGKAPKL...          45  109.6995   \n",
       "3  DIQMTQSPSSVSASVGDRVTITCRASQGISSWLAWYQHKPGKAPKL...          49  112.6290   \n",
       "4  DIQMTQSPSSLSASVGDRVTITCRASQGIRNYLAWYQQKPGKAPKL...          48  111.2512   \n",
       "\n",
       "      PPC     PNC  SFvCSP  Y  \n",
       "0  0.0000  0.0000   16.32  1  \n",
       "1  0.0954  0.0421   -3.10  1  \n",
       "2  0.0000  0.8965   -4.00  1  \n",
       "3  0.0000  1.1247    3.10  1  \n",
       "4  0.0485  1.1364  -19.50  1  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tap_data = pd.read_csv(path.join(DATA_DIR, \"tap/TAP_data.csv\"))\n",
    "tap_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5cc5b7cb-1cc4-450d-a52d-2a027f00a1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tap_data[\"seq\"] = tap_data[\"heavy\"] +  tap_data[\"light\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b06ed7a5-5e02-48e8-8e69-6355c776ef1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_20-17:25:41] TAP set: Filtered out 0 of 241 (0.0%) records of lengths exceeding 510.\n"
     ]
    }
   ],
   "source": [
    "encoded_tap_set = encode_dataset(tap_data[\"seq\"], tap_data[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'TAP set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e96bc721-e479-400a-8d09-e0b6c9635a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tap_X, tap_Y, tap_sample_weigths = encoded_tap_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49e9d36a-63d9-45b6-bcf8-b66e75fd506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = path.join(DATA_DIR, \"protein_bert/2022_03_30__05.pkl\")\n",
    "model = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1959c14b-c779-4741-9a0a-b2a453fd1fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6486486486486486"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(test_X, batch_size=32)\n",
    "y_pred_classes = (y_pred >= 0.5)\n",
    "f1_score(test_Y, y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c95a893-f467-405f-81b6-59b0277c0a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34285714285714286"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(tap_X, batch_size=32)\n",
    "y_pred_classes = (y_pred >= 0.5)\n",
    "f1_score(tap_Y, y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "345121cc-6ccb-46c2-aef0-5b173263f069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23651452282157676"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(tap_Y, y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "566d023a-b6e2-48e9-b884-c48037cc54bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_20-17:24:54] Test set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "seq_len = 512\n",
    "encoded_test_set = encode_dataset(test_data[\"seq\"], test_data[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'Test set')\n",
    "test_X, test_Y, test_sample_weigths = encoded_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb44bed8-ab18-4ae3-b16e-8c90e38c027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_name, dir_name, x_test, y_test, x_tap, y_tap):\n",
    "    model_path = path.join(DATA_DIR, f\"protein_bert/{dir_name}/{model_name}\")\n",
    "    model = keras.models.load_model(model_path)\n",
    "    parts = model_name[11:].split(\"_\")\n",
    "    patience_stop = parts[0]\n",
    "    patience_lr = parts[1]\n",
    "    lr = parts[2]\n",
    "    \n",
    "    y_pred = model.predict(x_test, batch_size=32)\n",
    "    y_pred_classes = (y_pred >= 0.5)\n",
    "    #print(f\"Model {model_name}\")\n",
    "    #print(f\"Test F1: {f1}\")\n",
    "    metric_dict = {\n",
    "        \"f1\": float(metrics.f1_score(y_test, y_pred_classes)),\n",
    "        \"acc\": float(metrics.accuracy_score(y_test, y_pred_classes)),\n",
    "        \"mcc\": float(metrics.matthews_corrcoef(y_test, y_pred_classes)),\n",
    "        \"auc\": float(metrics.roc_auc_score(y_test, y_pred_classes)),\n",
    "        \"precision\": float(metrics.precision_score(y_test, y_pred_classes)),\n",
    "        \"recall\": float(metrics.recall_score(y_test, y_pred_classes))\n",
    "    }\n",
    "    filename = path.join(DATA_DIR, f\"evaluations/protein_bert/{dir_name}/{model_name}_preds.csv\")\n",
    "    str_preds = [str(int(pred)) for pred in y_pred_classes]\n",
    "    with open(filename, \"wt\") as f:\n",
    "        f.write(\",\".join(str_preds) + \"\\n\")\n",
    "        \n",
    "    filename_sum = path.join(DATA_DIR, f\"evaluations/protein_bert/{dir_name}/all.csv\")\n",
    "    line = [\"protein_bert\", patience_stop, patience_lr, lr, metric_dict[\"f1\"], metric_dict[\"mcc\"], metric_dict[\"acc\"],metric_dict[\"precision\"],metric_dict[\"recall\"],metric_dict[\"auc\"]]\n",
    "    with open(filename_sum, 'a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile, delimiter='\\t')\n",
    "        csvwriter.writerow(line)\n",
    "    \n",
    "    \n",
    "    y_pred = model.predict(x_tap, batch_size=32)\n",
    "    y_pred_classes = (y_pred >= 0.5)\n",
    "    acc = accuracy_score(y_tap, y_pred_classes)\n",
    "    metric_dict = {\n",
    "        \"f1\": float(metrics.f1_score(y_tap, y_pred_classes)),\n",
    "        \"acc\": float(metrics.accuracy_score(y_tap, y_pred_classes)),\n",
    "        \"mcc\": float(metrics.matthews_corrcoef(y_tap, y_pred_classes)),\n",
    "        \"auc\": float(metrics.roc_auc_score(y_tap, y_pred_classes)),\n",
    "        \"precision\": float(metrics.precision_score(y_tap, y_pred_classes)),\n",
    "        \"recall\": float(metrics.recall_score(y_tap, y_pred_classes))\n",
    "    }\n",
    "        \n",
    "    filename_sum = path.join(DATA_DIR, f\"evaluations/protein_bert/{dir_name}/tap.csv\")\n",
    "    line = [\"protein_bert\", patience_stop, patience_lr, lr, metric_dict[\"f1\"], metric_dict[\"mcc\"], metric_dict[\"acc\"],metric_dict[\"precision\"],metric_dict[\"recall\"],metric_dict[\"auc\"]]\n",
    "    with open(filename_sum, 'a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile, delimiter='\\t')\n",
    "        csvwriter.writerow(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ab02fee7-bc99-42f9-8a64-5d7c60f12e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_10-09:53:46] Test set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n"
     ]
    }
   ],
   "source": [
    "seed = 4\n",
    "split = \"a\"\n",
    "train_data = pd.read_csv(path.join(DATA_DIR, f\"chen/deduplicated/crossval/chen_{seed}_a.csv\"), index_col=0)\n",
    "test_data = pd.read_csv(path.join(DATA_DIR, f\"chen/deduplicated/crossval/chen_{seed}_b.csv\"), index_col=0)\n",
    "train_data[\"seq\"] = train_data[\"heavy\"] + train_data[\"light\"]\n",
    "test_data[\"seq\"] = test_data[\"heavy\"] + test_data[\"light\"]\n",
    "\n",
    "encoded_test_set = encode_dataset(test_data[\"seq\"], test_data[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'Test set')\n",
    "test_X, test_Y, test_sample_weigths = encoded_test_set\n",
    "\n",
    "test_model(\"2022_04_09_3_3_0.0001\", \"4a\", test_X, test_Y, tap_X, tap_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bddc1b4-70ec-40c7-82b7-86c006cf6412",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [4] # [ 18, 27, 36, 42] # 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b626b4f1-a24a-46ee-8621-3f22949b228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f201d8c-c65c-4ebb-9b78-6f2ff4bf4991",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for seed in seeds:\n",
    "    train_data = pd.read_csv(path.join(DATA_DIR, f\"chen/deduplicated/crossval/chen_{seed}_a.csv\"), index_col=0)\n",
    "    test_data = pd.read_csv(path.join(DATA_DIR, f\"chen/deduplicated/crossval/chen_{seed}_b.csv\"), index_col=0)\n",
    "    train_data[\"seq\"] = train_data[\"heavy\"] + train_data[\"light\"]\n",
    "    test_data[\"seq\"] = test_data[\"heavy\"] + test_data[\"light\"]\n",
    "    encoded_test_set = encode_dataset(test_data[\"seq\"], test_data[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'Test set')\n",
    "    test_X, test_Y, test_sample_weigths = encoded_test_set\n",
    "    encoded_train_set = encode_dataset(train_data[\"seq\"], train_data[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'Train set')\n",
    "    train_X, train_Y, test_sample_weigths = encoded_train_set\n",
    "    \n",
    "    for split in [\"a\", \"b\"]:\n",
    "        for model in os.listdir(path.join(DATA_DIR, f\"protein_bert/{seed}{split}\")):\n",
    "            if model.startswith(\"2022_04_20\"):\n",
    "                if split == \"a\":\n",
    "                    test_model(model, f\"{seed}{split}\", test_X, test_Y, tap_X, tap_Y)\n",
    "                else:\n",
    "                    test_model(model, f\"{seed}{split}\", train_X, train_Y, tap_X, tap_Y)\n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc2be9b-108a-4163-a579-07ab6ca4e2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76574dfa-f777-4b64-a8f5-08df816879ab",
   "metadata": {},
   "source": [
    "# Full train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71fbac67-086d-41da-8141-aef3c17a6dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "760d1c1a-68dd-42c8-958b-0cbc2bbb4796",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_train.csv\"), index_col=0)\n",
    "test_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_test.csv\"), index_col=0)\n",
    "train_data[\"seq\"] = train_data[\"heavy\"] + train_data[\"light\"]\n",
    "test_data[\"seq\"] = test_data[\"heavy\"] + test_data[\"light\"]\n",
    "valid_data, test_data = train_test_split(test_data, test_size=0.5, random_state=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "358e32a7-9fdf-4a98-a33d-2352cdf0a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_old = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_train_data_old.csv\"), index_col=0)\n",
    "test_data_old = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_test_data_old.csv\"), index_col=0)\n",
    "valid_data_old = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_valid_data_old.csv\"), index_col=0)\n",
    "train_data_old[\"seq\"] = train_data_old[\"heavy\"] + train_data_old[\"light\"]\n",
    "test_data_old[\"seq\"] = test_data_old[\"heavy\"] + test_data_old[\"light\"]\n",
    "valid_data_old[\"seq\"] = valid_data_old[\"heavy\"] + valid_data_old[\"light\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a42ee61a-9dde-446f-8e80-040956e551b8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:rc09kp9o) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 3994... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▄▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▂▂▂▂▁▂▂▁▁▂▁▂▁▁▁▁▁▁▁▂▁▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>13</td></tr><tr><td>best_val_loss</td><td>0.45486</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32702</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.46158</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">rosy-cloud-6</strong>: <a href=\"https://wandb.ai/kvetab/Easter/runs/rc09kp9o\" target=\"_blank\">https://wandb.ai/kvetab/Easter/runs/rc09kp9o</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_143331-rc09kp9o/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:rc09kp9o). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/2icd7p5e\" target=\"_blank\">pious-voice-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_15-14:37:59] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:37:59] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:37:59] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 286ms/step - loss: 0.8872 - val_loss: 0.7528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.5899 - val_loss: 0.5730\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.5244 - val_loss: 0.5120\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4718 - val_loss: 0.4757\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4801 - val_loss: 0.4985\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4512 - val_loss: 0.4987\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4273 - val_loss: 0.4444\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4183 - val_loss: 0.4401\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4154 - val_loss: 0.4394\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4138 - val_loss: 0.4271\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4076 - val_loss: 0.4960\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4425 - val_loss: 0.4456\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4032 - val_loss: 0.4281\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_15-14:38:26] Training the entire fine-tuned model...\n",
      "[2022_04_15-14:38:34] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.4165WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1160s vs `on_train_batch_end` time: 0.1431s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1160s vs `on_train_batch_end` time: 0.1431s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 427ms/step - loss: 0.4031 - val_loss: 0.4263\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.4097 - val_loss: 0.4271\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.4015 - val_loss: 0.4251\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.4034 - val_loss: 0.4243\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.4046 - val_loss: 0.4236\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.4002 - val_loss: 0.4234\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3964 - val_loss: 0.4219\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.4003 - val_loss: 0.4215\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3964 - val_loss: 0.4216\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3974 - val_loss: 0.4200\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3905 - val_loss: 0.4197\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3886 - val_loss: 0.4183\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3860 - val_loss: 0.4182\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3920 - val_loss: 0.4167\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3899 - val_loss: 0.4156\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3962 - val_loss: 0.4159\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3902 - val_loss: 0.4143\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3896 - val_loss: 0.4132\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3836 - val_loss: 0.4128\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3864 - val_loss: 0.4123\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3838 - val_loss: 0.4114\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3764 - val_loss: 0.4114\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3784 - val_loss: 0.4100\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3802 - val_loss: 0.4105\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3739 - val_loss: 0.4085\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3807 - val_loss: 0.4078\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3725 - val_loss: 0.4074\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3730 - val_loss: 0.4068\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3686 - val_loss: 0.4061\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 3s 262ms/step - loss: 0.3682 - val_loss: 0.4064\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3684 - val_loss: 0.4051\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3657 - val_loss: 0.4041\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3686 - val_loss: 0.4035\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3720 - val_loss: 0.4035\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3621 - val_loss: 0.4029\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3628 - val_loss: 0.4025\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 3s 263ms/step - loss: 0.3566 - val_loss: 0.4025\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3629 - val_loss: 0.4004\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3578 - val_loss: 0.4002\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3503 - val_loss: 0.3997\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3554 - val_loss: 0.3982\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3515 - val_loss: 0.3979\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3504 - val_loss: 0.3971\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3478 - val_loss: 0.3980\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3442 - val_loss: 0.3976\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3556 - val_loss: 0.4003\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "[2022_04_15-14:40:55] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-14:40:55] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:40:55] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3607WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1116s vs `on_train_batch_end` time: 0.1497s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1116s vs `on_train_batch_end` time: 0.1497s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 442ms/step - loss: 0.3514 - val_loss: 0.3973\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  20  6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36363636363636365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2icd7p5e) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 4331... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▃▂▂▂▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>42</td></tr><tr><td>best_val_loss</td><td>0.39712</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35142</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.39726</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">pious-voice-1</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/2icd7p5e\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/2icd7p5e</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_143743-2icd7p5e/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2icd7p5e). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/12ryvo2g\" target=\"_blank\">decent-field-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_15-14:41:49] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:41:49] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:41:49] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 292ms/step - loss: 0.7868 - val_loss: 0.5233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.5189 - val_loss: 0.5264\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.4777 - val_loss: 0.4837\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4511 - val_loss: 0.4648\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.4381 - val_loss: 0.4581\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4323 - val_loss: 0.4499\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4276 - val_loss: 0.4476\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4202 - val_loss: 0.4336\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4110 - val_loss: 0.4281\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4066 - val_loss: 0.4312\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4042 - val_loss: 0.4202\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4117 - val_loss: 0.4244\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4000 - val_loss: 0.4191\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4057 - val_loss: 0.4178\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3917 - val_loss: 0.4095\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4015 - val_loss: 0.4206\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4042 - val_loss: 0.4054\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3935 - val_loss: 0.4044\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3775 - val_loss: 0.4023\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3899 - val_loss: 0.4222\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3847 - val_loss: 0.4142\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3762 - val_loss: 0.4094\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_15-14:42:27] Training the entire fine-tuned model...\n",
      "[2022_04_15-14:42:36] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3674WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1171s vs `on_train_batch_end` time: 0.1435s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1171s vs `on_train_batch_end` time: 0.1435s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 424ms/step - loss: 0.3748 - val_loss: 0.3993\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3819 - val_loss: 0.3959\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3636 - val_loss: 0.4047\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3652 - val_loss: 0.4182\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3663 - val_loss: 0.3890\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 262ms/step - loss: 0.3703 - val_loss: 0.3905\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3569 - val_loss: 0.3861\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3584 - val_loss: 0.3878\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3531 - val_loss: 0.3829\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3461 - val_loss: 0.3797\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3426 - val_loss: 0.3802\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3333 - val_loss: 0.3733\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3299 - val_loss: 0.3742\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3307 - val_loss: 0.4043\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3361 - val_loss: 0.3978\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_15-14:43:28] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-14:43:28] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:43:28] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3072WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1135s vs `on_train_batch_end` time: 0.1459s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1135s vs `on_train_batch_end` time: 0.1459s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 427ms/step - loss: 0.3293 - val_loss: 0.3771\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  20  6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36363636363636365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:12ryvo2g) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 4749... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>██▆▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▃▃▃▂▂▂▃▂▂▂▂▁▁▁▁▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>11</td></tr><tr><td>best_val_loss</td><td>0.37327</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32934</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.37711</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">decent-field-2</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/12ryvo2g\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/12ryvo2g</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_144133-12ryvo2g/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:12ryvo2g). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/2kcqor19\" target=\"_blank\">skilled-music-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_15-14:44:22] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:44:23] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:44:23] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 11s 410ms/step - loss: 0.8471 - val_loss: 0.8598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 112ms/step - loss: 0.6624 - val_loss: 0.7048\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.5235 - val_loss: 0.5406\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4893 - val_loss: 0.4924\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.5007 - val_loss: 0.4811\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4561 - val_loss: 0.4555\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4313 - val_loss: 0.4444\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4252 - val_loss: 0.4371\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4193 - val_loss: 0.4361\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4121 - val_loss: 0.4511\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.4223 - val_loss: 0.4237\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4149 - val_loss: 0.4291\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4070 - val_loss: 0.4199\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3938 - val_loss: 0.4149\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3858 - val_loss: 0.4216\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4110 - val_loss: 0.4349\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4049 - val_loss: 0.4332\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_15-14:44:55] Training the entire fine-tuned model...\n",
      "[2022_04_15-14:45:03] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3821WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1147s vs `on_train_batch_end` time: 0.1436s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1147s vs `on_train_batch_end` time: 0.1436s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 436ms/step - loss: 0.3912 - val_loss: 0.4092\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3843 - val_loss: 0.4085\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3894 - val_loss: 0.4173\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3941 - val_loss: 0.3999\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3899 - val_loss: 0.4064\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3802 - val_loss: 0.3943\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3711 - val_loss: 0.3874\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3669 - val_loss: 0.3930\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3479 - val_loss: 0.3836\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3405 - val_loss: 0.3854\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3461 - val_loss: 0.3908\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3344 - val_loss: 0.4013\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_15-14:45:46] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-14:45:46] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:45:50] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3296WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1143s vs `on_train_batch_end` time: 0.1446s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1143s vs `on_train_batch_end` time: 0.1446s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 426ms/step - loss: 0.3517 - val_loss: 0.3826\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  20  6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36363636363636365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2kcqor19) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 5035... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██▁▁▂▂▃▃▄▄▅▅▅▆▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▃▃▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.38264</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35167</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.38264</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">skilled-music-3</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/2kcqor19\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/2kcqor19</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_144406-2kcqor19/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2kcqor19). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/3og46y6h\" target=\"_blank\">dark-energy-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_15-14:46:45] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:46:45] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:46:45] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 11s 292ms/step - loss: 0.7612 - val_loss: 0.6272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.5468 - val_loss: 0.4896\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4684 - val_loss: 0.4755\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4512 - val_loss: 0.4935\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4544 - val_loss: 0.4561\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4271 - val_loss: 0.4412\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4230 - val_loss: 0.4592\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4347 - val_loss: 0.4376\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4187 - val_loss: 0.4279\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4091 - val_loss: 0.4224\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.4093 - val_loss: 0.4206\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4192 - val_loss: 0.4737\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4227 - val_loss: 0.4522\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.4240 - val_loss: 0.4149\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3966 - val_loss: 0.4456\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4198 - val_loss: 0.4099\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3848 - val_loss: 0.4184\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3835 - val_loss: 0.4026\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4016 - val_loss: 0.4089\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3846 - val_loss: 0.4051\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3662 - val_loss: 0.4075\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_15-14:47:22] Training the entire fine-tuned model...\n",
      "[2022_04_15-14:47:31] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.7090WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1136s vs `on_train_batch_end` time: 0.1461s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1136s vs `on_train_batch_end` time: 0.1461s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 431ms/step - loss: 0.6239 - val_loss: 0.4103\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.4389 - val_loss: 0.4141\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.4006 - val_loss: 0.4025\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3978 - val_loss: 0.3925\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 261ms/step - loss: 0.3651 - val_loss: 0.3878\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3655 - val_loss: 0.3916\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3615 - val_loss: 0.3831\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3431 - val_loss: 0.3794\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3294 - val_loss: 0.4244\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3166 - val_loss: 0.3822\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2888 - val_loss: 0.3879\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_15-14:48:10] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-14:48:10] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:48:11] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3102WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1125s vs `on_train_batch_end` time: 0.1470s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1125s vs `on_train_batch_end` time: 0.1470s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 423ms/step - loss: 0.3266 - val_loss: 0.3858\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  21  5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3og46y6h) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 5283... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▅▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▃▂▂▆▃▃▃▂▂▂▂▂▁▁▂</td></tr><tr><td>lr</td><td>█████████████████████▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▄▄▃▃▃▃▂▂▂▄▃▂▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.37945</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3266</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.38577</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dark-energy-4</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/3og46y6h\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/3og46y6h</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_144628-3og46y6h/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3og46y6h). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/1td139wd\" target=\"_blank\">fast-donkey-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_15-14:49:05] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:49:05] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:49:05] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 289ms/step - loss: 0.7580 - val_loss: 0.6996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.5862 - val_loss: 0.5701\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4914 - val_loss: 0.4816\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4537 - val_loss: 0.4963\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4644 - val_loss: 0.4558\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4447 - val_loss: 0.4844\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4242 - val_loss: 0.4440\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4100 - val_loss: 0.4411\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.4419 - val_loss: 0.4274\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4130 - val_loss: 0.4249\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4055 - val_loss: 0.4190\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4090 - val_loss: 0.4465\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4065 - val_loss: 0.4108\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4002 - val_loss: 0.4094\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3985 - val_loss: 0.4652\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3937 - val_loss: 0.4074\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3829 - val_loss: 0.4118\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3887 - val_loss: 0.4039\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3833 - val_loss: 0.4063\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.4028 - val_loss: 0.4312\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 112ms/step - loss: 0.4020 - val_loss: 0.3983\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3887 - val_loss: 0.3978\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3833 - val_loss: 0.4020\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3829 - val_loss: 0.3955\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3772 - val_loss: 0.3896\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3676 - val_loss: 0.3908\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3584 - val_loss: 0.3864\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3682 - val_loss: 0.4442\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3837 - val_loss: 0.4133\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3937 - val_loss: 0.4148\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3660 - val_loss: 0.3907\n",
      "[2022_04_15-14:49:54] Training the entire fine-tuned model...\n",
      "[2022_04_15-14:50:03] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3714WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1144s vs `on_train_batch_end` time: 0.1462s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1144s vs `on_train_batch_end` time: 0.1462s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 419ms/step - loss: 0.3625 - val_loss: 0.3931\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3573 - val_loss: 0.3862\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3661 - val_loss: 0.3893\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3645 - val_loss: 0.3878\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3721 - val_loss: 0.3858\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3614 - val_loss: 0.3873\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3549 - val_loss: 0.3872\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3484 - val_loss: 0.3872\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3561 - val_loss: 0.3863\n",
      "[2022_04_15-14:50:37] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-14:50:37] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:50:37] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3617WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1146s vs `on_train_batch_end` time: 0.1494s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1146s vs `on_train_batch_end` time: 0.1494s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 424ms/step - loss: 0.3587 - val_loss: 0.3858\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  20  6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36363636363636365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1td139wd) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 5544... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████████████████▃▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▃▃▃▂▂▂▂▂▂▂▂▃▁▂▁▁▂▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.3858</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35865</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.3858</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fast-donkey-5</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/1td139wd\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/1td139wd</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_144849-1td139wd/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1td139wd). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/2q37nhq7\" target=\"_blank\">gentle-yogurt-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_15-14:51:33] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:51:33] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:51:33] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 11s 408ms/step - loss: 0.8012 - val_loss: 0.6419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.5295 - val_loss: 0.4864\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4758 - val_loss: 0.4709\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4437 - val_loss: 0.4611\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4418 - val_loss: 0.4907\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4542 - val_loss: 0.4529\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4329 - val_loss: 0.4376\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4194 - val_loss: 0.4333\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4152 - val_loss: 0.4509\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4057 - val_loss: 0.4198\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4258 - val_loss: 0.4440\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4461 - val_loss: 0.4147\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4121 - val_loss: 0.4151\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4354 - val_loss: 0.4109\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3995 - val_loss: 0.4084\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3930 - val_loss: 0.4049\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3941 - val_loss: 0.4222\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3878 - val_loss: 0.4083\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3848 - val_loss: 0.4101\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3743 - val_loss: 0.4068\n",
      "[2022_04_15-14:52:09] Training the entire fine-tuned model...\n",
      "[2022_04_15-14:52:30] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3754WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1142s vs `on_train_batch_end` time: 0.1456s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1142s vs `on_train_batch_end` time: 0.1456s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 432ms/step - loss: 0.3842 - val_loss: 0.4167\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3932 - val_loss: 0.4001\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3817 - val_loss: 0.4084\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3627 - val_loss: 0.3947\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3709 - val_loss: 0.4067\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3854 - val_loss: 0.3902\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3754 - val_loss: 0.3991\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3699 - val_loss: 0.3932\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3685 - val_loss: 0.3862\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3519 - val_loss: 0.3845\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 262ms/step - loss: 0.3502 - val_loss: 0.3916\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3578 - val_loss: 0.4095\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3513 - val_loss: 0.3977\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3365 - val_loss: 0.3794\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3315 - val_loss: 0.3798\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3367 - val_loss: 0.3858\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3336 - val_loss: 0.3798\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3404 - val_loss: 0.3804\n",
      "[2022_04_15-14:53:30] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-14:53:30] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:53:30] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3405WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1152s vs `on_train_batch_end` time: 0.1471s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1152s vs `on_train_batch_end` time: 0.1471s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 434ms/step - loss: 0.3325 - val_loss: 0.3797\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  19  7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4117647058823529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2q37nhq7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 5863... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▃▃▂▂▂▂▃▂▃▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▃▄▃▃▂▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>13</td></tr><tr><td>best_val_loss</td><td>0.37944</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33247</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.37972</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">gentle-yogurt-6</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/2q37nhq7\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/2q37nhq7</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_145115-2q37nhq7/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2q37nhq7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/1fg0uvtb\" target=\"_blank\">elated-jazz-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_15-14:54:25] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:54:25] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:54:25] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 11s 294ms/step - loss: 0.8363 - val_loss: 0.6832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.5742 - val_loss: 0.5053\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.4685 - val_loss: 0.4820\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.4608 - val_loss: 0.4775\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 104ms/step - loss: 0.4610 - val_loss: 0.5137\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4616 - val_loss: 0.4510\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4408 - val_loss: 0.4732\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4390 - val_loss: 0.4525\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4555 - val_loss: 0.4746\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4265 - val_loss: 0.4324\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4069 - val_loss: 0.4350\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4094 - val_loss: 0.4311\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4021 - val_loss: 0.4284\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4025 - val_loss: 0.4354\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4015 - val_loss: 0.4263\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3965 - val_loss: 0.4304\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3982 - val_loss: 0.4239\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3930 - val_loss: 0.4227\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3922 - val_loss: 0.4219\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 112ms/step - loss: 0.4027 - val_loss: 0.4210\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4029 - val_loss: 0.4287\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3998 - val_loss: 0.4572\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4059 - val_loss: 0.4188\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3888 - val_loss: 0.4178\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3832 - val_loss: 0.4164\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3875 - val_loss: 0.4171\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3898 - val_loss: 0.4309\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3962 - val_loss: 0.4142\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3848 - val_loss: 0.4123\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3848 - val_loss: 0.4111\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3804 - val_loss: 0.4122\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3893 - val_loss: 0.4168\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3906 - val_loss: 0.4121\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3841 - val_loss: 0.4100\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3844 - val_loss: 0.4089\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3821 - val_loss: 0.4107\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3762 - val_loss: 0.4089\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3816 - val_loss: 0.4100\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 112ms/step - loss: 0.3834 - val_loss: 0.4094\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3801 - val_loss: 0.4088\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3735 - val_loss: 0.4087\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3764 - val_loss: 0.4087\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3836 - val_loss: 0.4087\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3728 - val_loss: 0.4089\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3877 - val_loss: 0.4088\n",
      "[2022_04_15-14:55:32] Training the entire fine-tuned model...\n",
      "[2022_04_15-14:56:04] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3976WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1139s vs `on_train_batch_end` time: 0.1454s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1139s vs `on_train_batch_end` time: 0.1454s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 435ms/step - loss: 0.3842 - val_loss: 0.4042\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3736 - val_loss: 0.4150\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3841 - val_loss: 0.3934\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3797 - val_loss: 0.3890\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3703 - val_loss: 0.3852\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3578 - val_loss: 0.3825\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3522 - val_loss: 0.3793\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3520 - val_loss: 0.3773\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3476 - val_loss: 0.3825\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3215 - val_loss: 0.3952\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3173 - val_loss: 0.3780\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3182 - val_loss: 0.3917\n",
      "[2022_04_15-14:56:47] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-14:56:47] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:56:52] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3347WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1144s vs `on_train_batch_end` time: 0.1471s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1144s vs `on_train_batch_end` time: 0.1471s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 428ms/step - loss: 0.3369 - val_loss: 0.3775\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  19  7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4117647058823529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1fg0uvtb) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 6154... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▄▃▃▃▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.37734</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33692</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.37746</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">elated-jazz-7</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/1fg0uvtb\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/1fg0uvtb</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_145408-1fg0uvtb/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1fg0uvtb). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/3aeeekdw\" target=\"_blank\">desert-brook-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_15-14:57:46] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:57:46] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:57:46] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 287ms/step - loss: 0.6981 - val_loss: 0.5152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.5291 - val_loss: 0.5591\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4891 - val_loss: 0.4721\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4612 - val_loss: 0.4988\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4604 - val_loss: 0.5260\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4563 - val_loss: 0.4502\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4317 - val_loss: 0.4608\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.4206 - val_loss: 0.4394\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4160 - val_loss: 0.4281\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4078 - val_loss: 0.4230\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4087 - val_loss: 0.4335\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3994 - val_loss: 0.4190\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3960 - val_loss: 0.4178\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.4081 - val_loss: 0.4096\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3842 - val_loss: 0.4086\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3940 - val_loss: 0.4934\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4363 - val_loss: 0.5024\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4307 - val_loss: 0.4430\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3994 - val_loss: 0.4037\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3824 - val_loss: 0.4130\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3767 - val_loss: 0.4019\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3748 - val_loss: 0.4021\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3836 - val_loss: 0.4061\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3721 - val_loss: 0.4047\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3878 - val_loss: 0.4054\n",
      "[2022_04_15-14:58:28] Training the entire fine-tuned model...\n",
      "[2022_04_15-14:58:45] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.6450WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1151s vs `on_train_batch_end` time: 0.1471s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1151s vs `on_train_batch_end` time: 0.1471s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 434ms/step - loss: 0.5680 - val_loss: 0.4705\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.4341 - val_loss: 0.4418\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.4188 - val_loss: 0.3979\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3815 - val_loss: 0.3914\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3696 - val_loss: 0.3928\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3473 - val_loss: 0.3913\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3549 - val_loss: 0.3830\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3263 - val_loss: 0.3739\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3415 - val_loss: 0.3704\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3130 - val_loss: 0.4170\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3102 - val_loss: 0.4432\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.2882 - val_loss: 0.4251\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.2467 - val_loss: 0.3991\n",
      "[2022_04_15-14:59:30] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-14:59:30] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:59:31] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.2886WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1122s vs `on_train_batch_end` time: 0.1484s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1122s vs `on_train_batch_end` time: 0.1484s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 427ms/step - loss: 0.3016 - val_loss: 0.3653\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  91  3\n",
       "1  17  9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47368421052631576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3aeeekdw) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 6570... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▄▄▄▅▁</td></tr><tr><td>loss</td><td>█▅▅▄▄▄▄▄▄▃▄▃▃▄▃▃▄▄▃▃▃▃▃▃▃▆▄▄▃▃▃▃▂▂▂▂▂▁▂</td></tr><tr><td>lr</td><td>██████████████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▆█▅▆▇▄▄▄▃▃▃▃▃▃▃▆▆▄▂▃▂▂▂▂▂▅▄▂▂▂▂▂▁▁▃▄▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.36531</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30157</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.36531</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">desert-brook-8</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/3aeeekdw\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/3aeeekdw</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_145730-3aeeekdw/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3aeeekdw). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/2a15f8bb\" target=\"_blank\">warm-salad-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_15-15:00:24] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:00:24] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:00:24] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 295ms/step - loss: 0.8954 - val_loss: 0.8255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.6537 - val_loss: 0.6861\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.5594 - val_loss: 0.5544\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4767 - val_loss: 0.4685\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4627 - val_loss: 0.5020\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4529 - val_loss: 0.4568\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.4414 - val_loss: 0.4820\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4399 - val_loss: 0.4590\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.4370 - val_loss: 0.4494\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4278 - val_loss: 0.4618\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4326 - val_loss: 0.4626\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4117 - val_loss: 0.4239\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.4019 - val_loss: 0.4277\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3991 - val_loss: 0.4339\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3945 - val_loss: 0.4172\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3998 - val_loss: 0.4168\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3989 - val_loss: 0.4120\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3864 - val_loss: 0.4110\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3819 - val_loss: 0.4213\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3894 - val_loss: 0.4083\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3920 - val_loss: 0.4560\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3937 - val_loss: 0.4113\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.3926 - val_loss: 0.4014\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.3939 - val_loss: 0.4356\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4070 - val_loss: 0.4055\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3914 - val_loss: 0.3999\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3701 - val_loss: 0.3977\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3750 - val_loss: 0.4020\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3842 - val_loss: 0.3974\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3688 - val_loss: 0.4273\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3804 - val_loss: 0.3959\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3725 - val_loss: 0.4072\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3642 - val_loss: 0.5025\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3970 - val_loss: 0.3868\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3606 - val_loss: 0.3949\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3676 - val_loss: 0.3964\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3539 - val_loss: 0.4023\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3633 - val_loss: 0.3867\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3569 - val_loss: 0.3872\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3648 - val_loss: 0.4052\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.3540 - val_loss: 0.3861\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 1s 103ms/step - loss: 0.3629 - val_loss: 0.3868\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 1s 101ms/step - loss: 0.3638 - val_loss: 0.3895\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 1s 102ms/step - loss: 0.3590 - val_loss: 0.3874\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 1s 103ms/step - loss: 0.3520 - val_loss: 0.3877\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.3475 - val_loss: 0.3871\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3596 - val_loss: 0.3879\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_15-15:01:32] Training the entire fine-tuned model...\n",
      "[2022_04_15-15:01:41] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3647WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1121s vs `on_train_batch_end` time: 0.1458s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1121s vs `on_train_batch_end` time: 0.1458s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 420ms/step - loss: 0.3745 - val_loss: 0.3893\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3457 - val_loss: 0.3880\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3460 - val_loss: 0.3856\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3441 - val_loss: 0.3858\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3444 - val_loss: 0.3831\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3409 - val_loss: 0.3845\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3548 - val_loss: 0.3814\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 261ms/step - loss: 0.3409 - val_loss: 0.3852\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3484 - val_loss: 0.3814\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3408 - val_loss: 0.3808\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 0.3469 - val_loss: 0.3826\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3519 - val_loss: 0.3858\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3327 - val_loss: 0.3785\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3383 - val_loss: 0.3859\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3528 - val_loss: 0.3798\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3517 - val_loss: 0.3792\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3325 - val_loss: 0.3798\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3296 - val_loss: 0.3797\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 3s 262ms/step - loss: 0.3422 - val_loss: 0.3792\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_15-15:02:44] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-15:02:44] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:03:09] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3311WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1123s vs `on_train_batch_end` time: 0.1457s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1123s vs `on_train_batch_end` time: 0.1457s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 424ms/step - loss: 0.3336 - val_loss: 0.3789\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  91  3\n",
       "1  19  7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3888888888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2a15f8bb) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 6927... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇███▁▁▁▂▂▂▃▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▅▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████████▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▂▂▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>12</td></tr><tr><td>best_val_loss</td><td>0.37848</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33362</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.37895</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">warm-salad-10</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/2a15f8bb\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/2a15f8bb</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_150009-2a15f8bb/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2a15f8bb). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/3mhjtiio\" target=\"_blank\">elated-river-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_15-15:04:03] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:04:03] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:04:03] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 294ms/step - loss: 0.8277 - val_loss: 0.7845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.5852 - val_loss: 0.5718\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.5028 - val_loss: 0.4990\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4845 - val_loss: 0.4995\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4431 - val_loss: 0.4567\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4341 - val_loss: 0.4511\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4217 - val_loss: 0.4444\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4198 - val_loss: 0.4376\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4175 - val_loss: 0.4548\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4227 - val_loss: 0.4294\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 113ms/step - loss: 0.4097 - val_loss: 0.4280\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4098 - val_loss: 0.4234\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4161 - val_loss: 0.4662\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4006 - val_loss: 0.4087\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3974 - val_loss: 0.4091\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3816 - val_loss: 0.4395\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3877 - val_loss: 0.4055\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3898 - val_loss: 0.4267\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3884 - val_loss: 0.4626\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3746 - val_loss: 0.4000\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3730 - val_loss: 0.4016\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3875 - val_loss: 0.4031\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3838 - val_loss: 0.3999\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3901 - val_loss: 0.3954\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3968 - val_loss: 0.4305\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4057 - val_loss: 0.4116\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3780 - val_loss: 0.4177\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3757 - val_loss: 0.3966\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3721 - val_loss: 0.4108\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3697 - val_loss: 0.3904\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3514 - val_loss: 0.3874\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3591 - val_loss: 0.3881\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3630 - val_loss: 0.3910\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3619 - val_loss: 0.4010\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3602 - val_loss: 0.3866\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3619 - val_loss: 0.3870\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3630 - val_loss: 0.3927\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3638 - val_loss: 0.3865\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3625 - val_loss: 0.3899\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3662 - val_loss: 0.3881\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3555 - val_loss: 0.3874\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3537 - val_loss: 0.3872\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3541 - val_loss: 0.3874\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3569 - val_loss: 0.3877\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_15-15:05:08] Training the entire fine-tuned model...\n",
      "[2022_04_15-15:05:17] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3664WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1143s vs `on_train_batch_end` time: 0.1450s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1143s vs `on_train_batch_end` time: 0.1450s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 427ms/step - loss: 0.3788 - val_loss: 0.4072\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3651 - val_loss: 0.3821\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3397 - val_loss: 0.3816\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3504 - val_loss: 0.3789\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3407 - val_loss: 0.3826\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3472 - val_loss: 0.3767\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3331 - val_loss: 0.3707\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3340 - val_loss: 0.3751\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3389 - val_loss: 0.3981\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3329 - val_loss: 0.3718\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3258 - val_loss: 0.3702\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3210 - val_loss: 0.3719\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3236 - val_loss: 0.3717\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3273 - val_loss: 0.3692\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3289 - val_loss: 0.3689\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3117 - val_loss: 0.3683\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3140 - val_loss: 0.3687\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3227 - val_loss: 0.3690\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3168 - val_loss: 0.3690\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3101 - val_loss: 0.3701\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3147 - val_loss: 0.3673\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3046 - val_loss: 0.3670\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3154 - val_loss: 0.3679\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3115 - val_loss: 0.3685\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3097 - val_loss: 0.3667\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3086 - val_loss: 0.3671\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3085 - val_loss: 0.3696\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3054 - val_loss: 0.3676\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3174 - val_loss: 0.3673\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3041 - val_loss: 0.3674\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3058 - val_loss: 0.3674\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "[2022_04_15-15:06:54] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-15:06:54] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:06:54] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.2986WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1145s vs `on_train_batch_end` time: 0.1436s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1145s vs `on_train_batch_end` time: 0.1436s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 425ms/step - loss: 0.2999 - val_loss: 0.3721\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  92  2\n",
       "1  19  7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3mhjtiio) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 7461... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▃▄▄▅▅▅▅▆▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>24</td></tr><tr><td>best_val_loss</td><td>0.36675</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.29995</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.37205</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">elated-river-12</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/3mhjtiio\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/3mhjtiio</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_150348-3mhjtiio/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3mhjtiio). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/1oh19b80\" target=\"_blank\">elated-plasma-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_15-15:07:49] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:07:49] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:07:49] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 11s 296ms/step - loss: 0.7848 - val_loss: 0.6447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.5266 - val_loss: 0.4901\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4639 - val_loss: 0.4793\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4496 - val_loss: 0.4714\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4401 - val_loss: 0.4590\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4415 - val_loss: 0.4438\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4460 - val_loss: 0.4397\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4439 - val_loss: 0.4321\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.4228 - val_loss: 0.5120\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4418 - val_loss: 0.4459\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4079 - val_loss: 0.4194\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.4111 - val_loss: 0.4398\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4061 - val_loss: 0.4114\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4012 - val_loss: 0.4212\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.4021 - val_loss: 0.4340\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4003 - val_loss: 0.4336\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3985 - val_loss: 0.4138\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3848 - val_loss: 0.4030\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3795 - val_loss: 0.4018\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3836 - val_loss: 0.4101\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3852 - val_loss: 0.4017\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3764 - val_loss: 0.4018\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3721 - val_loss: 0.4021\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 112ms/step - loss: 0.3702 - val_loss: 0.4016\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3762 - val_loss: 0.4009\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3757 - val_loss: 0.4016\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3715 - val_loss: 0.4011\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3633 - val_loss: 0.4012\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3768 - val_loss: 0.4009\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3726 - val_loss: 0.4004\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3733 - val_loss: 0.4007\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3716 - val_loss: 0.4012\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3693 - val_loss: 0.4010\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3580 - val_loss: 0.4011\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3661 - val_loss: 0.4009\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3659 - val_loss: 0.4009\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "[2022_04_15-15:08:45] Training the entire fine-tuned model...\n",
      "[2022_04_15-15:08:54] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.4098WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1138s vs `on_train_batch_end` time: 0.1460s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1138s vs `on_train_batch_end` time: 0.1460s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 427ms/step - loss: 0.3939 - val_loss: 0.3958\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3863 - val_loss: 0.3933\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3806 - val_loss: 0.3927\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3719 - val_loss: 0.3828\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 261ms/step - loss: 0.3485 - val_loss: 0.3784\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3643 - val_loss: 0.3899\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3532 - val_loss: 0.3831\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3405 - val_loss: 0.3714\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3299 - val_loss: 0.3905\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3236 - val_loss: 0.3765\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3261 - val_loss: 0.3639\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3091 - val_loss: 0.3668\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3033 - val_loss: 0.3779\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2843 - val_loss: 0.3744\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2960 - val_loss: 0.3717\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2838 - val_loss: 0.3687\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2882 - val_loss: 0.3682\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_15-15:09:50] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-15:09:50] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:09:50] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3191WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1147s vs `on_train_batch_end` time: 0.1469s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1147s vs `on_train_batch_end` time: 0.1469s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 431ms/step - loss: 0.3123 - val_loss: 0.3696\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  19  7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4117647058823529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1oh19b80) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 7950... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▄▃▃▃▅▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>10</td></tr><tr><td>best_val_loss</td><td>0.36389</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31229</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.3696</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">elated-plasma-13</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/1oh19b80\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/1oh19b80</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_150732-1oh19b80/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1oh19b80). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/5ve9efob\" target=\"_blank\">pretty-sunset-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_15-15:10:45] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:10:45] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:10:45] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 11s 288ms/step - loss: 0.7877 - val_loss: 0.6235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.5436 - val_loss: 0.5044\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4831 - val_loss: 0.4821\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4596 - val_loss: 0.4665\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4459 - val_loss: 0.4518\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4500 - val_loss: 0.4451\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4605 - val_loss: 0.4474\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4533 - val_loss: 0.5045\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4236 - val_loss: 0.4346\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4244 - val_loss: 0.4447\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4135 - val_loss: 0.4258\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4237 - val_loss: 0.4732\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4006 - val_loss: 0.4169\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4058 - val_loss: 0.4391\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4050 - val_loss: 0.4573\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4024 - val_loss: 0.4260\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3860 - val_loss: 0.4114\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3863 - val_loss: 0.4064\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3793 - val_loss: 0.4015\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3701 - val_loss: 0.4032\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3788 - val_loss: 0.4043\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3821 - val_loss: 0.3988\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3860 - val_loss: 0.4009\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3739 - val_loss: 0.3991\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3777 - val_loss: 0.3982\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3805 - val_loss: 0.4031\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3773 - val_loss: 0.3966\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.3785 - val_loss: 0.3992\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.3822 - val_loss: 0.4085\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3720 - val_loss: 0.3979\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3812 - val_loss: 0.3961\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3622 - val_loss: 0.3983\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3699 - val_loss: 0.3959\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3742 - val_loss: 0.3970\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3721 - val_loss: 0.3966\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3720 - val_loss: 0.3963\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3699 - val_loss: 0.3957\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3690 - val_loss: 0.3963\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3640 - val_loss: 0.3968\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3703 - val_loss: 0.3958\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3658 - val_loss: 0.3958\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3764 - val_loss: 0.3959\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3737 - val_loss: 0.3959\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "[2022_04_15-15:11:49] Training the entire fine-tuned model...\n",
      "[2022_04_15-15:11:58] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.7044WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1156s vs `on_train_batch_end` time: 0.1457s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1156s vs `on_train_batch_end` time: 0.1457s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 428ms/step - loss: 0.5852 - val_loss: 0.4028\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.4228 - val_loss: 0.3934\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3777 - val_loss: 0.3874\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3835 - val_loss: 0.4050\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 263ms/step - loss: 0.4174 - val_loss: 0.4378\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 261ms/step - loss: 0.3685 - val_loss: 0.3829\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3653 - val_loss: 0.3788\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3423 - val_loss: 0.3809\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3364 - val_loss: 0.3602\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3167 - val_loss: 0.3619\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.2959 - val_loss: 0.3615\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2796 - val_loss: 0.3648\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2480 - val_loss: 0.3672\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2250 - val_loss: 0.3860\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2092 - val_loss: 0.4218\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_15-15:12:49] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-15:12:49] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:12:49] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3076WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1166s vs `on_train_batch_end` time: 0.1460s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1166s vs `on_train_batch_end` time: 0.1460s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 539ms/step - loss: 0.3049 - val_loss: 0.3639\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  18  8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45714285714285713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:5ve9efob) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 8325... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▅▃▃▃▃▂▂▂▁▁▂</td></tr><tr><td>lr</td><td>███████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▅▃▃▄▃▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▁▂▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_val_loss</td><td>0.36023</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30494</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.36395</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">pretty-sunset-14</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/5ve9efob\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/5ve9efob</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_151028-5ve9efob/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:5ve9efob). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/gknzs0q3\" target=\"_blank\">logical-moon-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_15-15:13:43] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:13:43] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:13:43] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 290ms/step - loss: 0.8528 - val_loss: 0.7081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.5688 - val_loss: 0.5597\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4918 - val_loss: 0.4820\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4518 - val_loss: 0.4693\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4398 - val_loss: 0.4552\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.4357 - val_loss: 0.4504\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4283 - val_loss: 0.4570\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 113ms/step - loss: 0.4111 - val_loss: 0.4493\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4102 - val_loss: 0.4356\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4041 - val_loss: 0.4295\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4174 - val_loss: 0.4261\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4017 - val_loss: 0.4257\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4059 - val_loss: 0.4203\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3912 - val_loss: 0.4147\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.4028 - val_loss: 0.4128\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3977 - val_loss: 0.4114\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4027 - val_loss: 0.4207\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3834 - val_loss: 0.4086\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3856 - val_loss: 0.4114\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4393 - val_loss: 0.4075\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4049 - val_loss: 0.4025\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3849 - val_loss: 0.4038\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3771 - val_loss: 0.4191\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4049 - val_loss: 0.3993\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3626 - val_loss: 0.4069\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3798 - val_loss: 0.3966\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3649 - val_loss: 0.4049\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3603 - val_loss: 0.3978\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3747 - val_loss: 0.4434\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3946 - val_loss: 0.3892\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3847 - val_loss: 0.3926\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3778 - val_loss: 0.4162\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3642 - val_loss: 0.3875\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3692 - val_loss: 0.4412\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 113ms/step - loss: 0.3939 - val_loss: 0.3927\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3587 - val_loss: 0.3901\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3494 - val_loss: 0.3898\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3620 - val_loss: 0.3947\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3614 - val_loss: 0.3899\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3620 - val_loss: 0.3920\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3678 - val_loss: 0.3876\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_15-15:14:44] Training the entire fine-tuned model...\n",
      "[2022_04_15-15:14:54] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3685WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1142s vs `on_train_batch_end` time: 0.1442s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1142s vs `on_train_batch_end` time: 0.1442s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 429ms/step - loss: 0.3500 - val_loss: 0.3962\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3630 - val_loss: 0.3865\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3530 - val_loss: 0.3893\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 0.3555 - val_loss: 0.3874\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3540 - val_loss: 0.3874\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3520 - val_loss: 0.3872\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3469 - val_loss: 0.3866\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3430 - val_loss: 0.3861\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3524 - val_loss: 0.3863\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3558 - val_loss: 0.3864\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3485 - val_loss: 0.3854\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3515 - val_loss: 0.3857\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3515 - val_loss: 0.3876\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3482 - val_loss: 0.3870\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3587 - val_loss: 0.3847\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3582 - val_loss: 0.3858\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3505 - val_loss: 0.3859\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3511 - val_loss: 0.3864\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3469 - val_loss: 0.3871\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3538 - val_loss: 0.3868\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3448 - val_loss: 0.3863\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3538 - val_loss: 0.3858\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3407 - val_loss: 0.3854\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      "[2022_04_15-15:16:08] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-15:16:08] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:16:22] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3404WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1136s vs `on_train_batch_end` time: 0.1473s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1136s vs `on_train_batch_end` time: 0.1473s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 545ms/step - loss: 0.3493 - val_loss: 0.3845\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  21  5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:gknzs0q3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 8728... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▄▄▄▄▅▁</td></tr><tr><td>loss</td><td>█▄▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████████▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▃▃▂▂▂▂▂▂▂▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.38452</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34929</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.38452</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">logical-moon-15</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/gknzs0q3\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/gknzs0q3</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_151327-gknzs0q3/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:gknzs0q3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/3j8vlelm\" target=\"_blank\">kind-serenity-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_15-15:17:17] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:17:17] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:17:17] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 287ms/step - loss: 0.8497 - val_loss: 0.9034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.5952 - val_loss: 0.5578\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4940 - val_loss: 0.4785\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4798 - val_loss: 0.5324\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4946 - val_loss: 0.5045\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4619 - val_loss: 0.5121\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4456 - val_loss: 0.4449\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4217 - val_loss: 0.4361\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4272 - val_loss: 0.4370\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4086 - val_loss: 0.4272\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3959 - val_loss: 0.4460\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4005 - val_loss: 0.4193\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3980 - val_loss: 0.4172\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4087 - val_loss: 0.4207\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4005 - val_loss: 0.4172\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.4044 - val_loss: 0.4271\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4002 - val_loss: 0.4126\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3939 - val_loss: 0.4083\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3872 - val_loss: 0.4079\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3833 - val_loss: 0.4152\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3948 - val_loss: 0.4060\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3890 - val_loss: 0.4111\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3750 - val_loss: 0.4401\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4211 - val_loss: 0.5040\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3824 - val_loss: 0.3939\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3975 - val_loss: 0.4189\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.4098 - val_loss: 0.5248\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3833 - val_loss: 0.3936\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3864 - val_loss: 0.4196\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3735 - val_loss: 0.3966\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3656 - val_loss: 0.3854\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3809 - val_loss: 0.4092\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3762 - val_loss: 0.3969\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4207 - val_loss: 0.4420\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4305 - val_loss: 0.3983\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3683 - val_loss: 0.4063\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3556 - val_loss: 0.4052\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3747 - val_loss: 0.3847\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3574 - val_loss: 0.3840\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3582 - val_loss: 0.3975\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3574 - val_loss: 0.3940\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3594 - val_loss: 0.3928\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3611 - val_loss: 0.3839\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3630 - val_loss: 0.3854\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 1s 112ms/step - loss: 0.3550 - val_loss: 0.3871\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3728 - val_loss: 0.3856\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3496 - val_loss: 0.3859\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3494 - val_loss: 0.3857\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3537 - val_loss: 0.3869\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3525 - val_loss: 0.3867\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3490 - val_loss: 0.3863\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_15-15:18:31] Training the entire fine-tuned model...\n",
      "[2022_04_15-15:18:57] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3447WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1119s vs `on_train_batch_end` time: 0.1476s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1119s vs `on_train_batch_end` time: 0.1476s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 433ms/step - loss: 0.3644 - val_loss: 0.3862\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3565 - val_loss: 0.3843\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3584 - val_loss: 0.3881\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3499 - val_loss: 0.3792\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3436 - val_loss: 0.3864\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3392 - val_loss: 0.3938\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3507 - val_loss: 0.4045\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 261ms/step - loss: 0.3386 - val_loss: 0.3769\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 0.3355 - val_loss: 0.3730\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3278 - val_loss: 0.3742\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3175 - val_loss: 0.3744\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3267 - val_loss: 0.3727\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3093 - val_loss: 0.3922\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3221 - val_loss: 0.4002\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3112 - val_loss: 0.3820\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3073 - val_loss: 0.3692\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3012 - val_loss: 0.3803\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.2995 - val_loss: 0.3756\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.2960 - val_loss: 0.3662\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.2916 - val_loss: 0.3670\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2917 - val_loss: 0.3735\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2865 - val_loss: 0.3634\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2838 - val_loss: 0.3792\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2913 - val_loss: 0.4171\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2824 - val_loss: 0.4114\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2782 - val_loss: 0.3760\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2699 - val_loss: 0.3659\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.2654 - val_loss: 0.3706\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2574 - val_loss: 0.3665\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2584 - val_loss: 0.3722\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_15-15:20:31] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-15:20:31] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:20:31] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.2816WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1126s vs `on_train_batch_end` time: 0.1486s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1126s vs `on_train_batch_end` time: 0.1486s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 438ms/step - loss: 0.2853 - val_loss: 0.3657\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  90   4\n",
       "1  16  10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3j8vlelm) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9158... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▃▃▃▃▄▄▄▅▅▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▃▃▃▃▂▃▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▃▂▂▂▂▂▂▂▂▂▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>21</td></tr><tr><td>best_val_loss</td><td>0.36335</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.28526</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.36567</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">kind-serenity-16</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/3j8vlelm\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/3j8vlelm</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_151701-3j8vlelm/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3j8vlelm). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/1azrrqg7\" target=\"_blank\">grateful-thunder-17</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_15-15:21:25] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:21:25] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:21:25] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 288ms/step - loss: 0.8104 - val_loss: 0.5915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 116ms/step - loss: 0.5080 - val_loss: 0.4967\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4933 - val_loss: 0.4771\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4499 - val_loss: 0.4649\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4446 - val_loss: 0.4875\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4642 - val_loss: 0.4796\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4450 - val_loss: 0.4536\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4551 - val_loss: 0.4320\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4113 - val_loss: 0.4396\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4132 - val_loss: 0.4250\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.4088 - val_loss: 0.4265\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4210 - val_loss: 0.4349\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4169 - val_loss: 0.4122\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4378 - val_loss: 0.4113\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4294 - val_loss: 0.4850\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4380 - val_loss: 0.4179\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3985 - val_loss: 0.4600\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3916 - val_loss: 0.4074\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3803 - val_loss: 0.4282\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3917 - val_loss: 0.4030\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3769 - val_loss: 0.3994\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3932 - val_loss: 0.4298\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4184 - val_loss: 0.4095\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3925 - val_loss: 0.4070\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3873 - val_loss: 0.4021\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3722 - val_loss: 0.4075\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3646 - val_loss: 0.3956\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3729 - val_loss: 0.3948\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3647 - val_loss: 0.3949\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3659 - val_loss: 0.3952\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3644 - val_loss: 0.3982\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3618 - val_loss: 0.3984\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3598 - val_loss: 0.3947\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3630 - val_loss: 0.3941\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3644 - val_loss: 0.3969\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3650 - val_loss: 0.3938\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3575 - val_loss: 0.3971\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3601 - val_loss: 0.3948\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3633 - val_loss: 0.3961\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3620 - val_loss: 0.3935\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3606 - val_loss: 0.3981\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3688 - val_loss: 0.3946\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3633 - val_loss: 0.3942\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3662 - val_loss: 0.3946\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3636 - val_loss: 0.3949\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3697 - val_loss: 0.3944\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3602 - val_loss: 0.3949\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3670 - val_loss: 0.3950\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_15-15:22:35] Training the entire fine-tuned model...\n",
      "[2022_04_15-15:22:44] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3987WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1137s vs `on_train_batch_end` time: 0.1469s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1137s vs `on_train_batch_end` time: 0.1469s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 426ms/step - loss: 0.3929 - val_loss: 0.3915\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3789 - val_loss: 0.3896\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3595 - val_loss: 0.3896\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3632 - val_loss: 0.3874\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3423 - val_loss: 0.3825\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3424 - val_loss: 0.3803\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3346 - val_loss: 0.3797\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3455 - val_loss: 0.3730\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3459 - val_loss: 0.3955\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3272 - val_loss: 0.3709\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3203 - val_loss: 0.3693\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3205 - val_loss: 0.3669\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2997 - val_loss: 0.3731\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3148 - val_loss: 0.3717\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2924 - val_loss: 0.3812\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.2988 - val_loss: 0.3923\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3008 - val_loss: 0.3754\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2854 - val_loss: 0.3779\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2750 - val_loss: 0.3652\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2726 - val_loss: 0.3672\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2578 - val_loss: 0.3677\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2669 - val_loss: 0.3677\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2596 - val_loss: 0.3679\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.2458 - val_loss: 0.3712\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 0.2571 - val_loss: 0.3712\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2484 - val_loss: 0.3696\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2618 - val_loss: 0.3736\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "[2022_04_15-15:24:10] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-15:24:10] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:24:12] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.2608WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1141s vs `on_train_batch_end` time: 0.1461s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1141s vs `on_train_batch_end` time: 0.1461s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 427ms/step - loss: 0.2705 - val_loss: 0.3720\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  91   3\n",
       "1  16  10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5128205128205128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1azrrqg7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9734... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇██▁▁▁▂▂▂▃▃▃▄▄▄▅▅▁</td></tr><tr><td>loss</td><td>█▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▄▅▃▃▃▂▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>18</td></tr><tr><td>best_val_loss</td><td>0.36521</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.27045</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.37203</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">grateful-thunder-17</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/1azrrqg7\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/1azrrqg7</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_152109-1azrrqg7/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1azrrqg7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/30skiikj\" target=\"_blank\">apricot-dragon-18</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_15-15:25:08] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:25:08] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:25:08] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 292ms/step - loss: 0.8022 - val_loss: 0.6105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.5308 - val_loss: 0.5151\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4713 - val_loss: 0.4786\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4585 - val_loss: 0.4622\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4384 - val_loss: 0.4639\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.4365 - val_loss: 0.4458\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4301 - val_loss: 0.5041\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4568 - val_loss: 0.4528\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4152 - val_loss: 0.4562\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4044 - val_loss: 0.4256\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4026 - val_loss: 0.4286\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4140 - val_loss: 0.4207\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4122 - val_loss: 0.4189\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3959 - val_loss: 0.4144\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3936 - val_loss: 0.4153\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3846 - val_loss: 0.4568\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3998 - val_loss: 0.4169\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3893 - val_loss: 0.4033\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3821 - val_loss: 0.4016\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3879 - val_loss: 0.4040\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3768 - val_loss: 0.4027\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3793 - val_loss: 0.4098\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3881 - val_loss: 0.4005\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3997 - val_loss: 0.4137\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4236 - val_loss: 0.4698\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.4013 - val_loss: 0.4474\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4242 - val_loss: 0.5761\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4293 - val_loss: 0.3927\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3691 - val_loss: 0.4026\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3652 - val_loss: 0.4107\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3602 - val_loss: 0.3917\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3734 - val_loss: 0.3976\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3669 - val_loss: 0.3893\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3527 - val_loss: 0.3883\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3738 - val_loss: 0.3892\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3562 - val_loss: 0.3877\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3538 - val_loss: 0.3883\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3586 - val_loss: 0.3886\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3484 - val_loss: 0.3883\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3697 - val_loss: 0.3969\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3678 - val_loss: 0.3886\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3523 - val_loss: 0.3947\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3530 - val_loss: 0.3890\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3553 - val_loss: 0.3898\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_15-15:26:12] Training the entire fine-tuned model...\n",
      "[2022_04_15-15:26:21] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 1.0330WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1110s vs `on_train_batch_end` time: 0.1471s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1110s vs `on_train_batch_end` time: 0.1471s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 426ms/step - loss: 0.7918 - val_loss: 0.3976\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.4121 - val_loss: 0.3941\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 262ms/step - loss: 0.4068 - val_loss: 0.4007\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3922 - val_loss: 0.3810\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 261ms/step - loss: 0.3726 - val_loss: 0.3808\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3529 - val_loss: 0.3783\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3494 - val_loss: 0.3975\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3348 - val_loss: 0.3681\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3143 - val_loss: 0.3715\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3380 - val_loss: 0.3858\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3073 - val_loss: 0.3953\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.2746 - val_loss: 0.3685\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 264ms/step - loss: 0.2365 - val_loss: 0.3736\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.2225 - val_loss: 0.3818\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2131 - val_loss: 0.3906\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2003 - val_loss: 0.4062\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_15-15:27:15] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-15:27:15] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:27:15] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3056WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1158s vs `on_train_batch_end` time: 0.1439s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1158s vs `on_train_batch_end` time: 0.1439s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 425ms/step - loss: 0.3067 - val_loss: 0.3806\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  18  8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45714285714285713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "for pat in patience:\n",
    "    for lr in learning_rate:\n",
    "        finetune_by_settings_and_data(pat, lr, f\"Full old\", train_data_old, valid_data_old, f\"old_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310c43cb-2358-40c5-b3b6-2027bfc4106a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "559e98e6-c31c-4650-9886-ad9dfc11f0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_15-12:45:47] Test set: Filtered out 0 of 260 (0.0%) records of lengths exceeding 510.\n"
     ]
    }
   ],
   "source": [
    "encoded_test_set = encode_dataset(test_data[\"seq\"], test_data[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'Test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4cc4820c-1011-4813-9e0e-397cfa976103",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X, test_Y, test_sample_weigths = encoded_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c5e115f7-c70f-41df-b3a3-a0da075721c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_15-15:27:54] Test set old: Filtered out 0 of 119 (0.0%) records of lengths exceeding 510.\n"
     ]
    }
   ],
   "source": [
    "encoded_test_set_old = encode_dataset(test_data_old[\"seq\"], test_data_old[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'Test set old')\n",
    "test_X_old, test_Y_old, test_sample_weigths_old = encoded_test_set_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288f2631-ab65-4dea-a81d-871aeea43932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c197e30e-5ddf-453c-ad43-397bc6f1df66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for model in os.listdir(path.join(DATA_DIR, f\"protein_bert/full\")):\n",
    "    if model.startswith(\"2022_04_09\"):\n",
    "        test_model(model, f\"full\", test_X, test_Y, tap_X, tap_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8eedd51e-cb7d-4e6f-aa6d-6f8c6f8daf5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10224... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model in os.listdir(path.join(DATA_DIR, f\"protein_bert/old_full\")):\n",
    "    if model.startswith(\"2022_04_09\"):\n",
    "        test_model(model, f\"old_full\", test_X_old, test_Y_old, tap_X, tap_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ef191c63-a713-48fe-a41e-16f8ab7fe56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"2022_04_09_6_3_0.0001\"\n",
    "model_path = path.join(DATA_DIR, f\"protein_bert/full/{model_name}\")\n",
    "model = keras.models.load_model(model_path)\n",
    "parts = model_name[11:].split(\"_\")\n",
    "patience_stop = parts[0]\n",
    "patience_lr = parts[1]\n",
    "lr = parts[2]\n",
    "\n",
    "y_pred = model.predict(test_X, batch_size=32)\n",
    "y_pred_classes = (y_pred >= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c783e834-4cf9-4049-a9d2-662d35b7bee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15384615384615383"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(test_Y, y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9e3db90a-6ef4-4e06-b6fe-0454584a2dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[200,   6],\n",
       "       [ 49,   5]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(test_Y, y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8543f4a1-2176-4479-b0f7-30fbfd54df04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1291"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e0b6ff60-cf54-4985-8f9e-82fe5a31a5bc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:344ub7bf) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 3731... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█▁▁▂▃▃▃▄▅▅▅▆▇▁</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▃▂▁▂▃▁▄▃▂▁▂▃▂▂▃▁▁▁▂▁▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.45819</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35927</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.48604</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">still-butterfly-5</strong>: <a href=\"https://wandb.ai/kvetab/Easter/runs/344ub7bf\" target=\"_blank\">https://wandb.ai/kvetab/Easter/runs/344ub7bf</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_142948-344ub7bf/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:344ub7bf). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Easter/runs/rc09kp9o\" target=\"_blank\">rosy-cloud-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Easter\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_15-14:33:47] Training set: Filtered out 0 of 1291 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:33:48] Validation set: Filtered out 0 of 130 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:33:48] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 281ms/step - loss: 0.7999 - val_loss: 0.8898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.5859 - val_loss: 0.6600\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.5152 - val_loss: 0.5327\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4500 - val_loss: 0.5070\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4278 - val_loss: 0.4914\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4082 - val_loss: 0.5343\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.4290 - val_loss: 0.5193\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4161 - val_loss: 0.5199\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4100 - val_loss: 0.4779\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.4006 - val_loss: 0.4866\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3921 - val_loss: 0.4959\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3946 - val_loss: 0.4784\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3938 - val_loss: 0.4817\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3883 - val_loss: 0.4907\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3903 - val_loss: 0.4741\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3859 - val_loss: 0.4760\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3910 - val_loss: 0.4913\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3948 - val_loss: 0.4747\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3923 - val_loss: 0.4738\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3854 - val_loss: 0.4771\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3881 - val_loss: 0.4790\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3888 - val_loss: 0.4770\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3918 - val_loss: 0.4763\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3905 - val_loss: 0.4754\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3873 - val_loss: 0.4762\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "[2022_04_15-14:34:29] Training the entire fine-tuned model...\n",
      "[2022_04_15-14:34:45] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.4002WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1148s vs `on_train_batch_end` time: 0.1441s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1148s vs `on_train_batch_end` time: 0.1441s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 418ms/step - loss: 0.4045 - val_loss: 0.4907\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 247ms/step - loss: 0.3969 - val_loss: 0.4714\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3894 - val_loss: 0.4817\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 248ms/step - loss: 0.3789 - val_loss: 0.4876\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3825 - val_loss: 0.4609\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 248ms/step - loss: 0.3693 - val_loss: 0.4934\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3705 - val_loss: 0.5067\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3528 - val_loss: 0.4917\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3563 - val_loss: 0.4596\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 248ms/step - loss: 0.3530 - val_loss: 0.4644\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 248ms/step - loss: 0.3520 - val_loss: 0.4660\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3386 - val_loss: 0.4575\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3348 - val_loss: 0.4666\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3422 - val_loss: 0.4549\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3419 - val_loss: 0.4625\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3339 - val_loss: 0.4622\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 250ms/step - loss: 0.3254 - val_loss: 0.4636\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3271 - val_loss: 0.4599\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 3s 250ms/step - loss: 0.3225 - val_loss: 0.4595\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3249 - val_loss: 0.4624\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "[2022_04_15-14:35:49] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-14:35:49] Training set: Filtered out 0 of 1291 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:35:49] Validation set: Filtered out 0 of 130 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3351WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1126s vs `on_train_batch_end` time: 0.1469s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1126s vs `on_train_batch_end` time: 0.1469s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 418ms/step - loss: 0.3270 - val_loss: 0.4616\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"Easter\", entity=\"kvetab\")\n",
    "model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "    get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)\n",
    "training_callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(patience = 3, factor = 0.25, min_lr = 1e-07, verbose = 1),\n",
    "    keras.callbacks.EarlyStopping(patience = 6, restore_best_weights = True),\n",
    "    WandbCallback()\n",
    "]\n",
    "epoch_num = 100\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "wandb.config = {\n",
    "      \"learning_rate\": lr,\n",
    "      \"epochs\": epoch_num * 2,\n",
    "      \"batch_size\": batch_size\n",
    "    }\n",
    "print(type(lr))\n",
    "print(lr)\n",
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data['seq'], train_data['Y'], valid_data['seq'], valid_data['Y'], \\\n",
    "        seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = lr, begin_with_frozen_pretrained_layers = True, \\\n",
    "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 512, final_lr = lr / 10, callbacks = training_callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a4565283-afc4-452e-a7bf-edf3343d243f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>104</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  104  2\n",
       "1   21  3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "        start_seq_len = 512, start_batch_size = 32)\n",
    "print('Confusion matrix:')\n",
    "display(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "44a664fb-2244-4ead-836c-849c0c33179f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20689655172413793"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "26b31976-7e75-40d7-bfcc-aeb4a46995ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1291"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "377ee2c0-1ef8-42c7-a8c6-494d0514daf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "49746582-eab0-4f59-8411-ca38d6f35c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8e26aa-4435-49dd-958d-0318b364b644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
