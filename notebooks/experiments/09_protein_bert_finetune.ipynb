{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff8a9d94-4216-4b46-92e0-67298b77bf45",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from os import path\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3394e398-aa33-45b9-981f-ee3ead567624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from proteinbert.finetuning import encode_train_and_valid_sets, encode_dataset\n",
    "from proteinbert import OutputType, OutputSpec, evaluate_by_len, load_pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a869012f-8189-4cd3-b7a7-f614ce87bb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from proteinbert import OutputType, OutputSpec, FinetuningModelGenerator, load_pretrained_model, \\\n",
    "finetune, evaluate_by_len\n",
    "\n",
    "from proteinbert.conv_and_global_attention_model import get_model_with_hidden_layers_as_outputs\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d2ed9b0-3023-4f0c-b93e-3330a86f35ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4798e75-9168-4aa2-b76c-bf37d692efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33c6481c-b3ce-47c3-86c4-5258a9553fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/\"\n",
    "VALID_SPLIT_SEED = 333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5db7bda-d751-4876-916f-23e824fd7b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_TYPE = OutputType(False, 'binary')\n",
    "UNIQUE_LABELS = [0, 1]\n",
    "OUTPUT_SPEC = OutputSpec(OUTPUT_TYPE, UNIQUE_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78d4d0ff-52b0-4152-9447-182709e45923",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_generator, input_encoder = load_pretrained_model(\"../../data/protein_bert/\", \"epoch_92400_sample_23500000.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59ffc4b8-405d-4244-a4bc-f0ab0190c73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbdaf9de-0881-43dc-8d2e-317db6302948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkvetab\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Old_split_finetune/runs/vv174smt\" target=\"_blank\">glamorous-dawn-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Old_split_finetune\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/kvetab/Old_split_finetune/runs/vv174smt?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fe74148e9d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=f\"Old_split_finetune\", entity=\"kvetab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c5d33f-1465-4902-b24e-9a0e45c99d44",
   "metadata": {},
   "source": [
    "# Split 4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1743c445-302b-4f4e-9325-a6ef4fb17aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antibody_ID</th>\n",
       "      <th>heavy</th>\n",
       "      <th>light</th>\n",
       "      <th>Y</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6aod</td>\n",
       "      <td>EVQLVQSGAEVKKPGASVKVSCKASGYTFTGYYMHWVRQAPGQGLE...</td>\n",
       "      <td>DIVMTKSPSSLSASVGDRVTITCRASQGIRNDLGWYQQKPGKAPKR...</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6and</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGYEFSRSWMNWVRQAPGKGLE...</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCRSSQSIVHSVGNTFLEWYQQKPG...</td>\n",
       "      <td>1</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3esu</td>\n",
       "      <td>EVQLQQSGPELVKPGASVKISCKDSGYAFSSSWMNWVKQRPGQGPE...</td>\n",
       "      <td>DIVLIQSTSSLSASLGDRVTISCRASQDIRNYLNWYQQKPDGTVKL...</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3esv</td>\n",
       "      <td>EVQLQQSGPELVKPGASVKISCKDSGYAFNSSWMNWVKQRPGQGLE...</td>\n",
       "      <td>DIQMTQTTSSLSASLGDRVTVSCRASQDIRNYLNWYQQKPDGTVKF...</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3et9</td>\n",
       "      <td>EVQLQQSGPELVKPGASVKISCKDSGYAFSSSWMNWVKQRPGQGPE...</td>\n",
       "      <td>DIVLIQSTSSLSASLGDRVTISCRASQDIRNYLNWYQQKPDGTVKL...</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Antibody_ID                                              heavy  \\\n",
       "0         6aod  EVQLVQSGAEVKKPGASVKVSCKASGYTFTGYYMHWVRQAPGQGLE...   \n",
       "2         6and  EVQLVESGGGLVQPGGSLRLSCAASGYEFSRSWMNWVRQAPGKGLE...   \n",
       "9         3esu  EVQLQQSGPELVKPGASVKISCKDSGYAFSSSWMNWVKQRPGQGPE...   \n",
       "10        3esv  EVQLQQSGPELVKPGASVKISCKDSGYAFNSSWMNWVKQRPGQGLE...   \n",
       "11        3et9  EVQLQQSGPELVKPGASVKISCKDSGYAFSSSWMNWVKQRPGQGPE...   \n",
       "\n",
       "                                                light  Y  cluster  \n",
       "0   DIVMTKSPSSLSASVGDRVTITCRASQGIRNDLGWYQQKPGKAPKR...  0      165  \n",
       "2   DIQMTQSPSSLSASVGDRVTITCRSSQSIVHSVGNTFLEWYQQKPG...  1      135  \n",
       "9   DIVLIQSTSSLSASLGDRVTISCRASQDIRNYLNWYQQKPDGTVKL...  0       46  \n",
       "10  DIQMTQTTSSLSASLGDRVTVSCRASQDIRNYLNWYQQKPDGTVKF...  0       46  \n",
       "11  DIVLIQSTSSLSASLGDRVTISCRASQDIRNYLNWYQQKPDGTVKL...  0       46  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/crossval/chen_4_a.csv\"), index_col=0)\n",
    "test_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/crossval/chen_4_b.csv\"), index_col=0)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c09b813-ffc8-48f7-99a5-586c76ac33f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"seq\"] = train_data[\"heavy\"] + train_data[\"light\"]\n",
    "test_data[\"seq\"] = test_data[\"heavy\"] + test_data[\"light\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fe63df3-96bf-40ed-9065-216a8e67b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(patience = 3, factor = 0.25, min_lr = 1e-07, verbose = 1),\n",
    "    keras.callbacks.EarlyStopping(patience = 6, restore_best_weights = True),\n",
    "    WandbCallback()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faeffd00-5c33-40ed-ad02-1c71cb739527",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 50\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd45319f-0dd8-489a-b105-7163d1679298",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config = {\n",
    "      \"learning_rate\": learning_rate,\n",
    "      \"epochs\": epoch_num * 2,\n",
    "      \"batch_size\": batch_size\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16a259a0-5115-4000-a24a-42193248ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Validating using test data!!! #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72bd9e53-8469-410c-9d42-6daa3811fc7e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_20-11:55:49] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-11:55:49] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-11:55:49] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-20 11:55:49.746183: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-20 11:55:50.305130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9656 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:41:00.0, compute capability: 7.5\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "2022-04-20 11:55:52.250985: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-20 11:55:59.787175: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 7605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 11s 791ms/step - loss: 0.8133 - val_loss: 0.6563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.6111 - val_loss: 0.7012\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.5588 - val_loss: 0.5636\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.5089 - val_loss: 0.5047\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4533 - val_loss: 0.4726\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.4418 - val_loss: 0.4622\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4370 - val_loss: 0.4592\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4249 - val_loss: 0.4527\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4141 - val_loss: 0.4612\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.4101 - val_loss: 0.4456\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3975 - val_loss: 0.4485\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3892 - val_loss: 0.4472\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3928 - val_loss: 0.4430\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3757 - val_loss: 0.4401\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3700 - val_loss: 0.4430\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3719 - val_loss: 0.4494\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 0.3749 - val_loss: 0.4394\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3670 - val_loss: 0.4486\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3763 - val_loss: 0.4381\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3568 - val_loss: 0.4341\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3690 - val_loss: 0.4543\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3591 - val_loss: 0.4769\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3581 - val_loss: 0.4336\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.3468 - val_loss: 0.4326\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3443 - val_loss: 0.4397\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3428 - val_loss: 0.4531\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3618 - val_loss: 0.4275\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3829 - val_loss: 0.5017\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3678 - val_loss: 0.4766\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3617 - val_loss: 0.4267\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3323 - val_loss: 0.4459\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3367 - val_loss: 0.4600\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3325 - val_loss: 0.4285\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3188 - val_loss: 0.4303\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3208 - val_loss: 0.4289\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3086 - val_loss: 0.4255\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3090 - val_loss: 0.4251\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3159 - val_loss: 0.4251\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.3163 - val_loss: 0.4269\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.3255 - val_loss: 0.4280\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3359 - val_loss: 0.4250\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3107 - val_loss: 0.4245\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3069 - val_loss: 0.4248\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3077 - val_loss: 0.4244\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.3211 - val_loss: 0.4242\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3201 - val_loss: 0.4241\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3036 - val_loss: 0.4242\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.3213 - val_loss: 0.4241\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.3172 - val_loss: 0.4241\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.3154 - val_loss: 0.4240\n",
      "[2022_04_20-11:57:20] Training the entire fine-tuned model...\n",
      "[2022_04_20-11:57:29] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 9s 854ms/step - loss: 0.3087 - val_loss: 0.4627\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.3090 - val_loss: 0.4285\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.2965 - val_loss: 0.4371\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.3196 - val_loss: 0.4361\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 2s 372ms/step - loss: 0.3072 - val_loss: 0.4292\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.2870 - val_loss: 0.4215\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.2831 - val_loss: 0.4237\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.2804 - val_loss: 0.4200\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.2918 - val_loss: 0.4206\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.2800 - val_loss: 0.4196\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 2s 372ms/step - loss: 0.2767 - val_loss: 0.4213\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.2830 - val_loss: 0.4196\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2852 - val_loss: 0.4211\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.2743 - val_loss: 0.4205\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.2669 - val_loss: 0.4201\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.2741 - val_loss: 0.4210\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.2610 - val_loss: 0.4209\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.2736 - val_loss: 0.4206\n",
      "[2022_04_20-11:58:09] Training on final epochs of sequence length 1024...\n",
      "[2022_04_20-11:58:09] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 1022.\n",
      "[2022_04_20-11:58:09] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 1022.\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.3028WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1113s vs `on_train_batch_end` time: 0.1379s). Check your callbacks.\n",
      "10/10 [==============================] - 11s 603ms/step - loss: 0.2931 - val_loss: 0.4196\n"
     ]
    }
   ],
   "source": [
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data['seq'], train_data['Y'], test_data['seq'], test_data['Y'], \\\n",
    "        seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = learning_rate, begin_with_frozen_pretrained_layers = True, \\\n",
    "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = learning_rate / 10, callbacks = training_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f7e7d4f-4ca7-47fd-ad0f-cedd9de6c46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "        start_seq_len = 512, start_batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d14dac3-1751-4172-96e6-7636bb9ad8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-set performance:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># records</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model seq len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>733</td>\n",
       "      <td>0.785587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>733</td>\n",
       "      <td>0.785587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               # records       AUC\n",
       "Model seq len                     \n",
       "512                  733  0.785587\n",
       "All                  733  0.785587"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>496</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  496  86\n",
       "1   78  73"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Test-set performance:')\n",
    "display(results)\n",
    "\n",
    "print('Confusion matrix:')\n",
    "display(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "601cc41a-442a-4b5a-a651-a22f09e5ad2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47096774193548385"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c444b8d-daf7-4e24-a86c-c97f47b80d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "2022-04-09 11:32:18.361248: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_01/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/2022_04_09_01/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "mod = model_generator.create_model(seq_len = 512)\n",
    "mod.save(path.join(DATA_DIR, \"protein_bert/2022_04_09_01_x\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8230234c-f488-4a51-91d1-14a68aa24d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = [(3,3), (4,3), (6,3), (8,4)]\n",
    "learning_rate = [1e-5, 5e-5, 1e-4, 5e-4]\n",
    "prepro = [\"scaling\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4969f0aa-0567-4f3c-aa9f-c9681cf164d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_by_settings(patience, lr, project_name):\n",
    "    wandb.init(project=project_name, entity=\"kvetab\")\n",
    "    model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)\n",
    "    training_callbacks = [\n",
    "        keras.callbacks.ReduceLROnPlateau(patience = patience[1], factor = 0.25, min_lr = 1e-07, verbose = 1),\n",
    "        keras.callbacks.EarlyStopping(patience = patience[0], restore_best_weights = True),\n",
    "        WandbCallback()\n",
    "    ]\n",
    "    epoch_num = 100\n",
    "    batch_size = 128\n",
    "    wandb.config = {\n",
    "          \"learning_rate\": lr,\n",
    "          \"epochs\": epoch_num * 2,\n",
    "          \"batch_size\": batch_size\n",
    "        }\n",
    "    print(type(lr))\n",
    "    print(lr)\n",
    "    finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data['seq'], train_data['Y'], test_data['seq'], test_data['Y'], \\\n",
    "            seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = lr, begin_with_frozen_pretrained_layers = True, \\\n",
    "            lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 512, final_lr = lr / 10, callbacks = training_callbacks)\n",
    "    \n",
    "    results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "            start_seq_len = 512, start_batch_size = 32)\n",
    "    print('Confusion matrix:')\n",
    "    display(confusion_matrix)\n",
    "    fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "    f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "    print(f1)\n",
    "    \n",
    "    mod = model_generator.create_model(seq_len = 512)\n",
    "    mod.save(path.join(DATA_DIR, f\"protein_bert/4b/2022_04_20_{patience[0]}_{patience[1]}_{lr}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "591b1a01-6aee-4329-936e-30c6cdfa6091",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:vv174smt) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 18232... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇█▃▂▂▂▂▂▂▂▂▁▂▁▁▁▃▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>11</td></tr><tr><td>best_val_loss</td><td>0.41962</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.29309</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.41963</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">glamorous-dawn-1</strong>: <a href=\"https://wandb.ai/kvetab/Old_split_finetune/runs/vv174smt\" target=\"_blank\">https://wandb.ai/kvetab/Old_split_finetune/runs/vv174smt</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220420_115539-vv174smt/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:vv174smt). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/lrlucoz2\" target=\"_blank\">dry-forest-18</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_20-12:01:14] Training set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:01:14] Validation set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-12:01:14] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 710ms/step - loss: 0.9246 - val_loss: 0.5503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.5783 - val_loss: 0.7069\n",
      "Epoch 3/100\n",
      "2/5 [===========>..................] - ETA: 0s - loss: 0.6474"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/pbs.42056.lich-compute.vscht.cz/ipykernel_18199/780394418.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mfinetune_by_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Split 4a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/tmp/pbs.42056.lich-compute.vscht.cz/ipykernel_18199/1053265744.py\u001b[0m in \u001b[0;36mfinetune_by_settings\u001b[0;34m(patience, lr, project_name)\u001b[0m\n\u001b[1;32m     19\u001b[0m     finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data['seq'], train_data['Y'], test_data['seq'], test_data['Y'], \\\n\u001b[1;32m     20\u001b[0m             \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs_per_stage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbegin_with_frozen_pretrained_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 512, final_lr = lr / 10, callbacks = training_callbacks)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/proteinbert/finetuning.py\u001b[0m in \u001b[0;36mfinetune\u001b[0;34m(model_generator, input_encoder, output_spec, train_seqs, train_raw_Y, valid_seqs, valid_raw_Y, seq_len, batch_size, max_epochs_per_stage, lr, begin_with_frozen_pretrained_layers, lr_with_frozen_pretrained_layers, n_final_epochs, final_seq_len, final_lr, callbacks)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training with frozen pretrained layers...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         model_generator.train(encoded_train_set, encoded_valid_set, seq_len, batch_size, max_epochs_per_stage, lr = lr_with_frozen_pretrained_layers, \\\n\u001b[0;32m---> 53\u001b[0;31m                 callbacks = callbacks, freeze_pretrained_layers = True)\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training the entire fine-tuned model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/proteinbert/model_generation.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, encoded_train_set, encoded_valid_set, seq_len, batch_size, n_epochs, lr, callbacks, **create_model_kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         model.fit(train_X, train_Y, sample_weight = train_sample_weigths, batch_size = batch_size, epochs = n_epochs, validation_data = encoded_valid_set, \\\n\u001b[0;32m---> 30\u001b[0;31m                 callbacks = callbacks)\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/wandb/integration/keras/keras.py\u001b[0m in \u001b[0;36mnew_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0mset_wandb_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mold_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0mtraining_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_fit_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for pat in patience:\n",
    "    for lr in learning_rate:\n",
    "        finetune_by_settings(pat, lr, \"Split 4a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ffa85cd4-1674-44cb-9f4b-47fed90ec037",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/iq605716\" target=\"_blank\">proud-dragon-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-17:22:57] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:22:57] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:22:57] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 54s 581ms/step - loss: 0.8790 - val_loss: 0.4708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.6815 - val_loss: 0.5763\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5838 - val_loss: 0.4740\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5411 - val_loss: 0.5429\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-17:23:56] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:24:08] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5247WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1173s vs `on_train_batch_end` time: 0.1209s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1173s vs `on_train_batch_end` time: 0.1209s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 682ms/step - loss: 0.5247 - val_loss: 0.4662\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.5210 - val_loss: 0.4659\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.5201 - val_loss: 0.4630\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.5215 - val_loss: 0.4605\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.5191 - val_loss: 0.4575\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 0.5173 - val_loss: 0.4555\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.5183 - val_loss: 0.4542\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.5246 - val_loss: 0.4540\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.5200 - val_loss: 0.4555\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 343ms/step - loss: 0.5233 - val_loss: 0.4566\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 0.5164 - val_loss: 0.4576\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "[2022_04_09-17:24:37] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:24:37] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:24:37] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.5157WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0956s vs `on_train_batch_end` time: 0.1247s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0956s vs `on_train_batch_end` time: 0.1247s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 673ms/step - loss: 0.5177 - val_loss: 0.4539\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>529</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  529  0\n",
       "1  108  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:iq605716) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 34491... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▁▂▂▃▄▅▅▆▇▇█▁</td></tr><tr><td>loss</td><td>█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▂█▂▆▂▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.45389</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.51768</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.45389</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">proud-dragon-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/iq605716\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/iq605716</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_172247-iq605716/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:iq605716). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/uualznsv\" target=\"_blank\">sandy-planet-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-17:25:28] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:25:28] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:25:28] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 571ms/step - loss: 0.9884 - val_loss: 0.8532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.7348 - val_loss: 0.4670\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.5681 - val_loss: 0.4281\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5252 - val_loss: 0.5680\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5567 - val_loss: 0.4570\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4968 - val_loss: 0.5835\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-17:25:45] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:25:55] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4882WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0981s vs `on_train_batch_end` time: 0.1216s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0981s vs `on_train_batch_end` time: 0.1216s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 699ms/step - loss: 0.4896 - val_loss: 0.4518\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4907 - val_loss: 0.4375\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4800 - val_loss: 0.4252\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.4774 - val_loss: 0.4387\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4732 - val_loss: 0.4328\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4716 - val_loss: 0.4195\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4676 - val_loss: 0.4244\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.4605 - val_loss: 0.4311\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4517 - val_loss: 0.4116\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.4455 - val_loss: 0.4132\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4344 - val_loss: 0.4121\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4168 - val_loss: 0.4165\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_09-17:26:27] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:26:27] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:26:27] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4424WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0978s vs `on_train_batch_end` time: 0.1227s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0978s vs `on_train_batch_end` time: 0.1227s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 674ms/step - loss: 0.4387 - val_loss: 0.4123\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>521</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  521  8\n",
       "1  105  3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05042016806722689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:uualznsv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 34667... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▄▁▂▂▃▄▄▅▅▆▇▇█▁</td></tr><tr><td>loss</td><td>█▅▃▂▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▁▃▂▄▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_val_loss</td><td>0.41161</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.43875</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41233</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">sandy-planet-2</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/uualznsv\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/uualznsv</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_172513-uualznsv/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:uualznsv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/1yigexn9\" target=\"_blank\">breezy-durian-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-17:27:21] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:27:21] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:27:21] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 564ms/step - loss: 0.9048 - val_loss: 0.7479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.7445 - val_loss: 0.4370\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.6348 - val_loss: 0.4283\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.5251 - val_loss: 0.4871\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4881 - val_loss: 0.4153\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.4796 - val_loss: 0.5000\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4763 - val_loss: 0.4090\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4636 - val_loss: 0.4435\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4476 - val_loss: 0.4009\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4436 - val_loss: 0.4027\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4501 - val_loss: 0.4195\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4368 - val_loss: 0.4038\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-17:27:44] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:27:52] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4465WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0964s vs `on_train_batch_end` time: 0.1238s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0964s vs `on_train_batch_end` time: 0.1238s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 708ms/step - loss: 0.4487 - val_loss: 0.4005\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4485 - val_loss: 0.4159\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4274 - val_loss: 0.4109\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4474 - val_loss: 0.4227\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_09-17:28:09] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:28:09] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:28:09] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4393WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0952s vs `on_train_batch_end` time: 0.1246s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0952s vs `on_train_batch_end` time: 0.1246s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 703ms/step - loss: 0.4399 - val_loss: 0.4003\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>520</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  520  9\n",
       "1  104  4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06611570247933884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1yigexn9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 34856... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█▁▂▂▃▁</td></tr><tr><td>loss</td><td>█▆▄▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▃▁▃▁▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.40032</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.43986</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.40032</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">breezy-durian-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/1yigexn9\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/1yigexn9</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_172704-1yigexn9/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1yigexn9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/3ca7hirr\" target=\"_blank\">volcanic-wood-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-17:29:02] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:29:02] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:29:02] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 571ms/step - loss: 1.0263 - val_loss: 1.1799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.7311 - val_loss: 0.5796\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.6268 - val_loss: 0.4795\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.5593 - val_loss: 0.4428\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5221 - val_loss: 0.4226\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4706 - val_loss: 0.4370\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4593 - val_loss: 0.4257\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.4614 - val_loss: 0.4040\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4459 - val_loss: 0.3997\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4459 - val_loss: 0.4188\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4327 - val_loss: 0.4026\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4237 - val_loss: 0.4009\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_09-17:29:26] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:29:40] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.5546WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0954s vs `on_train_batch_end` time: 0.1235s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0954s vs `on_train_batch_end` time: 0.1235s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 706ms/step - loss: 0.5525 - val_loss: 0.4014\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.4700 - val_loss: 0.4042\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4854 - val_loss: 0.4126\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4412 - val_loss: 0.3991\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4482 - val_loss: 0.4337\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4410 - val_loss: 0.4020\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4531 - val_loss: 0.4382\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_09-17:30:01] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:30:01] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:30:01] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4324WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1004s vs `on_train_batch_end` time: 0.1234s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1004s vs `on_train_batch_end` time: 0.1234s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 700ms/step - loss: 0.4296 - val_loss: 0.4038\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>516</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  516  13\n",
       "1  102   6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09448818897637795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3ca7hirr) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 35036... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█▁▂▂▃▄▄▅▁</td></tr><tr><td>loss</td><td>█▅▃▃▂▂▁▁▁▁▁▁▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>3</td></tr><tr><td>best_val_loss</td><td>0.39912</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.42957</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.40377</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">volcanic-wood-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/3ca7hirr\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/3ca7hirr</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_172847-3ca7hirr/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3ca7hirr). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/3j7id2qm\" target=\"_blank\">ancient-valley-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-17:30:55] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:30:55] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:30:55] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 590ms/step - loss: 0.9398 - val_loss: 0.9604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.6932 - val_loss: 0.5022\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.5938 - val_loss: 0.4416\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.5174 - val_loss: 0.5099\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.5201 - val_loss: 0.4230\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4692 - val_loss: 0.4910\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4686 - val_loss: 0.4065\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4658 - val_loss: 0.4110\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4389 - val_loss: 0.4138\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4437 - val_loss: 0.4018\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4348 - val_loss: 0.3964\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4743 - val_loss: 0.4227\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4383 - val_loss: 0.3959\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4220 - val_loss: 0.3964\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4226 - val_loss: 0.3885\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4229 - val_loss: 0.3913\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4037 - val_loss: 0.4050\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4144 - val_loss: 0.4456\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4331 - val_loss: 0.3800\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4022 - val_loss: 0.3808\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4080 - val_loss: 0.3983\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3934 - val_loss: 0.3794\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3896 - val_loss: 0.4036\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4079 - val_loss: 0.3862\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4035 - val_loss: 0.3833\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4048 - val_loss: 0.3857\n",
      "[2022_04_09-17:31:35] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:31:54] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4081WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0968s vs `on_train_batch_end` time: 0.1243s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0968s vs `on_train_batch_end` time: 0.1243s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 658ms/step - loss: 0.4082 - val_loss: 0.3896\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3987 - val_loss: 0.3897\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4128 - val_loss: 0.3941\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4005 - val_loss: 0.3888\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3961 - val_loss: 0.3837\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3811 - val_loss: 0.3792\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3948 - val_loss: 0.3797\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3915 - val_loss: 0.3840\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3982 - val_loss: 0.3820\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3873 - val_loss: 0.3825\n",
      "[2022_04_09-17:32:20] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:32:20] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:32:20] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3958WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1003s vs `on_train_batch_end` time: 0.1247s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1003s vs `on_train_batch_end` time: 0.1247s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 688ms/step - loss: 0.3958 - val_loss: 0.3797\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>510</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  510  19\n",
       "1   89  19"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2602739726027397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3j7id2qm) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 35237... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▃▂▂▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.37917</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.39584</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.37969</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">ancient-valley-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/3j7id2qm\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/3j7id2qm</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_173039-3j7id2qm/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3j7id2qm). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/xxj6t3tk\" target=\"_blank\">major-cosmos-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-17:33:15] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:33:15] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:33:15] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 572ms/step - loss: 0.9796 - val_loss: 0.7330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.6496 - val_loss: 0.4352\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5586 - val_loss: 0.4319\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5078 - val_loss: 0.4981\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.5081 - val_loss: 0.4373\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4765 - val_loss: 0.4085\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4544 - val_loss: 0.4543\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.4662 - val_loss: 0.4008\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4473 - val_loss: 0.4415\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4573 - val_loss: 0.4020\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.4528 - val_loss: 0.3908\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4710 - val_loss: 0.4208\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4241 - val_loss: 0.4625\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4563 - val_loss: 0.4297\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4758 - val_loss: 0.4018\n",
      "[2022_04_09-17:33:41] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:33:49] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4313WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0995s vs `on_train_batch_end` time: 0.1222s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0995s vs `on_train_batch_end` time: 0.1222s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 670ms/step - loss: 0.4301 - val_loss: 0.4042\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 0.4243 - val_loss: 0.3918\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4270 - val_loss: 0.3901\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4166 - val_loss: 0.4090\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4090 - val_loss: 0.3861\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4108 - val_loss: 0.3929\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.4020 - val_loss: 0.4064\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4006 - val_loss: 0.3833\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3936 - val_loss: 0.3902\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3896 - val_loss: 0.3843\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3890 - val_loss: 0.3873\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3851 - val_loss: 0.3857\n",
      "[2022_04_09-17:34:21] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:34:21] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:34:21] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3908WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0956s vs `on_train_batch_end` time: 0.1242s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0956s vs `on_train_batch_end` time: 0.1242s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 672ms/step - loss: 0.3907 - val_loss: 0.3837\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>515</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  515  14\n",
       "1   95  13"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1925925925925926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:xxj6t3tk) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 35518... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█▁▁▂▃▃▃▄▅▅▅▆▇▁</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▃▂▂▂▁▂▁▁▂▃▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.38329</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.39074</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.38366</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">major-cosmos-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/xxj6t3tk\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/xxj6t3tk</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_173300-xxj6t3tk/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:xxj6t3tk). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/3pflob8e\" target=\"_blank\">easy-night-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-17:35:31] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:35:31] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:35:31] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 568ms/step - loss: 0.8316 - val_loss: 0.7136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 0.6507 - val_loss: 0.4898\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.5513 - val_loss: 0.4772\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.5333 - val_loss: 0.4468\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 0.4890 - val_loss: 0.4713\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 0.4778 - val_loss: 0.4139\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.4720 - val_loss: 0.4745\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 0.5251 - val_loss: 0.5055\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 0.4613 - val_loss: 0.4186\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 0.4424 - val_loss: 0.4154\n",
      "[2022_04_09-17:35:53] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:36:01] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4531WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0971s vs `on_train_batch_end` time: 0.1240s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0971s vs `on_train_batch_end` time: 0.1240s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 667ms/step - loss: 0.4557 - val_loss: 0.4999\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4774 - val_loss: 0.4085\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4567 - val_loss: 0.4199\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.4388 - val_loss: 0.4136\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.4282 - val_loss: 0.4091\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.4232 - val_loss: 0.4053\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4296 - val_loss: 0.4149\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4278 - val_loss: 0.4170\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4273 - val_loss: 0.4134\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4183 - val_loss: 0.4123\n",
      "[2022_04_09-17:36:28] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:36:28] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:36:28] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4286WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0940s vs `on_train_batch_end` time: 0.1273s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0940s vs `on_train_batch_end` time: 0.1273s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 673ms/step - loss: 0.4269 - val_loss: 0.4068\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>513</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  513  16\n",
       "1   99   9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13533834586466165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3pflob8e) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 35754... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█▁▂▃▃▄▅▆▆▇█▁</td></tr><tr><td>loss</td><td>█▅▃▃▂▂▂▃▂▁▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▂▂▁▃▃▁▁▃▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.40534</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.42693</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.4068</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">easy-night-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/3pflob8e\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/3pflob8e</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_173515-3pflob8e/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3pflob8e). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/am0668wt\" target=\"_blank\">exalted-lake-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-17:37:21] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:37:21] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:37:21] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 573ms/step - loss: 0.8241 - val_loss: 0.5921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.6161 - val_loss: 0.5308\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5331 - val_loss: 0.4581\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.5192 - val_loss: 0.4286\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.5814 - val_loss: 0.6019\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5168 - val_loss: 0.4159\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4813 - val_loss: 0.4081\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4614 - val_loss: 0.4573\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4681 - val_loss: 0.4743\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4738 - val_loss: 0.4788\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.5501 - val_loss: 0.4040\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4613 - val_loss: 0.4878\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4567 - val_loss: 0.3983\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4427 - val_loss: 0.4172\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4302 - val_loss: 0.4084\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4274 - val_loss: 0.4012\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4249 - val_loss: 0.3993\n",
      "[2022_04_09-17:37:50] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:38:00] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4812WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1234s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1234s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 670ms/step - loss: 0.4771 - val_loss: 0.3990\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.4190 - val_loss: 0.3873\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.4155 - val_loss: 0.4008\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 308ms/step - loss: 0.4072 - val_loss: 0.4027\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.3951 - val_loss: 0.4106\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.4177 - val_loss: 0.4133\n",
      "[2022_04_09-17:38:19] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:38:19] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:38:26] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4220WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0986s vs `on_train_batch_end` time: 0.1212s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0986s vs `on_train_batch_end` time: 0.1212s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 687ms/step - loss: 0.4212 - val_loss: 0.4009\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>507</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  507  22\n",
       "1   89  19"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2550335570469799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:am0668wt) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 35954... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██▁▁▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▅▃▃▄▃▂▂▂▂▄▂▂▂▂▂▁▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▃▂█▂▂▃▄▄▂▄▁▂▂▁▁▁▁▁▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_val_loss</td><td>0.38733</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.42122</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.40093</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">exalted-lake-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/am0668wt\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/am0668wt</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_173706-am0668wt/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:am0668wt). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/1waglaf7\" target=\"_blank\">vibrant-glade-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-17:39:21] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:39:21] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:39:21] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 547ms/step - loss: 0.8813 - val_loss: 1.3722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.8270 - val_loss: 0.6514\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.6990 - val_loss: 0.4738\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 0.5945 - val_loss: 0.4451\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.6084 - val_loss: 0.5054\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 0.7313 - val_loss: 0.4767\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.6031 - val_loss: 0.4671\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 0.5647 - val_loss: 0.4207\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4659 - val_loss: 0.4939\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4907 - val_loss: 0.4297\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4549 - val_loss: 0.4138\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4590 - val_loss: 0.4253\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4567 - val_loss: 0.4437\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 180ms/step - loss: 0.4493 - val_loss: 0.4103\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 0.4511 - val_loss: 0.4091\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4416 - val_loss: 0.4166\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4461 - val_loss: 0.4071\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4406 - val_loss: 0.4127\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4436 - val_loss: 0.4111\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4474 - val_loss: 0.4060\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4320 - val_loss: 0.4198\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4525 - val_loss: 0.4177\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4349 - val_loss: 0.4000\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4296 - val_loss: 0.4420\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 177ms/step - loss: 0.4490 - val_loss: 0.4078\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 175ms/step - loss: 0.4428 - val_loss: 0.3960\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 177ms/step - loss: 0.4451 - val_loss: 0.4473\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4302 - val_loss: 0.3960\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4342 - val_loss: 0.3981\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4173 - val_loss: 0.4009\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 0.4186 - val_loss: 0.3993\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 0.4270 - val_loss: 0.4003\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 180ms/step - loss: 0.4207 - val_loss: 0.4012\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 181ms/step - loss: 0.4155 - val_loss: 0.4017\n",
      "[2022_04_09-17:40:07] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:40:20] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4370WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0953s vs `on_train_batch_end` time: 0.1259s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0953s vs `on_train_batch_end` time: 0.1259s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 694ms/step - loss: 0.4364 - val_loss: 0.3974\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4286 - val_loss: 0.4092\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 309ms/step - loss: 0.4229 - val_loss: 0.4043\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 0.4315 - val_loss: 0.3986\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 309ms/step - loss: 0.4223 - val_loss: 0.3988\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.4256 - val_loss: 0.4004\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4203 - val_loss: 0.4025\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_09-17:40:40] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:40:40] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:40:40] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4156WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0981s vs `on_train_batch_end` time: 0.1216s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0981s vs `on_train_batch_end` time: 0.1216s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 684ms/step - loss: 0.4188 - val_loss: 0.3979\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>512</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  512  17\n",
       "1   97  11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16176470588235295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1waglaf7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 36169... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███▁▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▇▅▄▄▆▄▃▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>27</td></tr><tr><td>best_val_loss</td><td>0.39603</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.41884</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.39785</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">vibrant-glade-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/1waglaf7\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/1waglaf7</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_173904-1waglaf7/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1waglaf7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/2or1kl6t\" target=\"_blank\">kind-field-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-17:41:35] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:41:35] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:41:35] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 558ms/step - loss: 0.9627 - val_loss: 0.9321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.6830 - val_loss: 0.4828\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.5887 - val_loss: 0.4431\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.5013 - val_loss: 0.5086\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.5022 - val_loss: 0.4210\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4904 - val_loss: 0.4119\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4621 - val_loss: 0.4129\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4486 - val_loss: 0.4035\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.4616 - val_loss: 0.4299\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.4311 - val_loss: 0.4123\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4397 - val_loss: 0.3968\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 177ms/step - loss: 0.4395 - val_loss: 0.5503\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 0.4825 - val_loss: 0.3960\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 175ms/step - loss: 0.4686 - val_loss: 0.4359\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4728 - val_loss: 0.4150\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4481 - val_loss: 0.5008\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 175ms/step - loss: 0.4669 - val_loss: 0.3878\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 0.4155 - val_loss: 0.3822\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4139 - val_loss: 0.3899\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.4179 - val_loss: 0.4030\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 181ms/step - loss: 0.4028 - val_loss: 0.3818\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4135 - val_loss: 0.4052\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.4042 - val_loss: 0.3802\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.4154 - val_loss: 0.3915\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3968 - val_loss: 0.3884\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 0.4075 - val_loss: 0.3823\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3929 - val_loss: 0.3935\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.3917 - val_loss: 0.3924\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.3904 - val_loss: 0.3830\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-17:42:16] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:42:33] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4045WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0964s vs `on_train_batch_end` time: 0.1234s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0964s vs `on_train_batch_end` time: 0.1234s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 682ms/step - loss: 0.4030 - val_loss: 0.3806\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 308ms/step - loss: 0.4105 - val_loss: 0.3821\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 0.3894 - val_loss: 0.3884\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3938 - val_loss: 0.3789\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4137 - val_loss: 0.3990\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3957 - val_loss: 0.3781\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3891 - val_loss: 0.3770\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3812 - val_loss: 0.4235\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3794 - val_loss: 0.3738\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3936 - val_loss: 0.3832\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3830 - val_loss: 0.3974\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3778 - val_loss: 0.3726\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3656 - val_loss: 0.3913\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3797 - val_loss: 0.3874\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3649 - val_loss: 0.3724\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3569 - val_loss: 0.4053\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3546 - val_loss: 0.3738\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.3794 - val_loss: 0.3765\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3409 - val_loss: 0.3894\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3320 - val_loss: 0.3893\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3446 - val_loss: 0.3805\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_09-17:43:20] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:43:20] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:43:20] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3592WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1219s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1219s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 662ms/step - loss: 0.3617 - val_loss: 0.3724\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>506</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  506  23\n",
       "1   87  21"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27631578947368424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2or1kl6t) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 36480... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▇▇▇▇█▁▁▂▂▂▃▃▃▃▄▄▅▅▅▅▆▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▂▂▂▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▃▁▂▁▂▁▃▁▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.3724</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36172</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.3724</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">kind-field-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/2or1kl6t\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/2or1kl6t</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_174118-2or1kl6t/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2or1kl6t). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/34c02zic\" target=\"_blank\">driven-sea-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-17:44:54] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:44:54] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:44:54] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 571ms/step - loss: 0.8470 - val_loss: 0.6149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.6706 - val_loss: 0.4347\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5361 - val_loss: 0.4264\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4792 - val_loss: 0.4379\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4780 - val_loss: 0.4633\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4828 - val_loss: 0.4182\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4631 - val_loss: 0.4058\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4560 - val_loss: 0.4204\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4425 - val_loss: 0.4475\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4525 - val_loss: 0.3960\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4579 - val_loss: 0.3936\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4329 - val_loss: 0.4101\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4620 - val_loss: 0.5389\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4499 - val_loss: 0.3891\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4290 - val_loss: 0.3879\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4475 - val_loss: 0.4246\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4289 - val_loss: 0.3934\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4178 - val_loss: 0.3801\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4044 - val_loss: 0.3972\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 0.3904 - val_loss: 0.3935\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.3899 - val_loss: 0.3865\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4282 - val_loss: 0.3901\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3987 - val_loss: 0.3957\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4032 - val_loss: 0.3731\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3962 - val_loss: 0.3869\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3840 - val_loss: 0.3778\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3964 - val_loss: 0.3884\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3966 - val_loss: 0.3846\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3818 - val_loss: 0.3746\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3880 - val_loss: 0.3725\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3791 - val_loss: 0.3785\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3869 - val_loss: 0.3827\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.3806 - val_loss: 0.3744\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3774 - val_loss: 0.3734\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.3815 - val_loss: 0.3739\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3889 - val_loss: 0.3741\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_09-17:45:44] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:45:53] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3966WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0960s vs `on_train_batch_end` time: 0.1237s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0960s vs `on_train_batch_end` time: 0.1237s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 680ms/step - loss: 0.4024 - val_loss: 0.4048\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4004 - val_loss: 0.3693\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3745 - val_loss: 0.3774\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 0.3822 - val_loss: 0.3780\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3625 - val_loss: 0.3702\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3687 - val_loss: 0.3734\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3679 - val_loss: 0.3869\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.3555 - val_loss: 0.3766\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_09-17:46:16] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:46:16] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:46:16] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3781WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0954s vs `on_train_batch_end` time: 0.1268s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0954s vs `on_train_batch_end` time: 0.1268s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 680ms/step - loss: 0.3780 - val_loss: 0.3702\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>508</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  508  21\n",
       "1   84  24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3137254901960784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:34c02zic) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 36845... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███▁▁▁▂▂▂▂▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▃▄▂▂▂▂▂▂▆▂▂▃▂▂▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_val_loss</td><td>0.36927</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37796</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.37018</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">driven-sea-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/34c02zic\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/34c02zic</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_174439-34c02zic/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:34c02zic). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/12czo9bj\" target=\"_blank\">hardy-terrain-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-17:47:11] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:47:11] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:47:11] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 590ms/step - loss: 0.8894 - val_loss: 0.6605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.6528 - val_loss: 0.4356\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5448 - val_loss: 0.4266\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4965 - val_loss: 0.4851\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4866 - val_loss: 0.4124\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4691 - val_loss: 0.4236\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4580 - val_loss: 0.4095\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4475 - val_loss: 0.4024\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4463 - val_loss: 0.4038\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4346 - val_loss: 0.4879\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4461 - val_loss: 0.3993\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4273 - val_loss: 0.4109\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4664 - val_loss: 0.4303\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4501 - val_loss: 0.4777\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4330 - val_loss: 0.3904\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4439 - val_loss: 0.3919\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4058 - val_loss: 0.4388\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4429 - val_loss: 0.3898\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4066 - val_loss: 0.3875\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4122 - val_loss: 0.4055\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4063 - val_loss: 0.3878\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4099 - val_loss: 0.3920\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3897 - val_loss: 0.3923\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3987 - val_loss: 0.3913\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4096 - val_loss: 0.3959\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_09-17:47:50] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:47:58] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.5813WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1002s vs `on_train_batch_end` time: 0.1246s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1002s vs `on_train_batch_end` time: 0.1246s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 676ms/step - loss: 0.5795 - val_loss: 0.3912\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4567 - val_loss: 0.3947\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4434 - val_loss: 0.4136\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4191 - val_loss: 0.3878\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4158 - val_loss: 0.4236\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4022 - val_loss: 0.3811\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3669 - val_loss: 0.3753\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3908 - val_loss: 0.5027\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3880 - val_loss: 0.3699\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3468 - val_loss: 0.3804\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3566 - val_loss: 0.3788\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3286 - val_loss: 0.3964\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3215 - val_loss: 0.3687\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.2910 - val_loss: 0.3649\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.2859 - val_loss: 0.3654\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.2685 - val_loss: 0.3739\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.2690 - val_loss: 0.3674\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.2664 - val_loss: 0.3660\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.2460 - val_loss: 0.3670\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.2488 - val_loss: 0.3700\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "[2022_04_09-17:48:44] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:48:44] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:48:44] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.2896WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0950s vs `on_train_batch_end` time: 0.1270s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0950s vs `on_train_batch_end` time: 0.1270s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 687ms/step - loss: 0.2891 - val_loss: 0.3666\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>494</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  494  35\n",
       "1   75  33"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:12czo9bj) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 37167... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▄▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇</td></tr><tr><td>loss</td><td>█▅▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▅▃▃▃▃▂▃▃▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▄▂▂▂▂▄▂▂▃▄▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▄▁▁▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>13</td></tr><tr><td>best_val_loss</td><td>0.36492</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.28906</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.3666</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">hardy-terrain-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/12czo9bj\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/12czo9bj</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_174654-12czo9bj/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:12czo9bj). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/29bpsf7n\" target=\"_blank\">efficient-cloud-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_09-17:49:38] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:49:38] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:49:38] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 579ms/step - loss: 0.8873 - val_loss: 0.4887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.6215 - val_loss: 0.5977\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5988 - val_loss: 0.5325\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.6058 - val_loss: 0.4518\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5484 - val_loss: 0.4476\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.5268 - val_loss: 0.4996\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5930 - val_loss: 0.6074\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4993 - val_loss: 0.4885\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5286 - val_loss: 0.4983\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4594 - val_loss: 0.4046\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4868 - val_loss: 0.4018\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4481 - val_loss: 0.4248\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4432 - val_loss: 0.4008\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4415 - val_loss: 0.4095\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4261 - val_loss: 0.4274\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4287 - val_loss: 0.3987\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4414 - val_loss: 0.3968\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4286 - val_loss: 0.4237\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4245 - val_loss: 0.3957\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4295 - val_loss: 0.4030\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4283 - val_loss: 0.4006\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4303 - val_loss: 0.3931\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4329 - val_loss: 0.3946\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4226 - val_loss: 0.4094\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4169 - val_loss: 0.3967\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4278 - val_loss: 0.3940\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4116 - val_loss: 0.3967\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4211 - val_loss: 0.3917\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4256 - val_loss: 0.3927\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4120 - val_loss: 0.4062\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4187 - val_loss: 0.4111\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4200 - val_loss: 0.3942\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4218 - val_loss: 0.3933\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4178 - val_loss: 0.3929\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4092 - val_loss: 0.3931\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4236 - val_loss: 0.3933\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_09-17:50:27] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:50:36] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4318WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0967s vs `on_train_batch_end` time: 0.1258s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0967s vs `on_train_batch_end` time: 0.1258s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 718ms/step - loss: 0.4296 - val_loss: 0.3933\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4123 - val_loss: 0.4034\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4230 - val_loss: 0.4009\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4200 - val_loss: 0.3928\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4106 - val_loss: 0.3910\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4157 - val_loss: 0.3922\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4039 - val_loss: 0.3974\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.4104 - val_loss: 0.3975\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4060 - val_loss: 0.3941\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.4086 - val_loss: 0.3953\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.4047 - val_loss: 0.3975\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.4117 - val_loss: 0.3968\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4030 - val_loss: 0.3964\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_09-17:51:10] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:51:10] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:51:10] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4073WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0980s vs `on_train_batch_end` time: 0.1238s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0980s vs `on_train_batch_end` time: 0.1238s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 685ms/step - loss: 0.4059 - val_loss: 0.3909\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>513</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  513  16\n",
       "1   94  14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2028985507246377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:29bpsf7n) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 37490... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▄▄▄▃▄▂▃▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄█▆▃▅█▄▄▁▂▁▂▁▁▂▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.39094</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.40587</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.39094</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">efficient-cloud-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/29bpsf7n\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/29bpsf7n</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_174922-29bpsf7n/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:29bpsf7n). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/3kgedl1m\" target=\"_blank\">dulcet-capybara-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_09-17:52:05] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:52:05] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:52:05] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 570ms/step - loss: 1.0845 - val_loss: 0.9874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.6758 - val_loss: 0.4772\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5902 - val_loss: 0.4290\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.5721 - val_loss: 0.5580\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.5690 - val_loss: 0.4551\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4999 - val_loss: 0.6714\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.5800 - val_loss: 0.4239\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4827 - val_loss: 0.4802\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4591 - val_loss: 0.4023\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4495 - val_loss: 0.4187\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4388 - val_loss: 0.4001\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4389 - val_loss: 0.4426\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4244 - val_loss: 0.3926\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4174 - val_loss: 0.4119\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4067 - val_loss: 0.3881\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4243 - val_loss: 0.4249\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4208 - val_loss: 0.3905\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4021 - val_loss: 0.3828\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3949 - val_loss: 0.3830\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4183 - val_loss: 0.4824\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4333 - val_loss: 0.3802\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4142 - val_loss: 0.3829\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4110 - val_loss: 0.3860\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4057 - val_loss: 0.4674\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4026 - val_loss: 0.3757\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4123 - val_loss: 0.3722\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3933 - val_loss: 0.4640\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4063 - val_loss: 0.3807\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4160 - val_loss: 0.3826\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3905 - val_loss: 0.3747\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3867 - val_loss: 0.3802\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3744 - val_loss: 0.3926\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3810 - val_loss: 0.3800\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.3753 - val_loss: 0.3755\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_09-17:52:53] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:53:01] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3631WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0989s vs `on_train_batch_end` time: 0.1234s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0989s vs `on_train_batch_end` time: 0.1234s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 693ms/step - loss: 0.3634 - val_loss: 0.3989\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3832 - val_loss: 0.3726\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3752 - val_loss: 0.3894\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3850 - val_loss: 0.3730\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.3776 - val_loss: 0.3801\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3873 - val_loss: 0.3729\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3710 - val_loss: 0.3785\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3654 - val_loss: 0.3784\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3575 - val_loss: 0.3762\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3438 - val_loss: 0.3849\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_09-17:53:28] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:53:28] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:53:28] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3696WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0959s vs `on_train_batch_end` time: 0.1236s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0959s vs `on_train_batch_end` time: 0.1236s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 710ms/step - loss: 0.3772 - val_loss: 0.3759\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>497</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  497  32\n",
       "1   82  26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3132530120481928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3kgedl1m) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 37842... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▂▃▂▂▂▂▂▂▁▂▂▁▂▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▃▂▄▂▂▂▁▂▁▁▁▂▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>25</td></tr><tr><td>best_val_loss</td><td>0.37216</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37723</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.3759</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dulcet-capybara-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/3kgedl1m\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/3kgedl1m</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_175149-3kgedl1m/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3kgedl1m). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/1nn5thbq\" target=\"_blank\">fanciful-terrain-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_09-17:54:23] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:54:23] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:54:23] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 560ms/step - loss: 0.9404 - val_loss: 0.9079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.6727 - val_loss: 0.4309\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.5459 - val_loss: 0.4397\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5427 - val_loss: 0.4532\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4814 - val_loss: 0.4186\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4775 - val_loss: 0.4336\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4623 - val_loss: 0.4297\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4535 - val_loss: 0.3993\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4494 - val_loss: 0.3944\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4461 - val_loss: 0.4145\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4468 - val_loss: 0.3917\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4192 - val_loss: 0.4354\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4370 - val_loss: 0.3879\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4172 - val_loss: 0.4090\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4276 - val_loss: 0.4224\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4179 - val_loss: 0.3884\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4016 - val_loss: 0.3839\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4071 - val_loss: 0.3870\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4112 - val_loss: 0.3779\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4034 - val_loss: 0.3754\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4259 - val_loss: 0.5353\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4371 - val_loss: 0.3760\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4251 - val_loss: 0.4063\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4284 - val_loss: 0.3744\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3985 - val_loss: 0.3943\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3903 - val_loss: 0.3841\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4267 - val_loss: 0.3712\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4211 - val_loss: 0.6304\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.5101 - val_loss: 0.3699\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3940 - val_loss: 0.3771\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 0.3672 - val_loss: 0.3893\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3936 - val_loss: 0.6460\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4898 - val_loss: 0.3703\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.3918 - val_loss: 0.4107\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.3781 - val_loss: 0.3714\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 181ms/step - loss: 0.3564 - val_loss: 0.3855\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 0.3782 - val_loss: 0.3748\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_09-17:55:14] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:55:22] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3664WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0945s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0945s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 669ms/step - loss: 0.3707 - val_loss: 0.3746\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 309ms/step - loss: 0.3793 - val_loss: 0.3924\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3685 - val_loss: 0.3700\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3753 - val_loss: 0.4017\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 0.3552 - val_loss: 0.3753\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.3819 - val_loss: 0.3951\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 0.3526 - val_loss: 0.3685\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3870 - val_loss: 0.3700\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.3619 - val_loss: 0.3792\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.3532 - val_loss: 0.3645\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.3356 - val_loss: 0.3859\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 309ms/step - loss: 0.3166 - val_loss: 0.3621\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 0.3132 - val_loss: 0.3723\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 0.3202 - val_loss: 0.3632\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3151 - val_loss: 0.3892\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.2973 - val_loss: 0.3725\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 0.2882 - val_loss: 0.3745\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3061 - val_loss: 0.3733\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 343ms/step - loss: 0.2958 - val_loss: 0.3698\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3005 - val_loss: 0.3785\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_09-17:56:07] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:56:07] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:56:07] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3271WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0972s vs `on_train_batch_end` time: 0.1252s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0972s vs `on_train_batch_end` time: 0.1252s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 687ms/step - loss: 0.3286 - val_loss: 0.3623\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>488</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  488  41\n",
       "1   74  34"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37158469945355194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1nn5thbq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 38172... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▃▃▃▃▄▄▄▅▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▃▂▂▂▂▂▂▂▃▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▂▂▁▁▁▂▂▂▁▁▁▃▁▁▁▁▄▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>11</td></tr><tr><td>best_val_loss</td><td>0.36209</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32863</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.36226</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fanciful-terrain-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/1nn5thbq\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/1nn5thbq</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220409_175407-1nn5thbq/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1nn5thbq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/305ts2o6\" target=\"_blank\">blooming-terrain-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_09-17:57:03] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:57:03] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:57:03] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 583ms/step - loss: 0.8860 - val_loss: 0.8218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.7015 - val_loss: 0.4321\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.6143 - val_loss: 0.5102\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.6983 - val_loss: 0.5746\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5711 - val_loss: 0.4482\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.5011 - val_loss: 0.5322\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.4948 - val_loss: 0.4104\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4657 - val_loss: 0.4103\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4718 - val_loss: 0.4719\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4672 - val_loss: 0.4059\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4611 - val_loss: 0.4112\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4519 - val_loss: 0.4146\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 0.4553 - val_loss: 0.4040\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4490 - val_loss: 0.4078\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 0.4455 - val_loss: 0.4159\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4469 - val_loss: 0.4026\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4416 - val_loss: 0.4086\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4412 - val_loss: 0.4087\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4416 - val_loss: 0.4031\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4433 - val_loss: 0.4187\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 0.4359 - val_loss: 0.4082\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.4291 - val_loss: 0.3978\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4404 - val_loss: 0.4001\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4297 - val_loss: 0.4110\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 0.4337 - val_loss: 0.4084\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4349 - val_loss: 0.4014\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4293 - val_loss: 0.4002\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4313 - val_loss: 0.4008\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 0.4248 - val_loss: 0.4031\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 0.4347 - val_loss: 0.4058\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_09-17:57:46] Training the entire fine-tuned model...\n",
      "[2022_04_09-17:57:55] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4720WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0937s vs `on_train_batch_end` time: 0.1264s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0937s vs `on_train_batch_end` time: 0.1264s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 698ms/step - loss: 0.4755 - val_loss: 0.3983\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.4571 - val_loss: 0.3995\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4739 - val_loss: 0.4057\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 0.4306 - val_loss: 0.3996\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.4125 - val_loss: 0.3885\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3955 - val_loss: 0.3898\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3918 - val_loss: 0.3889\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.3710 - val_loss: 0.3997\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3303 - val_loss: 0.4253\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3267 - val_loss: 0.3859\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 0.3193 - val_loss: 0.3874\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.3018 - val_loss: 0.4028\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.2842 - val_loss: 0.3901\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.2678 - val_loss: 0.4405\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.2534 - val_loss: 0.4133\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.2273 - val_loss: 0.4233\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.2194 - val_loss: 0.4215\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 0.2115 - val_loss: 0.4301\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "[2022_04_09-17:58:36] Training on final epochs of sequence length 512...\n",
      "[2022_04_09-17:58:36] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_09-17:58:37] Validation set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.2989WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0987s vs `on_train_batch_end` time: 0.1239s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0987s vs `on_train_batch_end` time: 0.1239s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 676ms/step - loss: 0.2966 - val_loss: 0.4033\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>458</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  458  71\n",
       "1   66  42"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38009049773755654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_09_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/crossval/chen_4_b.csv\"), index_col=0)\n",
    "test_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/crossval/chen_4_a.csv\"), index_col=0)\n",
    "train_data[\"seq\"] = train_data[\"heavy\"] + train_data[\"light\"]\n",
    "test_data[\"seq\"] = test_data[\"heavy\"] + test_data[\"light\"]\n",
    "\n",
    "for pat in patience:\n",
    "    for lr in learning_rate:\n",
    "        finetune_by_settings(pat, lr, \"Split 4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23db2b59-81eb-47da-a60b-31e2d4892411",
   "metadata": {},
   "source": [
    "# 5x2cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f16d5b5-7ac3-4e98-8dcc-e028c8923d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8f47b94-77ca-4071-9756-e9107195e46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_trained_model(model, dir_name, model_name, x_test, y_test, x_tap, y_tap, lr, patience_lr, patience_stop):\n",
    "    y_pred = model.predict(x_test, batch_size=32)\n",
    "    y_pred_classes = (y_pred >= 0.5)\n",
    "    metric_dict = {\n",
    "        \"f1\": float(metrics.f1_score(y_test, y_pred_classes)),\n",
    "        \"acc\": float(metrics.accuracy_score(y_test, y_pred_classes)),\n",
    "        \"mcc\": float(metrics.matthews_corrcoef(y_test, y_pred_classes)),\n",
    "        \"auc\": float(metrics.roc_auc_score(y_test, y_pred_classes)),\n",
    "        \"precision\": float(metrics.precision_score(y_test, y_pred_classes)),\n",
    "        \"recall\": float(metrics.recall_score(y_test, y_pred_classes))\n",
    "    }\n",
    "    filename = path.join(DATA_DIR, f\"evaluations/protein_bert/{dir_name}/{model_name}_preds.csv\")\n",
    "    str_preds = [str(int(pred)) for pred in y_pred_classes]\n",
    "    with open(filename, \"wt\") as f:\n",
    "        f.write(\",\".join(str_preds) + \"\\n\")\n",
    "        \n",
    "    filename_sum = path.join(DATA_DIR, f\"evaluations/protein_bert/{dir_name}/all.csv\")\n",
    "    line = [\"protein_bert\", patience_stop, patience_lr, lr, metric_dict[\"f1\"], metric_dict[\"mcc\"], metric_dict[\"acc\"],metric_dict[\"precision\"],metric_dict[\"recall\"],metric_dict[\"auc\"]]\n",
    "    with open(filename_sum, 'a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile, delimiter='\\t')\n",
    "        csvwriter.writerow(line)\n",
    "    \n",
    "    \n",
    "    y_pred = model.predict(x_tap, batch_size=32)\n",
    "    y_pred_classes = (y_pred >= 0.5)\n",
    "    metric_dict = {\n",
    "        \"f1\": float(metrics.f1_score(y_tap, y_pred_classes)),\n",
    "        \"acc\": float(metrics.accuracy_score(y_tap, y_pred_classes)),\n",
    "        \"mcc\": float(metrics.matthews_corrcoef(y_tap, y_pred_classes)),\n",
    "        \"auc\": float(metrics.roc_auc_score(y_tap, y_pred_classes)),\n",
    "        \"precision\": float(metrics.precision_score(y_tap, y_pred_classes)),\n",
    "        \"recall\": float(metrics.recall_score(y_tap, y_pred_classes))\n",
    "    }\n",
    "        \n",
    "    filename_sum = path.join(DATA_DIR, f\"evaluations/protein_bert/{dir_name}/tap.csv\")\n",
    "    line = [\"protein_bert\", patience_stop, patience_lr, lr, metric_dict[\"f1\"], metric_dict[\"mcc\"], metric_dict[\"acc\"],metric_dict[\"precision\"],metric_dict[\"recall\"],metric_dict[\"auc\"]]\n",
    "    with open(filename_sum, 'a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile, delimiter='\\t')\n",
    "        csvwriter.writerow(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6816ec8-f1f4-4cda-a4be-256d0fdf968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_by_settings_and_data(patience, lr, project_name, train_data, test_data, valid_data, tap_X, tap_Y, dir_name):\n",
    "    wandb.init(project=project_name, entity=\"kvetab\")\n",
    "    model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)\n",
    "    training_callbacks = [\n",
    "        keras.callbacks.ReduceLROnPlateau(patience = patience[1], factor = 0.25, min_lr = 1e-07, verbose = 1),\n",
    "        keras.callbacks.EarlyStopping(patience = patience[0], restore_best_weights = True),\n",
    "        WandbCallback()\n",
    "    ]\n",
    "    epoch_num = 100\n",
    "    batch_size = 128\n",
    "    wandb.config = {\n",
    "          \"learning_rate\": lr,\n",
    "          \"epochs\": epoch_num * 2,\n",
    "          \"batch_size\": batch_size\n",
    "        }\n",
    "    print(type(lr))\n",
    "    print(lr)\n",
    "    finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data['seq'], train_data['Y'], valid_data['seq'], valid_data['Y'], \\\n",
    "            seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = lr, begin_with_frozen_pretrained_layers = True, \\\n",
    "            lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 512, final_lr = lr / 10, callbacks = training_callbacks)\n",
    "    \n",
    "    \n",
    "    mod = model_generator.create_model(seq_len = 512)\n",
    "    mod_name = f\"2022_04_21_{patience[0]}_{patience[1]}_{lr}\"\n",
    "    mod.save(path.join(DATA_DIR, f\"protein_bert/{dir_name}/{mod_name}\"))\n",
    "    \n",
    "    encoded_test_set = encode_dataset(test_data[\"seq\"], test_data[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'Test set')\n",
    "    test_X, test_Y, test_sample_weigths = encoded_test_set\n",
    "\n",
    "    test_trained_model(mod, dir_name, mod_name, test_X, test_Y, tap_X, tap_Y, lr, patience[1], patience[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1750fff9-d429-4b0a-ac05-d14f629b52bc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-13:40:17] Tap set: Filtered out 0 of 184 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3uneka2z) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 38122... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▅▅▆▆▇▇█▁▂▂▃▁</td></tr><tr><td>loss</td><td>█▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▅█▂▅▂▂▁▂▁▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>9</td></tr><tr><td>best_val_loss</td><td>0.44473</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36251</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.44898</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">visionary-pine-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/3uneka2z\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/3uneka2z</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_133424-3uneka2z/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3uneka2z). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/63ktqb8x\" target=\"_blank\">eager-surf-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-13:40:35] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:40:35] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:40:35] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 647ms/step - loss: 0.9191 - val_loss: 0.7767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.5691 - val_loss: 0.6967\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.6098 - val_loss: 0.5736\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4874 - val_loss: 0.5342\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4884 - val_loss: 0.5390\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4572 - val_loss: 0.4985\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4236 - val_loss: 0.4731\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4132 - val_loss: 0.5146\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4008 - val_loss: 0.4549\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3759 - val_loss: 0.4806\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3888 - val_loss: 0.4456\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3739 - val_loss: 0.4780\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3718 - val_loss: 0.4471\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3696 - val_loss: 0.5164\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-13:40:57] Training the entire fine-tuned model...\n",
      "[2022_04_21-13:41:13] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 787ms/step - loss: 0.3650 - val_loss: 0.4496\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 311ms/step - loss: 0.3520 - val_loss: 0.4553\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3540 - val_loss: 0.4575\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3538 - val_loss: 0.4555\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "[2022_04_21-13:41:27] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-13:41:27] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:41:27] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 759ms/step - loss: 0.3607 - val_loss: 0.4495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-13:42:03] Test set: Filtered out 0 of 393 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:63ktqb8x) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 38343... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▄▅▅▆▆▇▇█▁▂▂▃▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▄▃▃▂▂▂▁▂▁▂▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>10</td></tr><tr><td>best_val_loss</td><td>0.44562</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36074</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.44953</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">eager-surf-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/63ktqb8x\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/63ktqb8x</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_134017-63ktqb8x/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:63ktqb8x). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/1f5kmacy\" target=\"_blank\">balmy-voice-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-13:42:22] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:42:22] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:42:22] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 553ms/step - loss: 0.9477 - val_loss: 1.0915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.7600 - val_loss: 0.5408\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.6112 - val_loss: 0.4643\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.5129 - val_loss: 0.4576\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.5221 - val_loss: 0.4515\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.5231 - val_loss: 0.4709\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4778 - val_loss: 0.4102\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.4546 - val_loss: 0.4184\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4438 - val_loss: 0.4002\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.4360 - val_loss: 0.4378\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.4447 - val_loss: 0.4034\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4284 - val_loss: 0.3928\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4427 - val_loss: 0.3859\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.4218 - val_loss: 0.4032\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4689 - val_loss: 0.4720\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4288 - val_loss: 0.3879\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-13:42:46] Training the entire fine-tuned model...\n",
      "[2022_04_21-13:43:00] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4260WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1153s vs `on_train_batch_end` time: 0.1243s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1153s vs `on_train_batch_end` time: 0.1243s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 630ms/step - loss: 0.4260 - val_loss: 0.3862\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 254ms/step - loss: 0.4120 - val_loss: 0.3944\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.4208 - val_loss: 0.3986\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.4144 - val_loss: 0.3905\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "[2022_04_21-13:43:15] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-13:43:15] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:43:16] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4150WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0963s vs `on_train_batch_end` time: 0.1236s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0963s vs `on_train_batch_end` time: 0.1236s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 628ms/step - loss: 0.4133 - val_loss: 0.3860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-13:43:50] Test set: Filtered out 0 of 383 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1f5kmacy) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 38538... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██▁▁▂▂▁</td></tr><tr><td>loss</td><td>█▆▄▂▂▂▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▂▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>12</td></tr><tr><td>best_val_loss</td><td>0.3859</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.41334</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.38598</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">balmy-voice-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/1f5kmacy\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/1f5kmacy</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_134205-1f5kmacy/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1f5kmacy). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/26p7bi7c\" target=\"_blank\">leafy-star-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-13:44:10] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:44:10] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:44:10] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 641ms/step - loss: 0.9224 - val_loss: 0.8689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5993 - val_loss: 0.6056\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 185ms/step - loss: 0.5474 - val_loss: 0.5755\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.4521 - val_loss: 0.4924\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4336 - val_loss: 0.4902\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4076 - val_loss: 0.4876\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3936 - val_loss: 0.4655\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3934 - val_loss: 0.4859\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.3840 - val_loss: 0.4525\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3790 - val_loss: 0.4486\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 189ms/step - loss: 0.3636 - val_loss: 0.4488\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3732 - val_loss: 0.4416\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3708 - val_loss: 0.5022\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3589 - val_loss: 0.4377\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3450 - val_loss: 0.4571\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3509 - val_loss: 0.4504\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3497 - val_loss: 0.4363\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.3443 - val_loss: 0.5176\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3462 - val_loss: 0.4314\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3359 - val_loss: 0.4390\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3342 - val_loss: 0.4562\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3247 - val_loss: 0.4354\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-13:44:39] Training the entire fine-tuned model...\n",
      "[2022_04_21-13:44:47] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 785ms/step - loss: 0.3269 - val_loss: 0.4442\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.3283 - val_loss: 0.4304\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 317ms/step - loss: 0.3312 - val_loss: 0.4387\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3214 - val_loss: 0.4349\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3201 - val_loss: 0.4342\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_21-13:45:04] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-13:45:04] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:45:04] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 747ms/step - loss: 0.3290 - val_loss: 0.4313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-13:45:37] Test set: Filtered out 0 of 393 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:26p7bi7c) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 38742... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▄▄▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████████▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_val_loss</td><td>0.43037</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32901</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43131</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">leafy-star-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/26p7bi7c\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/26p7bi7c</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_134353-26p7bi7c/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:26p7bi7c). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/gl381aad\" target=\"_blank\">unique-flower-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-13:45:55] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:45:55] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:45:55] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 505ms/step - loss: 0.9124 - val_loss: 0.8822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.7411 - val_loss: 0.4681\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.5701 - val_loss: 0.4274\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.5269 - val_loss: 0.4513\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4803 - val_loss: 0.4393\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4922 - val_loss: 0.4136\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.4623 - val_loss: 0.4250\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4474 - val_loss: 0.4049\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.4604 - val_loss: 0.4998\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4744 - val_loss: 0.4852\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.5364 - val_loss: 0.4118\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-13:46:16] Training the entire fine-tuned model...\n",
      "[2022_04_21-13:46:33] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4605WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0976s vs `on_train_batch_end` time: 0.1226s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0976s vs `on_train_batch_end` time: 0.1226s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 612ms/step - loss: 0.4595 - val_loss: 0.4203\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 255ms/step - loss: 0.4403 - val_loss: 0.4013\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 255ms/step - loss: 0.4466 - val_loss: 0.4044\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.4468 - val_loss: 0.4237\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 255ms/step - loss: 0.4476 - val_loss: 0.4154\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_21-13:46:49] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-13:46:49] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:46:49] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4460WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0962s vs `on_train_batch_end` time: 0.1244s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0962s vs `on_train_batch_end` time: 0.1244s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 599ms/step - loss: 0.4489 - val_loss: 0.4015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-13:47:23] Test set: Filtered out 0 of 383 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:gl381aad) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 38982... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▅▅▆▇▇█▁▂▂▃▄▁</td></tr><tr><td>loss</td><td>█▅▃▂▂▂▁▁▁▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▁▂▂▁▁▁▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_val_loss</td><td>0.4013</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.44894</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.40155</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">unique-flower-2</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/gl381aad\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/gl381aad</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_134539-gl381aad/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:gl381aad). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/2acbcs3x\" target=\"_blank\">usual-moon-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-13:47:42] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:47:42] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:47:42] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 653ms/step - loss: 0.8326 - val_loss: 0.5915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.6017 - val_loss: 0.8408\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5905 - val_loss: 0.5004\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4740 - val_loss: 0.6256\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4635 - val_loss: 0.4751\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4227 - val_loss: 0.5146\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3982 - val_loss: 0.4646\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3774 - val_loss: 0.4800\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 0.3808 - val_loss: 0.4511\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3783 - val_loss: 0.4604\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3645 - val_loss: 0.4488\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3604 - val_loss: 0.4673\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3526 - val_loss: 0.4405\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3486 - val_loss: 0.4412\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3488 - val_loss: 0.4523\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3389 - val_loss: 0.4328\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 188ms/step - loss: 0.3416 - val_loss: 0.4506\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3345 - val_loss: 0.4461\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3236 - val_loss: 0.4272\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3321 - val_loss: 0.4286\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 186ms/step - loss: 0.3154 - val_loss: 0.4289\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3202 - val_loss: 0.4425\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-13:48:11] Training the entire fine-tuned model...\n",
      "[2022_04_21-13:48:20] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 754ms/step - loss: 0.3307 - val_loss: 0.4294\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 337ms/step - loss: 0.3324 - val_loss: 0.4432\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3157 - val_loss: 0.4288\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3240 - val_loss: 0.4517\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3138 - val_loss: 0.4250\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.3119 - val_loss: 0.4368\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3028 - val_loss: 0.4229\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3052 - val_loss: 0.4476\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3146 - val_loss: 0.4228\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3032 - val_loss: 0.4194\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.2973 - val_loss: 0.4409\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.2843 - val_loss: 0.4208\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.2713 - val_loss: 0.4418\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_21-13:48:49] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-13:48:49] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:48:49] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 789ms/step - loss: 0.2891 - val_loss: 0.4205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-13:49:23] Test set: Filtered out 0 of 393 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2acbcs3x) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 39157... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▄▅▅▁</td></tr><tr><td>loss</td><td>█▅▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄█▂▄▂▃▂▂▂▂▁▂▁▁▂▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>9</td></tr><tr><td>best_val_loss</td><td>0.41944</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.28912</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.4205</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">usual-moon-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/2acbcs3x\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/2acbcs3x</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_134725-2acbcs3x/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2acbcs3x). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/2rc5dk7r\" target=\"_blank\">blooming-surf-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-13:49:47] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:49:47] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:49:47] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 520ms/step - loss: 0.8969 - val_loss: 0.7575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.8266 - val_loss: 0.4366\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.6000 - val_loss: 0.4713\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.5806 - val_loss: 0.5275\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4912 - val_loss: 0.4201\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4718 - val_loss: 0.4392\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.4637 - val_loss: 0.4136\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4695 - val_loss: 0.4052\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4545 - val_loss: 0.4300\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4472 - val_loss: 0.4207\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4381 - val_loss: 0.4008\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4367 - val_loss: 0.3984\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.4283 - val_loss: 0.4177\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 0.4266 - val_loss: 0.3886\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4140 - val_loss: 0.3918\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4196 - val_loss: 0.4322\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4649 - val_loss: 0.4237\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-13:50:11] Training the entire fine-tuned model...\n",
      "[2022_04_21-13:50:21] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4209WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1247s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1247s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 619ms/step - loss: 0.4216 - val_loss: 0.4014\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 278ms/step - loss: 0.4112 - val_loss: 0.3862\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 255ms/step - loss: 0.3970 - val_loss: 0.3910\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.4163 - val_loss: 0.3869\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.4159 - val_loss: 0.3836\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3958 - val_loss: 0.3990\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 255ms/step - loss: 0.3881 - val_loss: 0.3840\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3862 - val_loss: 0.3773\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3842 - val_loss: 0.3935\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3894 - val_loss: 0.3794\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.4109 - val_loss: 0.3972\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_21-13:50:46] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-13:50:46] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:50:47] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3730WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0990s vs `on_train_batch_end` time: 0.1219s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0990s vs `on_train_batch_end` time: 0.1219s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 614ms/step - loss: 0.3743 - val_loss: 0.3827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-13:51:21] Test set: Filtered out 0 of 383 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2rc5dk7r) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 39438... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██▁▁▂▂▃▃▄▄▅▅▅▁</td></tr><tr><td>loss</td><td>█▇▄▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▃▄▂▂▂▂▂▂▁▁▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.3773</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37429</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.38272</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">blooming-surf-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/2rc5dk7r\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/2rc5dk7r</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_134926-2rc5dk7r/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2rc5dk7r). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/cn2kz098\" target=\"_blank\">distinctive-music-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-13:51:39] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:51:39] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:51:39] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 624ms/step - loss: 0.9281 - val_loss: 0.5508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.5111 - val_loss: 0.6576\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4571 - val_loss: 0.5288\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4423 - val_loss: 0.5915\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4226 - val_loss: 0.4696\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3966 - val_loss: 0.4892\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3927 - val_loss: 0.4770\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3773 - val_loss: 0.4519\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3698 - val_loss: 0.4650\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3701 - val_loss: 0.4693\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3732 - val_loss: 0.4438\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3586 - val_loss: 0.5276\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3666 - val_loss: 0.4331\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3499 - val_loss: 0.4426\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3378 - val_loss: 0.4484\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3396 - val_loss: 0.4291\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3470 - val_loss: 0.4853\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3203 - val_loss: 0.4286\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3251 - val_loss: 0.4321\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3277 - val_loss: 0.4395\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3284 - val_loss: 0.4347\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-13:52:06] Training the entire fine-tuned model...\n",
      "[2022_04_21-13:52:18] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 768ms/step - loss: 0.5681 - val_loss: 0.4681\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.4043 - val_loss: 0.5076\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3606 - val_loss: 0.4369\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3341 - val_loss: 0.4845\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3321 - val_loss: 0.4331\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.3118 - val_loss: 0.4466\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.2960 - val_loss: 0.4277\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.2856 - val_loss: 0.4312\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.2743 - val_loss: 0.4330\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.2567 - val_loss: 0.4299\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_21-13:52:41] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-13:52:42] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:52:44] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 748ms/step - loss: 0.2831 - val_loss: 0.4265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-13:53:19] Test set: Filtered out 0 of 393 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:cn2kz098) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 39694... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▄▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████████▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▅█▄▆▂▃▃▂▂▂▂▄▁▁▂▁▃▁▁▁▁▂▃▁▃▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.42646</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.28308</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.42646</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">distinctive-music-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/cn2kz098\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/cn2kz098</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_135123-cn2kz098/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:cn2kz098). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/3b3dxbsd\" target=\"_blank\">olive-energy-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-13:53:37] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:53:37] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:53:37] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 500ms/step - loss: 0.8513 - val_loss: 0.7227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.6960 - val_loss: 0.4395\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.5875 - val_loss: 0.4582\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.5256 - val_loss: 0.4995\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.5030 - val_loss: 0.4561\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-13:53:52] Training the entire fine-tuned model...\n",
      "[2022_04_21-13:54:01] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.5118WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0965s vs `on_train_batch_end` time: 0.1221s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0965s vs `on_train_batch_end` time: 0.1221s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 619ms/step - loss: 0.5110 - val_loss: 0.4270\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 255ms/step - loss: 0.4831 - val_loss: 0.4238\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 255ms/step - loss: 0.4820 - val_loss: 0.4083\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.4515 - val_loss: 0.4129\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 278ms/step - loss: 0.4229 - val_loss: 1.7524\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.8027 - val_loss: 0.5131\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_21-13:54:19] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-13:54:19] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:54:19] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4526WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0977s vs `on_train_batch_end` time: 0.1252s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0977s vs `on_train_batch_end` time: 0.1252s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 622ms/step - loss: 0.4526 - val_loss: 0.4077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-13:54:54] Test set: Filtered out 0 of 383 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3b3dxbsd) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 39955... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▄▅▇▁▂▄▅▇█▁</td></tr><tr><td>loss</td><td>█▅▄▃▂▂▂▂▁▁▇▁</td></tr><tr><td>lr</td><td>█████▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃▁▁▁▁▁▁▁▁█▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.40765</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.4526</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.40765</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">olive-energy-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/3b3dxbsd\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/3b3dxbsd</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_135322-3b3dxbsd/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3b3dxbsd). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/18s902h6\" target=\"_blank\">lively-voice-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-13:55:13] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:55:13] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:55:13] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 629ms/step - loss: 0.9448 - val_loss: 0.7306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.6052 - val_loss: 0.7306\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5676 - val_loss: 0.5295\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4674 - val_loss: 0.5293\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4512 - val_loss: 0.5030\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4298 - val_loss: 0.4880\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 186ms/step - loss: 0.4139 - val_loss: 0.4616\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4074 - val_loss: 0.5276\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 186ms/step - loss: 0.4146 - val_loss: 0.4486\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4092 - val_loss: 0.5565\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4144 - val_loss: 0.4445\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3965 - val_loss: 0.5282\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 188ms/step - loss: 0.3786 - val_loss: 0.4547\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3539 - val_loss: 0.5120\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3635 - val_loss: 0.4530\n",
      "[2022_04_21-13:55:35] Training the entire fine-tuned model...\n",
      "[2022_04_21-13:55:51] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 763ms/step - loss: 0.3691 - val_loss: 0.4437\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3680 - val_loss: 0.4518\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 318ms/step - loss: 0.3607 - val_loss: 0.4580\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3592 - val_loss: 0.4558\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3514 - val_loss: 0.4547\n",
      "[2022_04_21-13:56:06] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-13:56:06] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:56:07] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 737ms/step - loss: 0.3613 - val_loss: 0.4437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-13:56:42] Test set: Filtered out 0 of 393 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:18s902h6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 40115... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█▁▁▂▃▃▁</td></tr><tr><td>loss</td><td>█▄▄▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████▃▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>██▃▃▂▂▁▃▁▄▁▃▁▃▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.44371</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36128</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.44371</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">lively-voice-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/18s902h6\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/18s902h6</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_135456-18s902h6/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:18s902h6). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/2yebbjf9\" target=\"_blank\">fresh-deluge-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-13:57:02] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:57:02] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:57:02] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 505ms/step - loss: 0.9221 - val_loss: 0.5399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.7155 - val_loss: 0.4397\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.5690 - val_loss: 0.4385\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.5260 - val_loss: 0.5658\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.5390 - val_loss: 0.4307\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4886 - val_loss: 0.4341\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4789 - val_loss: 0.4213\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4531 - val_loss: 0.4142\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4488 - val_loss: 0.4028\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4517 - val_loss: 0.4475\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.4584 - val_loss: 0.3981\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4303 - val_loss: 0.4200\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.4854 - val_loss: 0.5720\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.5032 - val_loss: 0.3900\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4505 - val_loss: 0.3931\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.4140 - val_loss: 0.3848\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4037 - val_loss: 0.3853\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4150 - val_loss: 0.4226\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4139 - val_loss: 0.3959\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.4078 - val_loss: 0.3907\n",
      "[2022_04_21-13:57:29] Training the entire fine-tuned model...\n",
      "[2022_04_21-13:57:40] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4064WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0942s vs `on_train_batch_end` time: 0.1244s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0942s vs `on_train_batch_end` time: 0.1244s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 633ms/step - loss: 0.4085 - val_loss: 0.3865\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 254ms/step - loss: 0.4090 - val_loss: 0.3922\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.4091 - val_loss: 0.3899\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.4156 - val_loss: 0.3912\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 255ms/step - loss: 0.4079 - val_loss: 0.3902\n",
      "[2022_04_21-13:57:56] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-13:57:56] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:57:56] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3983WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0968s vs `on_train_batch_end` time: 0.1254s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0968s vs `on_train_batch_end` time: 0.1254s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 620ms/step - loss: 0.3978 - val_loss: 0.3864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-13:58:30] Test set: Filtered out 0 of 383 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2yebbjf9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 40323... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▃▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇▃▃█▃▃▂▂▂▃▁▂█▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>15</td></tr><tr><td>best_val_loss</td><td>0.38483</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3978</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.38638</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fresh-deluge-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/2yebbjf9\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/2yebbjf9</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_135645-2yebbjf9/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2yebbjf9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/i8c5qhsc\" target=\"_blank\">valiant-galaxy-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-13:58:50] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:58:50] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:58:50] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 624ms/step - loss: 0.8600 - val_loss: 0.7691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.6015 - val_loss: 0.7683\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.6762 - val_loss: 0.5944\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4881 - val_loss: 0.5079\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4659 - val_loss: 0.5471\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4380 - val_loss: 0.4777\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4330 - val_loss: 0.4746\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3932 - val_loss: 0.4697\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3820 - val_loss: 0.4545\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3729 - val_loss: 0.4760\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3671 - val_loss: 0.4457\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3582 - val_loss: 0.4658\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3525 - val_loss: 0.4397\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3545 - val_loss: 0.4571\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3473 - val_loss: 0.4369\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3459 - val_loss: 0.4476\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3369 - val_loss: 0.4396\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3420 - val_loss: 0.4388\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3319 - val_loss: 0.4401\n",
      "[2022_04_21-13:59:15] Training the entire fine-tuned model...\n",
      "[2022_04_21-13:59:25] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 760ms/step - loss: 0.3458 - val_loss: 0.4620\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3399 - val_loss: 0.4360\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3427 - val_loss: 0.4405\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3361 - val_loss: 0.4523\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 338ms/step - loss: 0.3309 - val_loss: 0.4366\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.3265 - val_loss: 0.4391\n",
      "[2022_04_21-13:59:42] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-13:59:42] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-13:59:42] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 742ms/step - loss: 0.3419 - val_loss: 0.4373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-14:00:24] Test set: Filtered out 0 of 393 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:i8c5qhsc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 40553... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██▁▁▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▅▆▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████▃▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>██▄▃▃▂▂▂▁▂▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_val_loss</td><td>0.43598</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34195</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43726</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">valiant-galaxy-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/i8c5qhsc\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/i8c5qhsc</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_135832-i8c5qhsc/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:i8c5qhsc). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/181y9qn7\" target=\"_blank\">flowing-pine-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-14:00:42] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:00:42] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:00:42] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 494ms/step - loss: 1.1161 - val_loss: 1.2136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.7980 - val_loss: 0.6334\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 0.7050 - val_loss: 0.5269\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.5584 - val_loss: 0.4232\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 127ms/step - loss: 0.4927 - val_loss: 0.4182\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.5014 - val_loss: 0.4895\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4891 - val_loss: 0.4107\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 0.4576 - val_loss: 0.4399\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 0.4547 - val_loss: 0.4014\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4394 - val_loss: 0.4048\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 0.4434 - val_loss: 0.4241\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.4426 - val_loss: 0.4074\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.4298 - val_loss: 0.4046\n",
      "[2022_04_21-14:01:04] Training the entire fine-tuned model...\n",
      "[2022_04_21-14:01:20] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4506WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0967s vs `on_train_batch_end` time: 0.1231s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0967s vs `on_train_batch_end` time: 0.1231s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 607ms/step - loss: 0.4518 - val_loss: 0.4238\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 255ms/step - loss: 0.4388 - val_loss: 0.3995\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 255ms/step - loss: 0.4419 - val_loss: 0.4047\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.4366 - val_loss: 0.4010\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 255ms/step - loss: 0.4412 - val_loss: 0.4193\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 255ms/step - loss: 0.4359 - val_loss: 0.4188\n",
      "[2022_04_21-14:01:38] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-14:01:38] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:01:38] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4320WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0967s vs `on_train_batch_end` time: 0.1229s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0967s vs `on_train_batch_end` time: 0.1229s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 601ms/step - loss: 0.4313 - val_loss: 0.3995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-14:02:11] Test set: Filtered out 0 of 383 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:181y9qn7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 40788... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▅▅▆▆▇▇█▁▂▂▃▃▄▁</td></tr><tr><td>loss</td><td>█▅▄▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▃▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_val_loss</td><td>0.39949</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.43126</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.39952</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">flowing-pine-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/181y9qn7\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/181y9qn7</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_140026-181y9qn7/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:181y9qn7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/3nw7ybmh\" target=\"_blank\">ruby-snowball-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-14:02:29] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:02:29] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:02:30] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 619ms/step - loss: 0.9923 - val_loss: 0.9124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.6624 - val_loss: 0.6437\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 0.6078 - val_loss: 0.6692\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5311 - val_loss: 0.4872\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4530 - val_loss: 0.5825\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4447 - val_loss: 0.4674\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4178 - val_loss: 0.5140\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3899 - val_loss: 0.4572\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3868 - val_loss: 0.4754\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3716 - val_loss: 0.4467\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3686 - val_loss: 0.4692\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 181ms/step - loss: 0.3603 - val_loss: 0.4410\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3589 - val_loss: 0.4514\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3511 - val_loss: 0.4410\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3523 - val_loss: 0.4493\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3434 - val_loss: 0.4508\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3450 - val_loss: 0.4369\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3396 - val_loss: 0.4420\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3406 - val_loss: 0.4401\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3442 - val_loss: 0.4413\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3393 - val_loss: 0.4387\n",
      "[2022_04_21-14:02:57] Training the entire fine-tuned model...\n",
      "[2022_04_21-14:03:06] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 762ms/step - loss: 0.3460 - val_loss: 0.4386\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 316ms/step - loss: 0.3501 - val_loss: 0.4353\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3387 - val_loss: 0.4457\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.3328 - val_loss: 0.4357\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.3323 - val_loss: 0.4330\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.3336 - val_loss: 0.4518\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.3328 - val_loss: 0.4253\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3115 - val_loss: 0.4513\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 316ms/step - loss: 0.2990 - val_loss: 0.4245\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.3050 - val_loss: 0.4752\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.2988 - val_loss: 0.4249\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.2876 - val_loss: 0.4968\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.2831 - val_loss: 0.4485\n",
      "[2022_04_21-14:03:34] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-14:03:34] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:03:34] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 763ms/step - loss: 0.3045 - val_loss: 0.4249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-14:04:08] Test set: Filtered out 0 of 393 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3nw7ybmh) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 41003... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▅▅▅▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▅▂▃▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_val_loss</td><td>0.42451</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30454</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.42489</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">ruby-snowball-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/3nw7ybmh\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/3nw7ybmh</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_140214-3nw7ybmh/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3nw7ybmh). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/n0803rll\" target=\"_blank\">different-sunset-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-14:04:27] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:04:27] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:04:27] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 519ms/step - loss: 0.9619 - val_loss: 0.8458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.6965 - val_loss: 0.4376\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.5859 - val_loss: 0.4879\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.6066 - val_loss: 0.6159\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.5420 - val_loss: 0.4261\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.5158 - val_loss: 0.4213\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.4658 - val_loss: 0.4130\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4710 - val_loss: 0.4277\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.4452 - val_loss: 0.4067\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4410 - val_loss: 0.4070\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4682 - val_loss: 0.4663\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.4366 - val_loss: 0.4013\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4579 - val_loss: 0.3931\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4410 - val_loss: 0.4500\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4402 - val_loss: 0.3854\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.4326 - val_loss: 0.3904\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4497 - val_loss: 0.4866\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4591 - val_loss: 0.4209\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.4217 - val_loss: 0.3838\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4167 - val_loss: 0.3782\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4022 - val_loss: 0.3859\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3922 - val_loss: 0.3779\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 0.4024 - val_loss: 0.3901\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3931 - val_loss: 0.3927\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4063 - val_loss: 0.3772\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4029 - val_loss: 0.3771\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3997 - val_loss: 0.3928\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4039 - val_loss: 0.3859\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4019 - val_loss: 0.3809\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3932 - val_loss: 0.3776\n",
      "[2022_04_21-14:05:01] Training the entire fine-tuned model...\n",
      "[2022_04_21-14:05:11] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3888WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0999s vs `on_train_batch_end` time: 0.1206s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0999s vs `on_train_batch_end` time: 0.1206s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 617ms/step - loss: 0.3921 - val_loss: 0.3926\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 280ms/step - loss: 0.4108 - val_loss: 0.3874\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.4344 - val_loss: 0.4095\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 280ms/step - loss: 0.4321 - val_loss: 0.3764\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 279ms/step - loss: 0.3709 - val_loss: 0.3717\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 279ms/step - loss: 0.3896 - val_loss: 0.3818\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3698 - val_loss: 0.3737\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 279ms/step - loss: 0.3659 - val_loss: 0.3891\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3497 - val_loss: 0.3802\n",
      "[2022_04_21-14:05:34] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-14:05:34] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:05:37] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3702WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0990s vs `on_train_batch_end` time: 0.1240s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0990s vs `on_train_batch_end` time: 0.1240s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 603ms/step - loss: 0.3661 - val_loss: 0.3716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-14:06:12] Test set: Filtered out 0 of 383 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:n0803rll) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 41278... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▂▂▂▁▁▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▃▅▂▂▂▂▂▂▂▁▁▂▁▁▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.37159</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36606</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.37159</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">different-sunset-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/n0803rll\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/n0803rll</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_140410-n0803rll/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:n0803rll). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/307dpzi9\" target=\"_blank\">upbeat-plasma-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-14:06:31] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:06:31] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:06:31] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 648ms/step - loss: 0.8523 - val_loss: 0.5980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.5281 - val_loss: 0.7472\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5064 - val_loss: 0.5235\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4589 - val_loss: 0.5893\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4375 - val_loss: 0.4722\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4139 - val_loss: 0.4763\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3953 - val_loss: 0.4794\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3806 - val_loss: 0.4515\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3888 - val_loss: 0.4568\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3687 - val_loss: 0.4626\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.3679 - val_loss: 0.4405\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3723 - val_loss: 0.5034\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3865 - val_loss: 0.4570\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3938 - val_loss: 0.4464\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3668 - val_loss: 0.4376\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3571 - val_loss: 0.4959\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3515 - val_loss: 0.4329\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3434 - val_loss: 0.4322\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3390 - val_loss: 0.4615\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3348 - val_loss: 0.4398\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3345 - val_loss: 0.4315\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3343 - val_loss: 0.4567\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3288 - val_loss: 0.4364\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3396 - val_loss: 0.4324\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3209 - val_loss: 0.4365\n",
      "[2022_04_21-14:07:01] Training the entire fine-tuned model...\n",
      "[2022_04_21-14:07:29] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 782ms/step - loss: 0.4670 - val_loss: 0.4826\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.4016 - val_loss: 0.4493\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3735 - val_loss: 0.4762\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3704 - val_loss: 0.4565\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3388 - val_loss: 0.4373\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 336ms/step - loss: 0.3223 - val_loss: 0.4754\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3168 - val_loss: 0.4349\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3204 - val_loss: 0.4912\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 337ms/step - loss: 0.3042 - val_loss: 0.4294\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.2852 - val_loss: 0.4555\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 317ms/step - loss: 0.2738 - val_loss: 0.4276\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.2605 - val_loss: 0.4489\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.2323 - val_loss: 0.4407\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.2230 - val_loss: 0.5145\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.1838 - val_loss: 0.4728\n",
      "[2022_04_21-14:08:01] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-14:08:01] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:08:01] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 745ms/step - loss: 0.2586 - val_loss: 0.4293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-14:08:36] Test set: Filtered out 0 of 393 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:307dpzi9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 41579... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▄▄▄▅▅▁</td></tr><tr><td>loss</td><td>█▄▄▄▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▅█▃▅▂▂▂▂▂▂▁▃▂▁▁▂▁▁▂▁▁▂▁▁▁▂▁▂▂▁▂▁▂▁▂▁▁▁▃▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>10</td></tr><tr><td>best_val_loss</td><td>0.42764</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.25856</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.42925</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">upbeat-plasma-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/307dpzi9\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/307dpzi9</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_140616-307dpzi9/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:307dpzi9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/3rykci2t\" target=\"_blank\">fanciful-firebrand-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-14:08:57] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:08:57] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:08:57] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 537ms/step - loss: 0.8826 - val_loss: 0.5924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.6992 - val_loss: 0.4750\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.5807 - val_loss: 0.4434\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.5197 - val_loss: 0.4932\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4861 - val_loss: 0.4178\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4762 - val_loss: 0.4232\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4674 - val_loss: 0.4354\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4635 - val_loss: 0.4254\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4469 - val_loss: 0.4085\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4454 - val_loss: 0.4124\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4448 - val_loss: 0.4064\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.4460 - val_loss: 0.3974\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.4446 - val_loss: 0.3933\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.4397 - val_loss: 0.4193\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4358 - val_loss: 0.3967\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4287 - val_loss: 0.3975\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.4305 - val_loss: 0.4004\n",
      "[2022_04_21-14:09:22] Training the entire fine-tuned model...\n",
      "[2022_04_21-14:09:31] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.5286WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1003s vs `on_train_batch_end` time: 0.1238s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1003s vs `on_train_batch_end` time: 0.1238s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 602ms/step - loss: 0.5409 - val_loss: 0.3962\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 255ms/step - loss: 0.4965 - val_loss: 0.4021\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.4616 - val_loss: 0.3952\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 259ms/step - loss: 0.4347 - val_loss: 0.3981\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.4403 - val_loss: 0.3920\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.4144 - val_loss: 0.4209\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.4198 - val_loss: 0.3859\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 255ms/step - loss: 0.4066 - val_loss: 0.3808\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3832 - val_loss: 0.3950\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3695 - val_loss: 0.4003\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 260ms/step - loss: 0.3651 - val_loss: 0.3885\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3409 - val_loss: 0.3795\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3209 - val_loss: 0.3722\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3113 - val_loss: 0.3724\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3117 - val_loss: 0.3705\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3004 - val_loss: 0.3694\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.2860 - val_loss: 0.3706\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.2786 - val_loss: 0.3780\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.2621 - val_loss: 0.3876\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 259ms/step - loss: 0.2552 - val_loss: 0.3864\n",
      "[2022_04_21-14:10:11] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-14:10:11] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:10:11] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.2914WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0961s vs `on_train_batch_end` time: 0.1257s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0961s vs `on_train_batch_end` time: 0.1257s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 622ms/step - loss: 0.2901 - val_loss: 0.3715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-14:10:45] Test set: Filtered out 0 of 383 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3rykci2t) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 41910... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██▁</td></tr><tr><td>loss</td><td>█▆▅▄▄▃▃▃▃▃▃▃▃▃▃▃▃▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▅▃▃▃▃▂▂▂▂▂▃▂▂▂▂▂▂▂▂▃▂▁▂▂▂▁▁▁▁▁▁▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>15</td></tr><tr><td>best_val_loss</td><td>0.36938</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.29009</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.37148</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fanciful-firebrand-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/3rykci2t\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/3rykci2t</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_140840-3rykci2t/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3rykci2t). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/1wx8cqoa\" target=\"_blank\">upbeat-valley-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-14:11:04] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:11:04] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:11:04] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 637ms/step - loss: 0.9399 - val_loss: 0.8146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 168ms/step - loss: 0.6365 - val_loss: 0.7288\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.6278 - val_loss: 0.6536\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4854 - val_loss: 0.4856\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.4941 - val_loss: 0.5654\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4522 - val_loss: 0.4690\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 185ms/step - loss: 0.4351 - val_loss: 0.5233\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3876 - val_loss: 0.4542\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3880 - val_loss: 0.4780\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3852 - val_loss: 0.4558\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3658 - val_loss: 0.4526\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3648 - val_loss: 0.4791\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3823 - val_loss: 0.4509\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3929 - val_loss: 0.5249\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3770 - val_loss: 0.4387\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3462 - val_loss: 0.4443\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 185ms/step - loss: 0.3359 - val_loss: 0.4439\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3369 - val_loss: 0.4352\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3377 - val_loss: 0.4679\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3343 - val_loss: 0.4337\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3240 - val_loss: 0.4512\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3277 - val_loss: 0.4380\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3340 - val_loss: 0.4314\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3195 - val_loss: 0.4338\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3119 - val_loss: 0.4460\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3205 - val_loss: 0.4544\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3249 - val_loss: 0.4448\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3112 - val_loss: 0.4326\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3132 - val_loss: 0.4341\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_21-14:11:39] Training the entire fine-tuned model...\n",
      "[2022_04_21-14:12:03] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 776ms/step - loss: 0.3155 - val_loss: 0.4390\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3332 - val_loss: 0.4416\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 318ms/step - loss: 0.3127 - val_loss: 0.4388\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 337ms/step - loss: 0.3182 - val_loss: 0.4375\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.3203 - val_loss: 0.4348\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3145 - val_loss: 0.4361\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.3117 - val_loss: 0.4386\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3169 - val_loss: 0.4393\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3043 - val_loss: 0.4380\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3109 - val_loss: 0.4370\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.3128 - val_loss: 0.4366\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_21-14:12:28] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-14:12:28] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:12:28] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 755ms/step - loss: 0.3154 - val_loss: 0.4348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-14:13:02] Test set: Filtered out 0 of 393 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1wx8cqoa) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 42207... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██▁▁▁▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▅▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▅▂▃▂▃▁▂▁▁▂▁▃▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>22</td></tr><tr><td>best_val_loss</td><td>0.4314</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31537</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43476</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">upbeat-valley-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/1wx8cqoa\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/1wx8cqoa</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_141047-1wx8cqoa/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1wx8cqoa). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/1h1us3wo\" target=\"_blank\">bright-oath-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-14:13:22] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:13:22] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:13:22] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 490ms/step - loss: 0.8908 - val_loss: 0.7728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.6344 - val_loss: 0.4621\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.6673 - val_loss: 0.5584\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.8030 - val_loss: 0.5196\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.6057 - val_loss: 0.4909\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.6169 - val_loss: 0.4244\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4864 - val_loss: 0.5611\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.5314 - val_loss: 0.4295\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4861 - val_loss: 0.4262\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.5037 - val_loss: 0.4162\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4625 - val_loss: 0.4175\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4631 - val_loss: 0.4271\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4616 - val_loss: 0.4327\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4602 - val_loss: 0.4324\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4617 - val_loss: 0.4285\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4555 - val_loss: 0.4256\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_21-14:13:45] Training the entire fine-tuned model...\n",
      "[2022_04_21-14:14:00] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4752WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0995s vs `on_train_batch_end` time: 0.1226s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0995s vs `on_train_batch_end` time: 0.1226s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 634ms/step - loss: 0.4740 - val_loss: 0.4148\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.4620 - val_loss: 0.4188\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 278ms/step - loss: 0.4641 - val_loss: 0.4265\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 279ms/step - loss: 0.4585 - val_loss: 0.4327\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.4539 - val_loss: 0.4332\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.4648 - val_loss: 0.4317\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.4659 - val_loss: 0.4312\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_21-14:14:19] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-14:14:19] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:14:19] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4638WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0990s vs `on_train_batch_end` time: 0.1215s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0990s vs `on_train_batch_end` time: 0.1215s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 633ms/step - loss: 0.4678 - val_loss: 0.4149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-14:14:53] Test set: Filtered out 0 of 383 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1h1us3wo) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 42527... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██▁▁▂▂▃▃▄▁</td></tr><tr><td>loss</td><td>█▄▄▇▃▄▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▄▃▂▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.41483</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.46783</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41488</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">bright-oath-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/1h1us3wo\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/1h1us3wo</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_141304-1h1us3wo/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1h1us3wo). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/14exk95w\" target=\"_blank\">desert-bee-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-14:15:13] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:15:13] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:15:13] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 638ms/step - loss: 0.8706 - val_loss: 0.7058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5323 - val_loss: 0.6600\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.5285 - val_loss: 0.4942\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4746 - val_loss: 0.6427\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.4761 - val_loss: 0.4759\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.4307 - val_loss: 0.5671\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.4306 - val_loss: 0.4700\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.4094 - val_loss: 0.5516\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.4004 - val_loss: 0.4741\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3912 - val_loss: 0.5729\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4025 - val_loss: 0.4600\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3796 - val_loss: 0.4452\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3748 - val_loss: 0.4625\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3753 - val_loss: 0.4655\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3601 - val_loss: 0.4390\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3600 - val_loss: 0.4496\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.3562 - val_loss: 0.4640\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3547 - val_loss: 0.4384\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3586 - val_loss: 0.4509\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3488 - val_loss: 0.4418\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3524 - val_loss: 0.4420\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3487 - val_loss: 0.4445\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.3495 - val_loss: 0.4440\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3525 - val_loss: 0.4440\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-14:15:44] Training the entire fine-tuned model...\n",
      "[2022_04_21-14:16:12] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 742ms/step - loss: 0.3814 - val_loss: 0.4448\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3572 - val_loss: 0.4365\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3580 - val_loss: 0.4617\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3428 - val_loss: 0.4415\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.3434 - val_loss: 0.4336\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3342 - val_loss: 0.4494\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3358 - val_loss: 0.4342\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3338 - val_loss: 0.4345\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3214 - val_loss: 0.4349\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3177 - val_loss: 0.4335\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 317ms/step - loss: 0.3164 - val_loss: 0.4316\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 338ms/step - loss: 0.3147 - val_loss: 0.4335\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.3139 - val_loss: 0.4318\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3116 - val_loss: 0.4352\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3131 - val_loss: 0.4352\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.3052 - val_loss: 0.4348\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.3124 - val_loss: 0.4330\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "[2022_04_21-14:16:47] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-14:16:47] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:16:47] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 745ms/step - loss: 0.3142 - val_loss: 0.4313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-14:17:22] Test set: Filtered out 0 of 393 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:14exk95w) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 42774... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇██▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▃▆▂▄▂▄▂▅▂▁▂▂▁▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.4313</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31423</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.4313</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">desert-bee-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/14exk95w\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/14exk95w</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_141456-14exk95w/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:14exk95w). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/9h1hfnzp\" target=\"_blank\">splendid-planet-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-14:17:41] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:17:41] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:17:41] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 524ms/step - loss: 0.8865 - val_loss: 0.5495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.6293 - val_loss: 0.6761\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.6848 - val_loss: 0.5072\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.5648 - val_loss: 0.5361\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4948 - val_loss: 0.4226\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4756 - val_loss: 0.4265\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.5096 - val_loss: 0.4662\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.4845 - val_loss: 0.4413\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 0.5140 - val_loss: 0.4029\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4541 - val_loss: 0.4261\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4532 - val_loss: 0.3990\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4463 - val_loss: 0.4010\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4393 - val_loss: 0.4080\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.4413 - val_loss: 0.3976\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4443 - val_loss: 0.3965\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4422 - val_loss: 0.4097\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4384 - val_loss: 0.4005\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.4389 - val_loss: 0.4033\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4366 - val_loss: 0.4022\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4347 - val_loss: 0.3953\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4341 - val_loss: 0.3963\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4341 - val_loss: 0.4066\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4366 - val_loss: 0.4012\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4379 - val_loss: 0.3998\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4365 - val_loss: 0.3981\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4242 - val_loss: 0.3988\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_21-14:18:13] Training the entire fine-tuned model...\n",
      "[2022_04_21-14:18:39] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4239WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0959s vs `on_train_batch_end` time: 0.1232s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0959s vs `on_train_batch_end` time: 0.1232s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 599ms/step - loss: 0.4249 - val_loss: 0.4007\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.4290 - val_loss: 0.3917\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.4307 - val_loss: 0.3994\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 255ms/step - loss: 0.4270 - val_loss: 0.3924\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.4225 - val_loss: 0.3924\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.4210 - val_loss: 0.3954\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.4210 - val_loss: 0.4003\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 254ms/step - loss: 0.4132 - val_loss: 0.4056\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_21-14:19:00] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-14:19:00] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:19:00] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4391WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0964s vs `on_train_batch_end` time: 0.1253s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0964s vs `on_train_batch_end` time: 0.1253s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 611ms/step - loss: 0.4367 - val_loss: 0.3917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-14:19:35] Test set: Filtered out 0 of 383 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:9h1hfnzp) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 43085... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▄▅▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▅█▄▅▂▂▃▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.39169</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.43675</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.39169</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">splendid-planet-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/9h1hfnzp\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/9h1hfnzp</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_141725-9h1hfnzp/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:9h1hfnzp). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/2go6j9e9\" target=\"_blank\">leafy-shape-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-14:19:55] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:19:55] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:19:55] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 654ms/step - loss: 0.9555 - val_loss: 0.7137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5984 - val_loss: 0.7297\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.6967 - val_loss: 0.7391\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.5573 - val_loss: 0.4838\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4721 - val_loss: 0.6196\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4499 - val_loss: 0.4712\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3896 - val_loss: 0.5526\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 167ms/step - loss: 0.4019 - val_loss: 0.4616\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3845 - val_loss: 0.5109\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 186ms/step - loss: 0.3899 - val_loss: 0.4483\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 171ms/step - loss: 0.3686 - val_loss: 0.4622\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3543 - val_loss: 0.4408\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3620 - val_loss: 0.4515\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3503 - val_loss: 0.4578\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3525 - val_loss: 0.4345\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3480 - val_loss: 0.4475\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3415 - val_loss: 0.4383\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3446 - val_loss: 0.4302\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3421 - val_loss: 0.4641\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3335 - val_loss: 0.4289\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3289 - val_loss: 0.4732\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3349 - val_loss: 0.4273\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3208 - val_loss: 0.4336\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3238 - val_loss: 0.4512\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3169 - val_loss: 0.4299\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 167ms/step - loss: 0.3136 - val_loss: 0.4337\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3065 - val_loss: 0.4339\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3228 - val_loss: 0.4342\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_21-14:20:28] Training the entire fine-tuned model...\n",
      "[2022_04_21-14:20:37] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 749ms/step - loss: 0.3246 - val_loss: 0.4283\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3337 - val_loss: 0.4514\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.3098 - val_loss: 0.4278\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.3214 - val_loss: 0.4452\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.3055 - val_loss: 0.4249\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 337ms/step - loss: 0.2959 - val_loss: 0.4692\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3085 - val_loss: 0.4246\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.2902 - val_loss: 0.4409\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 336ms/step - loss: 0.2745 - val_loss: 0.4219\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.2651 - val_loss: 0.4537\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.2546 - val_loss: 0.4332\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 338ms/step - loss: 0.2678 - val_loss: 0.5125\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.2563 - val_loss: 0.4414\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.2294 - val_loss: 0.4342\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 338ms/step - loss: 0.2435 - val_loss: 0.4532\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_21-14:21:08] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-14:21:08] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:21:09] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 784ms/step - loss: 0.2693 - val_loss: 0.4312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-14:21:57] Test set: Filtered out 0 of 393 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2go6j9e9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 43360... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▃▃▃▃▄▄▄▄▁</td></tr><tr><td>loss</td><td>█▅▆▄▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇██▂▅▂▄▂▃▂▁▂▂▁▂▁▁▂▁▂▁▂▁▁▁▁▁▂▁▂▂▁▁▁▂▁▃▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_val_loss</td><td>0.42192</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.26928</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.43117</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">leafy-shape-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/2go6j9e9\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/2go6j9e9</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_141938-2go6j9e9/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2go6j9e9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/2wqszq4c\" target=\"_blank\">ruby-meadow-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-14:22:16] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:22:16] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:22:16] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 515ms/step - loss: 0.8627 - val_loss: 0.5999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.6547 - val_loss: 0.4717\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.5826 - val_loss: 0.4410\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.5146 - val_loss: 0.4639\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4806 - val_loss: 0.5052\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.5423 - val_loss: 0.4401\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.5159 - val_loss: 0.4311\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4523 - val_loss: 0.4236\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4584 - val_loss: 0.4161\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4639 - val_loss: 0.4503\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.4633 - val_loss: 0.4210\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4481 - val_loss: 0.3985\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4306 - val_loss: 0.4504\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4384 - val_loss: 0.3966\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4287 - val_loss: 0.3912\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4403 - val_loss: 0.4147\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.4283 - val_loss: 0.4002\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4164 - val_loss: 0.4111\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4478 - val_loss: 0.3775\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4120 - val_loss: 0.3996\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4012 - val_loss: 0.3746\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 0.3995 - val_loss: 0.3777\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.4016 - val_loss: 0.3800\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4102 - val_loss: 0.3749\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3934 - val_loss: 0.3757\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3983 - val_loss: 0.3815\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3963 - val_loss: 0.3833\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-14:22:50] Training the entire fine-tuned model...\n",
      "[2022_04_21-14:23:15] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4040WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0976s vs `on_train_batch_end` time: 0.1247s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0976s vs `on_train_batch_end` time: 0.1247s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 611ms/step - loss: 0.4052 - val_loss: 0.3735\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3979 - val_loss: 0.3704\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 255ms/step - loss: 0.3954 - val_loss: 0.3772\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3997 - val_loss: 0.3698\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3826 - val_loss: 0.3813\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 280ms/step - loss: 0.3735 - val_loss: 0.3679\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 279ms/step - loss: 0.3884 - val_loss: 0.3738\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3808 - val_loss: 0.3655\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3632 - val_loss: 0.3679\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 280ms/step - loss: 0.3565 - val_loss: 0.3744\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3509 - val_loss: 0.3792\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3650 - val_loss: 0.3753\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3450 - val_loss: 0.3830\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3536 - val_loss: 0.3768\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_21-14:23:46] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-14:23:46] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:23:46] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3558WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0956s vs `on_train_batch_end` time: 0.1259s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0956s vs `on_train_batch_end` time: 0.1259s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 623ms/step - loss: 0.3563 - val_loss: 0.3661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-14:24:37] Test set: Filtered out 0 of 383 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2wqszq4c) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 43688... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▃▄▄▄▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▄▃▂▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▄▅▃▃▃▃▄▃▂▄▂▂▂▂▂▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.36553</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35628</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.36614</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">ruby-meadow-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/2wqszq4c\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/2wqszq4c</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_142200-2wqszq4c/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2wqszq4c). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/2exgvo6v\" target=\"_blank\">visionary-universe-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-14:24:56] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:24:56] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:24:56] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 644ms/step - loss: 0.8702 - val_loss: 0.5780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.6241 - val_loss: 0.9132\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.6930 - val_loss: 0.5627\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5206 - val_loss: 0.5363\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.4802 - val_loss: 0.5309\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 168ms/step - loss: 0.4263 - val_loss: 0.4670\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4224 - val_loss: 0.4865\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.4073 - val_loss: 0.4720\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3979 - val_loss: 0.4589\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3714 - val_loss: 0.4671\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 167ms/step - loss: 0.3801 - val_loss: 0.4458\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3690 - val_loss: 0.4533\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3603 - val_loss: 0.4427\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3509 - val_loss: 0.4397\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3519 - val_loss: 0.4489\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3482 - val_loss: 0.4636\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3496 - val_loss: 0.4343\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3369 - val_loss: 0.4500\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3383 - val_loss: 0.4460\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3245 - val_loss: 0.4438\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3309 - val_loss: 0.4335\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3209 - val_loss: 0.4392\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 167ms/step - loss: 0.3208 - val_loss: 0.4416\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3280 - val_loss: 0.4364\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3338 - val_loss: 0.4341\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3254 - val_loss: 0.4342\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3196 - val_loss: 0.4361\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-14:25:29] Training the entire fine-tuned model...\n",
      "[2022_04_21-14:25:55] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 755ms/step - loss: 0.4223 - val_loss: 0.4766\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 337ms/step - loss: 0.3682 - val_loss: 0.4467\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3462 - val_loss: 0.4795\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3219 - val_loss: 0.4437\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3206 - val_loss: 0.4487\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.3084 - val_loss: 0.4360\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.2961 - val_loss: 0.4374\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.3064 - val_loss: 0.4362\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.2750 - val_loss: 0.4589\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.2550 - val_loss: 0.4373\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.2560 - val_loss: 0.4524\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.2310 - val_loss: 0.4523\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_21-14:26:23] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-14:26:23] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:26:23] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 757ms/step - loss: 0.2995 - val_loss: 0.4356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-14:26:57] Test set: Filtered out 0 of 393 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2exgvo6v) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 44008... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▅▆▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▂▂▂▂▂▂▁▁▁▁▂</td></tr><tr><td>lr</td><td>████████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃█▃▃▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>20</td></tr><tr><td>best_val_loss</td><td>0.43347</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.29945</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.43563</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">visionary-universe-16</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/2exgvo6v\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/2exgvo6v</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_142439-2exgvo6v/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2exgvo6v). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/21syst6x\" target=\"_blank\">jumping-firefly-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-14:27:16] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:27:16] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:27:16] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 534ms/step - loss: 0.9177 - val_loss: 0.6749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.6245 - val_loss: 0.4709\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.5263 - val_loss: 0.4393\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.5034 - val_loss: 0.4246\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4796 - val_loss: 0.4292\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4642 - val_loss: 0.4087\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4783 - val_loss: 0.4976\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4530 - val_loss: 0.4070\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4723 - val_loss: 0.3992\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4580 - val_loss: 0.3919\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4351 - val_loss: 0.4046\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4438 - val_loss: 0.3915\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4459 - val_loss: 0.4789\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4390 - val_loss: 0.3851\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4093 - val_loss: 0.5961\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.5077 - val_loss: 0.4729\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.5150 - val_loss: 0.3789\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4083 - val_loss: 0.4131\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4101 - val_loss: 0.3744\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4039 - val_loss: 0.3710\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4002 - val_loss: 0.4673\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4335 - val_loss: 0.3679\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3934 - val_loss: 0.3766\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3965 - val_loss: 0.4026\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.3699 - val_loss: 0.3714\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3951 - val_loss: 0.3790\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3680 - val_loss: 0.3774\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3726 - val_loss: 0.3682\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_21-14:27:51] Training the entire fine-tuned model...\n",
      "[2022_04_21-14:27:59] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.6064WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0984s vs `on_train_batch_end` time: 0.1218s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0984s vs `on_train_batch_end` time: 0.1218s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 637ms/step - loss: 0.6005 - val_loss: 0.3982\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.4457 - val_loss: 0.3697\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 255ms/step - loss: 0.4558 - val_loss: 0.3718\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 255ms/step - loss: 0.4123 - val_loss: 0.4279\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.4249 - val_loss: 0.3909\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 280ms/step - loss: 0.3961 - val_loss: 0.3709\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3842 - val_loss: 0.3982\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3846 - val_loss: 0.3736\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_21-14:28:20] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-14:28:20] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:28:20] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4061WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0969s vs `on_train_batch_end` time: 0.1239s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0969s vs `on_train_batch_end` time: 0.1239s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 622ms/step - loss: 0.4031 - val_loss: 0.3694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-14:28:55] Test set: Filtered out 0 of 383 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:21syst6x) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 44309... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▂▃▃▂▂▁▁▂▁▁▁▁▁▁▄▂▂▂▂▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████████████▃▃▃▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▂▂▂▄▂▂▂▂▂▄▁▆▃▁▂▁▁▃▁▁▂▁▁▁▁▂▁▁▂▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>21</td></tr><tr><td>best_val_loss</td><td>0.36794</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.40315</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.36941</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">jumping-firefly-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/21syst6x\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/21syst6x</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_142700-21syst6x/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:21syst6x). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/2hrqc6jt\" target=\"_blank\">bumbling-snowflake-17</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-14:29:13] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:29:14] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:29:14] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 652ms/step - loss: 0.7282 - val_loss: 0.5877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4740 - val_loss: 0.7250\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4902 - val_loss: 0.4934\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4224 - val_loss: 0.5081\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4095 - val_loss: 0.5128\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3963 - val_loss: 0.4659\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.3823 - val_loss: 0.4701\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3752 - val_loss: 0.4544\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3658 - val_loss: 0.4541\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3542 - val_loss: 0.4649\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3596 - val_loss: 0.4662\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3497 - val_loss: 0.4483\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3515 - val_loss: 0.4375\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3513 - val_loss: 0.4410\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3404 - val_loss: 0.4439\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3453 - val_loss: 0.4822\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 187ms/step - loss: 0.3472 - val_loss: 0.4316\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3256 - val_loss: 0.4415\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 167ms/step - loss: 0.3301 - val_loss: 0.4525\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.3303 - val_loss: 0.4308\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3375 - val_loss: 0.4295\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3212 - val_loss: 0.4920\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3267 - val_loss: 0.4334\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3175 - val_loss: 0.4350\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3143 - val_loss: 0.4485\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3091 - val_loss: 0.4315\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3083 - val_loss: 0.4367\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3048 - val_loss: 0.4488\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3108 - val_loss: 0.4265\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 186ms/step - loss: 0.3046 - val_loss: 0.4488\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3057 - val_loss: 0.4365\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.2960 - val_loss: 0.4328\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3075 - val_loss: 0.4307\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3025 - val_loss: 0.4374\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.2961 - val_loss: 0.4404\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 187ms/step - loss: 0.2983 - val_loss: 0.4382\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.2941 - val_loss: 0.4326\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-14:29:55] Training the entire fine-tuned model...\n",
      "[2022_04_21-14:30:03] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 774ms/step - loss: 0.3136 - val_loss: 0.4363\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3037 - val_loss: 0.4414\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.3009 - val_loss: 0.4378\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3002 - val_loss: 0.4375\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.3027 - val_loss: 0.4323\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.2991 - val_loss: 0.4311\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.3147 - val_loss: 0.4347\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.2920 - val_loss: 0.4362\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 316ms/step - loss: 0.2986 - val_loss: 0.4339\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 339ms/step - loss: 0.3066 - val_loss: 0.4343\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.2846 - val_loss: 0.4349\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.3005 - val_loss: 0.4336\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 342ms/step - loss: 0.2875 - val_loss: 0.4337\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.2968 - val_loss: 0.4345\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_21-14:30:34] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-14:30:34] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:30:50] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 763ms/step - loss: 0.2952 - val_loss: 0.4314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-14:31:25] Test set: Filtered out 0 of 393 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2hrqc6jt) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 44597... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇███▁▁▂▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▅█▃▃▂▂▂▂▂▂▁▁▂▁▂▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>28</td></tr><tr><td>best_val_loss</td><td>0.42647</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.29515</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43143</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">bumbling-snowflake-17</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/2hrqc6jt\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/2hrqc6jt</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_142858-2hrqc6jt/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2hrqc6jt). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/y1dq0u9x\" target=\"_blank\">young-field-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-14:31:43] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:31:43] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:31:43] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 521ms/step - loss: 0.9631 - val_loss: 1.0577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.7761 - val_loss: 0.4791\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.5910 - val_loss: 0.4284\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.5499 - val_loss: 0.4353\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4752 - val_loss: 0.4130\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4594 - val_loss: 0.4054\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4658 - val_loss: 0.4136\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.4547 - val_loss: 0.4000\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4410 - val_loss: 0.4036\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4395 - val_loss: 0.3900\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4472 - val_loss: 0.6165\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.5176 - val_loss: 0.4169\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4416 - val_loss: 0.3936\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4454 - val_loss: 0.6312\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.5429 - val_loss: 0.3904\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4322 - val_loss: 0.3886\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4297 - val_loss: 0.4272\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4331 - val_loss: 0.3867\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4176 - val_loss: 0.3853\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.4075 - val_loss: 0.3937\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4074 - val_loss: 0.3820\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4059 - val_loss: 0.3898\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4032 - val_loss: 0.3828\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4058 - val_loss: 0.3807\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4079 - val_loss: 0.3850\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4066 - val_loss: 0.3848\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4029 - val_loss: 0.3784\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.4120 - val_loss: 0.3819\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4000 - val_loss: 0.3823\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3994 - val_loss: 0.3779\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4012 - val_loss: 0.3747\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3864 - val_loss: 0.3841\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3944 - val_loss: 0.3772\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4017 - val_loss: 0.3774\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3993 - val_loss: 0.3798\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.3970 - val_loss: 0.3756\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3841 - val_loss: 0.3757\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3826 - val_loss: 0.3804\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3866 - val_loss: 0.3786\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-14:32:27] Training the entire fine-tuned model...\n",
      "[2022_04_21-14:32:35] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4043WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1008s vs `on_train_batch_end` time: 0.1215s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1008s vs `on_train_batch_end` time: 0.1215s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 639ms/step - loss: 0.4070 - val_loss: 0.3798\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3938 - val_loss: 0.3859\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3902 - val_loss: 0.3810\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3937 - val_loss: 0.3777\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3987 - val_loss: 0.3756\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 255ms/step - loss: 0.3993 - val_loss: 0.3745\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3995 - val_loss: 0.3756\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3879 - val_loss: 0.3743\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 260ms/step - loss: 0.3869 - val_loss: 0.3751\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3922 - val_loss: 0.3780\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3846 - val_loss: 0.3843\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3953 - val_loss: 0.3872\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3814 - val_loss: 0.3851\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3823 - val_loss: 0.3807\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3836 - val_loss: 0.3776\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3848 - val_loss: 0.3759\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_21-14:33:09] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-14:33:09] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:33:10] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3936WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0970s vs `on_train_batch_end` time: 0.1241s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0970s vs `on_train_batch_end` time: 0.1241s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 633ms/step - loss: 0.3956 - val_loss: 0.3742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-14:33:44] Test set: Filtered out 0 of 383 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:y1dq0u9x) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 44966... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▂▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▆▄▂▂▂▂▂▃▂▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.37417</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.39565</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.37417</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">young-field-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/y1dq0u9x\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/y1dq0u9x</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_143128-y1dq0u9x/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:y1dq0u9x). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/3phdpk68\" target=\"_blank\">dutiful-meadow-18</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-14:34:03] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:34:03] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:34:03] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 631ms/step - loss: 0.9258 - val_loss: 0.6897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.5765 - val_loss: 0.7696\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.5838 - val_loss: 0.5061\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4694 - val_loss: 0.5798\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4918 - val_loss: 0.4777\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4312 - val_loss: 0.5434\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4250 - val_loss: 0.4619\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4002 - val_loss: 0.4870\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3808 - val_loss: 0.4507\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3737 - val_loss: 0.4717\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3624 - val_loss: 0.4487\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3601 - val_loss: 0.4481\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3593 - val_loss: 0.4526\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3490 - val_loss: 0.4521\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3464 - val_loss: 0.4370\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.3434 - val_loss: 0.4590\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3391 - val_loss: 0.4383\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3320 - val_loss: 0.4461\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3325 - val_loss: 0.4384\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3298 - val_loss: 0.4490\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3271 - val_loss: 0.4375\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3258 - val_loss: 0.4372\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3196 - val_loss: 0.4390\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_21-14:34:32] Training the entire fine-tuned model...\n",
      "[2022_04_21-14:34:40] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 757ms/step - loss: 0.3356 - val_loss: 0.4574\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3416 - val_loss: 0.4348\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3424 - val_loss: 0.4457\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 337ms/step - loss: 0.3314 - val_loss: 0.4376\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 336ms/step - loss: 0.3318 - val_loss: 0.4377\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.3179 - val_loss: 0.4491\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.3216 - val_loss: 0.4414\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3158 - val_loss: 0.4363\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3220 - val_loss: 0.4347\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.3182 - val_loss: 0.4388\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3176 - val_loss: 0.4398\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.3168 - val_loss: 0.4383\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 316ms/step - loss: 0.3168 - val_loss: 0.4349\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3121 - val_loss: 0.4348\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3157 - val_loss: 0.4350\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.3169 - val_loss: 0.4361\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3173 - val_loss: 0.4359\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "[2022_04_21-14:35:15] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-14:35:15] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:35:16] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 788ms/step - loss: 0.3218 - val_loss: 0.4363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-14:35:51] Test set: Filtered out 0 of 393 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3phdpk68) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 45418... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▆█▂▄▂▃▂▂▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_val_loss</td><td>0.43473</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32184</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43631</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dutiful-meadow-18</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/3phdpk68\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/3phdpk68</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_143347-3phdpk68/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3phdpk68). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/19tclus4\" target=\"_blank\">likely-firebrand-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-14:36:11] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:36:11] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:36:11] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 533ms/step - loss: 0.9136 - val_loss: 1.5337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.8972 - val_loss: 0.6341\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.6325 - val_loss: 0.6368\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.6089 - val_loss: 0.4241\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.5135 - val_loss: 0.4179\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.5111 - val_loss: 0.4590\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4746 - val_loss: 0.4097\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4525 - val_loss: 0.4340\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4498 - val_loss: 0.3971\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.4380 - val_loss: 0.4386\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4485 - val_loss: 0.4298\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4652 - val_loss: 0.4067\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4504 - val_loss: 0.3989\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 0.4168 - val_loss: 0.3895\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.4191 - val_loss: 0.3950\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4095 - val_loss: 0.3931\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4131 - val_loss: 0.3859\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4150 - val_loss: 0.3840\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.4067 - val_loss: 0.3935\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.4116 - val_loss: 0.3815\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4129 - val_loss: 0.3846\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4069 - val_loss: 0.3856\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4160 - val_loss: 0.3816\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4147 - val_loss: 0.3815\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.3985 - val_loss: 0.3817\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4041 - val_loss: 0.3817\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4141 - val_loss: 0.3797\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4090 - val_loss: 0.3864\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4007 - val_loss: 0.3886\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.4040 - val_loss: 0.3847\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4088 - val_loss: 0.3773\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4134 - val_loss: 0.3773\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4143 - val_loss: 0.3840\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.3992 - val_loss: 0.3927\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4028 - val_loss: 0.3807\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4009 - val_loss: 0.3787\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4068 - val_loss: 0.3782\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3920 - val_loss: 0.3784\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 0.3969 - val_loss: 0.3782\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3950 - val_loss: 0.3783\n",
      "[2022_04_21-14:36:55] Training the entire fine-tuned model...\n",
      "[2022_04_21-14:37:09] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4094WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0976s vs `on_train_batch_end` time: 0.1256s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0976s vs `on_train_batch_end` time: 0.1256s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 616ms/step - loss: 0.4077 - val_loss: 0.3942\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.4031 - val_loss: 0.3773\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 280ms/step - loss: 0.4020 - val_loss: 0.3921\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 280ms/step - loss: 0.3920 - val_loss: 0.3767\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 285ms/step - loss: 0.3893 - val_loss: 0.3796\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3961 - val_loss: 0.3794\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 280ms/step - loss: 0.4024 - val_loss: 0.3826\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3923 - val_loss: 0.3716\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 255ms/step - loss: 0.3767 - val_loss: 0.3897\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 280ms/step - loss: 0.3847 - val_loss: 0.3728\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3810 - val_loss: 0.3700\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 279ms/step - loss: 0.3798 - val_loss: 0.3951\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3833 - val_loss: 0.3672\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3628 - val_loss: 0.3721\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 259ms/step - loss: 0.3534 - val_loss: 0.3720\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3565 - val_loss: 0.3703\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3545 - val_loss: 0.3813\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3472 - val_loss: 0.3846\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3499 - val_loss: 0.3733\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3412 - val_loss: 0.3749\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3472 - val_loss: 0.3707\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_21-14:37:52] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-14:37:52] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:37:57] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3677WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0993s vs `on_train_batch_end` time: 0.1212s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0993s vs `on_train_batch_end` time: 0.1212s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 627ms/step - loss: 0.3642 - val_loss: 0.3670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-14:38:30] Test set: Filtered out 0 of 383 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:19tclus4) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 45722... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▄▄▄▄▁</td></tr><tr><td>loss</td><td>██▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.36702</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36415</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.36702</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">likely-firebrand-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/19tclus4\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/19tclus4</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_143553-19tclus4/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:19tclus4). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/1vmpa0uo\" target=\"_blank\">dry-salad-19</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-14:38:51] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:38:51] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:38:51] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 671ms/step - loss: 0.8620 - val_loss: 0.6247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.5598 - val_loss: 0.8392\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.6428 - val_loss: 0.4975\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4951 - val_loss: 0.6126\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4671 - val_loss: 0.4733\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4457 - val_loss: 0.5877\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4350 - val_loss: 0.4633\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.4006 - val_loss: 0.5489\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 167ms/step - loss: 0.4046 - val_loss: 0.4547\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3813 - val_loss: 0.5270\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3784 - val_loss: 0.4515\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3690 - val_loss: 0.4763\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3576 - val_loss: 0.4450\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3534 - val_loss: 0.4386\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3473 - val_loss: 0.4458\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3418 - val_loss: 0.4506\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3318 - val_loss: 0.4527\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3394 - val_loss: 0.4311\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3510 - val_loss: 0.5511\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3905 - val_loss: 0.4388\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3569 - val_loss: 0.4622\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3307 - val_loss: 0.4502\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3243 - val_loss: 0.4488\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3198 - val_loss: 0.4336\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3220 - val_loss: 0.4384\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3128 - val_loss: 0.4441\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_21-14:39:23] Training the entire fine-tuned model...\n",
      "[2022_04_21-14:39:31] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 760ms/step - loss: 0.3435 - val_loss: 0.4316\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.3482 - val_loss: 0.4534\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3257 - val_loss: 0.4292\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3202 - val_loss: 0.4376\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 316ms/step - loss: 0.3192 - val_loss: 0.4355\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.3134 - val_loss: 0.4320\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 338ms/step - loss: 0.3017 - val_loss: 0.4294\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.2985 - val_loss: 0.4277\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.2975 - val_loss: 0.4332\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.2964 - val_loss: 0.4351\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.2906 - val_loss: 0.4277\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.2859 - val_loss: 0.4249\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.2888 - val_loss: 0.4428\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.2917 - val_loss: 0.4322\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.2860 - val_loss: 0.4255\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.2796 - val_loss: 0.4415\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.2756 - val_loss: 0.4370\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 317ms/step - loss: 0.2741 - val_loss: 0.4333\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.2713 - val_loss: 0.4306\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.2765 - val_loss: 0.4326\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "[2022_04_21-14:40:11] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-14:40:11] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:40:11] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 976ms/step - loss: 0.2908 - val_loss: 0.4292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-14:40:46] Test set: Filtered out 0 of 393 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1vmpa0uo) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 46143... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▆▆▇▇▇▇██▁▂▂▂▂▃▃▄▄▄▄▅▅▅▆▆▁</td></tr><tr><td>loss</td><td>█▄▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄█▂▄▂▄▃▂▃▁▂▁▁▁▁▁▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>11</td></tr><tr><td>best_val_loss</td><td>0.42487</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.29077</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.4292</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dry-salad-19</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/1vmpa0uo\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/1vmpa0uo</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_143833-1vmpa0uo/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1vmpa0uo). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/1yg7xjzh\" target=\"_blank\">exalted-armadillo-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-14:41:04] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:41:04] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:41:04] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 527ms/step - loss: 0.9282 - val_loss: 0.5181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.6420 - val_loss: 0.4462\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.5838 - val_loss: 0.4726\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.5910 - val_loss: 0.5184\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.5117 - val_loss: 0.4166\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4936 - val_loss: 0.4152\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4911 - val_loss: 0.5406\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.4908 - val_loss: 0.4025\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4619 - val_loss: 0.3959\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4480 - val_loss: 0.4965\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4627 - val_loss: 0.4529\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4886 - val_loss: 0.5555\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4879 - val_loss: 0.3852\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.4632 - val_loss: 0.3890\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.4199 - val_loss: 0.3912\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.4271 - val_loss: 0.3946\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4173 - val_loss: 0.4048\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4058 - val_loss: 0.3846\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4075 - val_loss: 0.3831\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4055 - val_loss: 0.3917\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4107 - val_loss: 0.3821\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3984 - val_loss: 0.3886\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4007 - val_loss: 0.3873\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4083 - val_loss: 0.3774\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4109 - val_loss: 0.3922\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4008 - val_loss: 0.3738\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4055 - val_loss: 0.3902\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4140 - val_loss: 0.3766\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4007 - val_loss: 0.3726\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.4023 - val_loss: 0.3785\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3847 - val_loss: 0.3725\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3963 - val_loss: 0.3884\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3970 - val_loss: 0.3697\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.3952 - val_loss: 0.3797\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3941 - val_loss: 0.3751\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3978 - val_loss: 0.3739\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3900 - val_loss: 0.3703\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3907 - val_loss: 0.3779\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3913 - val_loss: 0.3740\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3888 - val_loss: 0.3683\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3808 - val_loss: 0.3672\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3843 - val_loss: 0.3721\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3925 - val_loss: 0.3703\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3895 - val_loss: 0.3673\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3918 - val_loss: 0.3668\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3946 - val_loss: 0.3650\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3873 - val_loss: 0.3651\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3951 - val_loss: 0.3674\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.3939 - val_loss: 0.3675\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.3829 - val_loss: 0.3694\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3808 - val_loss: 0.3700\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3799 - val_loss: 0.3710\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 0.3855 - val_loss: 0.3712\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.3836 - val_loss: 0.3694\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_21-14:41:58] Training the entire fine-tuned model...\n",
      "[2022_04_21-14:42:07] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3926WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0981s vs `on_train_batch_end` time: 0.1228s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0981s vs `on_train_batch_end` time: 0.1228s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 638ms/step - loss: 0.3891 - val_loss: 0.3649\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.4499 - val_loss: 0.3666\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.4288 - val_loss: 0.3720\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3848 - val_loss: 0.3640\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3874 - val_loss: 0.3719\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3743 - val_loss: 0.3587\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 259ms/step - loss: 0.3748 - val_loss: 0.3592\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3562 - val_loss: 0.3621\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3500 - val_loss: 0.3628\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3463 - val_loss: 0.3748\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3315 - val_loss: 0.3770\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3452 - val_loss: 0.3672\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3487 - val_loss: 0.3659\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3404 - val_loss: 0.3699\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_21-14:42:38] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-14:42:38] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:42:54] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3598WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0962s vs `on_train_batch_end` time: 0.1239s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0962s vs `on_train_batch_end` time: 0.1239s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 621ms/step - loss: 0.3574 - val_loss: 0.3594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-14:43:28] Test set: Filtered out 0 of 383 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1yg7xjzh) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 46484... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇██▁▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇▄▇▃█▂▅▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.35874</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35739</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.35936</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">exalted-armadillo-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/1yg7xjzh\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/1yg7xjzh</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_144048-1yg7xjzh/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1yg7xjzh). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204a/runs/3kyumj08\" target=\"_blank\">worthy-sun-20</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-14:43:47] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:43:47] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:43:47] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 662ms/step - loss: 0.9087 - val_loss: 0.5797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.6196 - val_loss: 0.9290\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.6812 - val_loss: 0.5688\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.5487 - val_loss: 0.5555\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.5224 - val_loss: 0.6135\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.4476 - val_loss: 0.4719\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4225 - val_loss: 0.5395\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3955 - val_loss: 0.4575\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3679 - val_loss: 0.5143\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3776 - val_loss: 0.4491\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3720 - val_loss: 0.5087\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3923 - val_loss: 0.4435\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3717 - val_loss: 0.4660\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3739 - val_loss: 0.4728\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.3665 - val_loss: 0.4371\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3513 - val_loss: 0.4851\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3488 - val_loss: 0.4345\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3305 - val_loss: 0.4782\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3422 - val_loss: 0.4381\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 167ms/step - loss: 0.3311 - val_loss: 0.4445\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3251 - val_loss: 0.4325\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3231 - val_loss: 0.4569\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3331 - val_loss: 0.4408\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3219 - val_loss: 0.4319\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3150 - val_loss: 0.4363\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3230 - val_loss: 0.4367\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3129 - val_loss: 0.4441\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.3135 - val_loss: 0.4483\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.2996 - val_loss: 0.4331\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3086 - val_loss: 0.4250\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3142 - val_loss: 0.4374\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 188ms/step - loss: 0.3073 - val_loss: 0.4329\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3054 - val_loss: 0.4321\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 186ms/step - loss: 0.3040 - val_loss: 0.4342\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3074 - val_loss: 0.4319\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.3001 - val_loss: 0.4311\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3076 - val_loss: 0.4320\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.3067 - val_loss: 0.4327\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-14:44:28] Training the entire fine-tuned model...\n",
      "[2022_04_21-14:44:46] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 766ms/step - loss: 0.4065 - val_loss: 0.5913\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3514 - val_loss: 0.4427\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3346 - val_loss: 0.4730\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3263 - val_loss: 0.4558\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.3019 - val_loss: 0.4317\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.2786 - val_loss: 0.4291\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.2802 - val_loss: 0.4351\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.2675 - val_loss: 0.4510\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.2539 - val_loss: 0.4299\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.2633 - val_loss: 0.4237\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.2443 - val_loss: 0.5591\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.2204 - val_loss: 0.4426\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 320ms/step - loss: 0.1999 - val_loss: 0.4571\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 339ms/step - loss: 0.1592 - val_loss: 0.4945\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.1461 - val_loss: 0.5625\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 318ms/step - loss: 0.1546 - val_loss: 0.5074\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.1257 - val_loss: 0.5508\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.1201 - val_loss: 0.5687\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_21-14:45:22] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-14:45:22] Training set: Filtered out 0 of 637 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:45:22] Validation set: Filtered out 0 of 261 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 11s 781ms/step - loss: 0.2182 - val_loss: 0.4471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4a/2022_04_21_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-14:45:56] Test set: Filtered out 0 of 393 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3kyumj08) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 46932... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▅▆▅▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▄▃▃▂▂▂▂▂▂▁▁▁▂</td></tr><tr><td>lr</td><td>████████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃█▃▄▂▁▂▂▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▂▁▁▁▁▁▃▁▂▂▃▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>9</td></tr><tr><td>best_val_loss</td><td>0.42372</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.21824</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.44714</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">worthy-sun-20</strong>: <a href=\"https://wandb.ai/kvetab/Split%204a/runs/3kyumj08\" target=\"_blank\">https://wandb.ai/kvetab/Split%204a/runs/3kyumj08</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_144331-3kyumj08/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3kyumj08). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%204b/runs/3ofg2v96\" target=\"_blank\">classic-eon-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%204b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-14:46:18] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:46:18] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:46:18] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 512ms/step - loss: 0.8653 - val_loss: 1.4388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.8386 - val_loss: 0.5689\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.6850 - val_loss: 0.4509\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.5325 - val_loss: 0.4566\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.4851 - val_loss: 0.4203\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4805 - val_loss: 0.4296\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4790 - val_loss: 0.4648\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.4961 - val_loss: 0.4054\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4498 - val_loss: 0.4154\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4491 - val_loss: 0.4037\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4347 - val_loss: 0.3958\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4518 - val_loss: 0.3923\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4301 - val_loss: 0.4192\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.4405 - val_loss: 0.3845\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4085 - val_loss: 0.4582\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4475 - val_loss: 0.3831\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4063 - val_loss: 0.4518\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.4494 - val_loss: 0.3935\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4318 - val_loss: 0.3634\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4402 - val_loss: 0.4252\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4167 - val_loss: 0.3711\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3978 - val_loss: 0.4056\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3983 - val_loss: 0.3655\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3906 - val_loss: 0.3722\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3840 - val_loss: 0.3638\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3970 - val_loss: 0.3656\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3870 - val_loss: 0.3655\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_21-14:46:52] Training the entire fine-tuned model...\n",
      "[2022_04_21-14:47:17] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.5927WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0965s vs `on_train_batch_end` time: 0.1237s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0965s vs `on_train_batch_end` time: 0.1237s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 631ms/step - loss: 0.5857 - val_loss: 0.3717\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 255ms/step - loss: 0.4256 - val_loss: 0.3731\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.4065 - val_loss: 0.3882\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 280ms/step - loss: 0.3990 - val_loss: 0.3856\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.4159 - val_loss: 0.3922\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.4249 - val_loss: 0.3721\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3886 - val_loss: 0.4075\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 255ms/step - loss: 0.3858 - val_loss: 0.3731\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3824 - val_loss: 0.3711\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3817 - val_loss: 0.3733\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3828 - val_loss: 0.3710\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 255ms/step - loss: 0.3635 - val_loss: 0.3741\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3487 - val_loss: 0.3694\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 280ms/step - loss: 0.3600 - val_loss: 0.3679\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3569 - val_loss: 0.3807\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3490 - val_loss: 0.3654\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 259ms/step - loss: 0.3402 - val_loss: 0.3629\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3326 - val_loss: 0.3614\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3285 - val_loss: 0.3609\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 259ms/step - loss: 0.3237 - val_loss: 0.3587\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3206 - val_loss: 0.3598\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3119 - val_loss: 0.3717\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.3144 - val_loss: 0.3633\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3038 - val_loss: 0.3631\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 2s 280ms/step - loss: 0.2848 - val_loss: 0.3635\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.2923 - val_loss: 0.3655\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.2785 - val_loss: 0.3659\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.2804 - val_loss: 0.3675\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "[2022_04_21-14:48:10] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-14:48:10] Training set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:48:11] Validation set: Filtered out 0 of 254 (0.0%) records of lengths exceeding 510.\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3235WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1003s vs `on_train_batch_end` time: 0.1221s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1003s vs `on_train_batch_end` time: 0.1221s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 611ms/step - loss: 0.3211 - val_loss: 0.3592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/4b/2022_04_21_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-14:49:19] Test set: Filtered out 0 of 383 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3ofg2v96) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 47316... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▇▇▇██▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██▁</td></tr><tr><td>loss</td><td>██▆▃▃▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂</td></tr><tr><td>lr</td><td>█████████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>19</td></tr><tr><td>best_val_loss</td><td>0.35872</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32107</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.35924</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">classic-eon-16</strong>: <a href=\"https://wandb.ai/kvetab/Split%204b/runs/3ofg2v96\" target=\"_blank\">https://wandb.ai/kvetab/Split%204b/runs/3ofg2v96</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_144559-3ofg2v96/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3ofg2v96). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/2ewnqf22\" target=\"_blank\">pleasant-sea-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-14:49:37] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:49:37] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:49:37] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 645ms/step - loss: 0.7829 - val_loss: 0.7631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.6275 - val_loss: 0.6592\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.6011 - val_loss: 0.4643\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5681 - val_loss: 0.5038\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5284 - val_loss: 0.4525\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.5165 - val_loss: 0.4444\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4772 - val_loss: 0.4582\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4903 - val_loss: 0.4371\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4449 - val_loss: 0.4743\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4566 - val_loss: 0.4305\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4216 - val_loss: 0.4586\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4447 - val_loss: 0.4435\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3965 - val_loss: 0.4527\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-14:49:57] Training the entire fine-tuned model...\n",
      "[2022_04_21-14:50:07] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 775ms/step - loss: 0.3908 - val_loss: 0.4291\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3886 - val_loss: 0.4284\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3985 - val_loss: 0.4291\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3885 - val_loss: 0.4285\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3968 - val_loss: 0.4276\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3906 - val_loss: 0.4277\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 328ms/step - loss: 0.3941 - val_loss: 0.4274\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 327ms/step - loss: 0.3834 - val_loss: 0.4271\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 300ms/step - loss: 0.3871 - val_loss: 0.4267\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3917 - val_loss: 0.4265\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3872 - val_loss: 0.4262\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3829 - val_loss: 0.4259\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3804 - val_loss: 0.4257\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3717 - val_loss: 0.4254\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3822 - val_loss: 0.4251\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3731 - val_loss: 0.4249\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3773 - val_loss: 0.4247\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3828 - val_loss: 0.4245\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3773 - val_loss: 0.4245\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3750 - val_loss: 0.4243\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3786 - val_loss: 0.4242\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 2s 327ms/step - loss: 0.3759 - val_loss: 0.4242\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3756 - val_loss: 0.4241\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3759 - val_loss: 0.4240\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3705 - val_loss: 0.4241\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3754 - val_loss: 0.4238\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3744 - val_loss: 0.4238\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3697 - val_loss: 0.4238\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3707 - val_loss: 0.4240\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3683 - val_loss: 0.4239\n",
      "[2022_04_21-14:51:02] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-14:51:02] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:51:02] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 759ms/step - loss: 0.3751 - val_loss: 0.4237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-14:52:10] Test set: Filtered out 0 of 406 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2ewnqf22) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 47706... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▆▆▇▇▇▇▇██▁</td></tr><tr><td>loss</td><td>█▅▅▄▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▂▃▂▁▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.42374</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37512</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42374</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">pleasant-sea-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/2ewnqf22\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/2ewnqf22</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_144921-2ewnqf22/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2ewnqf22). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/3digpi3y\" target=\"_blank\">spring-flower-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-14:52:28] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:52:28] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:52:28] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 530ms/step - loss: 0.9221 - val_loss: 0.8134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.6085 - val_loss: 0.5418\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.5173 - val_loss: 0.5430\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.5103 - val_loss: 0.5401\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 0.4618 - val_loss: 0.5072\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4330 - val_loss: 0.4694\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.4141 - val_loss: 0.4631\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.4199 - val_loss: 0.4748\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4141 - val_loss: 0.5489\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4612 - val_loss: 0.4666\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-14:52:48] Training the entire fine-tuned model...\n",
      "[2022_04_21-14:53:06] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4093WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1063s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1063s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 617ms/step - loss: 0.4093 - val_loss: 0.4610\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.4170 - val_loss: 0.4602\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.4092 - val_loss: 0.4600\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.4033 - val_loss: 0.4617\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.4100 - val_loss: 0.4619\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.4065 - val_loss: 0.4594\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 286ms/step - loss: 0.3992 - val_loss: 0.4577\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.4088 - val_loss: 0.4566\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.4055 - val_loss: 0.4571\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.4012 - val_loss: 0.4561\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.4011 - val_loss: 0.4553\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 268ms/step - loss: 0.4054 - val_loss: 0.4568\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3998 - val_loss: 0.4551\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.4062 - val_loss: 0.4523\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3912 - val_loss: 0.4523\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3913 - val_loss: 0.4528\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.394 - 2s 265ms/step - loss: 0.3942 - val_loss: 0.4522\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3935 - val_loss: 0.4521\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3946 - val_loss: 0.4514\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3929 - val_loss: 0.4516\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3850 - val_loss: 0.4516\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3961 - val_loss: 0.4515\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_21-14:53:50] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-14:53:50] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:53:50] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3983WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1001s vs `on_train_batch_end` time: 0.1304s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1001s vs `on_train_batch_end` time: 0.1304s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 609ms/step - loss: 0.3983 - val_loss: 0.4515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-14:54:26] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3digpi3y) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 48034... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██▁</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▃▂▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>18</td></tr><tr><td>best_val_loss</td><td>0.45141</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.39825</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.45148</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">spring-flower-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/3digpi3y\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/3digpi3y</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_145212-3digpi3y/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3digpi3y). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/3pwx2g7z\" target=\"_blank\">bright-smoke-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-14:54:45] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:54:45] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:54:45] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 643ms/step - loss: 0.8739 - val_loss: 0.6518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.6003 - val_loss: 0.6107\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5182 - val_loss: 0.5080\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4759 - val_loss: 0.4986\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4622 - val_loss: 0.4502\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.4470 - val_loss: 0.4458\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4264 - val_loss: 0.4457\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4136 - val_loss: 0.4342\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4025 - val_loss: 0.4395\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3899 - val_loss: 0.4385\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4123 - val_loss: 0.4292\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4007 - val_loss: 0.4375\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3885 - val_loss: 0.4275\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3767 - val_loss: 0.4223\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3744 - val_loss: 0.4288\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3692 - val_loss: 0.4217\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3549 - val_loss: 0.4209\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3683 - val_loss: 0.4264\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3503 - val_loss: 0.4221\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3647 - val_loss: 0.4162\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3501 - val_loss: 0.4222\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3329 - val_loss: 0.4170\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3709 - val_loss: 0.4164\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-14:55:15] Training the entire fine-tuned model...\n",
      "[2022_04_21-14:55:23] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 756ms/step - loss: 0.3590 - val_loss: 0.4148\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3570 - val_loss: 0.4184\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3652 - val_loss: 0.4211\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3433 - val_loss: 0.4155\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_21-14:55:39] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-14:55:39] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:55:39] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 781ms/step - loss: 0.3520 - val_loss: 0.4143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-14:56:12] Test set: Filtered out 0 of 406 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3pwx2g7z) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 48298... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██▁▁▂▂▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████████▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▄▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.41432</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35202</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41432</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">bright-smoke-2</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/3pwx2g7z\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/3pwx2g7z</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_145428-3pwx2g7z/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3pwx2g7z). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/jyq957u8\" target=\"_blank\">sparkling-planet-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-14:56:31] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:56:31] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:56:31] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 802ms/step - loss: 0.8766 - val_loss: 0.8134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.6505 - val_loss: 0.5073\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.5306 - val_loss: 0.5184\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4784 - val_loss: 0.4759\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4361 - val_loss: 0.4636\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 0.4275 - val_loss: 0.4653\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.4291 - val_loss: 0.4654\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.4032 - val_loss: 0.4506\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4009 - val_loss: 0.4536\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4141 - val_loss: 0.4449\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4005 - val_loss: 0.4427\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3856 - val_loss: 0.4744\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.3995 - val_loss: 0.4438\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3860 - val_loss: 0.4660\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-14:56:54] Training the entire fine-tuned model...\n",
      "[2022_04_21-14:57:08] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3928WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0987s vs `on_train_batch_end` time: 0.1277s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0987s vs `on_train_batch_end` time: 0.1277s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 628ms/step - loss: 0.3928 - val_loss: 0.4473\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 288ms/step - loss: 0.3836 - val_loss: 0.4363\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3765 - val_loss: 0.4368\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3672 - val_loss: 0.4358\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3728 - val_loss: 0.4322\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3796 - val_loss: 0.4305\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 268ms/step - loss: 0.3712 - val_loss: 0.4428\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3602 - val_loss: 0.4336\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 267ms/step - loss: 0.3512 - val_loss: 0.4379\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_21-14:57:31] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-14:57:31] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:57:31] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3603WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1004s vs `on_train_batch_end` time: 0.1276s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1004s vs `on_train_batch_end` time: 0.1276s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 604ms/step - loss: 0.3603 - val_loss: 0.4301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-14:58:04] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:jyq957u8) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 48539... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▄▅▅▆▆▇▇█▁▂▂▃▃▄▄▅▅▁</td></tr><tr><td>loss</td><td>█▅▃▃▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▃▂▂▂▂▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.43006</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36031</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43006</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">sparkling-planet-2</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/jyq957u8\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/jyq957u8</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_145615-jyq957u8/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:jyq957u8). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/1p76m8ce\" target=\"_blank\">apricot-sky-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-14:58:22] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:58:22] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:58:22] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 637ms/step - loss: 0.8466 - val_loss: 0.6203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5875 - val_loss: 0.6346\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5379 - val_loss: 0.5458\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 185ms/step - loss: 0.5156 - val_loss: 0.4881\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4694 - val_loss: 0.4470\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4807 - val_loss: 0.4482\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4506 - val_loss: 0.4955\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4518 - val_loss: 0.4828\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-14:58:40] Training the entire fine-tuned model...\n",
      "[2022_04_21-14:58:51] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 732ms/step - loss: 0.4348 - val_loss: 0.4445\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 301ms/step - loss: 0.4224 - val_loss: 0.4430\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 327ms/step - loss: 0.4175 - val_loss: 0.4396\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.4140 - val_loss: 0.4355\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.4096 - val_loss: 0.4323\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.4008 - val_loss: 0.4309\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3932 - val_loss: 0.4297\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3859 - val_loss: 0.4285\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3799 - val_loss: 0.4305\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3756 - val_loss: 0.4294\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3655 - val_loss: 0.4300\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_21-14:59:15] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-14:59:15] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-14:59:16] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 747ms/step - loss: 0.3772 - val_loss: 0.4284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-14:59:53] Test set: Filtered out 0 of 406 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1p76m8ce) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 48760... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▅▅▆▁▂▂▃▄▅▅▆▇▇█▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>██▅▃▂▂▃▃▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.42837</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37722</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.42837</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">apricot-sky-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/1p76m8ce\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/1p76m8ce</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_145806-1p76m8ce/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1p76m8ce). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/vdp1nb7c\" target=\"_blank\">driven-blaze-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-15:00:12] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:00:12] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:00:12] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 823ms/step - loss: 0.9452 - val_loss: 0.8103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.6181 - val_loss: 0.5354\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4881 - val_loss: 0.4940\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4841 - val_loss: 0.5195\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4523 - val_loss: 0.4820\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4404 - val_loss: 0.5075\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4456 - val_loss: 0.4827\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.4198 - val_loss: 0.4566\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.4031 - val_loss: 0.5002\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4239 - val_loss: 0.4590\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4075 - val_loss: 0.4470\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4255 - val_loss: 0.4438\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.4153 - val_loss: 0.5011\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.4044 - val_loss: 0.4398\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4024 - val_loss: 0.4494\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.3821 - val_loss: 0.4299\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3613 - val_loss: 0.4314\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3614 - val_loss: 0.4311\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3697 - val_loss: 0.4257\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3538 - val_loss: 0.4226\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3500 - val_loss: 0.4652\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3685 - val_loss: 0.4361\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3693 - val_loss: 0.4458\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-15:00:43] Training the entire fine-tuned model...\n",
      "[2022_04_21-15:00:51] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3548WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1057s vs `on_train_batch_end` time: 0.1252s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1057s vs `on_train_batch_end` time: 0.1252s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 625ms/step - loss: 0.3548 - val_loss: 0.4903\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3647 - val_loss: 0.4238\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 267ms/step - loss: 0.3430 - val_loss: 0.4254\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3372 - val_loss: 0.4270\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3477 - val_loss: 0.4172\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.3239 - val_loss: 0.4090\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3181 - val_loss: 0.4211\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3251 - val_loss: 0.4124\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 268ms/step - loss: 0.3133 - val_loss: 0.4164\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_21-15:01:14] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-15:01:14] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:01:14] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3274WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1050s vs `on_train_batch_end` time: 0.1271s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1050s vs `on_train_batch_end` time: 0.1271s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 634ms/step - loss: 0.3274 - val_loss: 0.4093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-15:01:48] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:vdp1nb7c) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 48960... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████████▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▃▂▃▂▂▃▂▂▂▃▂▂▁▁▁▁▁▂▁▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.40904</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32739</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.40929</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">driven-blaze-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/vdp1nb7c\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/vdp1nb7c</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_145955-vdp1nb7c/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:vdp1nb7c). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/27dz3dbx\" target=\"_blank\">usual-waterfall-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-15:02:13] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:02:13] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:02:13] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 673ms/step - loss: 0.8461 - val_loss: 0.5815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.6509 - val_loss: 0.6573\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5144 - val_loss: 0.5812\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4973 - val_loss: 0.5133\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4505 - val_loss: 0.4980\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.4718 - val_loss: 0.4655\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4330 - val_loss: 0.4404\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4057 - val_loss: 0.4356\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4095 - val_loss: 0.4299\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4017 - val_loss: 0.4270\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4074 - val_loss: 0.4402\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4108 - val_loss: 0.4568\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3915 - val_loss: 0.4508\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-15:02:35] Training the entire fine-tuned model...\n",
      "[2022_04_21-15:02:44] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 750ms/step - loss: 0.4809 - val_loss: 0.4293\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 301ms/step - loss: 0.3940 - val_loss: 0.4236\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3953 - val_loss: 0.4274\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3837 - val_loss: 0.4205\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 301ms/step - loss: 0.3631 - val_loss: 0.4389\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 328ms/step - loss: 0.3623 - val_loss: 0.4353\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3472 - val_loss: 0.4048\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3268 - val_loss: 0.4113\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 327ms/step - loss: 0.3225 - val_loss: 0.4532\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 0.3112 - val_loss: 0.4364\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_21-15:03:07] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-15:03:07] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:03:07] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 744ms/step - loss: 0.3177 - val_loss: 0.4062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-15:03:43] Test set: Filtered out 0 of 406 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:27dz3dbx) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 413... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▅▅▆▆▇▇█▁▂▂▃▃▄▅▅▆▆▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▂▂▂▂▂▂▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▆█▆▄▄▃▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>6</td></tr><tr><td>best_val_loss</td><td>0.40482</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31769</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.40619</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">usual-waterfall-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/27dz3dbx\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/27dz3dbx</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_150150-27dz3dbx/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:27dz3dbx). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/17cd0e3y\" target=\"_blank\">amber-cherry-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-15:04:02] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:04:02] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:04:02] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 804ms/step - loss: 0.8778 - val_loss: 0.8561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.6710 - val_loss: 0.5191\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.5329 - val_loss: 0.5470\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.5000 - val_loss: 0.5197\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 0.4602 - val_loss: 0.4825\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4358 - val_loss: 0.4620\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4178 - val_loss: 0.4614\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4180 - val_loss: 0.5128\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4374 - val_loss: 0.4476\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.4059 - val_loss: 0.4413\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3927 - val_loss: 0.4435\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.3845 - val_loss: 0.4383\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3754 - val_loss: 0.4399\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3864 - val_loss: 0.4418\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3700 - val_loss: 0.4360\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3799 - val_loss: 0.4507\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3694 - val_loss: 0.4444\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.3677 - val_loss: 0.4314\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3631 - val_loss: 0.4247\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3460 - val_loss: 0.4242\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3677 - val_loss: 0.4207\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3726 - val_loss: 0.4199\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3499 - val_loss: 0.4280\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3591 - val_loss: 0.4209\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3453 - val_loss: 0.4561\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-15:04:34] Training the entire fine-tuned model...\n",
      "[2022_04_21-15:04:59] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5456WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1052s vs `on_train_batch_end` time: 0.1247s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1052s vs `on_train_batch_end` time: 0.1247s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 631ms/step - loss: 0.5456 - val_loss: 0.4165\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3978 - val_loss: 0.4352\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3999 - val_loss: 0.4260\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3827 - val_loss: 0.4088\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3668 - val_loss: 0.4102\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3452 - val_loss: 0.4022\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3200 - val_loss: 0.4116\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3088 - val_loss: 0.4056\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.2985 - val_loss: 0.4095\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_21-15:05:22] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-15:05:22] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:05:26] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3253WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1006s vs `on_train_batch_end` time: 0.1274s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1006s vs `on_train_batch_end` time: 0.1274s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 628ms/step - loss: 0.3253 - val_loss: 0.4038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-15:06:00] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:17cd0e3y) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 631... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▆▄▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▄▂▂▂▂▂▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████████████▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▃▂▂▂▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.4022</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32529</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.40376</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">amber-cherry-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/17cd0e3y\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/17cd0e3y</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_150345-17cd0e3y/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:17cd0e3y). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/2lpoaguo\" target=\"_blank\">silvery-valley-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-15:06:20] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:06:20] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:06:20] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 630ms/step - loss: 0.7956 - val_loss: 0.8525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.6249 - val_loss: 0.6879\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.6056 - val_loss: 0.4839\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4706 - val_loss: 0.4716\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4562 - val_loss: 0.4505\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4437 - val_loss: 0.4455\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.4600 - val_loss: 0.5152\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4598 - val_loss: 0.4839\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4176 - val_loss: 0.4594\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4103 - val_loss: 0.4287\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4011 - val_loss: 0.4336\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4053 - val_loss: 0.4312\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3946 - val_loss: 0.4269\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3851 - val_loss: 0.4273\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 0.3919 - val_loss: 0.4252\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3891 - val_loss: 0.4276\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3881 - val_loss: 0.4259\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3779 - val_loss: 0.4239\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3830 - val_loss: 0.4238\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3814 - val_loss: 0.4229\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3789 - val_loss: 0.4225\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3809 - val_loss: 0.4224\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3790 - val_loss: 0.4215\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3825 - val_loss: 0.4210\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3796 - val_loss: 0.4204\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3722 - val_loss: 0.4213\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3717 - val_loss: 0.4196\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3635 - val_loss: 0.4199\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3696 - val_loss: 0.4185\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3699 - val_loss: 0.4226\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3691 - val_loss: 0.4184\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3679 - val_loss: 0.4178\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3684 - val_loss: 0.4182\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3592 - val_loss: 0.4172\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3649 - val_loss: 0.4179\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3701 - val_loss: 0.4161\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3576 - val_loss: 0.4180\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3585 - val_loss: 0.4169\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3597 - val_loss: 0.4165\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3635 - val_loss: 0.4172\n",
      "[2022_04_21-15:07:03] Training the entire fine-tuned model...\n",
      "[2022_04_21-15:07:19] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 756ms/step - loss: 0.3595 - val_loss: 0.4162\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3610 - val_loss: 0.4160\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 330ms/step - loss: 0.3594 - val_loss: 0.4159\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3614 - val_loss: 0.4158\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 326ms/step - loss: 0.3647 - val_loss: 0.4157\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3600 - val_loss: 0.4156\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3614 - val_loss: 0.4155\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3465 - val_loss: 0.4155\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3560 - val_loss: 0.4152\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3524 - val_loss: 0.4151\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 327ms/step - loss: 0.3536 - val_loss: 0.4150\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 328ms/step - loss: 0.3566 - val_loss: 0.4149\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3489 - val_loss: 0.4149\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3535 - val_loss: 0.4148\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3510 - val_loss: 0.4145\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 328ms/step - loss: 0.3585 - val_loss: 0.4143\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3514 - val_loss: 0.4142\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3592 - val_loss: 0.4140\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3562 - val_loss: 0.4140\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3504 - val_loss: 0.4138\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3528 - val_loss: 0.4137\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3548 - val_loss: 0.4136\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3410 - val_loss: 0.4135\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3430 - val_loss: 0.4135\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3451 - val_loss: 0.4134\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3512 - val_loss: 0.4134\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3411 - val_loss: 0.4134\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3397 - val_loss: 0.4133\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3431 - val_loss: 0.4133\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3377 - val_loss: 0.4133\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 2s 327ms/step - loss: 0.3398 - val_loss: 0.4133\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 2s 329ms/step - loss: 0.3528 - val_loss: 0.4133\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3358 - val_loss: 0.4133\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3415 - val_loss: 0.4133\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      "[2022_04_21-15:08:20] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-15:08:20] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:08:20] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 767ms/step - loss: 0.3427 - val_loss: 0.4133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-15:08:55] Test set: Filtered out 0 of 406 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2lpoaguo) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 950... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.4133</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34273</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.4133</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">silvery-valley-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/2lpoaguo\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/2lpoaguo</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_150602-2lpoaguo/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2lpoaguo). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/1v77uh4k\" target=\"_blank\">earnest-meadow-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-15:09:13] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:09:13] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:09:13] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 763ms/step - loss: 0.9254 - val_loss: 0.7127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 124ms/step - loss: 0.6125 - val_loss: 0.4845\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 126ms/step - loss: 0.5250 - val_loss: 0.5849\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.5085 - val_loss: 0.5674\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4894 - val_loss: 0.5454\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4553 - val_loss: 0.4745\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4578 - val_loss: 0.4725\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4438 - val_loss: 0.4929\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4297 - val_loss: 0.4681\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4244 - val_loss: 0.4653\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.4191 - val_loss: 0.4638\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4166 - val_loss: 0.4598\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4098 - val_loss: 0.4581\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4141 - val_loss: 0.4569\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4048 - val_loss: 0.4585\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4060 - val_loss: 0.4540\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.4061 - val_loss: 0.4530\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4072 - val_loss: 0.4540\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3975 - val_loss: 0.4520\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3988 - val_loss: 0.4510\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3925 - val_loss: 0.4462\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3988 - val_loss: 0.4464\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3999 - val_loss: 0.4445\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3891 - val_loss: 0.4439\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3872 - val_loss: 0.4440\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3897 - val_loss: 0.4431\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3937 - val_loss: 0.4412\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3777 - val_loss: 0.4464\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3844 - val_loss: 0.4400\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3967 - val_loss: 0.4414\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3851 - val_loss: 0.4390\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3841 - val_loss: 0.4393\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3841 - val_loss: 0.4427\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3670 - val_loss: 0.4380\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3715 - val_loss: 0.4398\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3783 - val_loss: 0.4377\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3758 - val_loss: 0.4368\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3731 - val_loss: 0.4436\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3670 - val_loss: 0.4367\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3727 - val_loss: 0.4363\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3725 - val_loss: 0.4369\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3645 - val_loss: 0.4335\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3594 - val_loss: 0.4383\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3634 - val_loss: 0.4314\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3707 - val_loss: 0.4325\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3586 - val_loss: 0.4319\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3590 - val_loss: 0.4281\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3615 - val_loss: 0.4299\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3517 - val_loss: 0.4279\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3578 - val_loss: 0.4264\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3587 - val_loss: 0.4349\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3650 - val_loss: 0.4278\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3528 - val_loss: 0.4270\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3510 - val_loss: 0.4257\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3476 - val_loss: 0.4302\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3509 - val_loss: 0.4270\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.3461 - val_loss: 0.4257\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3493 - val_loss: 0.4256\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3421 - val_loss: 0.4256\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3501 - val_loss: 0.4259\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.3516 - val_loss: 0.4261\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3474 - val_loss: 0.4260\n",
      "[2022_04_21-15:10:16] Training the entire fine-tuned model...\n",
      "[2022_04_21-15:10:51] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3557WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0999s vs `on_train_batch_end` time: 0.1294s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0999s vs `on_train_batch_end` time: 0.1294s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 640ms/step - loss: 0.3557 - val_loss: 0.4254\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3577 - val_loss: 0.4273\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3490 - val_loss: 0.4264\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3514 - val_loss: 0.4239\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3511 - val_loss: 0.4246\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 267ms/step - loss: 0.3434 - val_loss: 0.4247\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 287ms/step - loss: 0.3393 - val_loss: 0.4229\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3398 - val_loss: 0.4229\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3321 - val_loss: 0.4241\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 267ms/step - loss: 0.3294 - val_loss: 0.4264\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3384 - val_loss: 0.4267\n",
      "[2022_04_21-15:11:17] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-15:11:17] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:11:18] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3394WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1019s vs `on_train_batch_end` time: 0.1262s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1019s vs `on_train_batch_end` time: 0.1262s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 611ms/step - loss: 0.3394 - val_loss: 0.4228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-15:11:51] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1v77uh4k) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 1488... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▄▂▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.42275</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33944</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42275</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">earnest-meadow-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/1v77uh4k\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/1v77uh4k</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_150857-1v77uh4k/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1v77uh4k). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/3kiu6ltb\" target=\"_blank\">fast-glitter-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-15:12:10] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:12:10] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:12:10] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 664ms/step - loss: 0.8763 - val_loss: 0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.6098 - val_loss: 0.6738\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5188 - val_loss: 0.6020\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5169 - val_loss: 0.5333\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4717 - val_loss: 0.4775\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4563 - val_loss: 0.4482\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4257 - val_loss: 0.4440\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4045 - val_loss: 0.4367\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3968 - val_loss: 0.4324\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3949 - val_loss: 0.4359\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4017 - val_loss: 0.4417\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.4007 - val_loss: 0.4251\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3852 - val_loss: 0.4246\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3734 - val_loss: 0.4247\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3784 - val_loss: 0.4247\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3657 - val_loss: 0.4209\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3879 - val_loss: 0.4535\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 0.3743 - val_loss: 0.4348\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3795 - val_loss: 0.4174\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3551 - val_loss: 0.4206\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3465 - val_loss: 0.4157\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3476 - val_loss: 0.4176\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3494 - val_loss: 0.4211\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3584 - val_loss: 0.4130\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3517 - val_loss: 0.4227\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3849 - val_loss: 0.4219\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3477 - val_loss: 0.4374\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3518 - val_loss: 0.4124\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3328 - val_loss: 0.4157\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3333 - val_loss: 0.4204\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3348 - val_loss: 0.4119\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3348 - val_loss: 0.4165\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3320 - val_loss: 0.4176\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3353 - val_loss: 0.4134\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3252 - val_loss: 0.4122\n",
      "[2022_04_21-15:12:49] Training the entire fine-tuned model...\n",
      "[2022_04_21-15:12:58] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 751ms/step - loss: 0.3206 - val_loss: 0.4205\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3362 - val_loss: 0.4139\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3439 - val_loss: 0.4127\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3158 - val_loss: 0.4116\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3303 - val_loss: 0.4111\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3302 - val_loss: 0.4137\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3122 - val_loss: 0.4115\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3184 - val_loss: 0.4108\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3240 - val_loss: 0.4098\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3160 - val_loss: 0.4098\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.2941 - val_loss: 0.4101\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 332ms/step - loss: 0.3092 - val_loss: 0.4109\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.2988 - val_loss: 0.4112\n",
      "[2022_04_21-15:13:26] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-15:13:26] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:13:26] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 748ms/step - loss: 0.3175 - val_loss: 0.4098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-15:14:00] Test set: Filtered out 0 of 406 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3kiu6ltb) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2053... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>██▆▄▃▂▂▂▂▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_val_loss</td><td>0.40979</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31747</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.40982</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fast-glitter-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/3kiu6ltb\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/3kiu6ltb</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_151154-3kiu6ltb/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3kiu6ltb). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/2l8vf4p0\" target=\"_blank\">exalted-armadillo-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-15:14:18] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:14:18] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:14:18] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 497ms/step - loss: 0.9067 - val_loss: 0.9263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.6857 - val_loss: 0.5946\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.5249 - val_loss: 0.4777\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4802 - val_loss: 0.4945\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4529 - val_loss: 0.4867\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4229 - val_loss: 0.4714\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4279 - val_loss: 0.4613\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.4095 - val_loss: 0.4746\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4119 - val_loss: 0.4705\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3996 - val_loss: 0.5346\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.4336 - val_loss: 0.4475\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4117 - val_loss: 0.4452\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3983 - val_loss: 0.4556\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3955 - val_loss: 0.4458\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3893 - val_loss: 0.4432\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3899 - val_loss: 0.4409\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3797 - val_loss: 0.4394\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3791 - val_loss: 0.4387\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3870 - val_loss: 0.4373\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3862 - val_loss: 0.4455\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3830 - val_loss: 0.4386\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3774 - val_loss: 0.4399\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.3822 - val_loss: 0.4371\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3788 - val_loss: 0.4346\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 0.3694 - val_loss: 0.4346\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3752 - val_loss: 0.4349\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3770 - val_loss: 0.4363\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3754 - val_loss: 0.4363\n",
      "[2022_04_21-15:14:52] Training the entire fine-tuned model...\n",
      "[2022_04_21-15:15:01] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3823WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1305s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1305s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 616ms/step - loss: 0.3823 - val_loss: 0.4351\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3625 - val_loss: 0.4314\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3679 - val_loss: 0.4344\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3542 - val_loss: 0.4294\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3546 - val_loss: 0.4285\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 267ms/step - loss: 0.3600 - val_loss: 0.4283\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3367 - val_loss: 0.4308\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3402 - val_loss: 0.4282\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 289ms/step - loss: 0.3321 - val_loss: 0.4314\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3250 - val_loss: 0.4301\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 267ms/step - loss: 0.3188 - val_loss: 0.4288\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3263 - val_loss: 0.4295\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_21-15:15:29] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-15:15:29] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:15:29] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3304WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1007s vs `on_train_batch_end` time: 0.1297s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1007s vs `on_train_batch_end` time: 0.1297s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 647ms/step - loss: 0.3304 - val_loss: 0.4289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-15:16:03] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2l8vf4p0) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2408... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.42818</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33039</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42895</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">exalted-armadillo-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/2l8vf4p0\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/2l8vf4p0</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_151403-2l8vf4p0/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2l8vf4p0). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/2acpz291\" target=\"_blank\">dainty-sky-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-15:16:21] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:16:21] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:16:21] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 648ms/step - loss: 0.9441 - val_loss: 0.5004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.6337 - val_loss: 0.6581\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.6336 - val_loss: 0.5186\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5436 - val_loss: 0.5531\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5210 - val_loss: 0.4858\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4438 - val_loss: 0.4766\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4598 - val_loss: 0.4570\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4356 - val_loss: 0.4636\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4434 - val_loss: 0.4502\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4315 - val_loss: 0.4550\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4344 - val_loss: 0.4472\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4226 - val_loss: 0.4449\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4227 - val_loss: 0.4424\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4196 - val_loss: 0.4409\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4130 - val_loss: 0.4393\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4136 - val_loss: 0.4388\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4189 - val_loss: 0.4370\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4093 - val_loss: 0.4356\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4069 - val_loss: 0.4349\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4152 - val_loss: 0.4336\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4022 - val_loss: 0.4332\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4071 - val_loss: 0.4312\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3903 - val_loss: 0.4305\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3991 - val_loss: 0.4297\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3978 - val_loss: 0.4310\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3996 - val_loss: 0.4279\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 185ms/step - loss: 0.3956 - val_loss: 0.4270\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3985 - val_loss: 0.4259\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3893 - val_loss: 0.4259\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 0.3860 - val_loss: 0.4259\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3880 - val_loss: 0.4249\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3858 - val_loss: 0.4247\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3918 - val_loss: 0.4234\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3863 - val_loss: 0.4241\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3750 - val_loss: 0.4222\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3827 - val_loss: 0.4214\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3798 - val_loss: 0.4216\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3816 - val_loss: 0.4201\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3753 - val_loss: 0.4197\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3775 - val_loss: 0.4210\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 186ms/step - loss: 0.3765 - val_loss: 0.4190\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3777 - val_loss: 0.4181\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3700 - val_loss: 0.4175\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3652 - val_loss: 0.4186\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3632 - val_loss: 0.4167\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3671 - val_loss: 0.4170\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3622 - val_loss: 0.4167\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3631 - val_loss: 0.4158\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3621 - val_loss: 0.4155\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3589 - val_loss: 0.4157\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3557 - val_loss: 0.4152\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3642 - val_loss: 0.4155\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3531 - val_loss: 0.4150\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3609 - val_loss: 0.4161\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3628 - val_loss: 0.4139\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3499 - val_loss: 0.4141\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3559 - val_loss: 0.4142\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3514 - val_loss: 0.4133\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3516 - val_loss: 0.4173\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.3600 - val_loss: 0.4136\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 0.3579 - val_loss: 0.4136\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3567 - val_loss: 0.4117\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3491 - val_loss: 0.4123\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3493 - val_loss: 0.4129\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3439 - val_loss: 0.4114\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3488 - val_loss: 0.4113\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3462 - val_loss: 0.4112\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3458 - val_loss: 0.4112\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 0.3486 - val_loss: 0.4115\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3532 - val_loss: 0.4116\n",
      "\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3426 - val_loss: 0.4116\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3476 - val_loss: 0.4114\n",
      "[2022_04_21-15:17:31] Training the entire fine-tuned model...\n",
      "[2022_04_21-15:17:39] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 772ms/step - loss: 0.3543 - val_loss: 0.4145\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3576 - val_loss: 0.4111\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3402 - val_loss: 0.4085\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3352 - val_loss: 0.4114\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3302 - val_loss: 0.4083\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3188 - val_loss: 0.4069\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3089 - val_loss: 0.4082\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 309ms/step - loss: 0.3073 - val_loss: 0.4121\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.2994 - val_loss: 0.4143\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3002 - val_loss: 0.4127\n",
      "[2022_04_21-15:18:03] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-15:18:03] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:18:03] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 769ms/step - loss: 0.3034 - val_loss: 0.4066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-15:18:42] Test set: Filtered out 0 of 406 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2acpz291) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2719... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇███▁▁▁▂▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▂▁▁▁▁</td></tr><tr><td>lr</td><td>██▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇█▆▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.40664</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30338</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.40664</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dainty-sky-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/2acpz291\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/2acpz291</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_151605-2acpz291/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2acpz291). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/3s31wkog\" target=\"_blank\">fluent-shape-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-15:19:02] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:19:02] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:19:02] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 514ms/step - loss: 0.9241 - val_loss: 0.8615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.6554 - val_loss: 0.5413\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.5627 - val_loss: 0.5350\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.5114 - val_loss: 0.5349\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4736 - val_loss: 0.5184\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4252 - val_loss: 0.5431\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4691 - val_loss: 0.4891\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4569 - val_loss: 0.4925\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4238 - val_loss: 0.4624\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.4252 - val_loss: 0.4489\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4025 - val_loss: 0.4765\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4073 - val_loss: 0.4451\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3911 - val_loss: 0.4665\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.3924 - val_loss: 0.4439\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3772 - val_loss: 0.4588\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3918 - val_loss: 0.4426\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3730 - val_loss: 0.4786\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4083 - val_loss: 0.4340\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3543 - val_loss: 0.4462\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3525 - val_loss: 0.4290\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3691 - val_loss: 0.4462\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3697 - val_loss: 0.4384\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3596 - val_loss: 0.4415\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3513 - val_loss: 0.4244\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3472 - val_loss: 0.4246\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3416 - val_loss: 0.4304\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3341 - val_loss: 0.4265\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3348 - val_loss: 0.4250\n",
      "[2022_04_21-15:19:35] Training the entire fine-tuned model...\n",
      "[2022_04_21-15:19:45] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3621WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0987s vs `on_train_batch_end` time: 0.1310s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0987s vs `on_train_batch_end` time: 0.1310s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 619ms/step - loss: 0.3621 - val_loss: 0.4235\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3410 - val_loss: 0.4279\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3428 - val_loss: 0.4331\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3537 - val_loss: 0.4368\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3286 - val_loss: 0.4163\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3233 - val_loss: 0.4179\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3245 - val_loss: 0.4209\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3128 - val_loss: 0.4260\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3142 - val_loss: 0.4223\n",
      "[2022_04_21-15:20:08] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-15:20:08] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:20:11] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3172WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0996s vs `on_train_batch_end` time: 0.1303s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0996s vs `on_train_batch_end` time: 0.1303s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 615ms/step - loss: 0.3172 - val_loss: 0.4175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-15:20:47] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3s31wkog) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 3233... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▃▃▃▂▂▂▂▂▁▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>4</td></tr><tr><td>best_val_loss</td><td>0.41634</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3172</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.41753</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fluent-shape-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/3s31wkog\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/3s31wkog</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_151844-3s31wkog/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3s31wkog). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/wtgr8z40\" target=\"_blank\">sleek-bird-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-15:21:06] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:21:06] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:21:07] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 626ms/step - loss: 0.8968 - val_loss: 0.5540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.6163 - val_loss: 0.7670\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.6074 - val_loss: 0.5967\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5475 - val_loss: 0.5507\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4841 - val_loss: 0.4876\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4275 - val_loss: 0.4995\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4450 - val_loss: 0.4737\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4249 - val_loss: 0.4751\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4153 - val_loss: 0.4396\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3957 - val_loss: 0.4347\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4095 - val_loss: 0.4274\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.4055 - val_loss: 0.4263\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3839 - val_loss: 0.4235\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3909 - val_loss: 0.4243\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3764 - val_loss: 0.4364\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3712 - val_loss: 0.4677\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4092 - val_loss: 0.4201\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3812 - val_loss: 0.4404\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3831 - val_loss: 0.4296\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3666 - val_loss: 0.4194\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3743 - val_loss: 0.4201\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3609 - val_loss: 0.4178\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3652 - val_loss: 0.4175\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3540 - val_loss: 0.4163\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3608 - val_loss: 0.4163\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3605 - val_loss: 0.4152\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3622 - val_loss: 0.4163\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3682 - val_loss: 0.4149\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3519 - val_loss: 0.4151\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3565 - val_loss: 0.4195\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3524 - val_loss: 0.4144\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3646 - val_loss: 0.4151\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3556 - val_loss: 0.4172\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3447 - val_loss: 0.4150\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3504 - val_loss: 0.4149\n",
      "[2022_04_21-15:21:44] Training the entire fine-tuned model...\n",
      "[2022_04_21-15:22:05] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 738ms/step - loss: 0.3938 - val_loss: 0.4164\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3592 - val_loss: 0.4259\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3665 - val_loss: 0.4099\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3364 - val_loss: 0.4031\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3068 - val_loss: 0.4047\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3040 - val_loss: 0.4116\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3027 - val_loss: 0.5019\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3271 - val_loss: 0.4602\n",
      "[2022_04_21-15:22:25] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-15:22:25] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:22:25] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 749ms/step - loss: 0.3247 - val_loss: 0.4042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-15:22:59] Test set: Filtered out 0 of 406 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:wtgr8z40) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 3528... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇██▁▁▁▂▂▂▂▁</td></tr><tr><td>loss</td><td>█▅▅▄▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄█▅▄▃▃▂▂▂▂▁▁▁▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>3</td></tr><tr><td>best_val_loss</td><td>0.40308</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3247</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.40424</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">sleek-bird-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/wtgr8z40\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/wtgr8z40</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_152050-wtgr8z40/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:wtgr8z40). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/3r2xnttm\" target=\"_blank\">youthful-oath-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-15:23:18] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:23:18] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:23:18] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 532ms/step - loss: 0.8919 - val_loss: 0.8120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.6559 - val_loss: 0.5290\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4885 - val_loss: 0.4894\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.4736 - val_loss: 0.4926\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4496 - val_loss: 0.4781\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4361 - val_loss: 0.4588\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4151 - val_loss: 0.4657\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4483 - val_loss: 0.4717\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4261 - val_loss: 0.4462\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4069 - val_loss: 0.4487\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3993 - val_loss: 0.4655\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.4033 - val_loss: 0.4329\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3815 - val_loss: 0.4358\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3813 - val_loss: 0.4573\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4071 - val_loss: 0.5353\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4046 - val_loss: 0.4364\n",
      "[2022_04_21-15:23:43] Training the entire fine-tuned model...\n",
      "[2022_04_21-15:23:51] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5106WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0979s vs `on_train_batch_end` time: 0.1296s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0979s vs `on_train_batch_end` time: 0.1296s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 634ms/step - loss: 0.5106 - val_loss: 0.4481\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.4079 - val_loss: 0.4273\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3909 - val_loss: 0.4202\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 290ms/step - loss: 0.3679 - val_loss: 0.4185\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3422 - val_loss: 0.4252\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 289ms/step - loss: 0.3439 - val_loss: 0.4583\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 267ms/step - loss: 0.3515 - val_loss: 0.4122\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 288ms/step - loss: 0.3030 - val_loss: 0.4970\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3380 - val_loss: 0.4065\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 290ms/step - loss: 0.2814 - val_loss: 0.5204\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.2917 - val_loss: 0.4340\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.2234 - val_loss: 0.4345\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.1833 - val_loss: 0.4397\n",
      "[2022_04_21-15:24:22] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-15:24:22] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:24:39] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.2560WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0999s vs `on_train_batch_end` time: 0.1296s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0999s vs `on_train_batch_end` time: 0.1296s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 627ms/step - loss: 0.2560 - val_loss: 0.4076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-15:25:14] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3r2xnttm) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 3847... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██▁▁▂▂▃▃▄▄▅▅▆▆▇▁</td></tr><tr><td>loss</td><td>█▆▄▄▄▃▃▄▃▃▃▃▃▃▃▃▄▃▃▃▃▃▃▂▃▂▂▁▁▂</td></tr><tr><td>lr</td><td>███████████████▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▂▂▂▂▂▂▁▂▂▃▂▂▁▁▁▁▂▁▃▁▃▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_val_loss</td><td>0.40651</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.25602</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.40759</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">youthful-oath-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/3r2xnttm\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/3r2xnttm</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_152301-3r2xnttm/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3r2xnttm). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/ehma16sp\" target=\"_blank\">visionary-violet-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-15:25:33] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:25:33] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:25:33] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 645ms/step - loss: 0.7652 - val_loss: 0.6116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5288 - val_loss: 0.5035\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5205 - val_loss: 0.5193\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4668 - val_loss: 0.4918\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4511 - val_loss: 0.4533\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4274 - val_loss: 0.4454\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4140 - val_loss: 0.4553\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4073 - val_loss: 0.4328\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3945 - val_loss: 0.4301\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3908 - val_loss: 0.4284\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3857 - val_loss: 0.4250\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3737 - val_loss: 0.4227\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3833 - val_loss: 0.4537\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4044 - val_loss: 0.4299\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3866 - val_loss: 0.4476\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3846 - val_loss: 0.4228\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3626 - val_loss: 0.4316\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3669 - val_loss: 0.4249\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_21-15:25:58] Training the entire fine-tuned model...\n",
      "[2022_04_21-15:26:06] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 777ms/step - loss: 0.3742 - val_loss: 0.4228\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3558 - val_loss: 0.4226\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3788 - val_loss: 0.4225\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3660 - val_loss: 0.4224\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3730 - val_loss: 0.4222\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 328ms/step - loss: 0.3687 - val_loss: 0.4221\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3760 - val_loss: 0.4219\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3742 - val_loss: 0.4217\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3669 - val_loss: 0.4214\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 327ms/step - loss: 0.3659 - val_loss: 0.4213\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3676 - val_loss: 0.4211\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3635 - val_loss: 0.4210\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3624 - val_loss: 0.4209\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 328ms/step - loss: 0.3671 - val_loss: 0.4208\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3602 - val_loss: 0.4207\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3720 - val_loss: 0.4206\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3718 - val_loss: 0.4204\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3627 - val_loss: 0.4202\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3617 - val_loss: 0.4201\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3569 - val_loss: 0.4200\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3711 - val_loss: 0.4199\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 2s 309ms/step - loss: 0.3659 - val_loss: 0.4198\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3575 - val_loss: 0.4197\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 2s 329ms/step - loss: 0.3468 - val_loss: 0.4196\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3579 - val_loss: 0.4196\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3630 - val_loss: 0.4194\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3596 - val_loss: 0.4193\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3619 - val_loss: 0.4192\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3523 - val_loss: 0.4190\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3492 - val_loss: 0.4191\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3523 - val_loss: 0.4190\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3497 - val_loss: 0.4193\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 2s 330ms/step - loss: 0.3520 - val_loss: 0.4192\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3521 - val_loss: 0.4191\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3513 - val_loss: 0.4190\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 2s 328ms/step - loss: 0.3588 - val_loss: 0.4190\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3584 - val_loss: 0.4190\n",
      "[2022_04_21-15:27:13] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-15:27:13] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:27:14] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 1s/step - loss: 0.3507 - val_loss: 0.4190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-15:27:48] Test set: Filtered out 0 of 406 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ehma16sp) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 4099... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁</td></tr><tr><td>loss</td><td>█▄▄▃▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▅▂▂▂▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.41896</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35068</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41896</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">visionary-violet-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/ehma16sp\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/ehma16sp</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_152517-ehma16sp/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ehma16sp). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/ysx69536\" target=\"_blank\">curious-leaf-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-15:28:10] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:28:10] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:28:10] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 514ms/step - loss: 0.9599 - val_loss: 1.0930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.7043 - val_loss: 0.7957\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.6257 - val_loss: 0.5384\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4868 - val_loss: 0.4927\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4545 - val_loss: 0.4771\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4504 - val_loss: 0.4968\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4585 - val_loss: 0.5258\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4277 - val_loss: 0.4664\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4239 - val_loss: 0.4545\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4040 - val_loss: 0.5028\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.4087 - val_loss: 0.4714\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.4164 - val_loss: 0.4432\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3996 - val_loss: 0.4526\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3869 - val_loss: 0.4784\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4101 - val_loss: 0.4435\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3721 - val_loss: 0.4326\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3747 - val_loss: 0.4325\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3666 - val_loss: 0.4317\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3679 - val_loss: 0.4305\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3614 - val_loss: 0.4323\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3679 - val_loss: 0.4289\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3640 - val_loss: 0.4283\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3611 - val_loss: 0.4277\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3740 - val_loss: 0.4304\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3525 - val_loss: 0.4272\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3558 - val_loss: 0.4270\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3605 - val_loss: 0.4260\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3657 - val_loss: 0.4252\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3608 - val_loss: 0.4265\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3565 - val_loss: 0.4244\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3607 - val_loss: 0.4250\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3626 - val_loss: 0.4280\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3487 - val_loss: 0.4263\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3519 - val_loss: 0.4254\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3509 - val_loss: 0.4276\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3520 - val_loss: 0.4282\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-15:28:50] Training the entire fine-tuned model...\n",
      "[2022_04_21-15:28:59] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3592WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0997s vs `on_train_batch_end` time: 0.1275s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0997s vs `on_train_batch_end` time: 0.1275s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 628ms/step - loss: 0.3592 - val_loss: 0.4244\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 270ms/step - loss: 0.3568 - val_loss: 0.4305\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3561 - val_loss: 0.4272\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3531 - val_loss: 0.4235\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3534 - val_loss: 0.4234\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3487 - val_loss: 0.4240\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3500 - val_loss: 0.4239\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3580 - val_loss: 0.4237\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3566 - val_loss: 0.4236\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3465 - val_loss: 0.4235\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3444 - val_loss: 0.4234\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3469 - val_loss: 0.4235\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3433 - val_loss: 0.4235\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3473 - val_loss: 0.4235\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3564 - val_loss: 0.4235\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3470 - val_loss: 0.4235\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3521 - val_loss: 0.4234\n",
      "[2022_04_21-15:29:35] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-15:29:35] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:29:35] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3459WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0995s vs `on_train_batch_end` time: 0.1292s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0995s vs `on_train_batch_end` time: 0.1292s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 637ms/step - loss: 0.3459 - val_loss: 0.4233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-15:30:09] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ysx69536) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 4489... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▅▄▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▂▂▂▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.42334</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34594</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42334</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">curious-leaf-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/ysx69536\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/ysx69536</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_152750-ysx69536/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ysx69536). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/3s0v1xet\" target=\"_blank\">devout-water-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-15:32:28] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:32:28] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:32:28] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 640ms/step - loss: 0.7812 - val_loss: 0.6592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 180ms/step - loss: 0.5562 - val_loss: 0.4848\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4771 - val_loss: 0.4738\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4516 - val_loss: 0.4550\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4332 - val_loss: 0.4558\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4308 - val_loss: 0.4568\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4225 - val_loss: 0.4344\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.4063 - val_loss: 0.4342\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4071 - val_loss: 0.4293\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3924 - val_loss: 0.4235\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3913 - val_loss: 0.4222\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.3903 - val_loss: 0.4221\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3695 - val_loss: 0.4192\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3581 - val_loss: 0.4280\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3740 - val_loss: 0.4215\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3710 - val_loss: 0.4208\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3575 - val_loss: 0.4173\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3625 - val_loss: 0.4174\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3735 - val_loss: 0.4199\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3584 - val_loss: 0.4184\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3567 - val_loss: 0.4168\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3453 - val_loss: 0.4167\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3534 - val_loss: 0.4177\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3572 - val_loss: 0.4171\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3566 - val_loss: 0.4167\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3573 - val_loss: 0.4164\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3626 - val_loss: 0.4163\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3436 - val_loss: 0.4162\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3572 - val_loss: 0.4163\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3459 - val_loss: 0.4163\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3628 - val_loss: 0.4163\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3488 - val_loss: 0.4163\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3550 - val_loss: 0.4163\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3601 - val_loss: 0.4163\n",
      "[2022_04_21-15:33:05] Training the entire fine-tuned model...\n",
      "[2022_04_21-15:33:27] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 753ms/step - loss: 0.3392 - val_loss: 0.4183\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 301ms/step - loss: 0.3488 - val_loss: 0.4167\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3552 - val_loss: 0.4183\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 300ms/step - loss: 0.3429 - val_loss: 0.4224\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 298ms/step - loss: 0.3517 - val_loss: 0.4147\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3439 - val_loss: 0.4143\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 300ms/step - loss: 0.3472 - val_loss: 0.4137\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3337 - val_loss: 0.4132\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 300ms/step - loss: 0.3412 - val_loss: 0.4137\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 301ms/step - loss: 0.3356 - val_loss: 0.4130\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 328ms/step - loss: 0.3346 - val_loss: 0.4133\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 327ms/step - loss: 0.3236 - val_loss: 0.4136\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 327ms/step - loss: 0.3226 - val_loss: 0.4150\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 298ms/step - loss: 0.3311 - val_loss: 0.4139\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 299ms/step - loss: 0.3249 - val_loss: 0.4152\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 299ms/step - loss: 0.3218 - val_loss: 0.4150\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_21-15:33:59] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-15:33:59] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:33:59] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 11s 767ms/step - loss: 0.3356 - val_loss: 0.4126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-15:34:33] Test set: Filtered out 0 of 406 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3s0v1xet) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 4879... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▄▄▄▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.41262</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33564</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41262</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">devout-water-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/3s0v1xet\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/3s0v1xet</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_153012-3s0v1xet/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3s0v1xet). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/mj51lt1n\" target=\"_blank\">dazzling-wildflower-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-15:34:52] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:34:52] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:34:52] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 484ms/step - loss: 1.0437 - val_loss: 1.1348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.8474 - val_loss: 0.7548\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.5935 - val_loss: 0.6191\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.5132 - val_loss: 0.5432\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4779 - val_loss: 0.4706\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4523 - val_loss: 0.4654\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4354 - val_loss: 0.4798\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4322 - val_loss: 0.4841\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4244 - val_loss: 0.4673\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4068 - val_loss: 0.4528\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4049 - val_loss: 0.4523\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4022 - val_loss: 0.4547\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4069 - val_loss: 0.4494\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3920 - val_loss: 0.4485\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3930 - val_loss: 0.4493\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.3991 - val_loss: 0.4476\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3884 - val_loss: 0.4455\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3917 - val_loss: 0.4456\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3880 - val_loss: 0.4461\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 0.3829 - val_loss: 0.4436\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3860 - val_loss: 0.4423\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3859 - val_loss: 0.4443\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3819 - val_loss: 0.4403\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3799 - val_loss: 0.4390\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3839 - val_loss: 0.4380\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3857 - val_loss: 0.4381\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3701 - val_loss: 0.4373\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3800 - val_loss: 0.4381\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3794 - val_loss: 0.4372\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3706 - val_loss: 0.4364\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3745 - val_loss: 0.4354\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 0.3676 - val_loss: 0.4353\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.3688 - val_loss: 0.4361\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3669 - val_loss: 0.4339\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3660 - val_loss: 0.4338\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3724 - val_loss: 0.4323\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3706 - val_loss: 0.4312\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3642 - val_loss: 0.4308\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3633 - val_loss: 0.4318\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3594 - val_loss: 0.4296\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3580 - val_loss: 0.4285\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3561 - val_loss: 0.4316\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3629 - val_loss: 0.4271\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3584 - val_loss: 0.4265\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3566 - val_loss: 0.4260\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3518 - val_loss: 0.4260\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3543 - val_loss: 0.4268\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3483 - val_loss: 0.4265\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3531 - val_loss: 0.4277\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3471 - val_loss: 0.4271\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3562 - val_loss: 0.4268\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3532 - val_loss: 0.4269\n",
      "[2022_04_21-15:35:46] Training the entire fine-tuned model...\n",
      "[2022_04_21-15:35:55] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3600WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0986s vs `on_train_batch_end` time: 0.1316s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0986s vs `on_train_batch_end` time: 0.1316s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 635ms/step - loss: 0.3600 - val_loss: 0.4440\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3607 - val_loss: 0.4247\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 268ms/step - loss: 0.3489 - val_loss: 0.4229\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 268ms/step - loss: 0.3549 - val_loss: 0.4237\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3530 - val_loss: 0.4220\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 289ms/step - loss: 0.3428 - val_loss: 0.4285\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3242 - val_loss: 0.4246\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3353 - val_loss: 0.4284\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3250 - val_loss: 0.4279\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 289ms/step - loss: 0.3279 - val_loss: 0.4248\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3322 - val_loss: 0.4253\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_21-15:36:22] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-15:36:22] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:36:22] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3430WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1009s vs `on_train_batch_end` time: 0.1284s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1009s vs `on_train_batch_end` time: 0.1284s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 637ms/step - loss: 0.3430 - val_loss: 0.4221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-15:36:55] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:mj51lt1n) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 5235... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▁</td></tr><tr><td>loss</td><td>█▆▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>4</td></tr><tr><td>best_val_loss</td><td>0.42202</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34301</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42206</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dazzling-wildflower-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/mj51lt1n\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/mj51lt1n</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_153436-mj51lt1n/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:mj51lt1n). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/8ljqc336\" target=\"_blank\">fancy-firefly-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-15:37:15] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:37:15] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:37:15] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 653ms/step - loss: 0.9997 - val_loss: 0.4975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.6480 - val_loss: 0.6974\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.6218 - val_loss: 0.4978\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5594 - val_loss: 0.5618\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5362 - val_loss: 0.5085\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4642 - val_loss: 0.4801\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4659 - val_loss: 0.4598\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4488 - val_loss: 0.4654\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4352 - val_loss: 0.4522\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4340 - val_loss: 0.4527\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4253 - val_loss: 0.4487\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4256 - val_loss: 0.4480\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4232 - val_loss: 0.4455\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4174 - val_loss: 0.4446\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4164 - val_loss: 0.4427\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4176 - val_loss: 0.4426\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.4092 - val_loss: 0.4403\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4107 - val_loss: 0.4392\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4060 - val_loss: 0.4391\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.4048 - val_loss: 0.4369\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4026 - val_loss: 0.4366\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4111 - val_loss: 0.4363\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.4005 - val_loss: 0.4337\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3982 - val_loss: 0.4330\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.4000 - val_loss: 0.4319\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3966 - val_loss: 0.4315\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3909 - val_loss: 0.4349\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.3945 - val_loss: 0.4295\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3914 - val_loss: 0.4291\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3886 - val_loss: 0.4294\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.3796 - val_loss: 0.4278\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.3930 - val_loss: 0.4266\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.3879 - val_loss: 0.4262\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.3820 - val_loss: 0.4286\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.3784 - val_loss: 0.4249\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.3771 - val_loss: 0.4252\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.3769 - val_loss: 0.4279\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 0.3877 - val_loss: 0.4242\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3765 - val_loss: 0.4231\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3745 - val_loss: 0.4227\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.3656 - val_loss: 0.4217\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3831 - val_loss: 0.4218\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3773 - val_loss: 0.4220\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3675 - val_loss: 0.4204\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3772 - val_loss: 0.4189\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3691 - val_loss: 0.4226\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3672 - val_loss: 0.4194\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3641 - val_loss: 0.4197\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3659 - val_loss: 0.4202\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3711 - val_loss: 0.4182\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3676 - val_loss: 0.4181\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3700 - val_loss: 0.4181\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3587 - val_loss: 0.4180\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.3580 - val_loss: 0.4178\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3646 - val_loss: 0.4178\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3593 - val_loss: 0.4180\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3693 - val_loss: 0.4178\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3658 - val_loss: 0.4177\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3546 - val_loss: 0.4177\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3645 - val_loss: 0.4177\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3586 - val_loss: 0.4176\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3588 - val_loss: 0.4174\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3604 - val_loss: 0.4174\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3569 - val_loss: 0.4174\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3586 - val_loss: 0.4173\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3633 - val_loss: 0.4173\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3553 - val_loss: 0.4173\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3540 - val_loss: 0.4174\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3560 - val_loss: 0.4174\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3620 - val_loss: 0.4174\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3640 - val_loss: 0.4174\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3516 - val_loss: 0.4173\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3599 - val_loss: 0.4173\n",
      "[2022_04_21-15:38:24] Training the entire fine-tuned model...\n",
      "[2022_04_21-15:38:32] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 752ms/step - loss: 0.3577 - val_loss: 0.4287\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3725 - val_loss: 0.4162\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 327ms/step - loss: 0.3648 - val_loss: 0.4174\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3523 - val_loss: 0.4142\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3453 - val_loss: 0.4156\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3403 - val_loss: 0.4162\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3401 - val_loss: 0.4161\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3338 - val_loss: 0.4151\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3312 - val_loss: 0.4160\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3374 - val_loss: 0.4167\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_21-15:38:56] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-15:38:56] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:38:56] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 737ms/step - loss: 0.3470 - val_loss: 0.4134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-15:39:30] Test set: Filtered out 0 of 406 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:8ljqc336) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 5656... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇███▁▁▁▂▁</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇▇█▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.41344</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34696</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.41344</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fancy-firefly-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/8ljqc336\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/8ljqc336</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_153658-8ljqc336/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:8ljqc336). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/2gofp0zt\" target=\"_blank\">graceful-snow-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-15:39:48] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:39:48] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:39:48] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 509ms/step - loss: 0.8323 - val_loss: 0.5064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.5745 - val_loss: 0.5832\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.5947 - val_loss: 0.6314\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.5121 - val_loss: 0.4783\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4667 - val_loss: 0.4650\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4449 - val_loss: 0.4811\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.4200 - val_loss: 0.4689\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4203 - val_loss: 0.4649\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4031 - val_loss: 0.4646\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4155 - val_loss: 0.4533\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3942 - val_loss: 0.4531\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3890 - val_loss: 0.4456\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4088 - val_loss: 0.4534\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3986 - val_loss: 0.4377\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3763 - val_loss: 0.4449\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 0.3626 - val_loss: 0.4541\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3895 - val_loss: 0.4318\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3525 - val_loss: 0.4314\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3613 - val_loss: 0.4453\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3689 - val_loss: 0.4442\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3676 - val_loss: 0.4352\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3433 - val_loss: 0.4446\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3544 - val_loss: 0.4286\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3419 - val_loss: 0.4281\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3498 - val_loss: 0.4331\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3474 - val_loss: 0.4273\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3416 - val_loss: 0.4295\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3417 - val_loss: 0.4290\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3462 - val_loss: 0.4257\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3442 - val_loss: 0.4273\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3409 - val_loss: 0.4252\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3546 - val_loss: 0.4243\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3392 - val_loss: 0.4239\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3381 - val_loss: 0.4324\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3400 - val_loss: 0.4277\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3489 - val_loss: 0.4291\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3329 - val_loss: 0.4254\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3404 - val_loss: 0.4248\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3245 - val_loss: 0.4257\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-15:40:32] Training the entire fine-tuned model...\n",
      "[2022_04_21-15:40:41] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3412WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1003s vs `on_train_batch_end` time: 0.1285s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1003s vs `on_train_batch_end` time: 0.1285s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 631ms/step - loss: 0.3412 - val_loss: 0.4334\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 267ms/step - loss: 0.3366 - val_loss: 0.4244\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3198 - val_loss: 0.4269\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 287ms/step - loss: 0.3263 - val_loss: 0.4487\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 267ms/step - loss: 0.3273 - val_loss: 0.4243\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3347 - val_loss: 0.4258\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.2976 - val_loss: 0.4269\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.2997 - val_loss: 0.4260\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 267ms/step - loss: 0.2961 - val_loss: 0.4247\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.2930 - val_loss: 0.4352\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.2882 - val_loss: 0.4252\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_21-15:41:07] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-15:41:07] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:41:07] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3133WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1032s vs `on_train_batch_end` time: 0.1261s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1032s vs `on_train_batch_end` time: 0.1261s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 651ms/step - loss: 0.3133 - val_loss: 0.4184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-15:41:41] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2gofp0zt) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 6175... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇██▁▁▂▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▅▅▄▃▃▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▂▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄▆█▃▃▃▃▃▂▂▂▂▂▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.41837</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31329</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.41837</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">graceful-snow-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/2gofp0zt\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/2gofp0zt</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_153932-2gofp0zt/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2gofp0zt). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/3k7zkmn2\" target=\"_blank\">lively-plasma-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-15:42:01] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:42:01] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:42:01] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 639ms/step - loss: 0.8956 - val_loss: 0.7445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.6038 - val_loss: 0.6457\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.5454 - val_loss: 0.5370\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5343 - val_loss: 0.4798\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4524 - val_loss: 0.4489\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4403 - val_loss: 0.4418\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.4180 - val_loss: 0.4393\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4136 - val_loss: 0.4337\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4025 - val_loss: 0.4322\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4029 - val_loss: 0.4386\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3914 - val_loss: 0.4507\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4045 - val_loss: 0.4284\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3825 - val_loss: 0.4249\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3619 - val_loss: 0.4430\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3853 - val_loss: 0.4203\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3720 - val_loss: 0.4238\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3646 - val_loss: 0.4272\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3568 - val_loss: 0.4209\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3622 - val_loss: 0.4213\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3573 - val_loss: 0.4172\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3608 - val_loss: 0.4177\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3508 - val_loss: 0.4202\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3528 - val_loss: 0.4168\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3573 - val_loss: 0.4170\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3596 - val_loss: 0.4191\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3462 - val_loss: 0.4169\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3432 - val_loss: 0.4163\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3491 - val_loss: 0.4162\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3502 - val_loss: 0.4162\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3609 - val_loss: 0.4162\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3638 - val_loss: 0.4162\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3511 - val_loss: 0.4162\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3509 - val_loss: 0.4161\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3579 - val_loss: 0.4160\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3538 - val_loss: 0.4160\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3541 - val_loss: 0.4160\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3541 - val_loss: 0.4161\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3524 - val_loss: 0.4161\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3619 - val_loss: 0.4161\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3493 - val_loss: 0.4161\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3481 - val_loss: 0.4161\n",
      "[2022_04_21-15:42:43] Training the entire fine-tuned model...\n",
      "[2022_04_21-15:42:53] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 776ms/step - loss: 0.6108 - val_loss: 0.7187\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.5172 - val_loss: 0.4765\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.4058 - val_loss: 0.4691\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3900 - val_loss: 0.4325\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3690 - val_loss: 0.4227\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3566 - val_loss: 0.4092\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3469 - val_loss: 0.4072\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3411 - val_loss: 0.4050\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3300 - val_loss: 0.4041\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3132 - val_loss: 0.4016\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 326ms/step - loss: 0.3197 - val_loss: 0.4071\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.2818 - val_loss: 0.4052\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.2855 - val_loss: 0.4125\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.2585 - val_loss: 0.4117\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.2689 - val_loss: 0.4114\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.2541 - val_loss: 0.4119\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_21-15:43:26] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-15:43:26] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:43:26] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 757ms/step - loss: 0.2996 - val_loss: 0.4039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-15:44:00] Test set: Filtered out 0 of 406 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3k7zkmn2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 6543... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▄▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▄▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>9</td></tr><tr><td>best_val_loss</td><td>0.40158</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.29958</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.40391</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">lively-plasma-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/3k7zkmn2\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/3k7zkmn2</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_154144-3k7zkmn2/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3k7zkmn2). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/3qd86grj\" target=\"_blank\">prime-plant-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-15:44:19] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:44:19] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:44:19] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 513ms/step - loss: 0.8517 - val_loss: 0.6300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.5601 - val_loss: 0.4753\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.5022 - val_loss: 0.5003\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4634 - val_loss: 0.4666\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4404 - val_loss: 0.4784\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4338 - val_loss: 0.5052\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.4477 - val_loss: 0.4539\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4417 - val_loss: 0.5262\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4520 - val_loss: 0.5102\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4124 - val_loss: 0.4429\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3891 - val_loss: 0.4449\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3895 - val_loss: 0.4415\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.4076 - val_loss: 0.4903\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4122 - val_loss: 0.4409\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3951 - val_loss: 0.4799\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3964 - val_loss: 0.4530\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3727 - val_loss: 0.4816\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3826 - val_loss: 0.4623\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3836 - val_loss: 0.4454\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3635 - val_loss: 0.4452\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_21-15:44:46] Training the entire fine-tuned model...\n",
      "[2022_04_21-15:44:58] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4888WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0989s vs `on_train_batch_end` time: 0.1288s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0989s vs `on_train_batch_end` time: 0.1288s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 623ms/step - loss: 0.4888 - val_loss: 0.4925\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.4161 - val_loss: 0.4225\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3781 - val_loss: 0.4183\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 267ms/step - loss: 0.3734 - val_loss: 0.4075\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3514 - val_loss: 0.4117\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3337 - val_loss: 0.4263\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3250 - val_loss: 0.4683\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 268ms/step - loss: 0.3467 - val_loss: 0.4102\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3235 - val_loss: 0.4092\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3358 - val_loss: 0.4232\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_21-15:45:22] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-15:45:22] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:45:22] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3558WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0996s vs `on_train_batch_end` time: 0.1278s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0996s vs `on_train_batch_end` time: 0.1278s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 635ms/step - loss: 0.3558 - val_loss: 0.4092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-15:45:56] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3qd86grj) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 6938... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▂▃▃▄▄▄▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▂▃▃▃▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▄▃▃▄▂▅▄▂▂▂▄▂▃▂▃▃▂▂▄▁▁▁▁▂▃▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>3</td></tr><tr><td>best_val_loss</td><td>0.40746</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35576</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.40923</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">prime-plant-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/3qd86grj\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/3qd86grj</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_154402-3qd86grj/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3qd86grj). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/pqql7ko5\" target=\"_blank\">bright-smoke-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-15:46:16] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:46:16] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:46:16] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 609ms/step - loss: 0.9820 - val_loss: 0.4973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 181ms/step - loss: 0.6152 - val_loss: 0.6295\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.5562 - val_loss: 0.5346\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 0.5102 - val_loss: 0.4921\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4941 - val_loss: 0.4605\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4674 - val_loss: 0.4466\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4378 - val_loss: 0.4598\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4274 - val_loss: 0.4404\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4065 - val_loss: 0.4470\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4043 - val_loss: 0.4344\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3941 - val_loss: 0.4336\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3851 - val_loss: 0.4548\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 0.4079 - val_loss: 0.4799\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 185ms/step - loss: 0.4147 - val_loss: 0.4342\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 185ms/step - loss: 0.3913 - val_loss: 0.4264\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3820 - val_loss: 0.4230\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3740 - val_loss: 0.4308\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3953 - val_loss: 0.4596\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3786 - val_loss: 0.4223\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3607 - val_loss: 0.4177\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3539 - val_loss: 0.4290\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3668 - val_loss: 0.4261\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3601 - val_loss: 0.4285\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3700 - val_loss: 0.4140\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3384 - val_loss: 0.4158\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3429 - val_loss: 0.4602\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3678 - val_loss: 0.4630\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3934 - val_loss: 0.4130\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3600 - val_loss: 0.4154\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3553 - val_loss: 0.4127\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3403 - val_loss: 0.4097\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3305 - val_loss: 0.4105\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3326 - val_loss: 0.4440\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3573 - val_loss: 0.4863\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3772 - val_loss: 0.4679\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3592 - val_loss: 0.4204\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3329 - val_loss: 0.4109\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3261 - val_loss: 0.4194\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3318 - val_loss: 0.4121\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_21-15:46:56] Training the entire fine-tuned model...\n",
      "[2022_04_21-15:47:06] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 754ms/step - loss: 0.3427 - val_loss: 0.4100\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 300ms/step - loss: 0.3342 - val_loss: 0.4097\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3303 - val_loss: 0.4094\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3281 - val_loss: 0.4100\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3280 - val_loss: 0.4094\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3211 - val_loss: 0.4095\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3370 - val_loss: 0.4092\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3267 - val_loss: 0.4093\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3168 - val_loss: 0.4090\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3265 - val_loss: 0.4091\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3209 - val_loss: 0.4102\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3295 - val_loss: 0.4089\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 0.3235 - val_loss: 0.4087\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 332ms/step - loss: 0.3192 - val_loss: 0.4087\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3277 - val_loss: 0.4087\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3187 - val_loss: 0.4087\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3155 - val_loss: 0.4090\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3198 - val_loss: 0.4089\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3290 - val_loss: 0.4088\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3139 - val_loss: 0.4087\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3189 - val_loss: 0.4087\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 309ms/step - loss: 0.3188 - val_loss: 0.4086\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3200 - val_loss: 0.4086\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3073 - val_loss: 0.4086\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 309ms/step - loss: 0.3265 - val_loss: 0.4086\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3096 - val_loss: 0.4086\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3136 - val_loss: 0.4085\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3132 - val_loss: 0.4085\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3184 - val_loss: 0.4085\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 2s 330ms/step - loss: 0.3160 - val_loss: 0.4085\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3144 - val_loss: 0.4085\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 2s 330ms/step - loss: 0.3197 - val_loss: 0.4085\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3126 - val_loss: 0.4085\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3166 - val_loss: 0.4085\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3093 - val_loss: 0.4085\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 2s 329ms/step - loss: 0.3211 - val_loss: 0.4085\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 2s 329ms/step - loss: 0.3119 - val_loss: 0.4085\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 0.3240 - val_loss: 0.4085\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3337 - val_loss: 0.4085\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3162 - val_loss: 0.4085\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 2s 329ms/step - loss: 0.3202 - val_loss: 0.4085\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3165 - val_loss: 0.4085\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3166 - val_loss: 0.4085\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 309ms/step - loss: 0.3236 - val_loss: 0.4085\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3125 - val_loss: 0.4085\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3059 - val_loss: 0.4085\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3125 - val_loss: 0.4085\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3163 - val_loss: 0.4085\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3130 - val_loss: 0.4085\n",
      "[2022_04_21-15:48:29] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-15:48:29] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:48:30] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 752ms/step - loss: 0.3196 - val_loss: 0.4085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-15:49:08] Test set: Filtered out 0 of 406 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:pqql7ko5) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 7194... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▁▁▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██▁</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▆█▄▄▂▄▂▂▂▂▂▁▁▁▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>40</td></tr><tr><td>best_val_loss</td><td>0.4085</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31957</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.40852</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">bright-smoke-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/pqql7ko5\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/pqql7ko5</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_154559-pqql7ko5/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:pqql7ko5). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/2qqh79pt\" target=\"_blank\">comic-sponge-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-15:49:28] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:49:28] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:49:28] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 804ms/step - loss: 0.8584 - val_loss: 0.6906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.6320 - val_loss: 0.4908\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.5084 - val_loss: 0.5168\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4517 - val_loss: 0.5486\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.4719 - val_loss: 0.4792\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.4407 - val_loss: 0.4798\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4303 - val_loss: 0.4551\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4199 - val_loss: 0.4499\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4169 - val_loss: 0.4804\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.4137 - val_loss: 0.4516\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3804 - val_loss: 0.4446\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3950 - val_loss: 0.4995\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3954 - val_loss: 0.4462\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.3992 - val_loss: 0.5049\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.4085 - val_loss: 0.4648\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3799 - val_loss: 0.4346\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3668 - val_loss: 0.4373\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3722 - val_loss: 0.4309\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3648 - val_loss: 0.4286\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3618 - val_loss: 0.4274\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3588 - val_loss: 0.4251\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3595 - val_loss: 0.4251\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3516 - val_loss: 0.4270\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3658 - val_loss: 0.4253\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 139ms/step - loss: 0.3484 - val_loss: 0.4226\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3660 - val_loss: 0.4223\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3637 - val_loss: 0.4205\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3645 - val_loss: 0.4245\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3549 - val_loss: 0.4215\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3584 - val_loss: 0.4260\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3538 - val_loss: 0.4241\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3637 - val_loss: 0.4198\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 0.3463 - val_loss: 0.4245\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3379 - val_loss: 0.4225\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3532 - val_loss: 0.4196\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3585 - val_loss: 0.4202\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 0.3421 - val_loss: 0.4200\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3487 - val_loss: 0.4194\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3433 - val_loss: 0.4195\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3505 - val_loss: 0.4197\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3468 - val_loss: 0.4211\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3463 - val_loss: 0.4200\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3569 - val_loss: 0.4199\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3528 - val_loss: 0.4198\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3545 - val_loss: 0.4195\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 0.3440 - val_loss: 0.4196\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_21-15:50:18] Training the entire fine-tuned model...\n",
      "[2022_04_21-15:50:26] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3515WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1024s vs `on_train_batch_end` time: 0.1295s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1024s vs `on_train_batch_end` time: 0.1295s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 618ms/step - loss: 0.3515 - val_loss: 0.4207\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3502 - val_loss: 0.4189\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3548 - val_loss: 0.4195\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3435 - val_loss: 0.4235\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3423 - val_loss: 0.4215\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3364 - val_loss: 0.4182\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 289ms/step - loss: 0.3497 - val_loss: 0.4179\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3457 - val_loss: 0.4178\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3422 - val_loss: 0.4182\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 267ms/step - loss: 0.3333 - val_loss: 0.4194\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3462 - val_loss: 0.4184\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3376 - val_loss: 0.4179\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3437 - val_loss: 0.4172\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3307 - val_loss: 0.4170\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3337 - val_loss: 0.4169\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3381 - val_loss: 0.4174\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3294 - val_loss: 0.4180\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 270ms/step - loss: 0.3274 - val_loss: 0.4181\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3401 - val_loss: 0.4181\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3352 - val_loss: 0.4179\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3346 - val_loss: 0.4177\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 2s 289ms/step - loss: 0.3371 - val_loss: 0.4176\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3404 - val_loss: 0.4176\n",
      "[2022_04_21-15:51:12] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-15:51:12] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:51:13] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3369WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0975s vs `on_train_batch_end` time: 0.1305s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0975s vs `on_train_batch_end` time: 0.1305s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 637ms/step - loss: 0.3369 - val_loss: 0.4169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-15:51:47] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2qqh79pt) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 7737... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▁▂▂▂▃▃▃▃▄▄▄▁</td></tr><tr><td>loss</td><td>█▅▃▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▄▃▂▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.41687</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33692</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41687</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">comic-sponge-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/2qqh79pt\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/2qqh79pt</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_154910-2qqh79pt/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2qqh79pt). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/jgj2qd97\" target=\"_blank\">woven-voice-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-15:52:07] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:52:07] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:52:07] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 654ms/step - loss: 0.8368 - val_loss: 0.7195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.5906 - val_loss: 0.5971\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5290 - val_loss: 0.4785\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4912 - val_loss: 0.4572\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4615 - val_loss: 0.4817\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4691 - val_loss: 0.4630\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4455 - val_loss: 0.4502\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4160 - val_loss: 0.4312\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4063 - val_loss: 0.4276\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3928 - val_loss: 0.4412\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3922 - val_loss: 0.4365\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3936 - val_loss: 0.4360\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3826 - val_loss: 0.4214\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3723 - val_loss: 0.4195\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 0.3736 - val_loss: 0.4210\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3752 - val_loss: 0.4261\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3761 - val_loss: 0.4341\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3743 - val_loss: 0.4385\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3650 - val_loss: 0.4192\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3634 - val_loss: 0.4163\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3599 - val_loss: 0.4244\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3611 - val_loss: 0.4208\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3610 - val_loss: 0.4159\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3482 - val_loss: 0.4187\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3459 - val_loss: 0.4162\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3617 - val_loss: 0.4160\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3492 - val_loss: 0.4155\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3512 - val_loss: 0.4163\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3471 - val_loss: 0.4158\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3503 - val_loss: 0.4146\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3490 - val_loss: 0.4147\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3518 - val_loss: 0.4141\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3375 - val_loss: 0.4154\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3479 - val_loss: 0.4138\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3495 - val_loss: 0.4136\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3402 - val_loss: 0.4136\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 185ms/step - loss: 0.3336 - val_loss: 0.4133\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3321 - val_loss: 0.4144\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3364 - val_loss: 0.4132\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3408 - val_loss: 0.4152\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3433 - val_loss: 0.4126\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3420 - val_loss: 0.4130\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3380 - val_loss: 0.4125\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3323 - val_loss: 0.4116\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3309 - val_loss: 0.4132\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3315 - val_loss: 0.4121\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3420 - val_loss: 0.4129\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3367 - val_loss: 0.4143\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3440 - val_loss: 0.4127\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3322 - val_loss: 0.4118\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3383 - val_loss: 0.4117\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3314 - val_loss: 0.4119\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-15:53:00] Training the entire fine-tuned model...\n",
      "[2022_04_21-15:53:46] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 740ms/step - loss: 0.3314 - val_loss: 0.4164\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3412 - val_loss: 0.4127\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 327ms/step - loss: 0.3334 - val_loss: 0.4102\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3331 - val_loss: 0.4102\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3346 - val_loss: 0.4103\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3240 - val_loss: 0.4109\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3305 - val_loss: 0.4104\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 329ms/step - loss: 0.3196 - val_loss: 0.4108\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 328ms/step - loss: 0.3272 - val_loss: 0.4104\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 328ms/step - loss: 0.3199 - val_loss: 0.4102\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3059 - val_loss: 0.4102\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3238 - val_loss: 0.4102\n",
      "[2022_04_21-15:54:14] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-15:54:14] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:54:14] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 742ms/step - loss: 0.3294 - val_loss: 0.4101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-15:54:49] Test set: Filtered out 0 of 406 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:jgj2qd97) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 8194... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▂▃▂▁▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.4101</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32944</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.4101</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">woven-voice-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/jgj2qd97\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/jgj2qd97</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_155149-jgj2qd97/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:jgj2qd97). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/jhhm0r9i\" target=\"_blank\">exalted-wave-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-15:55:08] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:55:09] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:55:09] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 532ms/step - loss: 0.8809 - val_loss: 0.8036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.6663 - val_loss: 0.4916\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.5144 - val_loss: 0.4973\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.4530 - val_loss: 0.5086\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4618 - val_loss: 0.4798\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.4343 - val_loss: 0.4852\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4414 - val_loss: 0.4643\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4104 - val_loss: 0.4555\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4314 - val_loss: 0.5153\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4359 - val_loss: 0.4487\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3958 - val_loss: 0.4453\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3910 - val_loss: 0.4416\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3966 - val_loss: 0.4383\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.3845 - val_loss: 0.4385\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3729 - val_loss: 0.4333\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3658 - val_loss: 0.4346\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3760 - val_loss: 0.4515\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3801 - val_loss: 0.4236\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.3665 - val_loss: 0.4239\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 127ms/step - loss: 0.3576 - val_loss: 0.4223\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3517 - val_loss: 0.4308\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3746 - val_loss: 0.4511\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3864 - val_loss: 0.4768\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3730 - val_loss: 0.4329\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3365 - val_loss: 0.4402\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3501 - val_loss: 0.4238\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3453 - val_loss: 0.4208\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3463 - val_loss: 0.4208\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3472 - val_loss: 0.4231\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3413 - val_loss: 0.4183\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3383 - val_loss: 0.4213\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3383 - val_loss: 0.4181\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3374 - val_loss: 0.4236\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3378 - val_loss: 0.4173\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3393 - val_loss: 0.4182\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3369 - val_loss: 0.4186\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3339 - val_loss: 0.4151\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3317 - val_loss: 0.4174\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3431 - val_loss: 0.4150\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3355 - val_loss: 0.4145\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3274 - val_loss: 0.4139\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3169 - val_loss: 0.4181\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3393 - val_loss: 0.4156\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3181 - val_loss: 0.4147\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3240 - val_loss: 0.4163\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3210 - val_loss: 0.4156\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3316 - val_loss: 0.4145\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3238 - val_loss: 0.4144\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3198 - val_loss: 0.4151\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-15:55:59] Training the entire fine-tuned model...\n",
      "[2022_04_21-15:56:07] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3450WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1281s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1281s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 620ms/step - loss: 0.3450 - val_loss: 0.4159\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3333 - val_loss: 0.4134\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3190 - val_loss: 0.4136\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3187 - val_loss: 0.4269\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 267ms/step - loss: 0.3205 - val_loss: 0.4158\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 287ms/step - loss: 0.3168 - val_loss: 0.4210\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3087 - val_loss: 0.4210\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 267ms/step - loss: 0.3037 - val_loss: 0.4153\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3088 - val_loss: 0.4146\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.2972 - val_loss: 0.4175\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_21-15:56:33] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-15:56:33] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:56:33] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3197WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0986s vs `on_train_batch_end` time: 0.1293s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0986s vs `on_train_batch_end` time: 0.1293s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 825ms/step - loss: 0.3197 - val_loss: 0.4153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-15:57:06] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:jhhm0r9i) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 8631... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▂▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▃▂▂▂▂▂▁▁▁▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_val_loss</td><td>0.41344</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31974</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41533</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">exalted-wave-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/jhhm0r9i\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/jhhm0r9i</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_155452-jhhm0r9i/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:jhhm0r9i). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/38qfrygi\" target=\"_blank\">elated-vortex-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-15:57:24] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:57:24] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:57:25] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 621ms/step - loss: 0.7672 - val_loss: 0.7603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.6114 - val_loss: 0.5485\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 0.4961 - val_loss: 0.4723\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.4758 - val_loss: 0.4843\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 0.4693 - val_loss: 0.5091\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 0.4625 - val_loss: 0.4493\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 0.4181 - val_loss: 0.4384\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.4150 - val_loss: 0.4535\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 0.4027 - val_loss: 0.4303\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 172ms/step - loss: 0.3913 - val_loss: 0.4290\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 0.3843 - val_loss: 0.4257\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 172ms/step - loss: 0.3918 - val_loss: 0.4365\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.3828 - val_loss: 0.4540\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 0.3918 - val_loss: 0.4189\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.3797 - val_loss: 0.4201\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.3679 - val_loss: 0.4197\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3737 - val_loss: 0.4278\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3601 - val_loss: 0.4187\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3571 - val_loss: 0.4325\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3510 - val_loss: 0.4135\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.3506 - val_loss: 0.4268\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3606 - val_loss: 0.4182\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3495 - val_loss: 0.4164\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.3466 - val_loss: 0.4132\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3352 - val_loss: 0.4126\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3275 - val_loss: 0.4125\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3392 - val_loss: 0.4268\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.3417 - val_loss: 0.4122\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3285 - val_loss: 0.4112\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3377 - val_loss: 0.4195\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.3382 - val_loss: 0.4247\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.3412 - val_loss: 0.4159\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3402 - val_loss: 0.4112\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3242 - val_loss: 0.4095\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3352 - val_loss: 0.4135\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.3285 - val_loss: 0.4097\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.3235 - val_loss: 0.4132\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3255 - val_loss: 0.4086\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3281 - val_loss: 0.4087\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3124 - val_loss: 0.4131\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3130 - val_loss: 0.4081\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3276 - val_loss: 0.4087\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3246 - val_loss: 0.4189\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3132 - val_loss: 0.4097\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3299 - val_loss: 0.4081\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3198 - val_loss: 0.4077\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3271 - val_loss: 0.4083\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3185 - val_loss: 0.4085\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3153 - val_loss: 0.4076\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.2992 - val_loss: 0.4075\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3081 - val_loss: 0.4076\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3081 - val_loss: 0.4094\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3172 - val_loss: 0.4095\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3298 - val_loss: 0.4088\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 181ms/step - loss: 0.3196 - val_loss: 0.4082\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3008 - val_loss: 0.4080\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3274 - val_loss: 0.4078\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3101 - val_loss: 0.4078\n",
      "[2022_04_21-15:58:18] Training the entire fine-tuned model...\n",
      "[2022_04_21-15:58:28] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 767ms/step - loss: 0.3287 - val_loss: 0.4131\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3152 - val_loss: 0.4063\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3207 - val_loss: 0.4051\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 309ms/step - loss: 0.3062 - val_loss: 0.4046\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.2924 - val_loss: 0.4052\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.2975 - val_loss: 0.4161\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3128 - val_loss: 0.4167\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.2931 - val_loss: 0.4163\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 328ms/step - loss: 0.3012 - val_loss: 0.4063\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.2961 - val_loss: 0.4122\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.2870 - val_loss: 0.4078\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.2998 - val_loss: 0.4110\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_21-15:58:55] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-15:58:55] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-15:58:55] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 749ms/step - loss: 0.3101 - val_loss: 0.4048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-15:59:51] Test set: Filtered out 0 of 406 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:38qfrygi) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9033... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▆▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>3</td></tr><tr><td>best_val_loss</td><td>0.40464</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31008</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.40481</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">elated-vortex-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/38qfrygi\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/38qfrygi</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_155708-38qfrygi/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:38qfrygi). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/1tltqv6z\" target=\"_blank\">ethereal-salad-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-16:00:16] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:00:16] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:00:16] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 512ms/step - loss: 0.7900 - val_loss: 0.6874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.5976 - val_loss: 0.4797\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4836 - val_loss: 0.5014\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4496 - val_loss: 0.4724\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4394 - val_loss: 0.4673\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4243 - val_loss: 0.4656\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4179 - val_loss: 0.4570\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3997 - val_loss: 0.4573\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4079 - val_loss: 0.4522\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3963 - val_loss: 0.4431\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3810 - val_loss: 0.4687\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3997 - val_loss: 0.4643\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3771 - val_loss: 0.4321\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3728 - val_loss: 0.4304\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3668 - val_loss: 0.4310\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3602 - val_loss: 0.4300\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3522 - val_loss: 0.4357\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3510 - val_loss: 0.4292\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3707 - val_loss: 0.4343\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3661 - val_loss: 0.4511\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3724 - val_loss: 0.5248\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.4605 - val_loss: 0.4432\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3598 - val_loss: 0.4298\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3468 - val_loss: 0.4421\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3345 - val_loss: 0.4279\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3476 - val_loss: 0.4292\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3436 - val_loss: 0.4257\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3361 - val_loss: 0.4299\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 161ms/step - loss: 0.3314 - val_loss: 0.4236\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3408 - val_loss: 0.4227\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3412 - val_loss: 0.4250\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3406 - val_loss: 0.4212\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3385 - val_loss: 0.4273\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3305 - val_loss: 0.4199\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3341 - val_loss: 0.4187\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3317 - val_loss: 0.4204\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3416 - val_loss: 0.4168\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.3224 - val_loss: 0.4171\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3179 - val_loss: 0.4175\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3129 - val_loss: 0.4163\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3300 - val_loss: 0.4211\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3265 - val_loss: 0.4167\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3359 - val_loss: 0.4176\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3499 - val_loss: 0.4230\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3189 - val_loss: 0.4170\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3194 - val_loss: 0.4177\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3223 - val_loss: 0.4172\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3110 - val_loss: 0.4238\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-16:01:06] Training the entire fine-tuned model...\n",
      "[2022_04_21-16:01:14] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3196WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1023s vs `on_train_batch_end` time: 0.1300s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1023s vs `on_train_batch_end` time: 0.1300s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 640ms/step - loss: 0.3196 - val_loss: 0.4357\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3533 - val_loss: 0.4212\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3304 - val_loss: 0.4122\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 288ms/step - loss: 0.3151 - val_loss: 0.4148\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3236 - val_loss: 0.4134\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3002 - val_loss: 0.4159\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.2870 - val_loss: 0.4113\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.2953 - val_loss: 0.4277\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3154 - val_loss: 0.5144\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3268 - val_loss: 0.4692\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3005 - val_loss: 0.4730\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3002 - val_loss: 0.4217\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 268ms/step - loss: 0.2788 - val_loss: 0.4177\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.2740 - val_loss: 0.4129\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.2651 - val_loss: 0.4184\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_21-16:01:49] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-16:01:49] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:01:49] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3008WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0998s vs `on_train_batch_end` time: 0.1286s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0998s vs `on_train_batch_end` time: 0.1286s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 632ms/step - loss: 0.3008 - val_loss: 0.4115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-16:02:24] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1tltqv6z) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9554... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▃▃▃▂▂▂▂▂▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▂▂▂▂▂▂▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▄▃▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>6</td></tr><tr><td>best_val_loss</td><td>0.4113</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30083</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.41152</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">ethereal-salad-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/1tltqv6z\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/1tltqv6z</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_155953-1tltqv6z/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1tltqv6z). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018a/runs/22w62359\" target=\"_blank\">serene-cherry-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-16:02:44] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:02:44] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:02:44] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 617ms/step - loss: 0.9204 - val_loss: 0.5528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5928 - val_loss: 0.5980\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5240 - val_loss: 0.4865\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4718 - val_loss: 0.4622\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.4355 - val_loss: 0.4529\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4286 - val_loss: 0.4548\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4205 - val_loss: 0.4395\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4086 - val_loss: 0.4348\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4007 - val_loss: 0.4381\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4052 - val_loss: 0.4295\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3827 - val_loss: 0.4331\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3875 - val_loss: 0.4524\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3921 - val_loss: 0.4640\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3919 - val_loss: 0.4783\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3869 - val_loss: 0.4429\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3902 - val_loss: 0.4276\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3674 - val_loss: 0.4301\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3733 - val_loss: 0.4256\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3664 - val_loss: 0.4239\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3637 - val_loss: 0.4252\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3689 - val_loss: 0.4241\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3695 - val_loss: 0.4223\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3688 - val_loss: 0.4220\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3573 - val_loss: 0.4215\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3630 - val_loss: 0.4209\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3641 - val_loss: 0.4199\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3609 - val_loss: 0.4196\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3499 - val_loss: 0.4236\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.3646 - val_loss: 0.4186\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3577 - val_loss: 0.4187\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3592 - val_loss: 0.4177\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3568 - val_loss: 0.4181\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3499 - val_loss: 0.4168\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3534 - val_loss: 0.4169\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3481 - val_loss: 0.4174\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3542 - val_loss: 0.4166\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3521 - val_loss: 0.4168\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3497 - val_loss: 0.4173\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3473 - val_loss: 0.4166\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3406 - val_loss: 0.4165\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3496 - val_loss: 0.4167\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3493 - val_loss: 0.4157\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3509 - val_loss: 0.4160\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3302 - val_loss: 0.4150\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3401 - val_loss: 0.4150\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3406 - val_loss: 0.4151\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3485 - val_loss: 0.4145\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3478 - val_loss: 0.4146\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3580 - val_loss: 0.4164\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3467 - val_loss: 0.4164\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3422 - val_loss: 0.4163\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3477 - val_loss: 0.4153\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 0.3371 - val_loss: 0.4145\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3345 - val_loss: 0.4151\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3339 - val_loss: 0.4143\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3362 - val_loss: 0.4154\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3442 - val_loss: 0.4149\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3433 - val_loss: 0.4143\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3325 - val_loss: 0.4141\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3354 - val_loss: 0.4143\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3353 - val_loss: 0.4141\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3262 - val_loss: 0.4139\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3351 - val_loss: 0.4139\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3357 - val_loss: 0.4157\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3268 - val_loss: 0.4140\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3439 - val_loss: 0.4137\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3405 - val_loss: 0.4137\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3394 - val_loss: 0.4139\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3287 - val_loss: 0.4147\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3332 - val_loss: 0.4141\n",
      "\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3415 - val_loss: 0.4138\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.3353 - val_loss: 0.4135\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3432 - val_loss: 0.4135\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3361 - val_loss: 0.4135\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3368 - val_loss: 0.4135\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3335 - val_loss: 0.4136\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3243 - val_loss: 0.4136\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3437 - val_loss: 0.4137\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3312 - val_loss: 0.4137\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3315 - val_loss: 0.4136\n",
      "\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3302 - val_loss: 0.4136\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3356 - val_loss: 0.4136\n",
      "[2022_04_21-16:03:58] Training the entire fine-tuned model...\n",
      "[2022_04_21-16:04:22] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 765ms/step - loss: 0.3910 - val_loss: 0.4681\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3405 - val_loss: 0.4086\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3419 - val_loss: 0.4254\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3207 - val_loss: 0.4203\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 326ms/step - loss: 0.3101 - val_loss: 0.4175\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.2969 - val_loss: 0.4840\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 309ms/step - loss: 0.3030 - val_loss: 0.4370\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.2886 - val_loss: 0.4238\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.2651 - val_loss: 0.4276\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.2648 - val_loss: 0.4319\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_21-16:04:47] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-16:04:47] Training set: Filtered out 0 of 615 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:04:50] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 746ms/step - loss: 0.3224 - val_loss: 0.4107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18a/2022_04_21_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-16:05:23] Test set: Filtered out 0 of 406 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:22w62359) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10000... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇███▁▁▁▂▁</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▂▂▁▂▁▂▂▁▁▁</td></tr><tr><td>lr</td><td>██████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▂▅▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_val_loss</td><td>0.4086</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32236</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.41074</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">serene-cherry-16</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018a/runs/22w62359\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018a/runs/22w62359</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_160226-22w62359/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:22w62359). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2018b/runs/2w36s079\" target=\"_blank\">sparkling-smoke-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2018b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-16:05:43] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:05:43] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:05:43] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 535ms/step - loss: 0.9307 - val_loss: 0.7264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.6138 - val_loss: 0.4913\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4830 - val_loss: 0.4873\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4660 - val_loss: 0.4897\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4621 - val_loss: 0.4625\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4386 - val_loss: 0.4869\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4360 - val_loss: 0.4546\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.4159 - val_loss: 0.4702\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4309 - val_loss: 0.4454\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.4180 - val_loss: 0.5113\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.4335 - val_loss: 0.4508\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4070 - val_loss: 0.4579\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3911 - val_loss: 0.4403\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3818 - val_loss: 0.4808\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3839 - val_loss: 0.4350\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3637 - val_loss: 0.4318\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3594 - val_loss: 0.4407\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3742 - val_loss: 0.4324\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3742 - val_loss: 0.4297\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3409 - val_loss: 0.4460\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3560 - val_loss: 0.4246\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3386 - val_loss: 0.4735\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 0.3545 - val_loss: 0.4612\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 0.3729 - val_loss: 0.4569\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 124ms/step - loss: 0.3788 - val_loss: 0.4288\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 123ms/step - loss: 0.3533 - val_loss: 0.4259\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 123ms/step - loss: 0.3453 - val_loss: 0.4344\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 124ms/step - loss: 0.3459 - val_loss: 0.4268\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.3353 - val_loss: 0.4357\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_21-16:06:18] Training the entire fine-tuned model...\n",
      "[2022_04_21-16:06:41] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5540WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0987s vs `on_train_batch_end` time: 0.1317s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0987s vs `on_train_batch_end` time: 0.1317s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 634ms/step - loss: 0.5540 - val_loss: 0.6503\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.4960 - val_loss: 0.5429\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.4425 - val_loss: 0.4396\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3792 - val_loss: 0.4204\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3456 - val_loss: 0.4223\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3571 - val_loss: 0.4368\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 289ms/step - loss: 0.3299 - val_loss: 0.4323\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3105 - val_loss: 0.4208\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.2793 - val_loss: 0.4175\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 267ms/step - loss: 0.2710 - val_loss: 0.4153\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.2695 - val_loss: 0.4145\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.2641 - val_loss: 0.4116\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.2607 - val_loss: 0.4087\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.2439 - val_loss: 0.4090\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.2355 - val_loss: 0.4088\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.2406 - val_loss: 0.4089\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.2292 - val_loss: 0.4159\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 289ms/step - loss: 0.2191 - val_loss: 0.4147\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.2147 - val_loss: 0.4173\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.2105 - val_loss: 0.4220\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.2031 - val_loss: 0.4202\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "[2022_04_21-16:07:25] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-16:07:25] Training set: Filtered out 0 of 676 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:07:25] Validation set: Filtered out 0 of 246 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.2503WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1013s vs `on_train_batch_end` time: 0.1294s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1013s vs `on_train_batch_end` time: 0.1294s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 827ms/step - loss: 0.2503 - val_loss: 0.4102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/18b/2022_04_21_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-16:07:59] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2w36s079) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10566... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▇▇▇▇█▁▁▂▂▂▃▃▃▃▄▄▅▅▅▅▆▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▃▃▃▃▃▂▂▃▂▂▂▃▃▂▂▂▄▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▂▂▆▄▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>12</td></tr><tr><td>best_val_loss</td><td>0.40867</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.2503</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.41024</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">sparkling-smoke-16</strong>: <a href=\"https://wandb.ai/kvetab/Split%2018b/runs/2w36s079\" target=\"_blank\">https://wandb.ai/kvetab/Split%2018b/runs/2w36s079</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_160527-2w36s079/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2w36s079). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/3mgqa21g\" target=\"_blank\">glowing-yogurt-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-16:08:19] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:08:19] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:08:19] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 638ms/step - loss: 1.1044 - val_loss: 0.7124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.7197 - val_loss: 0.6161\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.7212 - val_loss: 0.5865\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.5810 - val_loss: 0.4683\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5267 - val_loss: 0.5343\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4842 - val_loss: 0.4619\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4388 - val_loss: 0.4906\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 0.4406 - val_loss: 0.4473\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4378 - val_loss: 0.4554\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4280 - val_loss: 0.4434\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4223 - val_loss: 0.4393\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3990 - val_loss: 0.4385\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3977 - val_loss: 0.4397\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3865 - val_loss: 0.4514\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3851 - val_loss: 0.4342\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3843 - val_loss: 0.4330\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3848 - val_loss: 0.4602\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3914 - val_loss: 0.4307\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3676 - val_loss: 0.4300\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3714 - val_loss: 0.4298\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3657 - val_loss: 0.4321\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3660 - val_loss: 0.4430\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3593 - val_loss: 0.4359\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-16:08:48] Training the entire fine-tuned model...\n",
      "[2022_04_21-16:08:56] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 819ms/step - loss: 0.3567 - val_loss: 0.4296\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3629 - val_loss: 0.4315\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3631 - val_loss: 0.4310\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3557 - val_loss: 0.4296\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3620 - val_loss: 0.4296\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3586 - val_loss: 0.4296\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3623 - val_loss: 0.4298\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_21-16:09:17] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-16:09:17] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:09:24] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 750ms/step - loss: 0.3668 - val_loss: 0.4296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-16:10:00] Test set: Filtered out 0 of 402 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3mgqa21g) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10921... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████████▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▅▂▄▂▃▁▂▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>3</td></tr><tr><td>best_val_loss</td><td>0.42956</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36679</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42959</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">glowing-yogurt-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/3mgqa21g\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/3mgqa21g</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_160801-3mgqa21g/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3mgqa21g). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/33hxes5c\" target=\"_blank\">olive-frost-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-16:10:19] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:10:19] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:10:19] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 537ms/step - loss: 0.8378 - val_loss: 0.5966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.5942 - val_loss: 0.5233\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.5129 - val_loss: 0.4989\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4737 - val_loss: 0.4520\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4452 - val_loss: 0.5442\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4589 - val_loss: 0.4533\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.4485 - val_loss: 0.4540\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-16:10:36] Training the entire fine-tuned model...\n",
      "[2022_04_21-16:10:46] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4335WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1057s vs `on_train_batch_end` time: 0.1253s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1057s vs `on_train_batch_end` time: 0.1253s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 639ms/step - loss: 0.4335 - val_loss: 0.4516\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 290ms/step - loss: 0.4360 - val_loss: 0.4504\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.4376 - val_loss: 0.4501\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.4328 - val_loss: 0.4493\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.4335 - val_loss: 0.4503\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.4348 - val_loss: 0.4507\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.4271 - val_loss: 0.4492\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.4266 - val_loss: 0.4489\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.4281 - val_loss: 0.4485\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.4219 - val_loss: 0.4482\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.4318 - val_loss: 0.4480\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.4264 - val_loss: 0.4480\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 290ms/step - loss: 0.4254 - val_loss: 0.4482\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.4305 - val_loss: 0.4486\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_21-16:11:19] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-16:11:19] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:11:19] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4202WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1002s vs `on_train_batch_end` time: 0.1277s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1002s vs `on_train_batch_end` time: 0.1277s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 614ms/step - loss: 0.4202 - val_loss: 0.4479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-16:11:52] Test set: Filtered out 0 of 373 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:33hxes5c) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 11183... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▄▁▂▂▃▃▄▄▅▅▆▆▇▇█▁</td></tr><tr><td>loss</td><td>█▄▃▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▁▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.44793</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.42019</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.44793</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">olive-frost-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/33hxes5c\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/33hxes5c</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_161002-33hxes5c/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:33hxes5c). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/1t4l4wu2\" target=\"_blank\">atomic-mountain-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-16:12:15] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:12:15] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:12:15] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 638ms/step - loss: 0.8576 - val_loss: 0.5773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.6013 - val_loss: 0.6244\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5302 - val_loss: 0.4820\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4778 - val_loss: 0.4743\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4652 - val_loss: 0.5022\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4474 - val_loss: 0.4752\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4296 - val_loss: 0.4725\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.4219 - val_loss: 0.4450\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4258 - val_loss: 0.4555\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4175 - val_loss: 0.5192\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4321 - val_loss: 0.4816\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-16:12:35] Training the entire fine-tuned model...\n",
      "[2022_04_21-16:12:53] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 769ms/step - loss: 0.4003 - val_loss: 0.4461\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3972 - val_loss: 0.4422\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3914 - val_loss: 0.4425\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3838 - val_loss: 0.4403\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3810 - val_loss: 0.4399\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3782 - val_loss: 0.4398\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3771 - val_loss: 0.4385\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3652 - val_loss: 0.4423\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3564 - val_loss: 0.4376\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3499 - val_loss: 0.4401\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3572 - val_loss: 0.4357\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3305 - val_loss: 0.4359\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3310 - val_loss: 0.4385\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3111 - val_loss: 0.4428\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_21-16:13:23] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-16:13:23] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:13:40] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 725ms/step - loss: 0.3419 - val_loss: 0.4357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-16:14:15] Test set: Filtered out 0 of 402 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1t4l4wu2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 11395... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▄▅▅▆▆▁▂▂▃▃▄▄▅▅▆▆▇▇█▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▂▂▂▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁</td></tr><tr><td>lr</td><td>███████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▆█▃▂▃▂▂▁▂▄▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.43571</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34192</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43571</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">atomic-mountain-2</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/1t4l4wu2\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/1t4l4wu2</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_161154-1t4l4wu2/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1t4l4wu2). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/2gwvmsev\" target=\"_blank\">feasible-shadow-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-16:14:35] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:14:35] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:14:35] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 507ms/step - loss: 0.9108 - val_loss: 0.8813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.7066 - val_loss: 0.5415\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.5497 - val_loss: 0.4682\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4788 - val_loss: 0.4893\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4526 - val_loss: 0.4524\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4447 - val_loss: 0.4428\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4370 - val_loss: 0.4368\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4141 - val_loss: 0.4546\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4146 - val_loss: 0.4406\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4137 - val_loss: 0.4293\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.4093 - val_loss: 0.4335\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3945 - val_loss: 0.4292\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3799 - val_loss: 0.4243\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3809 - val_loss: 0.4520\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3722 - val_loss: 0.4178\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3764 - val_loss: 0.4273\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.3650 - val_loss: 0.4529\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3793 - val_loss: 0.4377\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-16:15:00] Training the entire fine-tuned model...\n",
      "[2022_04_21-16:15:08] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3720WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1001s vs `on_train_batch_end` time: 0.1272s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1001s vs `on_train_batch_end` time: 0.1272s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 638ms/step - loss: 0.3720 - val_loss: 0.4169\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3577 - val_loss: 0.4163\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 260ms/step - loss: 0.3558 - val_loss: 0.4165\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3537 - val_loss: 0.4168\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3595 - val_loss: 0.4161\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3433 - val_loss: 0.4176\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3368 - val_loss: 0.4188\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3404 - val_loss: 0.4201\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_21-16:15:31] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-16:15:31] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:15:31] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3414WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0981s vs `on_train_batch_end` time: 0.1282s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0981s vs `on_train_batch_end` time: 0.1282s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 631ms/step - loss: 0.3414 - val_loss: 0.4166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-16:16:11] Test set: Filtered out 0 of 373 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2gwvmsev) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 11628... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██▁▁▂▂▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▆▄▃▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▁▁▂▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>4</td></tr><tr><td>best_val_loss</td><td>0.41611</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34143</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41664</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">feasible-shadow-2</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/2gwvmsev\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/2gwvmsev</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_161417-2gwvmsev/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2gwvmsev). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/25nm7man\" target=\"_blank\">peach-frog-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-16:16:30] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:16:30] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:16:30] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 626ms/step - loss: 0.9466 - val_loss: 0.5167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5948 - val_loss: 0.6481\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.5438 - val_loss: 0.5426\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4998 - val_loss: 0.6347\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-16:16:45] Training the entire fine-tuned model...\n",
      "[2022_04_21-16:16:58] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 741ms/step - loss: 0.4795 - val_loss: 0.5015\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.4773 - val_loss: 0.5012\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.4770 - val_loss: 0.5004\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 330ms/step - loss: 0.4693 - val_loss: 0.4983\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.4704 - val_loss: 0.4935\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.4628 - val_loss: 0.4878\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.4571 - val_loss: 0.4813\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.4480 - val_loss: 0.4730\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.4402 - val_loss: 0.4655\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.4208 - val_loss: 0.4553\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3978 - val_loss: 0.4477\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3823 - val_loss: 0.4539\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3614 - val_loss: 0.4503\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3517 - val_loss: 0.4796\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_21-16:17:28] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-16:17:28] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:17:45] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 732ms/step - loss: 0.3714 - val_loss: 0.4493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-16:18:18] Test set: Filtered out 0 of 402 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:25nm7man) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 11875... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▁▂▂▃▃▄▄▅▅▆▆▇▇█▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>lr</td><td>████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃█▄█▃▃▃▃▃▂▂▂▂▁▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>10</td></tr><tr><td>best_val_loss</td><td>0.44765</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37135</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.44925</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">peach-frog-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/25nm7man\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/25nm7man</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_161614-25nm7man/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:25nm7man). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/2nef8bvx\" target=\"_blank\">treasured-morning-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-16:18:44] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:18:44] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:18:44] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 525ms/step - loss: 0.8447 - val_loss: 0.9107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.6736 - val_loss: 0.5428\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.5204 - val_loss: 0.4717\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.5391 - val_loss: 0.5594\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.5012 - val_loss: 0.4721\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4569 - val_loss: 0.4490\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4759 - val_loss: 0.5440\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4493 - val_loss: 0.4455\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.4069 - val_loss: 0.4362\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3923 - val_loss: 0.4341\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4050 - val_loss: 0.4475\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3967 - val_loss: 0.4260\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3837 - val_loss: 0.4229\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3828 - val_loss: 0.4409\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3682 - val_loss: 0.4219\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3718 - val_loss: 0.4295\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3841 - val_loss: 0.4163\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3633 - val_loss: 0.4819\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3885 - val_loss: 0.4127\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3669 - val_loss: 0.4520\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.3619 - val_loss: 0.4434\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3626 - val_loss: 0.4090\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3338 - val_loss: 0.4101\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3401 - val_loss: 0.4459\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3550 - val_loss: 0.4449\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-16:19:16] Training the entire fine-tuned model...\n",
      "[2022_04_21-16:19:42] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3480WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1013s vs `on_train_batch_end` time: 0.1247s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1013s vs `on_train_batch_end` time: 0.1247s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 633ms/step - loss: 0.3480 - val_loss: 0.4097\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 260ms/step - loss: 0.3312 - val_loss: 0.4084\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.3459 - val_loss: 0.4348\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.3425 - val_loss: 0.4165\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3236 - val_loss: 0.4092\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_21-16:20:00] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-16:20:00] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:20:10] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3271WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0984s vs `on_train_batch_end` time: 0.1265s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0984s vs `on_train_batch_end` time: 0.1265s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 800ms/step - loss: 0.3271 - val_loss: 0.4090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-16:20:44] Test set: Filtered out 0 of 373 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2nef8bvx) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 12069... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▆▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████████████▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▃▂▂▃▂▁▁▂▁▁▁▁▁▁▂▁▂▁▁▁▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_val_loss</td><td>0.4084</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32713</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.40899</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">treasured-morning-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/2nef8bvx\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/2nef8bvx</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_161821-2nef8bvx/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2nef8bvx). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/3iqi5bwj\" target=\"_blank\">warm-totem-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-16:21:03] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:21:03] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:21:04] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 620ms/step - loss: 0.9193 - val_loss: 0.6507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.6630 - val_loss: 0.8375\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.6174 - val_loss: 0.5670\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5279 - val_loss: 0.6194\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5148 - val_loss: 0.5081\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4773 - val_loss: 0.5218\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4512 - val_loss: 0.4661\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4345 - val_loss: 0.4544\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4239 - val_loss: 0.4533\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4049 - val_loss: 0.4459\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4053 - val_loss: 0.4599\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3890 - val_loss: 0.4366\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3893 - val_loss: 0.4387\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3860 - val_loss: 0.4357\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3709 - val_loss: 0.4349\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3872 - val_loss: 0.4816\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3809 - val_loss: 0.4448\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3832 - val_loss: 0.4716\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-16:21:28] Training the entire fine-tuned model...\n",
      "[2022_04_21-16:21:40] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 767ms/step - loss: 0.4402 - val_loss: 0.4538\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 309ms/step - loss: 0.3796 - val_loss: 0.4490\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3689 - val_loss: 0.4881\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 0.3777 - val_loss: 0.4504\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3734 - val_loss: 0.4412\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3546 - val_loss: 0.4682\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3356 - val_loss: 0.4875\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3310 - val_loss: 0.4576\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_21-16:22:02] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-16:22:02] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:22:03] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 743ms/step - loss: 0.3431 - val_loss: 0.4384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-16:22:37] Test set: Filtered out 0 of 402 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3iqi5bwj) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 12394... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██▁▁▂▂▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▂▂▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▅█▃▄▂▃▂▁▁▁▁▁▁▁▁▂▁▂▁▁▂▁▁▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>14</td></tr><tr><td>best_val_loss</td><td>0.43486</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34315</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.43841</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">warm-totem-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/3iqi5bwj\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/3iqi5bwj</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_162046-3iqi5bwj/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3iqi5bwj). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/hieefcwt\" target=\"_blank\">dulcet-star-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-16:22:57] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:22:57] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:22:57] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 534ms/step - loss: 0.9061 - val_loss: 0.8755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.6526 - val_loss: 0.5106\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.5620 - val_loss: 0.5470\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.5730 - val_loss: 0.6118\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.5298 - val_loss: 0.5256\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-16:23:13] Training the entire fine-tuned model...\n",
      "[2022_04_21-16:23:25] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5028WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1295s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1295s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 625ms/step - loss: 0.5028 - val_loss: 0.4667\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 288ms/step - loss: 0.4896 - val_loss: 0.4638\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.4832 - val_loss: 0.4820\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.4683 - val_loss: 0.4585\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.4572 - val_loss: 0.4545\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.4198 - val_loss: 0.4508\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.4057 - val_loss: 0.4333\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 286ms/step - loss: 0.3526 - val_loss: 0.5225\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3767 - val_loss: 0.4875\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3530 - val_loss: 0.4481\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_21-16:23:51] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-16:23:51] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:23:51] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3676WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1279s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1279s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 630ms/step - loss: 0.3676 - val_loss: 0.4321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-16:24:25] Test set: Filtered out 0 of 373 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:hieefcwt) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 12627... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▁▂▃▃▄▅▆▆▇█▁</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▃▃▂▂▂▂▁▁▁▁</td></tr><tr><td>lr</td><td>█████▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▃▄▂▂▂▂▁▁▁▁▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.43206</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36755</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.43206</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dulcet-star-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/hieefcwt\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/hieefcwt</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_162239-hieefcwt/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:hieefcwt). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/kn5jphd0\" target=\"_blank\">valiant-surf-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-16:24:45] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:24:45] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:24:45] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 643ms/step - loss: 1.0045 - val_loss: 0.5082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.6978 - val_loss: 0.8015\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.7255 - val_loss: 0.4938\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5277 - val_loss: 0.4907\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4847 - val_loss: 0.4798\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4460 - val_loss: 0.4557\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4225 - val_loss: 0.4604\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4185 - val_loss: 0.4474\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4109 - val_loss: 0.4506\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4089 - val_loss: 0.4452\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4031 - val_loss: 0.4401\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3994 - val_loss: 0.4424\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3894 - val_loss: 0.4398\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3821 - val_loss: 0.4381\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3900 - val_loss: 0.4409\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3755 - val_loss: 0.4349\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3726 - val_loss: 0.4308\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3742 - val_loss: 0.4344\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3745 - val_loss: 0.4339\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3665 - val_loss: 0.4334\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3755 - val_loss: 0.4306\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3629 - val_loss: 0.4304\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3584 - val_loss: 0.4265\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3611 - val_loss: 0.4261\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3523 - val_loss: 0.4298\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3584 - val_loss: 0.4261\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3506 - val_loss: 0.4263\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3521 - val_loss: 0.4264\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3503 - val_loss: 0.4263\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3534 - val_loss: 0.4260\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3534 - val_loss: 0.4256\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3508 - val_loss: 0.4259\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3471 - val_loss: 0.4262\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3612 - val_loss: 0.4254\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3521 - val_loss: 0.4252\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3546 - val_loss: 0.4251\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3606 - val_loss: 0.4257\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3545 - val_loss: 0.4263\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3554 - val_loss: 0.4255\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3517 - val_loss: 0.4253\n",
      "[2022_04_21-16:25:27] Training the entire fine-tuned model...\n",
      "[2022_04_21-16:25:42] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 764ms/step - loss: 0.3554 - val_loss: 0.4252\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3453 - val_loss: 0.4249\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 0.3522 - val_loss: 0.4255\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3553 - val_loss: 0.4265\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3474 - val_loss: 0.4255\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3503 - val_loss: 0.4256\n",
      "[2022_04_21-16:26:01] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-16:26:01] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:26:01] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 758ms/step - loss: 0.3439 - val_loss: 0.4249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-16:26:36] Test set: Filtered out 0 of 402 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:kn5jphd0) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 12799... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███▁▁▁▂▂▁</td></tr><tr><td>loss</td><td>█▅▅▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_val_loss</td><td>0.42494</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3439</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42495</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">valiant-surf-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/kn5jphd0\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/kn5jphd0</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_162428-kn5jphd0/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:kn5jphd0). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/106nmhkt\" target=\"_blank\">jumping-wind-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-16:26:58] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:26:58] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:26:58] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 523ms/step - loss: 0.7586 - val_loss: 0.4887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.5771 - val_loss: 0.5332\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.5371 - val_loss: 0.4598\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4635 - val_loss: 0.4685\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.4437 - val_loss: 0.4661\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4255 - val_loss: 0.4472\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4004 - val_loss: 0.4407\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4052 - val_loss: 0.4394\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3992 - val_loss: 0.4802\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.4008 - val_loss: 0.4333\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3887 - val_loss: 0.4264\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3849 - val_loss: 0.4344\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3857 - val_loss: 0.4271\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3831 - val_loss: 0.4547\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3701 - val_loss: 0.4233\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3532 - val_loss: 0.4367\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3646 - val_loss: 0.4173\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3537 - val_loss: 0.4166\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3600 - val_loss: 0.4158\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3608 - val_loss: 0.4183\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3629 - val_loss: 0.4153\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3683 - val_loss: 0.4148\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3549 - val_loss: 0.4151\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 0.3527 - val_loss: 0.4142\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3609 - val_loss: 0.4153\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3511 - val_loss: 0.4145\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3529 - val_loss: 0.4179\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3435 - val_loss: 0.4143\n",
      "[2022_04_21-16:27:32] Training the entire fine-tuned model...\n",
      "[2022_04_21-16:27:40] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3484WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0983s vs `on_train_batch_end` time: 0.1274s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0983s vs `on_train_batch_end` time: 0.1274s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 639ms/step - loss: 0.3484 - val_loss: 0.4143\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3517 - val_loss: 0.4159\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3512 - val_loss: 0.4142\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.3555 - val_loss: 0.4138\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 286ms/step - loss: 0.3500 - val_loss: 0.4137\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3475 - val_loss: 0.4141\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 287ms/step - loss: 0.3486 - val_loss: 0.4148\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 289ms/step - loss: 0.3507 - val_loss: 0.4134\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3456 - val_loss: 0.4140\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3436 - val_loss: 0.4137\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3501 - val_loss: 0.4135\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3338 - val_loss: 0.4139\n",
      "[2022_04_21-16:28:09] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-16:28:09] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:28:09] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3458WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0959s vs `on_train_batch_end` time: 0.1297s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0959s vs `on_train_batch_end` time: 0.1297s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 610ms/step - loss: 0.3458 - val_loss: 0.4134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-16:29:03] Test set: Filtered out 0 of 373 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:106nmhkt) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 13145... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▅█▄▄▄▃▃▃▅▂▂▂▂▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.41338</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34575</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41338</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">jumping-wind-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/106nmhkt\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/106nmhkt</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_162639-106nmhkt/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:106nmhkt). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/3h9zcxjq\" target=\"_blank\">usual-morning-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-16:29:23] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:29:23] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:29:23] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 645ms/step - loss: 0.9690 - val_loss: 0.5033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.6300 - val_loss: 0.7608\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.6152 - val_loss: 0.5363\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5653 - val_loss: 0.6180\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5463 - val_loss: 0.5123\n",
      "[2022_04_21-16:29:38] Training the entire fine-tuned model...\n",
      "[2022_04_21-16:29:46] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 776ms/step - loss: 0.4781 - val_loss: 0.5016\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.4820 - val_loss: 0.5034\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.4775 - val_loss: 0.4996\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.4752 - val_loss: 0.4974\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.4752 - val_loss: 0.4956\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.4733 - val_loss: 0.4945\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.4728 - val_loss: 0.4914\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.4633 - val_loss: 0.4892\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.4656 - val_loss: 0.4878\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.4571 - val_loss: 0.4831\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.4545 - val_loss: 0.4790\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.4490 - val_loss: 0.4751\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.4364 - val_loss: 0.4712\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 331ms/step - loss: 0.4347 - val_loss: 0.4645\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 0.4268 - val_loss: 0.4615\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.4000 - val_loss: 0.4555\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3929 - val_loss: 0.4619\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3843 - val_loss: 0.4622\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3779 - val_loss: 0.4800\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3638 - val_loss: 0.4619\n",
      "[2022_04_21-16:30:26] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-16:30:26] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:30:34] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 747ms/step - loss: 0.3999 - val_loss: 0.4557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-16:31:07] Test set: Filtered out 0 of 402 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3h9zcxjq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 13455... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▂█▃▅▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>15</td></tr><tr><td>best_val_loss</td><td>0.45548</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.39992</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.4557</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">usual-morning-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/3h9zcxjq\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/3h9zcxjq</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_162906-3h9zcxjq/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3h9zcxjq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/3exymsk8\" target=\"_blank\">grateful-sponge-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-16:31:29] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:31:29] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:31:29] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 525ms/step - loss: 0.8855 - val_loss: 0.6865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.6631 - val_loss: 0.4677\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.5155 - val_loss: 0.5669\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.5863 - val_loss: 0.5811\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4778 - val_loss: 0.5036\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4718 - val_loss: 0.4507\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4421 - val_loss: 0.4655\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4338 - val_loss: 0.4536\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4357 - val_loss: 0.4489\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4245 - val_loss: 0.4464\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.4206 - val_loss: 0.4412\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4192 - val_loss: 0.4422\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4114 - val_loss: 0.4399\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4173 - val_loss: 0.4404\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4076 - val_loss: 0.4457\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.4119 - val_loss: 0.4364\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4116 - val_loss: 0.4358\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4032 - val_loss: 0.4379\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3933 - val_loss: 0.4339\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4018 - val_loss: 0.4325\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3899 - val_loss: 0.4328\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4000 - val_loss: 0.4303\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3951 - val_loss: 0.4299\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3928 - val_loss: 0.4303\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3793 - val_loss: 0.4280\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3882 - val_loss: 0.4271\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3843 - val_loss: 0.4292\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3770 - val_loss: 0.4269\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3813 - val_loss: 0.4265\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3759 - val_loss: 0.4245\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3784 - val_loss: 0.4240\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3787 - val_loss: 0.4251\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3697 - val_loss: 0.4232\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.3741 - val_loss: 0.4230\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3787 - val_loss: 0.4230\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3648 - val_loss: 0.4221\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3706 - val_loss: 0.4261\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3688 - val_loss: 0.4224\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3653 - val_loss: 0.4203\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3615 - val_loss: 0.4202\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3689 - val_loss: 0.4196\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3717 - val_loss: 0.4198\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3844 - val_loss: 0.4250\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3630 - val_loss: 0.4256\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3519 - val_loss: 0.4198\n",
      "[2022_04_21-16:32:17] Training the entire fine-tuned model...\n",
      "[2022_04_21-16:32:25] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3666WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1022s vs `on_train_batch_end` time: 0.1261s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1022s vs `on_train_batch_end` time: 0.1261s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 642ms/step - loss: 0.3666 - val_loss: 0.4247\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.3617 - val_loss: 0.4215\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.3558 - val_loss: 0.4189\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.3539 - val_loss: 0.4189\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3533 - val_loss: 0.4192\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3471 - val_loss: 0.4197\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3295 - val_loss: 0.4199\n",
      "[2022_04_21-16:32:46] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-16:32:46] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:32:53] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3580WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0978s vs `on_train_batch_end` time: 0.1289s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0978s vs `on_train_batch_end` time: 0.1289s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 626ms/step - loss: 0.3580 - val_loss: 0.4196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-16:33:28] Test set: Filtered out 0 of 373 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3exymsk8) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 13697... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███▁▁▁▂▂▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▅▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>2</td></tr><tr><td>best_val_loss</td><td>0.41888</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.358</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41958</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">grateful-sponge-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/3exymsk8\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/3exymsk8</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_163111-3exymsk8/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3exymsk8). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/1nkrfugz\" target=\"_blank\">desert-deluge-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-16:33:46] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:33:47] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:33:47] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 641ms/step - loss: 0.9115 - val_loss: 0.5489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5781 - val_loss: 0.6377\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5305 - val_loss: 0.4923\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4885 - val_loss: 0.4708\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4381 - val_loss: 0.4652\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4388 - val_loss: 0.4604\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4214 - val_loss: 0.4767\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4179 - val_loss: 0.4520\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4122 - val_loss: 0.4482\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.3963 - val_loss: 0.4394\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3866 - val_loss: 0.4385\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3910 - val_loss: 0.4342\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3925 - val_loss: 0.4340\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3763 - val_loss: 0.4343\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3775 - val_loss: 0.4335\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3673 - val_loss: 0.4320\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3670 - val_loss: 0.4271\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3757 - val_loss: 0.4267\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3642 - val_loss: 0.4262\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3569 - val_loss: 0.4308\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3536 - val_loss: 0.4451\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3539 - val_loss: 0.4355\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3564 - val_loss: 0.4292\n",
      "[2022_04_21-16:34:17] Training the entire fine-tuned model...\n",
      "[2022_04_21-16:34:25] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 763ms/step - loss: 0.3521 - val_loss: 0.4265\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3519 - val_loss: 0.4299\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3511 - val_loss: 0.4331\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3557 - val_loss: 0.4384\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 309ms/step - loss: 0.3365 - val_loss: 0.4374\n",
      "[2022_04_21-16:34:42] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-16:34:42] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:34:42] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 754ms/step - loss: 0.3575 - val_loss: 0.4262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-16:35:15] Test set: Filtered out 0 of 402 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1nkrfugz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 14062... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████████▃▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▅█▃▂▂▂▃▂▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.42616</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35752</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.42616</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">desert-deluge-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/1nkrfugz\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/1nkrfugz</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_163330-1nkrfugz/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1nkrfugz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/of282snk\" target=\"_blank\">cool-serenity-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-16:35:35] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:35:35] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:35:35] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 526ms/step - loss: 0.8556 - val_loss: 0.8360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.6409 - val_loss: 0.5022\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.5478 - val_loss: 0.5126\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.5110 - val_loss: 0.5406\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4672 - val_loss: 0.4461\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4385 - val_loss: 0.4609\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4351 - val_loss: 0.5099\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4448 - val_loss: 0.4544\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.4208 - val_loss: 0.4364\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4056 - val_loss: 0.4343\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3995 - val_loss: 0.4376\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.4024 - val_loss: 0.4354\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3960 - val_loss: 0.4297\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3955 - val_loss: 0.4293\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3925 - val_loss: 0.4327\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3942 - val_loss: 0.4292\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3902 - val_loss: 0.4293\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3838 - val_loss: 0.4293\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3846 - val_loss: 0.4286\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3837 - val_loss: 0.4274\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3874 - val_loss: 0.4271\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3877 - val_loss: 0.4277\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3882 - val_loss: 0.4288\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3890 - val_loss: 0.4291\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3824 - val_loss: 0.4285\n",
      "[2022_04_21-16:36:06] Training the entire fine-tuned model...\n",
      "[2022_04_21-16:36:32] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3946WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0963s vs `on_train_batch_end` time: 0.1277s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0963s vs `on_train_batch_end` time: 0.1277s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 624ms/step - loss: 0.3946 - val_loss: 0.4321\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.3779 - val_loss: 0.4235\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3826 - val_loss: 0.4265\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.3796 - val_loss: 0.4257\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3638 - val_loss: 0.4223\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3705 - val_loss: 0.4295\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3391 - val_loss: 0.4312\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.3375 - val_loss: 0.4515\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3396 - val_loss: 0.4258\n",
      "[2022_04_21-16:36:56] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-16:36:56] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:36:56] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3498WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1007s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1007s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 597ms/step - loss: 0.3498 - val_loss: 0.4228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-16:37:31] Test set: Filtered out 0 of 373 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:of282snk) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 14308... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▃▃▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>4</td></tr><tr><td>best_val_loss</td><td>0.42234</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34984</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.42281</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">cool-serenity-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/of282snk\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/of282snk</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_163518-of282snk/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:of282snk). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/3b2v4vov\" target=\"_blank\">denim-monkey-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-16:37:49] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:37:49] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:37:50] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 636ms/step - loss: 0.8006 - val_loss: 0.6510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5947 - val_loss: 0.6714\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5504 - val_loss: 0.4751\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4975 - val_loss: 0.4663\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4572 - val_loss: 0.4891\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4369 - val_loss: 0.4569\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4364 - val_loss: 0.4459\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4068 - val_loss: 0.4496\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4045 - val_loss: 0.4377\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3913 - val_loss: 0.4358\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3880 - val_loss: 0.4342\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 0.3835 - val_loss: 0.4456\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3754 - val_loss: 0.4338\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3734 - val_loss: 0.4488\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3936 - val_loss: 0.4307\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3778 - val_loss: 0.4286\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3563 - val_loss: 0.4330\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3599 - val_loss: 0.4260\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3504 - val_loss: 0.4326\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3526 - val_loss: 0.4317\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3609 - val_loss: 0.4303\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3380 - val_loss: 0.4255\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3477 - val_loss: 0.4235\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3419 - val_loss: 0.4233\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3570 - val_loss: 0.4221\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3377 - val_loss: 0.4218\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3494 - val_loss: 0.4241\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3510 - val_loss: 0.4222\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3471 - val_loss: 0.4209\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3414 - val_loss: 0.4288\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3484 - val_loss: 0.4203\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3429 - val_loss: 0.4202\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3509 - val_loss: 0.4213\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3314 - val_loss: 0.4207\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3402 - val_loss: 0.4204\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3375 - val_loss: 0.4202\n",
      "[2022_04_21-16:38:29] Training the entire fine-tuned model...\n",
      "[2022_04_21-16:38:38] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 750ms/step - loss: 0.6440 - val_loss: 0.4342\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.4918 - val_loss: 0.4855\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.4121 - val_loss: 0.4449\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.4041 - val_loss: 0.5094\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3891 - val_loss: 0.4405\n",
      "[2022_04_21-16:38:54] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-16:38:54] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:38:54] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 750ms/step - loss: 0.3517 - val_loss: 0.4281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-16:39:28] Test set: Filtered out 0 of 402 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3b2v4vov) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 14583... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇███▁▁▁▂▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▃▂▂▁</td></tr><tr><td>lr</td><td>████████████████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇█▃▂▃▂▂▂▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▂▃▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>31</td></tr><tr><td>best_val_loss</td><td>0.42016</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35169</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.42806</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">denim-monkey-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/3b2v4vov\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/3b2v4vov</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_163733-3b2v4vov/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3b2v4vov). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/2mvfjwme\" target=\"_blank\">prime-puddle-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-16:39:51] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:39:51] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:39:51] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 512ms/step - loss: 0.8691 - val_loss: 1.0954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 0.7520 - val_loss: 0.6232\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.5613 - val_loss: 0.4599\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.5101 - val_loss: 0.5105\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4615 - val_loss: 0.4967\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4723 - val_loss: 0.4592\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.4199 - val_loss: 0.4403\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4129 - val_loss: 0.4365\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.4056 - val_loss: 0.4336\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.4028 - val_loss: 0.4323\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.3960 - val_loss: 0.4294\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.3986 - val_loss: 0.4560\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 0.3883 - val_loss: 0.4330\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.3818 - val_loss: 0.4200\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3735 - val_loss: 0.4203\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3617 - val_loss: 0.4468\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3715 - val_loss: 0.4150\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3773 - val_loss: 0.4294\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3645 - val_loss: 0.4219\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4077 - val_loss: 0.4149\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3540 - val_loss: 0.5061\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.3712 - val_loss: 0.4157\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3718 - val_loss: 0.5089\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.4127 - val_loss: 0.4137\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 0.3671 - val_loss: 0.4331\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.3509 - val_loss: 0.4385\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3396 - val_loss: 0.4244\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3476 - val_loss: 0.4215\n",
      "[2022_04_21-16:40:25] Training the entire fine-tuned model...\n",
      "[2022_04_21-16:40:49] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5901WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1002s vs `on_train_batch_end` time: 0.1253s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1002s vs `on_train_batch_end` time: 0.1253s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 649ms/step - loss: 0.5901 - val_loss: 0.4204\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3822 - val_loss: 0.4608\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3805 - val_loss: 0.4580\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3501 - val_loss: 0.4332\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 286ms/step - loss: 0.3396 - val_loss: 0.4197\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.3429 - val_loss: 0.4314\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3279 - val_loss: 0.4224\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 287ms/step - loss: 0.3382 - val_loss: 0.4188\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 260ms/step - loss: 0.3296 - val_loss: 0.4191\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 260ms/step - loss: 0.2992 - val_loss: 0.4246\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3060 - val_loss: 0.4236\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3133 - val_loss: 0.4201\n",
      "[2022_04_21-16:41:18] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-16:41:18] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:41:18] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3343WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1016s vs `on_train_batch_end` time: 0.1261s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1016s vs `on_train_batch_end` time: 0.1261s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 631ms/step - loss: 0.3343 - val_loss: 0.4189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-16:41:51] Test set: Filtered out 0 of 373 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2mvfjwme) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 14893... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▇▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▅▂▂▂▁▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>23</td></tr><tr><td>best_val_loss</td><td>0.41373</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3343</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.4189</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">prime-puddle-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/2mvfjwme\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/2mvfjwme</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_163931-2mvfjwme/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2mvfjwme). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/154hqyde\" target=\"_blank\">earnest-darkness-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-16:42:11] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:42:11] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:42:11] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 610ms/step - loss: 0.9001 - val_loss: 0.5111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.6311 - val_loss: 0.7531\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.6247 - val_loss: 0.5428\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4804 - val_loss: 0.5556\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4798 - val_loss: 0.4800\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4623 - val_loss: 0.4809\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4426 - val_loss: 0.4677\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4371 - val_loss: 0.4785\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4321 - val_loss: 0.4595\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4285 - val_loss: 0.4580\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4241 - val_loss: 0.4591\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4269 - val_loss: 0.4560\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4222 - val_loss: 0.4545\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4112 - val_loss: 0.4531\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4111 - val_loss: 0.4553\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 0.4137 - val_loss: 0.4526\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4047 - val_loss: 0.4491\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.4115 - val_loss: 0.4480\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4032 - val_loss: 0.4490\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4063 - val_loss: 0.4460\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4071 - val_loss: 0.4466\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3975 - val_loss: 0.4440\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3965 - val_loss: 0.4434\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3962 - val_loss: 0.4448\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3960 - val_loss: 0.4413\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3967 - val_loss: 0.4434\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4077 - val_loss: 0.4480\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3882 - val_loss: 0.4416\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3894 - val_loss: 0.4388\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3918 - val_loss: 0.4412\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3870 - val_loss: 0.4464\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3914 - val_loss: 0.4405\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3898 - val_loss: 0.4396\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 167ms/step - loss: 0.3877 - val_loss: 0.4386\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3883 - val_loss: 0.4383\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3899 - val_loss: 0.4382\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3857 - val_loss: 0.4384\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3855 - val_loss: 0.4385\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3813 - val_loss: 0.4385\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3811 - val_loss: 0.4386\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3886 - val_loss: 0.4386\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3928 - val_loss: 0.4386\n",
      "[2022_04_21-16:42:55] Training the entire fine-tuned model...\n",
      "[2022_04_21-16:43:08] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 758ms/step - loss: 0.3832 - val_loss: 0.4384\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3894 - val_loss: 0.4393\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3855 - val_loss: 0.4382\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3852 - val_loss: 0.4384\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3815 - val_loss: 0.4384\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3872 - val_loss: 0.4377\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3827 - val_loss: 0.4376\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3816 - val_loss: 0.4376\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3830 - val_loss: 0.4381\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3852 - val_loss: 0.4382\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3819 - val_loss: 0.4382\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3782 - val_loss: 0.4381\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3813 - val_loss: 0.4380\n",
      "[2022_04_21-16:43:38] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-16:43:38] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:43:38] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 751ms/step - loss: 0.3809 - val_loss: 0.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-16:44:13] Test set: Filtered out 0 of 402 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:154hqyde) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 15206... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▄▄▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃█▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.43753</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.38085</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43753</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">earnest-darkness-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/154hqyde\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/154hqyde</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_164155-154hqyde/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:154hqyde). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/10hmwj56\" target=\"_blank\">dazzling-morning-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-16:44:32] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:44:32] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:44:32] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 527ms/step - loss: 0.8960 - val_loss: 1.0514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.7027 - val_loss: 0.6216\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.5566 - val_loss: 0.4641\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.4811 - val_loss: 0.4606\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4635 - val_loss: 0.4475\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4540 - val_loss: 0.4680\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4437 - val_loss: 0.4937\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.4172 - val_loss: 0.4369\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4162 - val_loss: 0.4374\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4074 - val_loss: 0.4286\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4011 - val_loss: 0.4256\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3802 - val_loss: 0.4238\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3782 - val_loss: 0.4319\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3857 - val_loss: 0.4350\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3870 - val_loss: 0.4419\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3784 - val_loss: 0.4320\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3828 - val_loss: 0.4161\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3753 - val_loss: 0.4215\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3656 - val_loss: 0.4298\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3615 - val_loss: 0.4152\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3622 - val_loss: 0.4155\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3525 - val_loss: 0.4153\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3660 - val_loss: 0.4143\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3442 - val_loss: 0.4140\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3589 - val_loss: 0.4170\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3525 - val_loss: 0.4159\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3542 - val_loss: 0.4153\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3563 - val_loss: 0.4133\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3444 - val_loss: 0.4161\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3490 - val_loss: 0.4154\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3404 - val_loss: 0.4130\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.3539 - val_loss: 0.4129\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.3504 - val_loss: 0.4135\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.3520 - val_loss: 0.4138\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3585 - val_loss: 0.4140\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3549 - val_loss: 0.4137\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3498 - val_loss: 0.4129\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.3594 - val_loss: 0.4128\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.3405 - val_loss: 0.4128\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3505 - val_loss: 0.4127\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3453 - val_loss: 0.4127\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3520 - val_loss: 0.4127\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3472 - val_loss: 0.4127\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3475 - val_loss: 0.4127\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3458 - val_loss: 0.4128\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3473 - val_loss: 0.4129\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3441 - val_loss: 0.4129\n",
      "[2022_04_21-16:45:23] Training the entire fine-tuned model...\n",
      "[2022_04_21-16:46:11] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3485WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0988s vs `on_train_batch_end` time: 0.1271s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0988s vs `on_train_batch_end` time: 0.1271s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 645ms/step - loss: 0.3485 - val_loss: 0.4142\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3457 - val_loss: 0.4126\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3502 - val_loss: 0.4125\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3549 - val_loss: 0.4123\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3412 - val_loss: 0.4128\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 285ms/step - loss: 0.3397 - val_loss: 0.4126\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 286ms/step - loss: 0.3502 - val_loss: 0.4129\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.3458 - val_loss: 0.4127\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.3484 - val_loss: 0.4126\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3353 - val_loss: 0.4126\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_21-16:46:37] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-16:46:37] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:46:39] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3423WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1009s vs `on_train_batch_end` time: 0.1263s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1009s vs `on_train_batch_end` time: 0.1263s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 610ms/step - loss: 0.3423 - val_loss: 0.4124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-16:47:12] Test set: Filtered out 0 of 373 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:10hmwj56) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 15589... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▁</td></tr><tr><td>loss</td><td>█▆▄▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>3</td></tr><tr><td>best_val_loss</td><td>0.41234</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34229</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41239</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dazzling-morning-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/10hmwj56\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/10hmwj56</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_164415-10hmwj56/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:10hmwj56). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/3mojbnnu\" target=\"_blank\">peachy-field-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-16:47:33] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:47:33] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:47:33] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 662ms/step - loss: 0.9080 - val_loss: 0.5296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.6123 - val_loss: 0.7533\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5838 - val_loss: 0.6034\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5025 - val_loss: 0.6394\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5555 - val_loss: 0.5206\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4513 - val_loss: 0.5078\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4729 - val_loss: 0.4640\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4506 - val_loss: 0.5025\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4374 - val_loss: 0.4595\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4377 - val_loss: 0.4610\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4269 - val_loss: 0.4688\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4379 - val_loss: 0.4586\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4221 - val_loss: 0.4539\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 0.4197 - val_loss: 0.4550\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4143 - val_loss: 0.4552\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4104 - val_loss: 0.4505\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4216 - val_loss: 0.4488\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4135 - val_loss: 0.4489\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4085 - val_loss: 0.4530\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4131 - val_loss: 0.4468\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4040 - val_loss: 0.4454\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3998 - val_loss: 0.4444\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4038 - val_loss: 0.4440\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3960 - val_loss: 0.4436\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3993 - val_loss: 0.4430\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3973 - val_loss: 0.4408\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3946 - val_loss: 0.4427\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3936 - val_loss: 0.4405\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3888 - val_loss: 0.4443\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3876 - val_loss: 0.4377\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3947 - val_loss: 0.4426\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3821 - val_loss: 0.4359\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3831 - val_loss: 0.4358\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3864 - val_loss: 0.4413\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3825 - val_loss: 0.4342\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3846 - val_loss: 0.4349\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3740 - val_loss: 0.4351\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3790 - val_loss: 0.4323\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3765 - val_loss: 0.4320\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3736 - val_loss: 0.4325\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3798 - val_loss: 0.4320\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3721 - val_loss: 0.4335\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3658 - val_loss: 0.4309\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3734 - val_loss: 0.4303\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3652 - val_loss: 0.4301\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3685 - val_loss: 0.4318\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3697 - val_loss: 0.4334\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3699 - val_loss: 0.4306\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3686 - val_loss: 0.4305\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3780 - val_loss: 0.4307\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3696 - val_loss: 0.4304\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_21-16:48:25] Training the entire fine-tuned model...\n",
      "[2022_04_21-16:48:33] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 746ms/step - loss: 0.3681 - val_loss: 0.4356\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3662 - val_loss: 0.4294\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3674 - val_loss: 0.4312\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 329ms/step - loss: 0.3621 - val_loss: 0.4291\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3626 - val_loss: 0.4303\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3510 - val_loss: 0.4293\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 330ms/step - loss: 0.3527 - val_loss: 0.4284\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3505 - val_loss: 0.4292\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3503 - val_loss: 0.4319\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 330ms/step - loss: 0.3454 - val_loss: 0.4288\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3417 - val_loss: 0.4288\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3342 - val_loss: 0.4295\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3343 - val_loss: 0.4303\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_21-16:49:02] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-16:49:02] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:49:02] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 744ms/step - loss: 0.3508 - val_loss: 0.4283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-16:49:55] Test set: Filtered out 0 of 402 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3mojbnnu) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 15989... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▄▃▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃█▆▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.42832</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35075</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42832</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">peachy-field-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/3mojbnnu\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/3mojbnnu</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_164714-3mojbnnu/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3mojbnnu). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/ziaogxtv\" target=\"_blank\">comic-terrain-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-16:50:15] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:50:15] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:50:15] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 527ms/step - loss: 0.9173 - val_loss: 0.9507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 0.6407 - val_loss: 0.5289\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.5455 - val_loss: 0.4737\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4795 - val_loss: 0.5246\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4656 - val_loss: 0.4576\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4572 - val_loss: 0.4464\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.4305 - val_loss: 0.5384\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4592 - val_loss: 0.5291\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4490 - val_loss: 0.5019\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4137 - val_loss: 0.4390\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4154 - val_loss: 0.4308\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3925 - val_loss: 0.4447\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3923 - val_loss: 0.4301\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3894 - val_loss: 0.4316\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3942 - val_loss: 0.4351\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3964 - val_loss: 0.4282\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3885 - val_loss: 0.4392\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3942 - val_loss: 0.4281\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3737 - val_loss: 0.4259\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3794 - val_loss: 0.4281\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3862 - val_loss: 0.4260\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3762 - val_loss: 0.4239\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3828 - val_loss: 0.4242\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.3852 - val_loss: 0.4236\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3780 - val_loss: 0.4365\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3723 - val_loss: 0.4213\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3772 - val_loss: 0.4212\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3688 - val_loss: 0.4238\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3662 - val_loss: 0.4194\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3702 - val_loss: 0.4187\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.3686 - val_loss: 0.4262\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3700 - val_loss: 0.4179\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3609 - val_loss: 0.4271\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3591 - val_loss: 0.4175\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3606 - val_loss: 0.4204\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3519 - val_loss: 0.4172\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3528 - val_loss: 0.4187\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3651 - val_loss: 0.4177\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3516 - val_loss: 0.4171\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3551 - val_loss: 0.4160\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3494 - val_loss: 0.4206\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3499 - val_loss: 0.4200\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3524 - val_loss: 0.4167\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3592 - val_loss: 0.4164\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3497 - val_loss: 0.4160\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3480 - val_loss: 0.4163\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3441 - val_loss: 0.4163\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3529 - val_loss: 0.4163\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3490 - val_loss: 0.4162\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3461 - val_loss: 0.4162\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3585 - val_loss: 0.4162\n",
      "[2022_04_21-16:51:08] Training the entire fine-tuned model...\n",
      "[2022_04_21-16:51:16] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3488WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1006s vs `on_train_batch_end` time: 0.1266s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1006s vs `on_train_batch_end` time: 0.1266s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 612ms/step - loss: 0.3488 - val_loss: 0.4211\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3495 - val_loss: 0.4286\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3489 - val_loss: 0.4144\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.3439 - val_loss: 0.4140\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3458 - val_loss: 0.4185\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 287ms/step - loss: 0.3388 - val_loss: 0.4149\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 260ms/step - loss: 0.3358 - val_loss: 0.4305\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3384 - val_loss: 0.4140\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3187 - val_loss: 0.4123\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3260 - val_loss: 0.4126\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3357 - val_loss: 0.4128\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3332 - val_loss: 0.4141\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 287ms/step - loss: 0.3239 - val_loss: 0.4145\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3341 - val_loss: 0.4144\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3273 - val_loss: 0.4140\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "[2022_04_21-16:51:50] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-16:51:50] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:51:50] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3225WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0980s vs `on_train_batch_end` time: 0.1285s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0980s vs `on_train_batch_end` time: 0.1285s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 612ms/step - loss: 0.3225 - val_loss: 0.4124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-16:52:24] Test set: Filtered out 0 of 373 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ziaogxtv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 16423... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▅▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▁▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_val_loss</td><td>0.41228</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32248</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41239</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">comic-terrain-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/ziaogxtv\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/ziaogxtv</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_164959-ziaogxtv/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ziaogxtv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/uy99sjbd\" target=\"_blank\">glowing-sun-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-16:52:46] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:52:46] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:52:46] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 660ms/step - loss: 0.8966 - val_loss: 0.5444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.6113 - val_loss: 0.7163\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5741 - val_loss: 0.5454\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.4839 - val_loss: 0.5208\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4439 - val_loss: 0.4747\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4357 - val_loss: 0.4645\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4210 - val_loss: 0.4522\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4112 - val_loss: 0.4557\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4049 - val_loss: 0.4457\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3994 - val_loss: 0.4676\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4141 - val_loss: 0.4380\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3834 - val_loss: 0.4350\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3933 - val_loss: 0.4545\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3751 - val_loss: 0.4365\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3846 - val_loss: 0.4340\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3803 - val_loss: 0.4309\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3757 - val_loss: 0.4442\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3732 - val_loss: 0.4277\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3605 - val_loss: 0.4295\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3600 - val_loss: 0.4243\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3625 - val_loss: 0.4266\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3450 - val_loss: 0.4406\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3503 - val_loss: 0.4336\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3408 - val_loss: 0.4315\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3594 - val_loss: 0.4302\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3380 - val_loss: 0.4240\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3560 - val_loss: 0.4251\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3476 - val_loss: 0.4238\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3455 - val_loss: 0.4193\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3408 - val_loss: 0.4219\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3444 - val_loss: 0.4189\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3456 - val_loss: 0.4204\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3456 - val_loss: 0.4188\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3448 - val_loss: 0.4217\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3402 - val_loss: 0.4202\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3322 - val_loss: 0.4184\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3357 - val_loss: 0.4188\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3410 - val_loss: 0.4247\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 185ms/step - loss: 0.3443 - val_loss: 0.4181\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3321 - val_loss: 0.4212\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3359 - val_loss: 0.4187\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3410 - val_loss: 0.4175\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3423 - val_loss: 0.4220\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 185ms/step - loss: 0.3338 - val_loss: 0.4170\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3350 - val_loss: 0.4179\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3371 - val_loss: 0.4168\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3315 - val_loss: 0.4189\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3287 - val_loss: 0.4162\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3316 - val_loss: 0.4166\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3385 - val_loss: 0.4172\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3307 - val_loss: 0.4160\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3267 - val_loss: 0.4158\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3162 - val_loss: 0.4225\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3394 - val_loss: 0.4169\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3221 - val_loss: 0.4202\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.3242 - val_loss: 0.4173\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3186 - val_loss: 0.4148\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3278 - val_loss: 0.4148\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3371 - val_loss: 0.4151\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3246 - val_loss: 0.4167\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3197 - val_loss: 0.4164\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3238 - val_loss: 0.4160\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3323 - val_loss: 0.4155\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_21-16:53:47] Training the entire fine-tuned model...\n",
      "[2022_04_21-16:53:55] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 761ms/step - loss: 0.3484 - val_loss: 0.4251\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3280 - val_loss: 0.4178\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3145 - val_loss: 0.4185\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 331ms/step - loss: 0.3089 - val_loss: 0.4221\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3176 - val_loss: 0.4193\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.2931 - val_loss: 0.4200\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3006 - val_loss: 0.4209\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3006 - val_loss: 0.4196\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_21-16:54:17] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-16:54:17] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:54:17] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 733ms/step - loss: 0.3133 - val_loss: 0.4180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-16:54:51] Test set: Filtered out 0 of 402 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:uy99sjbd) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 16862... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇███▁▁▁▂▁</td></tr><tr><td>loss</td><td>█▅▃▃▂▂▂▂▂▂▂▂▂▁▁▂▁▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄█▃▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>56</td></tr><tr><td>best_val_loss</td><td>0.41482</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31328</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.41797</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">glowing-sun-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/uy99sjbd\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/uy99sjbd</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_165227-uy99sjbd/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:uy99sjbd). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/ot9iswmb\" target=\"_blank\">lilac-leaf-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-16:55:10] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:55:10] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:55:10] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 515ms/step - loss: 0.9118 - val_loss: 0.9978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.6681 - val_loss: 0.5927\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.5586 - val_loss: 0.4612\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4804 - val_loss: 0.5277\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.5089 - val_loss: 0.4622\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4349 - val_loss: 0.4535\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4289 - val_loss: 0.4571\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4122 - val_loss: 0.4454\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.4258 - val_loss: 0.4293\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3898 - val_loss: 0.4276\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3995 - val_loss: 0.4473\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3943 - val_loss: 0.4316\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3775 - val_loss: 0.4231\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3731 - val_loss: 0.4242\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3668 - val_loss: 0.4218\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3654 - val_loss: 0.4388\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3655 - val_loss: 0.4269\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3704 - val_loss: 0.4410\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3539 - val_loss: 0.4232\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3643 - val_loss: 0.4211\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3569 - val_loss: 0.4213\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3510 - val_loss: 0.4302\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3583 - val_loss: 0.4190\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3455 - val_loss: 0.4161\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3487 - val_loss: 0.4156\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3473 - val_loss: 0.4212\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3529 - val_loss: 0.4151\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3414 - val_loss: 0.4162\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3436 - val_loss: 0.4151\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3471 - val_loss: 0.4136\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 126ms/step - loss: 0.3479 - val_loss: 0.4133\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3352 - val_loss: 0.4142\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3395 - val_loss: 0.4144\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3339 - val_loss: 0.4134\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3454 - val_loss: 0.4133\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3495 - val_loss: 0.4140\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3417 - val_loss: 0.4143\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3316 - val_loss: 0.4143\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3459 - val_loss: 0.4141\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3387 - val_loss: 0.4135\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3410 - val_loss: 0.4135\n",
      "[2022_04_21-16:55:55] Training the entire fine-tuned model...\n",
      "[2022_04_21-16:56:09] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3588WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0982s vs `on_train_batch_end` time: 0.1281s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0982s vs `on_train_batch_end` time: 0.1281s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 631ms/step - loss: 0.3588 - val_loss: 0.4149\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.3498 - val_loss: 0.4154\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3408 - val_loss: 0.4289\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3369 - val_loss: 0.4274\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.3312 - val_loss: 0.4155\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3313 - val_loss: 0.4233\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3296 - val_loss: 0.4144\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.3168 - val_loss: 0.4137\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 267ms/step - loss: 0.3302 - val_loss: 0.4140\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 287ms/step - loss: 0.3142 - val_loss: 0.4158\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3105 - val_loss: 0.4145\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3073 - val_loss: 0.4144\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3137 - val_loss: 0.4145\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 286ms/step - loss: 0.3089 - val_loss: 0.4145\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "[2022_04_21-16:56:41] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-16:56:41] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:56:57] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3122WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1000s vs `on_train_batch_end` time: 0.1266s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1000s vs `on_train_batch_end` time: 0.1266s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 622ms/step - loss: 0.3122 - val_loss: 0.4135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-16:57:30] Test set: Filtered out 0 of 373 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ot9iswmb) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 17323... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>34</td></tr><tr><td>best_val_loss</td><td>0.41331</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31216</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.41355</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">lilac-leaf-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/ot9iswmb\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/ot9iswmb</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_165454-ot9iswmb/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ot9iswmb). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/3siahrqw\" target=\"_blank\">serene-jazz-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-16:57:49] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:57:49] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:57:49] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 631ms/step - loss: 0.9380 - val_loss: 0.5524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.6124 - val_loss: 0.6704\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5935 - val_loss: 0.5044\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5068 - val_loss: 0.5735\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4823 - val_loss: 0.4773\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4627 - val_loss: 0.5345\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4570 - val_loss: 0.4682\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4214 - val_loss: 0.4755\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4237 - val_loss: 0.4416\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4188 - val_loss: 0.4382\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4097 - val_loss: 0.4481\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3990 - val_loss: 0.4349\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3824 - val_loss: 0.4353\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3789 - val_loss: 0.4341\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3758 - val_loss: 0.4377\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3776 - val_loss: 0.4295\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3700 - val_loss: 0.4344\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3653 - val_loss: 0.4283\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3555 - val_loss: 0.4293\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3622 - val_loss: 0.4261\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3820 - val_loss: 0.4473\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3628 - val_loss: 0.4317\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3621 - val_loss: 0.4301\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3442 - val_loss: 0.4245\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3559 - val_loss: 0.4239\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3526 - val_loss: 0.4287\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3562 - val_loss: 0.4242\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3503 - val_loss: 0.4237\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3508 - val_loss: 0.4252\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3410 - val_loss: 0.4258\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3519 - val_loss: 0.4229\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3362 - val_loss: 0.4244\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3536 - val_loss: 0.4228\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3338 - val_loss: 0.4222\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3410 - val_loss: 0.4222\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3339 - val_loss: 0.4241\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3451 - val_loss: 0.4211\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3404 - val_loss: 0.4212\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3475 - val_loss: 0.4279\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3426 - val_loss: 0.4216\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3438 - val_loss: 0.4210\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3465 - val_loss: 0.4220\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3336 - val_loss: 0.4243\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3269 - val_loss: 0.4220\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 185ms/step - loss: 0.3457 - val_loss: 0.4217\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3392 - val_loss: 0.4215\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3396 - val_loss: 0.4213\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_21-16:58:37] Training the entire fine-tuned model...\n",
      "[2022_04_21-16:58:47] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 756ms/step - loss: 0.6996 - val_loss: 0.7243\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.4644 - val_loss: 0.4348\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3957 - val_loss: 0.4440\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 330ms/step - loss: 0.3793 - val_loss: 0.4350\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3415 - val_loss: 0.4474\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3512 - val_loss: 0.4357\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3356 - val_loss: 0.4318\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3312 - val_loss: 0.4306\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 309ms/step - loss: 0.3297 - val_loss: 0.4303\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3320 - val_loss: 0.4295\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3209 - val_loss: 0.4294\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 331ms/step - loss: 0.3095 - val_loss: 0.4298\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3095 - val_loss: 0.4331\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3182 - val_loss: 0.4319\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3157 - val_loss: 0.4306\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3096 - val_loss: 0.4302\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.2995 - val_loss: 0.4302\n",
      "[2022_04_21-16:59:23] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-16:59:23] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-16:59:23] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 753ms/step - loss: 0.3223 - val_loss: 0.4298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-17:00:10] Test set: Filtered out 0 of 402 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3siahrqw) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 17713... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▅▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄▇▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>40</td></tr><tr><td>best_val_loss</td><td>0.42095</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32234</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.42979</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">serene-jazz-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/3siahrqw\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/3siahrqw</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_165732-3siahrqw/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3siahrqw). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/3p19jowc\" target=\"_blank\">fresh-morning-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-17:00:29] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:00:29] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:00:29] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 520ms/step - loss: 0.8886 - val_loss: 0.8173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.6380 - val_loss: 0.4798\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.5225 - val_loss: 0.5260\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.5306 - val_loss: 0.5284\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4587 - val_loss: 0.4540\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4381 - val_loss: 0.4536\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.4423 - val_loss: 0.4474\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4071 - val_loss: 0.4418\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4014 - val_loss: 0.4361\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4027 - val_loss: 0.4434\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4288 - val_loss: 0.4286\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3860 - val_loss: 0.4285\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3911 - val_loss: 0.5493\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4090 - val_loss: 0.4767\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3970 - val_loss: 0.4362\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3863 - val_loss: 0.4257\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3700 - val_loss: 0.4208\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3759 - val_loss: 0.4212\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3728 - val_loss: 0.4254\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3548 - val_loss: 0.4260\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3619 - val_loss: 0.4210\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3575 - val_loss: 0.4206\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3639 - val_loss: 0.4257\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3661 - val_loss: 0.4220\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3649 - val_loss: 0.4192\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3556 - val_loss: 0.4187\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3554 - val_loss: 0.4186\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3528 - val_loss: 0.4186\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3595 - val_loss: 0.4196\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3572 - val_loss: 0.4190\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3588 - val_loss: 0.4186\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3652 - val_loss: 0.4184\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3543 - val_loss: 0.4184\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3581 - val_loss: 0.4184\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3518 - val_loss: 0.4187\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3530 - val_loss: 0.4186\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3552 - val_loss: 0.4187\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3557 - val_loss: 0.4187\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3623 - val_loss: 0.4187\n",
      "[2022_04_21-17:01:12] Training the entire fine-tuned model...\n",
      "[2022_04_21-17:01:20] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4290WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0998s vs `on_train_batch_end` time: 0.1274s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0998s vs `on_train_batch_end` time: 0.1274s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 605ms/step - loss: 0.4290 - val_loss: 0.5311\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.4414 - val_loss: 0.4324\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.4107 - val_loss: 0.4790\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.3756 - val_loss: 0.4306\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.3344 - val_loss: 0.4204\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3218 - val_loss: 0.4189\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3047 - val_loss: 0.4199\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.2896 - val_loss: 0.4485\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.2608 - val_loss: 0.4587\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.2660 - val_loss: 0.4536\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.2489 - val_loss: 0.4505\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.2447 - val_loss: 0.4484\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_21-17:01:48] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-17:01:48] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:01:48] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3108WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0973s vs `on_train_batch_end` time: 0.1270s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0973s vs `on_train_batch_end` time: 0.1270s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 599ms/step - loss: 0.3108 - val_loss: 0.4179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-17:02:43] Test set: Filtered out 0 of 373 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3p19jowc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 18151... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▃▂▂▂▁▁▁▂</td></tr><tr><td>lr</td><td>███████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▃▃▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▂▁▁▁▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.41794</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31076</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.41794</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fresh-morning-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/3p19jowc\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/3p19jowc</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_170012-3p19jowc/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3p19jowc). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/lsfu5jff\" target=\"_blank\">splendid-galaxy-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-17:03:02] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:03:02] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:03:02] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 1s/step - loss: 1.0881 - val_loss: 0.6377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.6826 - val_loss: 0.6257\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5912 - val_loss: 0.4760\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5096 - val_loss: 0.5115\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4754 - val_loss: 0.4611\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4577 - val_loss: 0.4835\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.4411 - val_loss: 0.4560\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4344 - val_loss: 0.5091\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4446 - val_loss: 0.4467\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4095 - val_loss: 0.4536\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4074 - val_loss: 0.4394\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3943 - val_loss: 0.4386\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3888 - val_loss: 0.4375\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3913 - val_loss: 0.4345\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3743 - val_loss: 0.4365\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3768 - val_loss: 0.4325\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3734 - val_loss: 0.4325\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3685 - val_loss: 0.4361\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3725 - val_loss: 0.4322\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3662 - val_loss: 0.4320\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3537 - val_loss: 0.4319\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3636 - val_loss: 0.4376\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3626 - val_loss: 0.4254\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3469 - val_loss: 0.4235\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3473 - val_loss: 0.4265\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3462 - val_loss: 0.4252\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3414 - val_loss: 0.4258\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3426 - val_loss: 0.4221\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3386 - val_loss: 0.4288\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 0.3388 - val_loss: 0.4480\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3533 - val_loss: 0.4216\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3284 - val_loss: 0.4212\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3380 - val_loss: 0.4364\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3306 - val_loss: 0.4290\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3349 - val_loss: 0.4604\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3453 - val_loss: 0.4365\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3436 - val_loss: 0.4232\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3226 - val_loss: 0.4208\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3255 - val_loss: 0.4289\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3266 - val_loss: 0.4206\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3351 - val_loss: 0.4203\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 186ms/step - loss: 0.3213 - val_loss: 0.4258\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3260 - val_loss: 0.4200\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3116 - val_loss: 0.4210\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3211 - val_loss: 0.4223\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3318 - val_loss: 0.4198\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3217 - val_loss: 0.4230\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3281 - val_loss: 0.4207\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3230 - val_loss: 0.4242\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3195 - val_loss: 0.4202\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3283 - val_loss: 0.4196\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3074 - val_loss: 0.4205\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3231 - val_loss: 0.4207\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.3143 - val_loss: 0.4203\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3185 - val_loss: 0.4191\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3218 - val_loss: 0.4192\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3150 - val_loss: 0.4194\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3192 - val_loss: 0.4198\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3181 - val_loss: 0.4202\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3115 - val_loss: 0.4202\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3175 - val_loss: 0.4199\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3301 - val_loss: 0.4196\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3214 - val_loss: 0.4196\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_21-17:04:03] Training the entire fine-tuned model...\n",
      "[2022_04_21-17:04:12] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 760ms/step - loss: 0.3015 - val_loss: 0.4199\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3136 - val_loss: 0.4197\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3230 - val_loss: 0.4208\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 331ms/step - loss: 0.3160 - val_loss: 0.4246\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3064 - val_loss: 0.4207\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3133 - val_loss: 0.4203\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3287 - val_loss: 0.4204\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3034 - val_loss: 0.4206\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 0.3087 - val_loss: 0.4208\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3025 - val_loss: 0.4209\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_21-17:04:35] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-17:04:35] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:04:35] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 745ms/step - loss: 0.3250 - val_loss: 0.4197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-17:05:14] Test set: Filtered out 0 of 402 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:lsfu5jff) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 18525... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███▁▁▁▂▂▁</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>██▄▃▄▂▂▁▂▁▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>54</td></tr><tr><td>best_val_loss</td><td>0.41909</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32497</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41967</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">splendid-galaxy-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/lsfu5jff\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/lsfu5jff</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_170245-lsfu5jff/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:lsfu5jff). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/337rsvn0\" target=\"_blank\">vague-universe-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-17:05:35] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:05:35] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:05:35] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 523ms/step - loss: 0.8305 - val_loss: 0.6205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.5840 - val_loss: 0.4704\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.5192 - val_loss: 0.4882\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4812 - val_loss: 0.4649\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4479 - val_loss: 0.4465\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4282 - val_loss: 0.4728\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4207 - val_loss: 0.4652\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4186 - val_loss: 0.4966\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.4290 - val_loss: 0.4337\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4041 - val_loss: 0.4793\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.4230 - val_loss: 0.4697\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4150 - val_loss: 0.4644\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4117 - val_loss: 0.4256\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3980 - val_loss: 0.4470\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3815 - val_loss: 0.4214\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3573 - val_loss: 0.4211\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3523 - val_loss: 0.4157\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3646 - val_loss: 0.4161\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3563 - val_loss: 0.4126\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3661 - val_loss: 0.4288\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3693 - val_loss: 0.4177\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3780 - val_loss: 0.4532\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3594 - val_loss: 0.4114\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3463 - val_loss: 0.4156\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3446 - val_loss: 0.4312\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3453 - val_loss: 0.4290\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3421 - val_loss: 0.4147\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3270 - val_loss: 0.4133\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3418 - val_loss: 0.4097\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3365 - val_loss: 0.4099\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3234 - val_loss: 0.4122\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3167 - val_loss: 0.4130\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3252 - val_loss: 0.4100\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3171 - val_loss: 0.4098\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3143 - val_loss: 0.4102\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3136 - val_loss: 0.4131\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3184 - val_loss: 0.4109\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-17:06:16] Training the entire fine-tuned model...\n",
      "[2022_04_21-17:06:32] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3232WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0993s vs `on_train_batch_end` time: 0.1279s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0993s vs `on_train_batch_end` time: 0.1279s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 628ms/step - loss: 0.3232 - val_loss: 0.4094\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3372 - val_loss: 0.4107\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3236 - val_loss: 0.4095\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 285ms/step - loss: 0.3251 - val_loss: 0.4106\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3098 - val_loss: 0.4109\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3217 - val_loss: 0.4101\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3271 - val_loss: 0.4095\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 285ms/step - loss: 0.3160 - val_loss: 0.4092\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3215 - val_loss: 0.4092\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3172 - val_loss: 0.4092\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.3106 - val_loss: 0.4092\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3240 - val_loss: 0.4093\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3281 - val_loss: 0.4093\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3210 - val_loss: 0.4093\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3084 - val_loss: 0.4093\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 267ms/step - loss: 0.3119 - val_loss: 0.4093\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3202 - val_loss: 0.4093\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3203 - val_loss: 0.4093\n",
      "[2022_04_21-17:07:11] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-17:07:11] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:07:11] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3280WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0984s vs `on_train_batch_end` time: 0.1276s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0984s vs `on_train_batch_end` time: 0.1276s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 619ms/step - loss: 0.3280 - val_loss: 0.4092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-17:07:46] Test set: Filtered out 0 of 373 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:337rsvn0) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 19000... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▃▃▃▃▄▄▄▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▄▂▃▄▂▃▃▂▁▁▁▁▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.40917</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32805</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.40917</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">vague-universe-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/337rsvn0\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/337rsvn0</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_170516-337rsvn0/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:337rsvn0). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/200bggda\" target=\"_blank\">happy-dew-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-17:08:05] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:08:05] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:08:05] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 643ms/step - loss: 0.9335 - val_loss: 0.5013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.6240 - val_loss: 0.7468\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.6202 - val_loss: 0.5291\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4914 - val_loss: 0.5314\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4852 - val_loss: 0.4808\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4371 - val_loss: 0.4686\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.4167 - val_loss: 0.4497\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 177ms/step - loss: 0.4119 - val_loss: 0.4478\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 0.4134 - val_loss: 0.4463\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4036 - val_loss: 0.4405\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3959 - val_loss: 0.4494\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3844 - val_loss: 0.4382\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3845 - val_loss: 0.4405\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3919 - val_loss: 0.4714\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4139 - val_loss: 0.4371\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3843 - val_loss: 0.4287\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3771 - val_loss: 0.4827\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3892 - val_loss: 0.4692\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3771 - val_loss: 0.4617\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3702 - val_loss: 0.4260\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3890 - val_loss: 0.4359\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3667 - val_loss: 0.4776\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3860 - val_loss: 0.4282\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3636 - val_loss: 0.4262\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3539 - val_loss: 0.4226\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3514 - val_loss: 0.4302\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3403 - val_loss: 0.4222\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.3527 - val_loss: 0.4220\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3408 - val_loss: 0.4224\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3403 - val_loss: 0.4222\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3402 - val_loss: 0.4212\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3438 - val_loss: 0.4240\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3328 - val_loss: 0.4208\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3389 - val_loss: 0.4208\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3486 - val_loss: 0.4293\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3405 - val_loss: 0.4223\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3401 - val_loss: 0.4215\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3398 - val_loss: 0.4255\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3407 - val_loss: 0.4221\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3313 - val_loss: 0.4205\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3288 - val_loss: 0.4204\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3365 - val_loss: 0.4208\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3377 - val_loss: 0.4209\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3386 - val_loss: 0.4210\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3300 - val_loss: 0.4212\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3364 - val_loss: 0.4211\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3414 - val_loss: 0.4211\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3311 - val_loss: 0.4211\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3306 - val_loss: 0.4211\n",
      "[2022_04_21-17:08:55] Training the entire fine-tuned model...\n",
      "[2022_04_21-17:09:03] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 755ms/step - loss: 0.3417 - val_loss: 0.4222\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3432 - val_loss: 0.4259\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 307ms/step - loss: 0.3472 - val_loss: 0.4231\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3375 - val_loss: 0.4246\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3180 - val_loss: 0.4229\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3206 - val_loss: 0.4223\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3248 - val_loss: 0.4223\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3305 - val_loss: 0.4230\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3193 - val_loss: 0.4236\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_21-17:09:25] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-17:09:25] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:09:30] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 749ms/step - loss: 0.3358 - val_loss: 0.4211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-17:10:05] Test set: Filtered out 0 of 402 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:200bggda) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 19393... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███▁▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▄▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃█▃▂▂▂▂▂▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>40</td></tr><tr><td>best_val_loss</td><td>0.42042</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33575</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.42112</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">happy-dew-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/200bggda\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/200bggda</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_170748-200bggda/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:200bggda). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/2eln3iv8\" target=\"_blank\">generous-plasma-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-17:10:31] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:10:31] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:10:31] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 518ms/step - loss: 0.9328 - val_loss: 0.6731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.5547 - val_loss: 0.4816\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4919 - val_loss: 0.4650\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4589 - val_loss: 0.4820\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4522 - val_loss: 0.4491\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4300 - val_loss: 0.4494\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.4115 - val_loss: 0.4748\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4386 - val_loss: 0.4375\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4366 - val_loss: 0.4918\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4045 - val_loss: 0.4412\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3967 - val_loss: 0.4305\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3717 - val_loss: 0.4237\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3717 - val_loss: 0.4232\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3669 - val_loss: 0.4230\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3577 - val_loss: 0.4636\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3650 - val_loss: 0.4285\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3622 - val_loss: 0.4451\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3740 - val_loss: 0.4137\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3538 - val_loss: 0.4149\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3423 - val_loss: 0.4102\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3427 - val_loss: 0.4179\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3386 - val_loss: 0.4111\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3452 - val_loss: 0.4638\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3785 - val_loss: 0.4092\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3398 - val_loss: 0.4113\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3310 - val_loss: 0.4119\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3337 - val_loss: 0.4082\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3262 - val_loss: 0.4241\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3343 - val_loss: 0.4176\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3397 - val_loss: 0.4235\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3213 - val_loss: 0.4169\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3100 - val_loss: 0.4101\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 0.3243 - val_loss: 0.4080\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3058 - val_loss: 0.4081\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3340 - val_loss: 0.4099\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3125 - val_loss: 0.4071\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3213 - val_loss: 0.4062\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3241 - val_loss: 0.4114\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3061 - val_loss: 0.4128\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3108 - val_loss: 0.4098\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3157 - val_loss: 0.4060\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3195 - val_loss: 0.4076\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3100 - val_loss: 0.4058\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3159 - val_loss: 0.4056\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3118 - val_loss: 0.4051\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.2977 - val_loss: 0.4057\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3094 - val_loss: 0.4052\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3119 - val_loss: 0.4085\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3222 - val_loss: 0.4080\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3127 - val_loss: 0.4131\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3151 - val_loss: 0.4063\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3075 - val_loss: 0.4077\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3112 - val_loss: 0.4069\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-17:11:27] Training the entire fine-tuned model...\n",
      "[2022_04_21-17:11:35] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.2994WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1289s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1289s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 643ms/step - loss: 0.2994 - val_loss: 0.4070\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3168 - val_loss: 0.4076\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3034 - val_loss: 0.4065\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3093 - val_loss: 0.4084\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3014 - val_loss: 0.4110\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3188 - val_loss: 0.4126\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 286ms/step - loss: 0.2993 - val_loss: 0.4102\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.2860 - val_loss: 0.4108\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.2933 - val_loss: 0.4115\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.2925 - val_loss: 0.4102\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.2788 - val_loss: 0.4112\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_21-17:12:03] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-17:12:03] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:12:03] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.2971WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1008s vs `on_train_batch_end` time: 0.1272s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1008s vs `on_train_batch_end` time: 0.1272s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 626ms/step - loss: 0.2971 - val_loss: 0.4066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-17:12:38] Test set: Filtered out 0 of 373 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2eln3iv8) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 19799... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███▁▁▂▂▂▂▁</td></tr><tr><td>loss</td><td>█▄▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▂▃▃▂▁▁▃▂▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>44</td></tr><tr><td>best_val_loss</td><td>0.40508</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.29706</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.40655</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">generous-plasma-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/2eln3iv8\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/2eln3iv8</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_171007-2eln3iv8/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2eln3iv8). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/27znlkav\" target=\"_blank\">decent-sun-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-17:12:58] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:12:58] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:12:58] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 657ms/step - loss: 0.8280 - val_loss: 0.7408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.6388 - val_loss: 0.7100\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.5114 - val_loss: 0.6576\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5321 - val_loss: 0.6225\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4973 - val_loss: 0.5042\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4473 - val_loss: 0.5211\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4496 - val_loss: 0.4494\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4366 - val_loss: 0.4455\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4031 - val_loss: 0.4441\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4005 - val_loss: 0.4407\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4010 - val_loss: 0.4550\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3919 - val_loss: 0.4370\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3767 - val_loss: 0.4477\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3774 - val_loss: 0.4366\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3759 - val_loss: 0.4346\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3781 - val_loss: 0.4575\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3815 - val_loss: 0.4357\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3708 - val_loss: 0.4349\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3764 - val_loss: 0.4605\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3818 - val_loss: 0.4372\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3675 - val_loss: 0.4362\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3475 - val_loss: 0.4389\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3727 - val_loss: 0.4350\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_21-17:13:28] Training the entire fine-tuned model...\n",
      "[2022_04_21-17:13:36] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 774ms/step - loss: 0.3783 - val_loss: 0.4489\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3637 - val_loss: 0.4357\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3637 - val_loss: 0.4362\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3474 - val_loss: 0.4327\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3453 - val_loss: 0.4342\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 309ms/step - loss: 0.3454 - val_loss: 0.4332\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3376 - val_loss: 0.4550\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3256 - val_loss: 0.4435\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3290 - val_loss: 0.4330\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3226 - val_loss: 0.4399\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3235 - val_loss: 0.4322\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3159 - val_loss: 0.4337\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3178 - val_loss: 0.4333\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 331ms/step - loss: 0.3143 - val_loss: 0.4343\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3027 - val_loss: 0.4347\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3004 - val_loss: 0.4341\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 331ms/step - loss: 0.3033 - val_loss: 0.4342\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 331ms/step - loss: 0.2906 - val_loss: 0.4350\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 331ms/step - loss: 0.2930 - val_loss: 0.4350\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "[2022_04_21-17:14:13] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-17:14:13] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:14:13] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 753ms/step - loss: 0.3157 - val_loss: 0.4324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-17:14:47] Test set: Filtered out 0 of 402 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:27znlkav) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 20225... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▁</td></tr><tr><td>loss</td><td>█▆▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▆▅▃▃▁▁▁▁▂▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>10</td></tr><tr><td>best_val_loss</td><td>0.43218</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31574</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.43236</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">decent-sun-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/27znlkav\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/27znlkav</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_171240-27znlkav/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:27znlkav). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/1qnbzndt\" target=\"_blank\">distinctive-voice-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-17:15:07] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:15:07] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:15:07] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 510ms/step - loss: 0.7874 - val_loss: 0.6302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.5392 - val_loss: 0.4695\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4962 - val_loss: 0.4805\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4555 - val_loss: 0.4497\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4470 - val_loss: 0.4906\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4395 - val_loss: 0.4378\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.4161 - val_loss: 0.4337\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.3976 - val_loss: 0.4436\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4043 - val_loss: 0.4405\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3841 - val_loss: 0.4568\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.4010 - val_loss: 0.4231\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3781 - val_loss: 0.4347\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3689 - val_loss: 0.4201\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3671 - val_loss: 0.4251\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 127ms/step - loss: 0.3755 - val_loss: 0.4765\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4040 - val_loss: 0.4322\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3678 - val_loss: 0.4744\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3748 - val_loss: 0.4307\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3504 - val_loss: 0.4236\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3610 - val_loss: 0.4179\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3513 - val_loss: 0.4156\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3520 - val_loss: 0.4208\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3518 - val_loss: 0.4172\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3550 - val_loss: 0.4137\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3572 - val_loss: 0.4135\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3490 - val_loss: 0.4129\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3466 - val_loss: 0.4130\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3378 - val_loss: 0.4155\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3490 - val_loss: 0.4191\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3420 - val_loss: 0.4124\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3373 - val_loss: 0.4114\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3267 - val_loss: 0.4132\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3418 - val_loss: 0.4123\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3319 - val_loss: 0.4111\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3309 - val_loss: 0.4109\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3381 - val_loss: 0.4097\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.3325 - val_loss: 0.4096\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3226 - val_loss: 0.4091\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3272 - val_loss: 0.4104\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3374 - val_loss: 0.4099\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3412 - val_loss: 0.4096\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3287 - val_loss: 0.4150\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3298 - val_loss: 0.4095\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3240 - val_loss: 0.4103\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3287 - val_loss: 0.4094\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3219 - val_loss: 0.4103\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-17:15:55] Training the entire fine-tuned model...\n",
      "[2022_04_21-17:16:05] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3303WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1000s vs `on_train_batch_end` time: 0.1262s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1000s vs `on_train_batch_end` time: 0.1262s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 630ms/step - loss: 0.3303 - val_loss: 0.4093\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3313 - val_loss: 0.4095\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.3175 - val_loss: 0.4127\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3243 - val_loss: 0.4097\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3173 - val_loss: 0.4081\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.2893 - val_loss: 0.4149\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.2993 - val_loss: 0.4133\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.2916 - val_loss: 0.4577\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3156 - val_loss: 0.4523\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.2964 - val_loss: 0.4194\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.2696 - val_loss: 0.4254\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 286ms/step - loss: 0.2796 - val_loss: 0.4187\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.2777 - val_loss: 0.4199\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_21-17:16:34] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-17:16:34] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:16:34] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3082WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0998s vs `on_train_batch_end` time: 0.1280s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0998s vs `on_train_batch_end` time: 0.1280s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 648ms/step - loss: 0.3082 - val_loss: 0.4085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-17:17:09] Test set: Filtered out 0 of 373 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1qnbzndt) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 20540... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▃▂▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▄▂▂▃▁▁▂▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>4</td></tr><tr><td>best_val_loss</td><td>0.40808</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30821</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.40853</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">distinctive-voice-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/1qnbzndt\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/1qnbzndt</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_171450-1qnbzndt/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1qnbzndt). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027a/runs/2d7fgmuv\" target=\"_blank\">zany-water-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-17:17:28] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:17:28] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:17:28] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 615ms/step - loss: 0.8186 - val_loss: 0.5373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.6472 - val_loss: 0.7776\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5936 - val_loss: 0.6349\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5605 - val_loss: 0.6236\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4889 - val_loss: 0.5265\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4652 - val_loss: 0.5209\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4458 - val_loss: 0.4518\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4182 - val_loss: 0.4459\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.4088 - val_loss: 0.4421\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3967 - val_loss: 0.4395\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3962 - val_loss: 0.4389\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4004 - val_loss: 0.4514\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3995 - val_loss: 0.4330\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3827 - val_loss: 0.4320\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3762 - val_loss: 0.4319\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3710 - val_loss: 0.4367\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3663 - val_loss: 0.4322\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3667 - val_loss: 0.4282\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3636 - val_loss: 0.4283\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3619 - val_loss: 0.4261\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3556 - val_loss: 0.4241\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3554 - val_loss: 0.4224\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3554 - val_loss: 0.4209\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3499 - val_loss: 0.4325\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3406 - val_loss: 0.4256\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3464 - val_loss: 0.4277\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3538 - val_loss: 0.4187\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3406 - val_loss: 0.4207\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.3380 - val_loss: 0.4149\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3444 - val_loss: 0.4138\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3437 - val_loss: 0.4483\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3446 - val_loss: 0.4140\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3317 - val_loss: 0.4351\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3522 - val_loss: 0.4264\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3299 - val_loss: 0.4128\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3317 - val_loss: 0.4125\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3357 - val_loss: 0.4185\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3172 - val_loss: 0.4137\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3311 - val_loss: 0.4131\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3362 - val_loss: 0.4145\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3332 - val_loss: 0.4129\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3311 - val_loss: 0.4119\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3366 - val_loss: 0.4117\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3353 - val_loss: 0.4122\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3222 - val_loss: 0.4127\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3078 - val_loss: 0.4132\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3201 - val_loss: 0.4126\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3196 - val_loss: 0.4124\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3162 - val_loss: 0.4124\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3284 - val_loss: 0.4123\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3175 - val_loss: 0.4125\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_21-17:18:18] Training the entire fine-tuned model...\n",
      "[2022_04_21-17:19:06] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 771ms/step - loss: 0.4986 - val_loss: 0.5498\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.4101 - val_loss: 0.4182\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3309 - val_loss: 0.4177\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3082 - val_loss: 0.4473\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3188 - val_loss: 0.4160\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 331ms/step - loss: 0.2952 - val_loss: 0.4183\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.2797 - val_loss: 0.4269\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.2558 - val_loss: 0.4436\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.2465 - val_loss: 0.4375\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.2201 - val_loss: 0.4409\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.2171 - val_loss: 0.4429\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.2007 - val_loss: 0.4490\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.1895 - val_loss: 0.4593\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_21-17:19:36] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-17:19:36] Training set: Filtered out 0 of 621 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:19:36] Validation set: Filtered out 0 of 268 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 742ms/step - loss: 0.2825 - val_loss: 0.4176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27a/2022_04_21_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-17:20:28] Test set: Filtered out 0 of 402 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2d7fgmuv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 20943... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▆▅▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▃▂▃▂▃▂▂▂▂▂▃▂▂▂▂▁▁▂</td></tr><tr><td>lr</td><td>█████████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃█▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>42</td></tr><tr><td>best_val_loss</td><td>0.41173</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.28254</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.41758</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">zany-water-16</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027a/runs/2d7fgmuv\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027a/runs/2d7fgmuv</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_171711-2d7fgmuv/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2d7fgmuv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2027b/runs/xjh752fn\" target=\"_blank\">lyric-shadow-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2027b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-17:20:50] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:20:50] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:20:50] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 515ms/step - loss: 1.0557 - val_loss: 0.9675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.7415 - val_loss: 0.7722\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.6382 - val_loss: 0.7071\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.5354 - val_loss: 0.5647\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.5264 - val_loss: 0.4898\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4591 - val_loss: 0.4509\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4843 - val_loss: 0.5123\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4668 - val_loss: 0.5581\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4566 - val_loss: 0.4454\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4507 - val_loss: 0.4574\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4329 - val_loss: 0.4870\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4223 - val_loss: 0.4348\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3831 - val_loss: 0.4343\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3956 - val_loss: 0.5113\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4024 - val_loss: 0.4538\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3941 - val_loss: 0.4238\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 0.3768 - val_loss: 0.4463\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3718 - val_loss: 0.4246\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3699 - val_loss: 0.4232\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3680 - val_loss: 0.4926\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3775 - val_loss: 0.4843\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3989 - val_loss: 0.4243\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3488 - val_loss: 0.4239\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3564 - val_loss: 0.4196\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3405 - val_loss: 0.4179\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3477 - val_loss: 0.4180\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3411 - val_loss: 0.4202\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3561 - val_loss: 0.4169\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3417 - val_loss: 0.4175\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3461 - val_loss: 0.4179\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3427 - val_loss: 0.4199\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3425 - val_loss: 0.4164\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3396 - val_loss: 0.4178\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3441 - val_loss: 0.4147\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3419 - val_loss: 0.4146\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3362 - val_loss: 0.4197\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3359 - val_loss: 0.4143\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3381 - val_loss: 0.4142\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3426 - val_loss: 0.4162\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3293 - val_loss: 0.4156\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3362 - val_loss: 0.4207\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3429 - val_loss: 0.4144\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3296 - val_loss: 0.4144\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3280 - val_loss: 0.4145\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3280 - val_loss: 0.4148\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3264 - val_loss: 0.4165\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-17:21:39] Training the entire fine-tuned model...\n",
      "[2022_04_21-17:21:47] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5482WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1005s vs `on_train_batch_end` time: 0.1277s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1005s vs `on_train_batch_end` time: 0.1277s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 829ms/step - loss: 0.5482 - val_loss: 0.5377\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.4026 - val_loss: 0.5078\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3667 - val_loss: 0.4459\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3587 - val_loss: 0.4145\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3456 - val_loss: 0.4164\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3133 - val_loss: 0.4171\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3021 - val_loss: 0.4206\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.2796 - val_loss: 0.4277\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 288ms/step - loss: 0.2719 - val_loss: 0.4249\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.2696 - val_loss: 0.4246\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.2592 - val_loss: 0.4258\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.2586 - val_loss: 0.4256\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_21-17:22:16] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-17:22:16] Training set: Filtered out 0 of 670 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:22:16] Validation set: Filtered out 0 of 248 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3258WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0984s vs `on_train_batch_end` time: 0.1285s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0984s vs `on_train_batch_end` time: 0.1285s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 616ms/step - loss: 0.3258 - val_loss: 0.4169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/27b/2022_04_21_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-17:22:50] Test set: Filtered out 0 of 373 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:xjh752fn) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 21385... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▄▂▂▂▁▁▁▁▂</td></tr><tr><td>lr</td><td>████████████████▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▅▂▁▃▁▂▁▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>37</td></tr><tr><td>best_val_loss</td><td>0.41417</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32579</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.41695</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">lyric-shadow-16</strong>: <a href=\"https://wandb.ai/kvetab/Split%2027b/runs/xjh752fn\" target=\"_blank\">https://wandb.ai/kvetab/Split%2027b/runs/xjh752fn</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_172031-xjh752fn/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:xjh752fn). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/1qejtsli\" target=\"_blank\">firm-bee-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-17:23:09] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:23:10] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:23:10] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 831ms/step - loss: 0.7491 - val_loss: 0.9054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.7018 - val_loss: 0.6301\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.5588 - val_loss: 0.5835\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.5128 - val_loss: 0.4982\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.4684 - val_loss: 0.5118\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4197 - val_loss: 0.4581\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4170 - val_loss: 0.4493\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4164 - val_loss: 0.5623\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4251 - val_loss: 0.4516\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4025 - val_loss: 0.4559\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-17:23:29] Training the entire fine-tuned model...\n",
      "[2022_04_21-17:23:38] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3981WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1081s vs `on_train_batch_end` time: 0.1278s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1081s vs `on_train_batch_end` time: 0.1278s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 647ms/step - loss: 0.3981 - val_loss: 0.4507\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 267ms/step - loss: 0.3971 - val_loss: 0.4561\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3995 - val_loss: 0.4559\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3915 - val_loss: 0.4499\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 288ms/step - loss: 0.3918 - val_loss: 0.4467\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 289ms/step - loss: 0.3919 - val_loss: 0.4481\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3877 - val_loss: 0.4528\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3958 - val_loss: 0.4529\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "[2022_04_21-17:23:59] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-17:23:59] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:23:59] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3904WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0976s vs `on_train_batch_end` time: 0.1314s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0976s vs `on_train_batch_end` time: 0.1314s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 609ms/step - loss: 0.3904 - val_loss: 0.4468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-17:24:38] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1qejtsli) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 21780... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█▁▂▃▃▄▅▆▆▁</td></tr><tr><td>loss</td><td>█▇▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▁▁▃▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>4</td></tr><tr><td>best_val_loss</td><td>0.44665</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.39036</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.44682</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">firm-bee-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/1qejtsli\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/1qejtsli</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_172253-1qejtsli/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1qejtsli). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/16kyev14\" target=\"_blank\">revived-glade-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-17:24:57] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:24:58] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:24:58] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 635ms/step - loss: 0.7885 - val_loss: 1.0212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 0.7028 - val_loss: 0.5862\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5876 - val_loss: 0.4911\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4982 - val_loss: 0.4279\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4643 - val_loss: 0.4234\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 0.4441 - val_loss: 0.4171\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4301 - val_loss: 0.4044\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4266 - val_loss: 0.4004\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4234 - val_loss: 0.3963\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4019 - val_loss: 0.4004\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4088 - val_loss: 0.4304\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4292 - val_loss: 0.4106\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-17:25:18] Training the entire fine-tuned model...\n",
      "[2022_04_21-17:25:26] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 793ms/step - loss: 0.4198 - val_loss: 0.3974\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.4091 - val_loss: 0.3990\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 326ms/step - loss: 0.4137 - val_loss: 0.4009\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 301ms/step - loss: 0.4028 - val_loss: 0.4005\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "[2022_04_21-17:25:42] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-17:25:42] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:25:42] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 739ms/step - loss: 0.4127 - val_loss: 0.3973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-17:26:15] Test set: Filtered out 0 of 407 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:16kyev14) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 21975... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█▁▂▂▃▁</td></tr><tr><td>loss</td><td>█▆▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_val_loss</td><td>0.39628</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.41275</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.39734</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">revived-glade-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/16kyev14\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/16kyev14</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_172440-16kyev14/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:16kyev14). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/o52pcdvq\" target=\"_blank\">wandering-river-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-17:26:35] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:26:35] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:26:35] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 547ms/step - loss: 0.8106 - val_loss: 0.8326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.6371 - val_loss: 0.7665\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.5585 - val_loss: 0.4942\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.5424 - val_loss: 0.4768\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4669 - val_loss: 0.5983\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4530 - val_loss: 0.4593\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4064 - val_loss: 0.4722\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4016 - val_loss: 0.4567\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3916 - val_loss: 0.4366\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3814 - val_loss: 0.4340\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3802 - val_loss: 0.4434\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3711 - val_loss: 0.4470\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3872 - val_loss: 0.4321\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3686 - val_loss: 0.4318\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3887 - val_loss: 0.5149\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3835 - val_loss: 0.4287\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3584 - val_loss: 0.4188\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3627 - val_loss: 0.4556\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3675 - val_loss: 0.4120\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3467 - val_loss: 0.4176\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3677 - val_loss: 0.4807\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3580 - val_loss: 0.4130\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-17:27:05] Training the entire fine-tuned model...\n",
      "[2022_04_21-17:27:14] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3595WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1000s vs `on_train_batch_end` time: 0.1309s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1000s vs `on_train_batch_end` time: 0.1309s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 644ms/step - loss: 0.3595 - val_loss: 0.4133\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3536 - val_loss: 0.4235\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3429 - val_loss: 0.4139\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 268ms/step - loss: 0.3358 - val_loss: 0.4113\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3223 - val_loss: 0.4128\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3243 - val_loss: 0.4101\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3275 - val_loss: 0.4211\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3170 - val_loss: 0.4125\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3204 - val_loss: 0.4129\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_21-17:27:36] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-17:27:36] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:27:36] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3235WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0993s vs `on_train_batch_end` time: 0.1305s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0993s vs `on_train_batch_end` time: 0.1305s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 631ms/step - loss: 0.3235 - val_loss: 0.4101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-17:28:11] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:o52pcdvq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 22161... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▆▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████████▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▂▂▄▂▂▂▁▁▂▂▁▁▃▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.41012</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32353</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41012</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">wandering-river-2</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/o52pcdvq\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/o52pcdvq</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_172618-o52pcdvq/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:o52pcdvq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/2czu4zdv\" target=\"_blank\">hopeful-mountain-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-17:28:31] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:28:31] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:28:31] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 642ms/step - loss: 0.8785 - val_loss: 1.0077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.6760 - val_loss: 0.6596\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.6141 - val_loss: 0.5796\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5143 - val_loss: 0.4551\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4826 - val_loss: 0.4372\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4849 - val_loss: 0.4199\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4432 - val_loss: 0.4120\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4640 - val_loss: 0.4431\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4315 - val_loss: 0.4110\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4249 - val_loss: 0.4075\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4026 - val_loss: 0.3929\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4004 - val_loss: 0.3981\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3962 - val_loss: 0.3884\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3945 - val_loss: 0.3847\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3932 - val_loss: 0.4227\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3845 - val_loss: 0.3818\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3815 - val_loss: 0.3798\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3939 - val_loss: 0.4609\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4242 - val_loss: 0.3861\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3681 - val_loss: 0.3738\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3731 - val_loss: 0.3947\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3611 - val_loss: 0.3730\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3593 - val_loss: 0.3805\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3563 - val_loss: 0.3769\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3679 - val_loss: 0.3711\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3453 - val_loss: 0.3713\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3457 - val_loss: 0.3699\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3547 - val_loss: 0.3744\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3510 - val_loss: 0.3724\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3671 - val_loss: 0.3773\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-17:29:05] Training the entire fine-tuned model...\n",
      "[2022_04_21-17:29:13] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 757ms/step - loss: 0.3452 - val_loss: 0.3733\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3270 - val_loss: 0.3682\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3468 - val_loss: 0.3756\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3444 - val_loss: 0.3673\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3277 - val_loss: 0.3756\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3141 - val_loss: 0.3650\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 309ms/step - loss: 0.3270 - val_loss: 0.3644\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 300ms/step - loss: 0.3220 - val_loss: 0.3666\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3165 - val_loss: 0.3652\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3092 - val_loss: 0.3653\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_21-17:29:38] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-17:29:38] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:29:38] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 723ms/step - loss: 0.3139 - val_loss: 0.3645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-17:30:12] Test set: Filtered out 0 of 407 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2czu4zdv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 22422... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▅▅▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████████████████▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▂▂▂▂▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>6</td></tr><tr><td>best_val_loss</td><td>0.3644</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31394</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.3645</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">hopeful-mountain-2</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/2czu4zdv\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/2czu4zdv</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_172813-2czu4zdv/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2czu4zdv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/39cg2ccr\" target=\"_blank\">swift-sun-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-17:30:31] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:30:31] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:30:31] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 503ms/step - loss: 0.8402 - val_loss: 0.9981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.7191 - val_loss: 0.7976\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.5267 - val_loss: 0.5018\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4864 - val_loss: 0.5087\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4409 - val_loss: 0.4992\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4302 - val_loss: 0.4591\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4059 - val_loss: 0.4701\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4091 - val_loss: 0.4546\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3997 - val_loss: 0.4348\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3872 - val_loss: 0.4307\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3815 - val_loss: 0.4293\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3721 - val_loss: 0.4320\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3676 - val_loss: 0.4257\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3639 - val_loss: 0.4344\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3607 - val_loss: 0.4540\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3696 - val_loss: 0.4301\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-17:30:56] Training the entire fine-tuned model...\n",
      "[2022_04_21-17:31:09] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3716WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0998s vs `on_train_batch_end` time: 0.1280s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0998s vs `on_train_batch_end` time: 0.1280s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 630ms/step - loss: 0.3716 - val_loss: 0.4260\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3649 - val_loss: 0.4618\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3730 - val_loss: 0.4181\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3549 - val_loss: 0.4346\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3550 - val_loss: 0.4191\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3381 - val_loss: 0.4235\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_21-17:31:27] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-17:31:27] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:31:35] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3513WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0991s vs `on_train_batch_end` time: 0.1311s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0991s vs `on_train_batch_end` time: 0.1311s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 623ms/step - loss: 0.3513 - val_loss: 0.4179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-17:32:09] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:39cg2ccr) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 22734... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██▁▁▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▆▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.41788</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35129</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.41788</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">swift-sun-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/39cg2ccr\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/39cg2ccr</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_173014-39cg2ccr/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:39cg2ccr). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/1m88ix6w\" target=\"_blank\">dutiful-microwave-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-17:32:29] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:32:29] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:32:29] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 597ms/step - loss: 0.8631 - val_loss: 1.1059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.8249 - val_loss: 0.7295\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.6460 - val_loss: 0.7523\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5759 - val_loss: 0.5446\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5449 - val_loss: 0.5886\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4852 - val_loss: 0.4658\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4681 - val_loss: 0.5109\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4616 - val_loss: 0.4384\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.4529 - val_loss: 0.4423\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4219 - val_loss: 0.4001\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4053 - val_loss: 0.4361\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4203 - val_loss: 0.3932\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4064 - val_loss: 0.3998\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3923 - val_loss: 0.3925\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3822 - val_loss: 0.3868\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3814 - val_loss: 0.3906\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3773 - val_loss: 0.3911\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3682 - val_loss: 0.3829\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3840 - val_loss: 0.4161\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3923 - val_loss: 0.3846\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3823 - val_loss: 0.3819\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3764 - val_loss: 0.3865\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3727 - val_loss: 0.3741\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3477 - val_loss: 0.3730\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3592 - val_loss: 0.3838\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3655 - val_loss: 0.3732\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3425 - val_loss: 0.3707\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3622 - val_loss: 0.3705\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3484 - val_loss: 0.3696\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3688 - val_loss: 0.3881\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3419 - val_loss: 0.3685\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3401 - val_loss: 0.3695\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3335 - val_loss: 0.3902\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3275 - val_loss: 0.3681\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3382 - val_loss: 0.3679\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3349 - val_loss: 0.3650\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3515 - val_loss: 0.3835\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3251 - val_loss: 0.3691\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3302 - val_loss: 0.3679\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-17:33:09] Training the entire fine-tuned model...\n",
      "[2022_04_21-17:33:19] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 746ms/step - loss: 0.3716 - val_loss: 0.3747\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3526 - val_loss: 0.3761\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3391 - val_loss: 0.4005\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3311 - val_loss: 0.3715\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3278 - val_loss: 0.3673\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3208 - val_loss: 0.3664\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 327ms/step - loss: 0.3108 - val_loss: 0.3640\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 328ms/step - loss: 0.3094 - val_loss: 0.3752\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.2986 - val_loss: 0.3663\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3066 - val_loss: 0.3677\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_21-17:33:43] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-17:33:43] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:33:43] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 11s 742ms/step - loss: 0.3037 - val_loss: 0.3631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-17:34:19] Test set: Filtered out 0 of 407 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1m88ix6w) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 22950... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███▁▁▂▂▂▂▂▁</td></tr><tr><td>loss</td><td>██▅▄▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████████████████████▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▅▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.36314</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30371</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.36314</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dutiful-microwave-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/1m88ix6w\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/1m88ix6w</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_173212-1m88ix6w/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1m88ix6w). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/9z2jxbzu\" target=\"_blank\">wandering-tree-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-17:34:38] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:34:38] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:34:38] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 509ms/step - loss: 0.8448 - val_loss: 1.0066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.7060 - val_loss: 0.7659\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.5368 - val_loss: 0.4850\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4816 - val_loss: 0.4820\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4245 - val_loss: 0.5023\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.4367 - val_loss: 0.4735\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.4421 - val_loss: 0.5404\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4253 - val_loss: 0.4407\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4047 - val_loss: 0.4425\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3951 - val_loss: 0.4548\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3733 - val_loss: 0.4407\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3731 - val_loss: 0.4397\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3706 - val_loss: 0.4282\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3754 - val_loss: 0.4367\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.3729 - val_loss: 0.4336\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3722 - val_loss: 0.4249\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3769 - val_loss: 0.4410\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3664 - val_loss: 0.4239\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3684 - val_loss: 0.4380\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3633 - val_loss: 0.4233\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3658 - val_loss: 0.4303\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3629 - val_loss: 0.4263\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3588 - val_loss: 0.4300\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_21-17:35:08] Training the entire fine-tuned model...\n",
      "[2022_04_21-17:35:16] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4583WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1007s vs `on_train_batch_end` time: 0.1280s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1007s vs `on_train_batch_end` time: 0.1280s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 611ms/step - loss: 0.4583 - val_loss: 0.4660\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3768 - val_loss: 0.4493\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3910 - val_loss: 0.4731\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3651 - val_loss: 0.4226\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 288ms/step - loss: 0.3697 - val_loss: 0.4312\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3650 - val_loss: 0.5086\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 267ms/step - loss: 0.3420 - val_loss: 0.4351\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_21-17:35:35] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-17:35:35] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:35:35] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3408WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0971s vs `on_train_batch_end` time: 0.1298s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0971s vs `on_train_batch_end` time: 0.1298s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 606ms/step - loss: 0.3408 - val_loss: 0.4221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-17:36:10] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:9z2jxbzu) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 23299... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▆▄▃▂▂▂▂▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▃▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.42213</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34077</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.42213</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">wandering-tree-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/9z2jxbzu\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/9z2jxbzu</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_173422-9z2jxbzu/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:9z2jxbzu). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/101vxv0v\" target=\"_blank\">good-blaze-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-17:36:34] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:36:34] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:36:34] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 635ms/step - loss: 0.8745 - val_loss: 0.8711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.6745 - val_loss: 0.5575\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5618 - val_loss: 0.4915\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5352 - val_loss: 0.4279\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4955 - val_loss: 0.4197\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4588 - val_loss: 0.4512\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4478 - val_loss: 0.4078\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4349 - val_loss: 0.4315\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4407 - val_loss: 0.4012\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4164 - val_loss: 0.3950\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4065 - val_loss: 0.3930\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4014 - val_loss: 0.3964\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3978 - val_loss: 0.3927\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3925 - val_loss: 0.3854\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3900 - val_loss: 0.4048\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3956 - val_loss: 0.3827\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 187ms/step - loss: 0.3681 - val_loss: 0.3945\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3927 - val_loss: 0.4206\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3927 - val_loss: 0.3997\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-17:36:59] Training the entire fine-tuned model...\n",
      "[2022_04_21-17:37:09] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 771ms/step - loss: 0.6037 - val_loss: 0.4562\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.4424 - val_loss: 0.4207\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.4181 - val_loss: 0.4055\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3980 - val_loss: 0.4119\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3965 - val_loss: 0.3930\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3789 - val_loss: 0.4040\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3562 - val_loss: 0.3891\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 329ms/step - loss: 0.3665 - val_loss: 0.4046\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3395 - val_loss: 0.3792\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3185 - val_loss: 0.3828\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 328ms/step - loss: 0.3118 - val_loss: 0.3872\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3052 - val_loss: 0.3784\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 301ms/step - loss: 0.2796 - val_loss: 0.3857\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.2570 - val_loss: 0.3770\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 301ms/step - loss: 0.2347 - val_loss: 0.3941\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 327ms/step - loss: 0.2184 - val_loss: 0.4016\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.2045 - val_loss: 0.4084\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_21-17:37:44] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-17:37:44] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:37:44] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 725ms/step - loss: 0.2441 - val_loss: 0.3781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-17:38:17] Test set: Filtered out 0 of 407 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:101vxv0v) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 23555... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▁</td></tr><tr><td>loss</td><td>█▆▅▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▅▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▂▁▂▁▁▁▁▁▁▁▁▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>13</td></tr><tr><td>best_val_loss</td><td>0.37703</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.24414</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.37809</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">good-blaze-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/101vxv0v\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/101vxv0v</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_173613-101vxv0v/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:101vxv0v). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/2bk40mdx\" target=\"_blank\">young-frog-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-17:38:35] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:38:36] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:38:36] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 488ms/step - loss: 0.7920 - val_loss: 1.1567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.7558 - val_loss: 1.0316\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 0.6469 - val_loss: 0.6421\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 0.5348 - val_loss: 0.6041\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.4678 - val_loss: 0.4628\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.4264 - val_loss: 0.4717\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4012 - val_loss: 0.4605\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.4025 - val_loss: 0.4466\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3904 - val_loss: 0.4462\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.3922 - val_loss: 0.4389\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3784 - val_loss: 0.4309\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3810 - val_loss: 0.4444\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.3778 - val_loss: 0.4354\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.3711 - val_loss: 0.4335\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3799 - val_loss: 0.4354\n",
      "[2022_04_21-17:39:00] Training the entire fine-tuned model...\n",
      "[2022_04_21-17:39:13] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3841WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1039s vs `on_train_batch_end` time: 0.1267s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1039s vs `on_train_batch_end` time: 0.1267s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 640ms/step - loss: 0.3841 - val_loss: 0.4320\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 268ms/step - loss: 0.3695 - val_loss: 0.4367\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3794 - val_loss: 0.4400\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 268ms/step - loss: 0.3755 - val_loss: 0.4361\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 289ms/step - loss: 0.3706 - val_loss: 0.4343\n",
      "[2022_04_21-17:39:31] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-17:39:31] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:39:31] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3812WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1009s vs `on_train_batch_end` time: 0.1294s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1009s vs `on_train_batch_end` time: 0.1294s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 623ms/step - loss: 0.3812 - val_loss: 0.4318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-17:40:04] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2bk40mdx) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 23842... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█▁▁▂▃▃▁</td></tr><tr><td>loss</td><td>█▇▆▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████▃▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>10</td></tr><tr><td>best_val_loss</td><td>0.43091</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.38125</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.43179</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">young-frog-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/2bk40mdx\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/2bk40mdx</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_173819-2bk40mdx/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2bk40mdx). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/krme6qlu\" target=\"_blank\">splendid-plasma-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-17:40:23] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:40:23] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:40:23] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 1s/step - loss: 1.0402 - val_loss: 0.6850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.6721 - val_loss: 0.6329\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.6067 - val_loss: 0.5889\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5546 - val_loss: 0.4571\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5016 - val_loss: 0.4461\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4718 - val_loss: 0.4129\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4415 - val_loss: 0.4145\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4253 - val_loss: 0.4050\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4221 - val_loss: 0.3993\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4138 - val_loss: 0.4245\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.4167 - val_loss: 0.4011\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.4214 - val_loss: 0.4168\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4070 - val_loss: 0.3906\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4010 - val_loss: 0.3903\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3858 - val_loss: 0.4000\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4052 - val_loss: 0.3908\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3876 - val_loss: 0.3903\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3847 - val_loss: 0.3903\n",
      "[2022_04_21-17:40:49] Training the entire fine-tuned model...\n",
      "[2022_04_21-17:41:00] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 758ms/step - loss: 0.3937 - val_loss: 0.3926\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3915 - val_loss: 0.3942\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3998 - val_loss: 0.3915\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 301ms/step - loss: 0.3995 - val_loss: 0.3901\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 301ms/step - loss: 0.3818 - val_loss: 0.3899\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 328ms/step - loss: 0.3949 - val_loss: 0.3900\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3904 - val_loss: 0.3907\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3849 - val_loss: 0.3895\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 301ms/step - loss: 0.3892 - val_loss: 0.3897\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3797 - val_loss: 0.3892\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 331ms/step - loss: 0.3868 - val_loss: 0.3878\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3834 - val_loss: 0.3879\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 301ms/step - loss: 0.3909 - val_loss: 0.3869\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3763 - val_loss: 0.3885\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3818 - val_loss: 0.3880\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3785 - val_loss: 0.3862\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3799 - val_loss: 0.3854\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3805 - val_loss: 0.3866\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3805 - val_loss: 0.3868\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3752 - val_loss: 0.3866\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3732 - val_loss: 0.3866\n",
      "[2022_04_21-17:41:40] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-17:41:40] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:41:46] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 747ms/step - loss: 0.3760 - val_loss: 0.3854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-17:42:20] Test set: Filtered out 0 of 407 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:krme6qlu) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 24044... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██▁</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▆▃▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>16</td></tr><tr><td>best_val_loss</td><td>0.38535</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37603</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.38544</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">splendid-plasma-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/krme6qlu\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/krme6qlu</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_174006-krme6qlu/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:krme6qlu). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/9oyb4osn\" target=\"_blank\">hearty-smoke-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-17:42:41] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:42:41] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:42:41] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 508ms/step - loss: 0.8614 - val_loss: 0.6493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.5466 - val_loss: 0.5016\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4526 - val_loss: 0.4741\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4259 - val_loss: 0.5072\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4397 - val_loss: 0.4810\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4213 - val_loss: 0.4477\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3949 - val_loss: 0.4550\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3866 - val_loss: 0.4953\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3969 - val_loss: 0.4438\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4051 - val_loss: 0.5048\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4549 - val_loss: 0.5365\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.4211 - val_loss: 0.6051\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4809 - val_loss: 0.4624\n",
      "[2022_04_21-17:43:02] Training the entire fine-tuned model...\n",
      "[2022_04_21-17:43:10] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3796WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1020s vs `on_train_batch_end` time: 0.1258s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1020s vs `on_train_batch_end` time: 0.1258s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 1s/step - loss: 0.3796 - val_loss: 0.4311\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3716 - val_loss: 0.4463\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3700 - val_loss: 0.4299\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 267ms/step - loss: 0.3653 - val_loss: 0.4311\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3588 - val_loss: 0.4315\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3579 - val_loss: 0.4285\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 270ms/step - loss: 0.3551 - val_loss: 0.4248\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3584 - val_loss: 0.4271\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3587 - val_loss: 0.4280\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3462 - val_loss: 0.4269\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3497 - val_loss: 0.4331\n",
      "[2022_04_21-17:43:38] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-17:43:38] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:43:38] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3617WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1003s vs `on_train_batch_end` time: 0.1262s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1003s vs `on_train_batch_end` time: 0.1262s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 630ms/step - loss: 0.3617 - val_loss: 0.4258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-17:44:11] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:9oyb4osn) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 24359... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▅▅▆▆▇▇█▁▂▂▃▃▄▅▅▆▆▇▁</td></tr><tr><td>loss</td><td>█▄▂▂▂▂▂▂▂▂▂▂▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▄▃▂▂▃▂▃▄▇▂▁▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>6</td></tr><tr><td>best_val_loss</td><td>0.42484</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36175</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.4258</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">hearty-smoke-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/9oyb4osn\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/9oyb4osn</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_174223-9oyb4osn/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:9oyb4osn). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/20lsdomj\" target=\"_blank\">laced-energy-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-17:44:30] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:44:30] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:44:30] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 626ms/step - loss: 0.9418 - val_loss: 0.7671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.6976 - val_loss: 0.5878\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 185ms/step - loss: 0.5719 - val_loss: 0.5774\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.5133 - val_loss: 0.4538\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4944 - val_loss: 0.4423\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.4742 - val_loss: 0.4156\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4396 - val_loss: 0.4109\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4376 - val_loss: 0.4089\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4255 - val_loss: 0.4207\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4204 - val_loss: 0.4269\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4402 - val_loss: 0.4767\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4272 - val_loss: 0.3960\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4176 - val_loss: 0.3944\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3998 - val_loss: 0.4115\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3952 - val_loss: 0.3935\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4076 - val_loss: 0.3934\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3939 - val_loss: 0.4050\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3981 - val_loss: 0.3927\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3930 - val_loss: 0.3926\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3832 - val_loss: 0.3960\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3898 - val_loss: 0.3926\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3912 - val_loss: 0.3913\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3871 - val_loss: 0.3930\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3814 - val_loss: 0.3902\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3847 - val_loss: 0.3916\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3846 - val_loss: 0.3933\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3774 - val_loss: 0.3896\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3758 - val_loss: 0.3873\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3806 - val_loss: 0.3891\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3770 - val_loss: 0.3857\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3765 - val_loss: 0.3951\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3837 - val_loss: 0.3854\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3812 - val_loss: 0.3868\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3737 - val_loss: 0.3863\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3771 - val_loss: 0.3882\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3742 - val_loss: 0.3867\n",
      "[2022_04_21-17:45:09] Training the entire fine-tuned model...\n",
      "[2022_04_21-17:45:18] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 754ms/step - loss: 0.3794 - val_loss: 0.3846\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3717 - val_loss: 0.3946\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3716 - val_loss: 0.3832\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3648 - val_loss: 0.3841\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3715 - val_loss: 0.3838\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3575 - val_loss: 0.3797\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3546 - val_loss: 0.3782\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3564 - val_loss: 0.3769\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3447 - val_loss: 0.3767\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3429 - val_loss: 0.3766\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3399 - val_loss: 0.3746\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3438 - val_loss: 0.3796\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3314 - val_loss: 0.3735\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3277 - val_loss: 0.3805\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 329ms/step - loss: 0.3381 - val_loss: 0.3730\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3240 - val_loss: 0.3733\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3094 - val_loss: 0.3801\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.2911 - val_loss: 0.3738\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.2922 - val_loss: 0.3755\n",
      "[2022_04_21-17:45:55] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-17:45:55] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:45:55] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 11s 775ms/step - loss: 0.3201 - val_loss: 0.3728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-17:46:30] Test set: Filtered out 0 of 407 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:20lsdomj) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 24576... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▄▄▄▄▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁</td></tr><tr><td>lr</td><td>████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▅▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.37279</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3201</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.37279</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">laced-energy-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/20lsdomj\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/20lsdomj</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_174413-20lsdomj/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:20lsdomj). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/3kebfv38\" target=\"_blank\">magic-frost-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-17:46:54] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:46:54] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:46:54] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 504ms/step - loss: 0.8495 - val_loss: 1.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.7009 - val_loss: 0.8293\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.5842 - val_loss: 0.4899\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4535 - val_loss: 0.4867\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4215 - val_loss: 0.4726\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4082 - val_loss: 0.4631\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4028 - val_loss: 0.4610\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3969 - val_loss: 0.4442\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3913 - val_loss: 0.4385\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3867 - val_loss: 0.4452\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3767 - val_loss: 0.4370\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3923 - val_loss: 0.4365\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3823 - val_loss: 0.4241\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3757 - val_loss: 0.4791\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3776 - val_loss: 0.4473\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3733 - val_loss: 0.4225\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3510 - val_loss: 0.4336\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3524 - val_loss: 0.4353\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3725 - val_loss: 0.4190\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3559 - val_loss: 0.4184\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3530 - val_loss: 0.4163\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3470 - val_loss: 0.4199\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3358 - val_loss: 0.4194\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3394 - val_loss: 0.4735\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3523 - val_loss: 0.4093\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 0.3418 - val_loss: 0.4095\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3201 - val_loss: 0.4328\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3379 - val_loss: 0.4096\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3165 - val_loss: 0.4115\n",
      "[2022_04_21-17:47:29] Training the entire fine-tuned model...\n",
      "[2022_04_21-17:47:36] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3343WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1011s vs `on_train_batch_end` time: 0.1281s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1011s vs `on_train_batch_end` time: 0.1281s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 627ms/step - loss: 0.3343 - val_loss: 0.4401\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3321 - val_loss: 0.4290\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3353 - val_loss: 0.4442\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3382 - val_loss: 0.4071\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3083 - val_loss: 0.4324\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3059 - val_loss: 0.4111\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3107 - val_loss: 0.4473\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.2997 - val_loss: 0.4189\n",
      "[2022_04_21-17:47:57] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-17:47:57] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:47:57] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3222WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1020s vs `on_train_batch_end` time: 0.1277s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1020s vs `on_train_batch_end` time: 0.1277s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 629ms/step - loss: 0.3222 - val_loss: 0.4082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-17:48:39] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3kebfv38) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 24970... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██▁▁▁▂▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▆▅▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▂▂▂▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>3</td></tr><tr><td>best_val_loss</td><td>0.40708</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32219</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.40817</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">magic-frost-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/3kebfv38\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/3kebfv38</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_174632-3kebfv38/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3kebfv38). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/3dq5ufyc\" target=\"_blank\">prime-monkey-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-17:48:58] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:48:58] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:48:58] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 622ms/step - loss: 0.9412 - val_loss: 0.8020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.6884 - val_loss: 0.5812\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.5875 - val_loss: 0.5996\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5210 - val_loss: 0.4518\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4670 - val_loss: 0.4824\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4806 - val_loss: 0.4149\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 0.4401 - val_loss: 0.4157\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4356 - val_loss: 0.4063\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4223 - val_loss: 0.4171\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4298 - val_loss: 0.4026\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4172 - val_loss: 0.3964\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4056 - val_loss: 0.3988\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3999 - val_loss: 0.3917\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3898 - val_loss: 0.3904\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3906 - val_loss: 0.3912\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3761 - val_loss: 0.3989\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3831 - val_loss: 0.3925\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3805 - val_loss: 0.3857\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3640 - val_loss: 0.3835\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3688 - val_loss: 0.3854\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3760 - val_loss: 0.3888\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3654 - val_loss: 0.3824\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3607 - val_loss: 0.3883\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3762 - val_loss: 0.3818\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3709 - val_loss: 0.3803\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.3603 - val_loss: 0.3890\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3587 - val_loss: 0.3833\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3671 - val_loss: 0.3794\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3583 - val_loss: 0.3895\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3601 - val_loss: 0.3787\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3575 - val_loss: 0.3812\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3657 - val_loss: 0.3833\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3640 - val_loss: 0.3794\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3516 - val_loss: 0.3791\n",
      "[2022_04_21-17:49:35] Training the entire fine-tuned model...\n",
      "[2022_04_21-17:49:57] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 750ms/step - loss: 0.3710 - val_loss: 0.3786\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 301ms/step - loss: 0.3550 - val_loss: 0.3852\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3614 - val_loss: 0.3806\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3523 - val_loss: 0.3759\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3348 - val_loss: 0.3760\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 326ms/step - loss: 0.3381 - val_loss: 0.3744\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3375 - val_loss: 0.3740\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3339 - val_loss: 0.3694\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 328ms/step - loss: 0.3174 - val_loss: 0.3778\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3020 - val_loss: 0.3694\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.2957 - val_loss: 0.3815\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3011 - val_loss: 0.3712\n",
      "[2022_04_21-17:50:23] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-17:50:23] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:50:23] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 1s/step - loss: 0.3272 - val_loss: 0.3691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-17:50:57] Test set: Filtered out 0 of 407 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3dq5ufyc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 25273... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▅▂▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.3691</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32723</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.3691</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">prime-monkey-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/3dq5ufyc\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/3dq5ufyc</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_174841-3dq5ufyc/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3dq5ufyc). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/3h20lphv\" target=\"_blank\">autumn-wave-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-17:51:17] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:51:17] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:51:17] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 522ms/step - loss: 0.8280 - val_loss: 0.7925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.5979 - val_loss: 0.6496\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.5029 - val_loss: 0.5084\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4443 - val_loss: 0.4844\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.4263 - val_loss: 0.4826\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.4145 - val_loss: 0.4537\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.4261 - val_loss: 0.4822\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4469 - val_loss: 0.4558\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.4017 - val_loss: 0.5146\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.4080 - val_loss: 0.4393\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3920 - val_loss: 0.4347\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3832 - val_loss: 0.4729\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3869 - val_loss: 0.4328\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3750 - val_loss: 0.4406\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3706 - val_loss: 0.4365\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3756 - val_loss: 0.4303\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3694 - val_loss: 0.4430\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 0.3706 - val_loss: 0.4313\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.3723 - val_loss: 0.4306\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3617 - val_loss: 0.4295\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3624 - val_loss: 0.4329\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.3677 - val_loss: 0.4399\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3722 - val_loss: 0.4369\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3633 - val_loss: 0.4349\n",
      "[2022_04_21-17:51:49] Training the entire fine-tuned model...\n",
      "[2022_04_21-17:51:57] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4035WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1019s vs `on_train_batch_end` time: 0.1289s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1019s vs `on_train_batch_end` time: 0.1289s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 625ms/step - loss: 0.4035 - val_loss: 0.4311\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3784 - val_loss: 0.4250\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3615 - val_loss: 0.5052\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3636 - val_loss: 0.4638\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3589 - val_loss: 0.4270\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3067 - val_loss: 0.4439\n",
      "[2022_04_21-17:52:16] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-17:52:16] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:52:16] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3553WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0992s vs `on_train_batch_end` time: 0.1302s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0992s vs `on_train_batch_end` time: 0.1302s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 633ms/step - loss: 0.3553 - val_loss: 0.4267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-17:52:49] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3h20lphv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 25617... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇██▁▁▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂</td></tr><tr><td>lr</td><td>█████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▂▂▂▃▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_val_loss</td><td>0.42495</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35533</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.4267</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">autumn-wave-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/3h20lphv\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/3h20lphv</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_175059-3h20lphv/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3h20lphv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/2h2vq12f\" target=\"_blank\">trim-plasma-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-17:53:09] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:53:09] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:53:09] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 1s/step - loss: 0.8574 - val_loss: 0.6845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5769 - val_loss: 0.4657\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5150 - val_loss: 0.4408\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4882 - val_loss: 0.5100\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4689 - val_loss: 0.4268\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4392 - val_loss: 0.4378\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4348 - val_loss: 0.4041\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4180 - val_loss: 0.3989\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4062 - val_loss: 0.4046\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4081 - val_loss: 0.4003\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4205 - val_loss: 0.3941\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3923 - val_loss: 0.3901\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3966 - val_loss: 0.3940\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4102 - val_loss: 0.3934\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4004 - val_loss: 0.4989\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4052 - val_loss: 0.3839\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3924 - val_loss: 0.3819\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3696 - val_loss: 0.4102\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3639 - val_loss: 0.3793\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3707 - val_loss: 0.3791\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3716 - val_loss: 0.3859\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3679 - val_loss: 0.3784\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3712 - val_loss: 0.3780\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3622 - val_loss: 0.3852\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3645 - val_loss: 0.3775\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3704 - val_loss: 0.3780\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3617 - val_loss: 0.3791\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3532 - val_loss: 0.3789\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3693 - val_loss: 0.3783\n",
      "[2022_04_21-17:53:44] Training the entire fine-tuned model...\n",
      "[2022_04_21-17:53:52] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 751ms/step - loss: 0.4181 - val_loss: 0.4753\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.4207 - val_loss: 0.4023\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3906 - val_loss: 0.3846\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3887 - val_loss: 0.3908\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3676 - val_loss: 0.3973\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3274 - val_loss: 0.3744\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 326ms/step - loss: 0.3155 - val_loss: 0.3866\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3165 - val_loss: 0.3852\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3193 - val_loss: 0.3742\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.2957 - val_loss: 0.3692\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 327ms/step - loss: 0.2589 - val_loss: 0.3839\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.2524 - val_loss: 0.3745\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.2159 - val_loss: 0.3904\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.1859 - val_loss: 0.3985\n",
      "[2022_04_21-17:54:22] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-17:54:22] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:54:39] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 762ms/step - loss: 0.2645 - val_loss: 0.3683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-17:55:13] Test set: Filtered out 0 of 407 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2h2vq12f) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 25874... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▆▆▇▇▇▇▇██▁▁▁▂▂▃▃▃▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▅▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▃▃▃▃▃▂▂▂▂▂▁▁▁▂</td></tr><tr><td>lr</td><td>██████████████▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▄▂▃▂▂▂▂▁▂▂▄▁▁▂▁▁▁▁▁▁▁▁▁▁▃▂▁▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.36832</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.26446</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.36832</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">trim-plasma-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/2h2vq12f\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/2h2vq12f</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_175252-2h2vq12f/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2h2vq12f). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/2etymm0u\" target=\"_blank\">clear-eon-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-17:55:33] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:55:33] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:55:33] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 514ms/step - loss: 0.9631 - val_loss: 0.7736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.6811 - val_loss: 0.7582\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.5471 - val_loss: 0.4839\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4731 - val_loss: 0.4923\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4417 - val_loss: 0.5081\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4395 - val_loss: 0.4528\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4135 - val_loss: 0.4480\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3980 - val_loss: 0.4498\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3877 - val_loss: 0.4361\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4005 - val_loss: 0.5161\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4037 - val_loss: 0.4307\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.4028 - val_loss: 0.4245\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3791 - val_loss: 0.4237\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3697 - val_loss: 0.4242\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3673 - val_loss: 0.4186\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3668 - val_loss: 0.4435\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3583 - val_loss: 0.4357\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3834 - val_loss: 0.4511\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.3713 - val_loss: 0.4444\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3652 - val_loss: 0.4505\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3544 - val_loss: 0.4137\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3483 - val_loss: 0.4231\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3550 - val_loss: 0.4325\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3433 - val_loss: 0.4123\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3458 - val_loss: 0.4179\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3514 - val_loss: 0.4237\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3542 - val_loss: 0.4116\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3399 - val_loss: 0.4306\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3396 - val_loss: 0.4121\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.3378 - val_loss: 0.4178\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3398 - val_loss: 0.4153\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3382 - val_loss: 0.4136\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3357 - val_loss: 0.4110\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3443 - val_loss: 0.4117\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3360 - val_loss: 0.4127\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3390 - val_loss: 0.4169\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3406 - val_loss: 0.4190\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3385 - val_loss: 0.4181\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3356 - val_loss: 0.4165\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_21-17:56:15] Training the entire fine-tuned model...\n",
      "[2022_04_21-17:56:30] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3366WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0995s vs `on_train_batch_end` time: 0.1304s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0995s vs `on_train_batch_end` time: 0.1304s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 12s 659ms/step - loss: 0.3366 - val_loss: 0.4172\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3351 - val_loss: 0.4131\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3363 - val_loss: 0.4134\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 267ms/step - loss: 0.3435 - val_loss: 0.4131\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3302 - val_loss: 0.4132\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3299 - val_loss: 0.4134\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3284 - val_loss: 0.4130\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3316 - val_loss: 0.4129\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3280 - val_loss: 0.4139\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3279 - val_loss: 0.4141\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 268ms/step - loss: 0.3195 - val_loss: 0.4137\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3403 - val_loss: 0.4135\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3335 - val_loss: 0.4136\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 289ms/step - loss: 0.3299 - val_loss: 0.4134\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      "[2022_04_21-17:57:03] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-17:57:03] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:57:03] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3247WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0997s vs `on_train_batch_end` time: 0.1282s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0997s vs `on_train_batch_end` time: 0.1282s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 620ms/step - loss: 0.3247 - val_loss: 0.4129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-17:57:39] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2etymm0u) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 26197... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▃▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>██▂▃▂▂▁▃▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>32</td></tr><tr><td>best_val_loss</td><td>0.41103</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32466</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41286</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">clear-eon-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/2etymm0u\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/2etymm0u</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_175516-2etymm0u/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2etymm0u). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/70wb71qc\" target=\"_blank\">dashing-planet-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-17:57:57] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:57:57] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:57:57] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 653ms/step - loss: 0.8394 - val_loss: 0.9407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.6967 - val_loss: 0.5639\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5473 - val_loss: 0.5243\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5163 - val_loss: 0.4344\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4732 - val_loss: 0.4239\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4564 - val_loss: 0.4241\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4409 - val_loss: 0.4057\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4309 - val_loss: 0.4171\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4167 - val_loss: 0.4031\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4065 - val_loss: 0.3924\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4080 - val_loss: 0.3927\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3985 - val_loss: 0.3926\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3944 - val_loss: 0.4061\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3825 - val_loss: 0.3853\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.3912 - val_loss: 0.3846\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3793 - val_loss: 0.4014\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3887 - val_loss: 0.3846\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3825 - val_loss: 0.3839\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3837 - val_loss: 0.3893\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3742 - val_loss: 0.3823\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3776 - val_loss: 0.3859\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3709 - val_loss: 0.3837\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3763 - val_loss: 0.3816\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3750 - val_loss: 0.3851\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3711 - val_loss: 0.3826\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3769 - val_loss: 0.3815\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.3664 - val_loss: 0.3843\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3782 - val_loss: 0.3824\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3650 - val_loss: 0.3808\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3718 - val_loss: 0.3802\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3743 - val_loss: 0.3808\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3621 - val_loss: 0.3802\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3739 - val_loss: 0.3812\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3661 - val_loss: 0.3818\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3672 - val_loss: 0.3817\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3656 - val_loss: 0.3811\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3662 - val_loss: 0.3811\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3687 - val_loss: 0.3811\n",
      "[2022_04_21-17:58:38] Training the entire fine-tuned model...\n",
      "[2022_04_21-17:58:48] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 741ms/step - loss: 0.3745 - val_loss: 0.3841\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3712 - val_loss: 0.3813\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 301ms/step - loss: 0.3592 - val_loss: 0.3804\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3665 - val_loss: 0.3795\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 309ms/step - loss: 0.3691 - val_loss: 0.3794\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3642 - val_loss: 0.3794\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 326ms/step - loss: 0.3601 - val_loss: 0.3802\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3676 - val_loss: 0.3805\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3647 - val_loss: 0.3803\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3540 - val_loss: 0.3795\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 326ms/step - loss: 0.3613 - val_loss: 0.3790\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3643 - val_loss: 0.3786\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3566 - val_loss: 0.3785\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3537 - val_loss: 0.3786\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3540 - val_loss: 0.3790\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 325ms/step - loss: 0.3641 - val_loss: 0.3792\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 332ms/step - loss: 0.3602 - val_loss: 0.3792\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3644 - val_loss: 0.3792\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3504 - val_loss: 0.3792\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      "[2022_04_21-17:59:25] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-17:59:25] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-17:59:25] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 11s 758ms/step - loss: 0.3662 - val_loss: 0.3784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-18:00:10] Test set: Filtered out 0 of 407 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:70wb71qc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 26569... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▁▂▂▂▃▃▃▄▄▄▄▁</td></tr><tr><td>loss</td><td>█▆▄▃▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.37844</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36616</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.37844</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dashing-planet-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/70wb71qc\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/70wb71qc</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_175741-70wb71qc/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:70wb71qc). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/1unjnxt2\" target=\"_blank\">ruby-galaxy-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-18:00:31] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:00:31] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:00:32] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 508ms/step - loss: 0.9080 - val_loss: 0.7631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.5955 - val_loss: 0.6793\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.5391 - val_loss: 0.5361\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4799 - val_loss: 0.5158\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4657 - val_loss: 0.4977\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 0.4214 - val_loss: 0.4712\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4169 - val_loss: 0.4468\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3997 - val_loss: 0.4493\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3921 - val_loss: 0.4858\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4022 - val_loss: 0.4338\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3835 - val_loss: 0.4336\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3674 - val_loss: 0.4548\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3747 - val_loss: 0.4441\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3706 - val_loss: 0.4452\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3805 - val_loss: 0.4491\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3662 - val_loss: 0.4276\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3523 - val_loss: 0.4223\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3603 - val_loss: 0.4243\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3468 - val_loss: 0.4314\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3578 - val_loss: 0.4224\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3624 - val_loss: 0.4258\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3499 - val_loss: 0.4291\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3440 - val_loss: 0.4303\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-18:01:01] Training the entire fine-tuned model...\n",
      "[2022_04_21-18:01:29] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3524WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1025s vs `on_train_batch_end` time: 0.1282s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1025s vs `on_train_batch_end` time: 0.1282s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 914ms/step - loss: 0.3524 - val_loss: 0.4217\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3472 - val_loss: 0.4218\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3505 - val_loss: 0.4191\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3388 - val_loss: 0.4194\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3357 - val_loss: 0.4159\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 288ms/step - loss: 0.3380 - val_loss: 0.4176\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3336 - val_loss: 0.4157\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3269 - val_loss: 0.4204\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 289ms/step - loss: 0.3390 - val_loss: 0.4199\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3243 - val_loss: 0.4212\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3187 - val_loss: 0.4186\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3194 - val_loss: 0.4175\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3179 - val_loss: 0.4188\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_21-18:02:00] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-18:02:00] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:02:16] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3284WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1009s vs `on_train_batch_end` time: 0.1273s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1009s vs `on_train_batch_end` time: 0.1273s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 614ms/step - loss: 0.3284 - val_loss: 0.4144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-18:02:50] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1unjnxt2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 26969... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▄▅▅▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▃▃▃▂▂▂▂▁▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.41437</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32836</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41437</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">ruby-galaxy-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/1unjnxt2\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/1unjnxt2</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_180013-1unjnxt2/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1unjnxt2). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/niy5tuwy\" target=\"_blank\">glamorous-jazz-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-18:03:10] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:03:10] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:03:10] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 629ms/step - loss: 0.9822 - val_loss: 0.8752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.7343 - val_loss: 0.7313\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.7213 - val_loss: 0.5891\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.6062 - val_loss: 0.5094\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5511 - val_loss: 0.4789\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4752 - val_loss: 0.4267\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4572 - val_loss: 0.4445\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4408 - val_loss: 0.4074\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.4303 - val_loss: 0.4359\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.4202 - val_loss: 0.4027\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 0.4145 - val_loss: 0.4329\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4202 - val_loss: 0.4015\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4076 - val_loss: 0.4173\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 0.4042 - val_loss: 0.3898\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3975 - val_loss: 0.3899\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3997 - val_loss: 0.4076\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3933 - val_loss: 0.3853\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3857 - val_loss: 0.3824\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3873 - val_loss: 0.4031\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3785 - val_loss: 0.3861\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3615 - val_loss: 0.3820\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3653 - val_loss: 0.3771\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3558 - val_loss: 0.3848\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3527 - val_loss: 0.3735\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3561 - val_loss: 0.3988\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3813 - val_loss: 0.3759\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3596 - val_loss: 0.3696\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3489 - val_loss: 0.3892\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3517 - val_loss: 0.3780\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3390 - val_loss: 0.3914\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3358 - val_loss: 0.3680\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3439 - val_loss: 0.3683\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3438 - val_loss: 0.3724\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3413 - val_loss: 0.3667\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 0.3387 - val_loss: 0.3666\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3424 - val_loss: 0.3755\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3423 - val_loss: 0.3671\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3285 - val_loss: 0.3661\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3361 - val_loss: 0.3664\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3303 - val_loss: 0.3706\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3272 - val_loss: 0.3684\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3369 - val_loss: 0.3676\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3321 - val_loss: 0.3672\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3414 - val_loss: 0.3665\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_21-18:03:55] Training the entire fine-tuned model...\n",
      "[2022_04_21-18:04:04] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 777ms/step - loss: 0.3577 - val_loss: 0.3654\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3464 - val_loss: 0.3663\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3411 - val_loss: 0.3755\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3482 - val_loss: 0.3709\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3405 - val_loss: 0.3635\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3284 - val_loss: 0.3652\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3289 - val_loss: 0.3699\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3200 - val_loss: 0.3661\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3186 - val_loss: 0.3652\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 327ms/step - loss: 0.3291 - val_loss: 0.3643\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3349 - val_loss: 0.3635\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "[2022_04_21-18:04:29] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-18:04:29] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:04:29] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 1s/step - loss: 0.3415 - val_loss: 0.3630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-18:05:03] Test set: Filtered out 0 of 407 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:niy5tuwy) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 27279... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▁</td></tr><tr><td>loss</td><td>█▅▅▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▄▃▂▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.36303</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34152</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.36303</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">glamorous-jazz-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/niy5tuwy\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/niy5tuwy</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_180253-niy5tuwy/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:niy5tuwy). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/1lkkkimz\" target=\"_blank\">royal-rain-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-18:05:23] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:05:23] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:05:23] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 485ms/step - loss: 1.0328 - val_loss: 0.6633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.6712 - val_loss: 0.7892\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.5546 - val_loss: 0.5442\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4675 - val_loss: 0.5753\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4567 - val_loss: 0.4690\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.4176 - val_loss: 0.4603\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4103 - val_loss: 0.5079\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4183 - val_loss: 0.4424\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4142 - val_loss: 0.4359\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3971 - val_loss: 0.4325\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.3935 - val_loss: 0.4809\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3870 - val_loss: 0.4266\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3813 - val_loss: 0.4240\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3749 - val_loss: 0.4438\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3521 - val_loss: 0.4212\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3700 - val_loss: 0.4218\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3628 - val_loss: 0.4332\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3627 - val_loss: 0.4362\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3517 - val_loss: 0.4138\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 0.3540 - val_loss: 0.4104\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3519 - val_loss: 0.4214\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3445 - val_loss: 0.4147\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3371 - val_loss: 0.4101\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3364 - val_loss: 0.4214\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.3446 - val_loss: 0.4116\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.3435 - val_loss: 0.4168\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3400 - val_loss: 0.4146\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3385 - val_loss: 0.4168\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3382 - val_loss: 0.4186\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-18:05:59] Training the entire fine-tuned model...\n",
      "[2022_04_21-18:06:07] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3515WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0978s vs `on_train_batch_end` time: 0.1289s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0978s vs `on_train_batch_end` time: 0.1289s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 664ms/step - loss: 0.3515 - val_loss: 0.4368\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3404 - val_loss: 0.4077\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3311 - val_loss: 0.4221\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3218 - val_loss: 0.4236\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3145 - val_loss: 0.4209\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 290ms/step - loss: 0.3056 - val_loss: 0.4178\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3221 - val_loss: 0.4132\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.2994 - val_loss: 0.4248\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_21-18:06:29] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-18:06:29] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:06:29] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3420WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1016s vs `on_train_batch_end` time: 0.1285s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1016s vs `on_train_batch_end` time: 0.1285s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 607ms/step - loss: 0.3420 - val_loss: 0.4079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-18:07:03] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1lkkkimz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 27661... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██▁▁▁▂▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▆█▄▄▂▂▃▂▂▁▂▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_val_loss</td><td>0.40769</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34204</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.40791</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">royal-rain-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/1lkkkimz\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/1lkkkimz</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_180505-1lkkkimz/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1lkkkimz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/3odin0uw\" target=\"_blank\">smooth-resonance-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-18:07:23] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:07:24] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:07:24] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 606ms/step - loss: 0.9058 - val_loss: 0.8931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.7252 - val_loss: 0.6199\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.5618 - val_loss: 0.6174\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.5511 - val_loss: 0.4623\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 0.5031 - val_loss: 0.4359\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.4540 - val_loss: 0.4126\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.4399 - val_loss: 0.4184\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.4283 - val_loss: 0.4051\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 0.4191 - val_loss: 0.4070\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.4215 - val_loss: 0.3981\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.4167 - val_loss: 0.3958\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.4036 - val_loss: 0.3911\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 172ms/step - loss: 0.4016 - val_loss: 0.3947\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.3978 - val_loss: 0.4012\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.3939 - val_loss: 0.4065\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.4140 - val_loss: 0.3898\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 171ms/step - loss: 0.3852 - val_loss: 0.3981\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 0.3871 - val_loss: 0.3845\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.3746 - val_loss: 0.3845\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 0.3688 - val_loss: 0.3936\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.3802 - val_loss: 0.3824\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 172ms/step - loss: 0.3692 - val_loss: 0.3868\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.3800 - val_loss: 0.3852\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 173ms/step - loss: 0.3804 - val_loss: 0.3811\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 0.3683 - val_loss: 0.3850\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.3719 - val_loss: 0.3833\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.3574 - val_loss: 0.3802\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.3711 - val_loss: 0.3830\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.3718 - val_loss: 0.3878\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.3750 - val_loss: 0.3793\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 0.3686 - val_loss: 0.3818\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 0.3672 - val_loss: 0.3810\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.3673 - val_loss: 0.3783\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.3645 - val_loss: 0.3827\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.3643 - val_loss: 0.3793\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.3665 - val_loss: 0.3771\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 171ms/step - loss: 0.3596 - val_loss: 0.3831\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.3687 - val_loss: 0.3776\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.3620 - val_loss: 0.3756\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 0.3654 - val_loss: 0.3806\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.3578 - val_loss: 0.3778\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 0.3555 - val_loss: 0.3742\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 171ms/step - loss: 0.3561 - val_loss: 0.3800\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.3617 - val_loss: 0.3807\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.3566 - val_loss: 0.3731\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 171ms/step - loss: 0.3503 - val_loss: 0.3762\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.3487 - val_loss: 0.3750\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.3507 - val_loss: 0.3724\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 0.3507 - val_loss: 0.3797\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3523 - val_loss: 0.3717\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3488 - val_loss: 0.3713\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3516 - val_loss: 0.3778\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.3371 - val_loss: 0.3714\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 0.3483 - val_loss: 0.3758\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3380 - val_loss: 0.3777\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.3499 - val_loss: 0.3712\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3373 - val_loss: 0.3708\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3557 - val_loss: 0.3711\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.3387 - val_loss: 0.3711\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.3370 - val_loss: 0.3720\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.3419 - val_loss: 0.3719\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 0.3414 - val_loss: 0.3718\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3455 - val_loss: 0.3714\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_21-18:08:20] Training the entire fine-tuned model...\n",
      "[2022_04_21-18:08:30] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 765ms/step - loss: 0.3667 - val_loss: 0.3704\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 301ms/step - loss: 0.3466 - val_loss: 0.3753\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3608 - val_loss: 0.3678\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3414 - val_loss: 0.3745\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3320 - val_loss: 0.3661\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.3306 - val_loss: 0.3688\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3245 - val_loss: 0.3643\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3197 - val_loss: 0.3632\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 327ms/step - loss: 0.3129 - val_loss: 0.3674\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3094 - val_loss: 0.3635\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.2979 - val_loss: 0.3647\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.2922 - val_loss: 0.3642\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.2969 - val_loss: 0.3718\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.2876 - val_loss: 0.3649\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_21-18:08:59] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-18:08:59] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:08:59] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 1s/step - loss: 0.3076 - val_loss: 0.3626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-18:09:50] Test set: Filtered out 0 of 407 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3odin0uw) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 27951... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▁</td></tr><tr><td>loss</td><td>█▆▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.36264</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30764</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.36264</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">smooth-resonance-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/3odin0uw\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/3odin0uw</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_180706-3odin0uw/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3odin0uw). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/oh918szp\" target=\"_blank\">silver-paper-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-18:10:08] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:10:08] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:10:08] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 514ms/step - loss: 0.8672 - val_loss: 1.0133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.7314 - val_loss: 0.8531\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.5805 - val_loss: 0.4882\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4465 - val_loss: 0.5018\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4332 - val_loss: 0.4613\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4164 - val_loss: 0.4543\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4057 - val_loss: 0.4792\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4031 - val_loss: 0.4583\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3929 - val_loss: 0.4549\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3821 - val_loss: 0.4345\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3823 - val_loss: 0.4391\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3782 - val_loss: 0.4349\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3749 - val_loss: 0.4330\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3823 - val_loss: 0.4393\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3783 - val_loss: 0.4325\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3738 - val_loss: 0.4350\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3685 - val_loss: 0.4305\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3642 - val_loss: 0.4265\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3727 - val_loss: 0.4295\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3610 - val_loss: 0.4335\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.3694 - val_loss: 0.4253\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.3662 - val_loss: 0.4241\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3648 - val_loss: 0.4276\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3732 - val_loss: 0.4205\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3565 - val_loss: 0.4367\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3613 - val_loss: 0.4203\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3587 - val_loss: 0.4259\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3630 - val_loss: 0.4209\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3588 - val_loss: 0.4289\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3505 - val_loss: 0.4248\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3507 - val_loss: 0.4193\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3535 - val_loss: 0.4202\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3530 - val_loss: 0.4240\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3543 - val_loss: 0.4257\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3608 - val_loss: 0.4262\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3544 - val_loss: 0.4244\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3478 - val_loss: 0.4224\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_21-18:10:50] Training the entire fine-tuned model...\n",
      "[2022_04_21-18:10:58] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4982WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0997s vs `on_train_batch_end` time: 0.1299s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0997s vs `on_train_batch_end` time: 0.1299s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 644ms/step - loss: 0.4982 - val_loss: 0.5765\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.4318 - val_loss: 0.4496\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3833 - val_loss: 0.4671\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3555 - val_loss: 0.4326\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 269ms/step - loss: 0.3691 - val_loss: 0.4970\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3454 - val_loss: 0.4449\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3533 - val_loss: 0.4385\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3155 - val_loss: 0.4184\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 290ms/step - loss: 0.3192 - val_loss: 0.4243\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3176 - val_loss: 0.4216\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.2982 - val_loss: 0.4195\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 269ms/step - loss: 0.3006 - val_loss: 0.4219\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.2898 - val_loss: 0.4205\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.2994 - val_loss: 0.4203\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "[2022_04_21-18:11:30] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-18:11:30] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:11:46] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3115WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0991s vs `on_train_batch_end` time: 0.1314s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0991s vs `on_train_batch_end` time: 0.1314s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 627ms/step - loss: 0.3115 - val_loss: 0.4214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-18:12:19] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:oh918szp) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 28444... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇███▁▁▂▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▆▅▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▄▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▂▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.41837</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31146</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.42143</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">silver-paper-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/oh918szp\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/oh918szp</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_180952-oh918szp/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:oh918szp). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/1pa61my5\" target=\"_blank\">ruby-wave-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-18:12:38] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:12:38] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:12:38] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 649ms/step - loss: 0.8319 - val_loss: 0.8489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.6153 - val_loss: 0.4879\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5191 - val_loss: 0.4341\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 0.4934 - val_loss: 0.4662\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4778 - val_loss: 0.4405\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4520 - val_loss: 0.4787\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4475 - val_loss: 0.4056\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4442 - val_loss: 0.4052\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.4412 - val_loss: 0.4304\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4278 - val_loss: 0.4023\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4207 - val_loss: 0.4020\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4152 - val_loss: 0.4111\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4163 - val_loss: 0.4024\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4215 - val_loss: 0.3995\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4138 - val_loss: 0.4083\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4085 - val_loss: 0.3987\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4104 - val_loss: 0.3984\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4098 - val_loss: 0.3976\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4046 - val_loss: 0.4016\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.4026 - val_loss: 0.3965\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4093 - val_loss: 0.3949\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.3911 - val_loss: 0.3951\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3950 - val_loss: 0.3934\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3911 - val_loss: 0.3958\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4069 - val_loss: 0.3902\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3990 - val_loss: 0.4065\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3971 - val_loss: 0.3900\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3876 - val_loss: 0.3967\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3895 - val_loss: 0.3907\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3851 - val_loss: 0.3899\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3768 - val_loss: 0.3867\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3802 - val_loss: 0.3875\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3799 - val_loss: 0.3888\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3824 - val_loss: 0.3888\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3797 - val_loss: 0.3886\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3849 - val_loss: 0.3887\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3875 - val_loss: 0.3887\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_21-18:13:17] Training the entire fine-tuned model...\n",
      "[2022_04_21-18:13:37] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 744ms/step - loss: 0.5660 - val_loss: 0.4801\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 301ms/step - loss: 0.4656 - val_loss: 0.4546\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 300ms/step - loss: 0.4121 - val_loss: 0.4149\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.4071 - val_loss: 0.4160\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.4022 - val_loss: 0.3903\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 299ms/step - loss: 0.3808 - val_loss: 0.3854\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 301ms/step - loss: 0.3773 - val_loss: 0.3870\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3614 - val_loss: 0.3774\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3440 - val_loss: 0.3754\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3339 - val_loss: 0.3916\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3445 - val_loss: 0.3835\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.2935 - val_loss: 0.3990\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.2992 - val_loss: 0.3747\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.2966 - val_loss: 0.3748\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.2760 - val_loss: 0.3776\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.2637 - val_loss: 0.3791\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.2796 - val_loss: 0.3777\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 301ms/step - loss: 0.2591 - val_loss: 0.3794\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 328ms/step - loss: 0.2620 - val_loss: 0.3794\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "[2022_04_21-18:14:14] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-18:14:14] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:14:14] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 1s/step - loss: 0.2946 - val_loss: 0.3716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-18:14:47] Test set: Filtered out 0 of 407 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1pa61my5) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 28810... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇███▁▁▂▂▂▂▃▃▃▄▄▄▄▁</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▃▂▃▅▄▃▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▃▁▂▁▂▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▃▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.37159</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.29459</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.37159</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">ruby-wave-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/1pa61my5\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/1pa61my5</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_181221-1pa61my5/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1pa61my5). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/2fh5w95v\" target=\"_blank\">quiet-wood-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-18:15:07] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:15:08] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:15:08] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 535ms/step - loss: 0.9917 - val_loss: 0.5713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.6725 - val_loss: 0.8340\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.5625 - val_loss: 0.5922\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.5050 - val_loss: 0.5774\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.4712 - val_loss: 0.4855\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.4391 - val_loss: 0.4559\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4109 - val_loss: 0.4698\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.4005 - val_loss: 0.4432\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4048 - val_loss: 0.4391\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3837 - val_loss: 0.4380\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3750 - val_loss: 0.4409\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.3738 - val_loss: 0.4324\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3723 - val_loss: 0.4507\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3617 - val_loss: 0.4456\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3734 - val_loss: 0.4248\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 0.3605 - val_loss: 0.4258\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3513 - val_loss: 0.4264\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3614 - val_loss: 0.4715\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3535 - val_loss: 0.4633\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3821 - val_loss: 0.4511\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3651 - val_loss: 0.4458\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3529 - val_loss: 0.4166\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3514 - val_loss: 0.4310\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3453 - val_loss: 0.4224\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3415 - val_loss: 0.4137\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3426 - val_loss: 0.4191\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3303 - val_loss: 0.4164\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3446 - val_loss: 0.4143\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3498 - val_loss: 0.4157\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3445 - val_loss: 0.4167\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.3344 - val_loss: 0.4199\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3393 - val_loss: 0.4166\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.3228 - val_loss: 0.4183\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-18:15:46] Training the entire fine-tuned model...\n",
      "[2022_04_21-18:16:06] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3446WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0993s vs `on_train_batch_end` time: 0.1302s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0993s vs `on_train_batch_end` time: 0.1302s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 634ms/step - loss: 0.3446 - val_loss: 0.4199\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3364 - val_loss: 0.4163\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3371 - val_loss: 0.4166\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 289ms/step - loss: 0.3334 - val_loss: 0.4165\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3464 - val_loss: 0.4170\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 267ms/step - loss: 0.3398 - val_loss: 0.4192\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 268ms/step - loss: 0.3430 - val_loss: 0.4188\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.3253 - val_loss: 0.4184\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3350 - val_loss: 0.4183\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3274 - val_loss: 0.4187\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_21-18:16:32] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-18:16:32] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:16:34] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3368WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1001s vs `on_train_batch_end` time: 0.1281s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1001s vs `on_train_batch_end` time: 0.1281s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 614ms/step - loss: 0.3368 - val_loss: 0.4161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-18:17:07] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2fh5w95v) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 29196... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▂▂▂▂▂▂▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄█▄▄▂▂▂▁▁▁▁▂▂▁▁▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>24</td></tr><tr><td>best_val_loss</td><td>0.4137</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33683</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41607</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">quiet-wood-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/2fh5w95v\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/2fh5w95v</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_181450-2fh5w95v/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2fh5w95v). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/1hff7fzn\" target=\"_blank\">playful-eon-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-18:17:28] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:17:28] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:17:28] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 620ms/step - loss: 0.8758 - val_loss: 1.1086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.7548 - val_loss: 0.6645\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.6675 - val_loss: 0.6045\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5836 - val_loss: 0.5174\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5701 - val_loss: 0.4707\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5131 - val_loss: 0.4319\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4538 - val_loss: 0.4385\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4561 - val_loss: 0.4057\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4342 - val_loss: 0.4082\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4213 - val_loss: 0.3981\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.4099 - val_loss: 0.4058\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4103 - val_loss: 0.3953\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.4043 - val_loss: 0.4106\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3938 - val_loss: 0.3908\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4131 - val_loss: 0.4346\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3855 - val_loss: 0.3947\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3906 - val_loss: 0.4028\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3703 - val_loss: 0.3868\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 0.3747 - val_loss: 0.4109\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3883 - val_loss: 0.3821\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3746 - val_loss: 0.3821\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3671 - val_loss: 0.3810\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3687 - val_loss: 0.3781\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3708 - val_loss: 0.3765\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 0.3634 - val_loss: 0.3751\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3570 - val_loss: 0.3759\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3587 - val_loss: 0.3723\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3397 - val_loss: 0.3720\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 0.3481 - val_loss: 0.3703\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.3532 - val_loss: 0.3704\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3516 - val_loss: 0.3808\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3310 - val_loss: 0.3687\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3329 - val_loss: 0.3727\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3432 - val_loss: 0.3796\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3483 - val_loss: 0.3678\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3410 - val_loss: 0.3714\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3521 - val_loss: 0.4274\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3607 - val_loss: 0.3894\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3528 - val_loss: 0.3632\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3456 - val_loss: 0.3646\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3399 - val_loss: 0.3666\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3328 - val_loss: 0.3640\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3162 - val_loss: 0.3650\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3234 - val_loss: 0.3631\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3106 - val_loss: 0.3634\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3237 - val_loss: 0.3614\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3266 - val_loss: 0.3610\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3326 - val_loss: 0.3641\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3188 - val_loss: 0.3608\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3395 - val_loss: 0.3611\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3257 - val_loss: 0.3601\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3182 - val_loss: 0.3598\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3144 - val_loss: 0.3638\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3206 - val_loss: 0.3593\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3278 - val_loss: 0.3596\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3231 - val_loss: 0.3659\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3322 - val_loss: 0.3598\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3163 - val_loss: 0.3605\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3080 - val_loss: 0.3614\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3171 - val_loss: 0.3590\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3105 - val_loss: 0.3587\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3177 - val_loss: 0.3588\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.2986 - val_loss: 0.3587\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3226 - val_loss: 0.3590\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3197 - val_loss: 0.3593\n",
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3109 - val_loss: 0.3590\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3199 - val_loss: 0.3590\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3123 - val_loss: 0.3588\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3086 - val_loss: 0.3588\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_21-18:18:32] Training the entire fine-tuned model...\n",
      "[2022_04_21-18:19:08] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 750ms/step - loss: 0.3150 - val_loss: 0.3585\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3109 - val_loss: 0.3586\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3107 - val_loss: 0.3582\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3050 - val_loss: 0.3577\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 325ms/step - loss: 0.3114 - val_loss: 0.3577\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.2970 - val_loss: 0.3581\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.2981 - val_loss: 0.3578\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3140 - val_loss: 0.3578\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 309ms/step - loss: 0.3009 - val_loss: 0.3580\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 332ms/step - loss: 0.3002 - val_loss: 0.3584\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3115 - val_loss: 0.3582\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3160 - val_loss: 0.3581\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 328ms/step - loss: 0.3183 - val_loss: 0.3580\n",
      "[2022_04_21-18:19:36] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-18:19:36] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:19:36] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 733ms/step - loss: 0.3136 - val_loss: 0.3577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-18:20:10] Test set: Filtered out 0 of 407 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1hff7fzn) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 29527... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.35769</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31358</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.35769</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">playful-eon-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/1hff7fzn\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/1hff7fzn</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_181709-1hff7fzn/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1hff7fzn). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/32ucm9gi\" target=\"_blank\">leafy-mountain-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-18:20:30] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:20:30] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:20:30] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 510ms/step - loss: 0.7674 - val_loss: 0.6024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.5688 - val_loss: 0.5899\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.5093 - val_loss: 0.5642\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.4725 - val_loss: 0.5108\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 0.4797 - val_loss: 0.5191\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4280 - val_loss: 0.4554\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3976 - val_loss: 0.4642\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3999 - val_loss: 0.4435\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3872 - val_loss: 0.4328\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3805 - val_loss: 0.4845\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4009 - val_loss: 0.4256\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3807 - val_loss: 0.4225\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3652 - val_loss: 0.4566\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.3567 - val_loss: 0.4370\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3562 - val_loss: 0.4202\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3493 - val_loss: 0.4150\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3440 - val_loss: 0.5266\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 0.3799 - val_loss: 0.4446\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3901 - val_loss: 0.5384\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4598 - val_loss: 0.4825\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3572 - val_loss: 0.4240\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3566 - val_loss: 0.4402\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3422 - val_loss: 0.4145\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3362 - val_loss: 0.4159\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3305 - val_loss: 0.4211\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3303 - val_loss: 0.4196\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3385 - val_loss: 0.4185\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3265 - val_loss: 0.4210\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3341 - val_loss: 0.4204\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3376 - val_loss: 0.4158\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3321 - val_loss: 0.4166\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-18:21:07] Training the entire fine-tuned model...\n",
      "[2022_04_21-18:21:28] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3330WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0984s vs `on_train_batch_end` time: 0.1305s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0984s vs `on_train_batch_end` time: 0.1305s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 610ms/step - loss: 0.3330 - val_loss: 0.4147\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3317 - val_loss: 0.4202\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3383 - val_loss: 0.4183\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3309 - val_loss: 0.4200\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 288ms/step - loss: 0.3390 - val_loss: 0.4139\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3224 - val_loss: 0.4267\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3144 - val_loss: 0.4111\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 288ms/step - loss: 0.3246 - val_loss: 0.4180\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3329 - val_loss: 0.4114\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3134 - val_loss: 0.4314\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 269ms/step - loss: 0.3089 - val_loss: 0.4131\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3163 - val_loss: 0.4126\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.2996 - val_loss: 0.4162\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 267ms/step - loss: 0.3037 - val_loss: 0.4166\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3054 - val_loss: 0.4156\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_21-18:22:01] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-18:22:01] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:22:01] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3100WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1018s vs `on_train_batch_end` time: 0.1279s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1018s vs `on_train_batch_end` time: 0.1279s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 637ms/step - loss: 0.3100 - val_loss: 0.4113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-18:22:37] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:32ucm9gi) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 30050... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇███▁▁▂▂▂▂▃▃▃▄▄▄▁</td></tr><tr><td>loss</td><td>█▅▄▄▄▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>██▇▅▅▃▂▂▄▂▁▃▁▁▅▂▆▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>6</td></tr><tr><td>best_val_loss</td><td>0.41109</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31003</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.41126</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">leafy-mountain-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/32ucm9gi\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/32ucm9gi</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_182012-32ucm9gi/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:32ucm9gi). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/2859wew7\" target=\"_blank\">dainty-gorge-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-18:22:57] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:22:57] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:22:57] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 615ms/step - loss: 0.9408 - val_loss: 0.8637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.7035 - val_loss: 0.6311\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5947 - val_loss: 0.6661\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5624 - val_loss: 0.5116\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5470 - val_loss: 0.5075\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4874 - val_loss: 0.4374\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4887 - val_loss: 0.4207\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4627 - val_loss: 0.4091\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4300 - val_loss: 0.4042\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4205 - val_loss: 0.4117\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.4046 - val_loss: 0.3995\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4030 - val_loss: 0.3956\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3955 - val_loss: 0.3965\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3993 - val_loss: 0.3959\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3858 - val_loss: 0.3926\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3866 - val_loss: 0.3986\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3930 - val_loss: 0.3862\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3779 - val_loss: 0.3841\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3741 - val_loss: 0.3961\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3727 - val_loss: 0.3838\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3765 - val_loss: 0.3941\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3783 - val_loss: 0.3894\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3531 - val_loss: 0.3778\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3595 - val_loss: 0.3822\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3530 - val_loss: 0.3762\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3494 - val_loss: 0.3891\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3549 - val_loss: 0.3842\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3555 - val_loss: 0.3871\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3491 - val_loss: 0.3829\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3358 - val_loss: 0.3722\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3429 - val_loss: 0.3718\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3396 - val_loss: 0.3794\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3432 - val_loss: 0.3720\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3352 - val_loss: 0.3722\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 0.3361 - val_loss: 0.3713\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3437 - val_loss: 0.3764\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3455 - val_loss: 0.3705\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3467 - val_loss: 0.3718\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3284 - val_loss: 0.3735\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3486 - val_loss: 0.3705\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3348 - val_loss: 0.3698\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3264 - val_loss: 0.3726\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3316 - val_loss: 0.3708\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3287 - val_loss: 0.3684\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3278 - val_loss: 0.3706\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3267 - val_loss: 0.3680\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3394 - val_loss: 0.3756\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3275 - val_loss: 0.3678\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3321 - val_loss: 0.3698\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3324 - val_loss: 0.3670\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3399 - val_loss: 0.3678\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3212 - val_loss: 0.3688\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3241 - val_loss: 0.3661\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3353 - val_loss: 0.3730\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3308 - val_loss: 0.3655\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3227 - val_loss: 0.3668\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3235 - val_loss: 0.3675\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3193 - val_loss: 0.3658\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3296 - val_loss: 0.3711\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3192 - val_loss: 0.3693\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3166 - val_loss: 0.3670\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3346 - val_loss: 0.3657\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3312 - val_loss: 0.3655\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3109 - val_loss: 0.3655\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3219 - val_loss: 0.3657\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3289 - val_loss: 0.3659\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3191 - val_loss: 0.3663\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3315 - val_loss: 0.3664\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3190 - val_loss: 0.3665\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3160 - val_loss: 0.3665\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3161 - val_loss: 0.3665\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "[2022_04_21-18:24:02] Training the entire fine-tuned model...\n",
      "[2022_04_21-18:24:37] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 777ms/step - loss: 0.3297 - val_loss: 0.3702\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3357 - val_loss: 0.3648\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3043 - val_loss: 0.3639\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3208 - val_loss: 0.3627\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3106 - val_loss: 0.3676\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.2945 - val_loss: 0.3621\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3134 - val_loss: 0.3625\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3077 - val_loss: 0.3687\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 327ms/step - loss: 0.2935 - val_loss: 0.3673\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 327ms/step - loss: 0.2924 - val_loss: 0.3724\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3046 - val_loss: 0.3729\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.2886 - val_loss: 0.3639\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.2969 - val_loss: 0.3656\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 328ms/step - loss: 0.2970 - val_loss: 0.3643\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_21-18:25:06] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-18:25:06] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:25:06] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 747ms/step - loss: 0.2954 - val_loss: 0.3636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-18:25:40] Test set: Filtered out 0 of 407 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2859wew7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 30387... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▂</td></tr><tr><td>loss</td><td>█▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.36212</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.29543</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.36364</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dainty-gorge-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/2859wew7\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/2859wew7</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_182239-2859wew7/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2859wew7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/kbfma29q\" target=\"_blank\">upbeat-monkey-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-18:26:04] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:26:04] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:26:04] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 539ms/step - loss: 0.7784 - val_loss: 0.6818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 0.5533 - val_loss: 0.5121\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4586 - val_loss: 0.5640\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 124ms/step - loss: 0.4740 - val_loss: 0.4701\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4317 - val_loss: 0.4549\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4131 - val_loss: 0.5290\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4171 - val_loss: 0.4552\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4163 - val_loss: 0.4517\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3951 - val_loss: 0.4642\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3958 - val_loss: 0.5050\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4184 - val_loss: 0.4238\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3971 - val_loss: 0.4244\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.3642 - val_loss: 0.4238\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3675 - val_loss: 0.4847\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3680 - val_loss: 0.4230\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3698 - val_loss: 0.4173\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 0.3628 - val_loss: 0.4152\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3546 - val_loss: 0.4739\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3632 - val_loss: 0.4411\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3474 - val_loss: 0.4239\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3378 - val_loss: 0.4365\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3341 - val_loss: 0.4155\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.3370 - val_loss: 0.4223\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3284 - val_loss: 0.4142\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.3374 - val_loss: 0.4225\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3448 - val_loss: 0.4122\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3361 - val_loss: 0.4275\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3438 - val_loss: 0.4151\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3363 - val_loss: 0.4316\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3272 - val_loss: 0.4110\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3388 - val_loss: 0.4138\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 0.3325 - val_loss: 0.4154\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3370 - val_loss: 0.4094\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3205 - val_loss: 0.4280\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.3320 - val_loss: 0.4111\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3237 - val_loss: 0.4231\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3286 - val_loss: 0.4147\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3183 - val_loss: 0.4118\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3269 - val_loss: 0.4097\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3259 - val_loss: 0.4107\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3199 - val_loss: 0.4152\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-18:26:50] Training the entire fine-tuned model...\n",
      "[2022_04_21-18:27:03] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3402WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0991s vs `on_train_batch_end` time: 0.1310s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0991s vs `on_train_batch_end` time: 0.1310s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 636ms/step - loss: 0.3402 - val_loss: 0.4234\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3211 - val_loss: 0.4162\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3315 - val_loss: 0.4048\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3172 - val_loss: 0.4208\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 268ms/step - loss: 0.3137 - val_loss: 0.4054\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.3098 - val_loss: 0.4164\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 268ms/step - loss: 0.3076 - val_loss: 0.4121\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.2920 - val_loss: 0.4165\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 289ms/step - loss: 0.2971 - val_loss: 0.4136\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.2948 - val_loss: 0.4089\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3013 - val_loss: 0.4144\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_21-18:27:29] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-18:27:29] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:27:29] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3169WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0998s vs `on_train_batch_end` time: 0.1304s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0998s vs `on_train_batch_end` time: 0.1304s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 629ms/step - loss: 0.3169 - val_loss: 0.4058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-18:28:05] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:kbfma29q) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 30927... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇██▁▁▂▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▅▂▄▂▂▄▁▁▃▁▁▃▂▂▁▁▁▁▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>2</td></tr><tr><td>best_val_loss</td><td>0.40479</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31689</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.40585</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">upbeat-monkey-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/kbfma29q\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/kbfma29q</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_182542-kbfma29q/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:kbfma29q). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/3c5rpxns\" target=\"_blank\">astral-glitter-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-18:28:25] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:28:25] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:28:25] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 634ms/step - loss: 1.0438 - val_loss: 0.4939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 167ms/step - loss: 0.8127 - val_loss: 0.7087\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.8269 - val_loss: 0.4473\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.5953 - val_loss: 0.4335\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5363 - val_loss: 0.4372\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4941 - val_loss: 0.4422\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4750 - val_loss: 0.4154\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4511 - val_loss: 0.4286\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4406 - val_loss: 0.4036\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4334 - val_loss: 0.4021\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4247 - val_loss: 0.4023\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4150 - val_loss: 0.3962\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4029 - val_loss: 0.4032\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4007 - val_loss: 0.3908\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3965 - val_loss: 0.3905\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3876 - val_loss: 0.3883\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3919 - val_loss: 0.3872\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3769 - val_loss: 0.3927\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3735 - val_loss: 0.3845\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 0.3802 - val_loss: 0.3828\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3809 - val_loss: 0.3913\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3662 - val_loss: 0.3842\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3828 - val_loss: 0.3796\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3640 - val_loss: 0.3783\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3618 - val_loss: 0.3936\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3550 - val_loss: 0.3785\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.3591 - val_loss: 0.3833\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 0.3598 - val_loss: 0.3749\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3464 - val_loss: 0.3788\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3377 - val_loss: 0.3731\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3458 - val_loss: 0.3845\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.3492 - val_loss: 0.3738\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3412 - val_loss: 0.3709\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3475 - val_loss: 0.3691\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3415 - val_loss: 0.4026\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3624 - val_loss: 0.4100\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3586 - val_loss: 0.3979\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3504 - val_loss: 0.3682\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3331 - val_loss: 0.3713\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3272 - val_loss: 0.3711\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3143 - val_loss: 0.3689\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3333 - val_loss: 0.3681\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3274 - val_loss: 0.3758\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3223 - val_loss: 0.3657\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3156 - val_loss: 0.3658\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3313 - val_loss: 0.3681\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3328 - val_loss: 0.3646\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3289 - val_loss: 0.3624\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3297 - val_loss: 0.3915\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3242 - val_loss: 0.3708\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3256 - val_loss: 0.3709\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3585 - val_loss: 0.4212\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3455 - val_loss: 0.3628\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3401 - val_loss: 0.3692\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3097 - val_loss: 0.3724\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3216 - val_loss: 0.3612\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3108 - val_loss: 0.3594\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3064 - val_loss: 0.3608\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3167 - val_loss: 0.3601\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3038 - val_loss: 0.3590\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3087 - val_loss: 0.3627\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3132 - val_loss: 0.3605\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3025 - val_loss: 0.3588\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 0.3174 - val_loss: 0.3589\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3052 - val_loss: 0.3589\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3109 - val_loss: 0.3588\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.2983 - val_loss: 0.3589\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3061 - val_loss: 0.3585\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.2994 - val_loss: 0.3585\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.2966 - val_loss: 0.3584\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 0.3065 - val_loss: 0.3584\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3083 - val_loss: 0.3585\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.2955 - val_loss: 0.3586\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.2988 - val_loss: 0.3583\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 0.3102 - val_loss: 0.3583\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3170 - val_loss: 0.3583\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3082 - val_loss: 0.3583\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.2926 - val_loss: 0.3584\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3094 - val_loss: 0.3584\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3052 - val_loss: 0.3584\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3047 - val_loss: 0.3584\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.2984 - val_loss: 0.3584\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3191 - val_loss: 0.3584\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3031 - val_loss: 0.3584\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.2959 - val_loss: 0.3583\n",
      "[2022_04_21-18:29:42] Training the entire fine-tuned model...\n",
      "[2022_04_21-18:29:51] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 741ms/step - loss: 0.3315 - val_loss: 0.3809\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.3474 - val_loss: 0.3650\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.3144 - val_loss: 0.3683\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.2999 - val_loss: 0.3728\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3120 - val_loss: 0.3664\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 329ms/step - loss: 0.2955 - val_loss: 0.3639\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.2889 - val_loss: 0.3618\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.2889 - val_loss: 0.3633\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.2674 - val_loss: 0.3632\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 330ms/step - loss: 0.2824 - val_loss: 0.3618\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.2539 - val_loss: 0.3629\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.2592 - val_loss: 0.3629\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.2707 - val_loss: 0.3651\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 328ms/step - loss: 0.2609 - val_loss: 0.3629\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 326ms/step - loss: 0.2625 - val_loss: 0.3633\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_21-18:30:22] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-18:30:22] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:30:22] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 744ms/step - loss: 0.2810 - val_loss: 0.3616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-18:31:13] Test set: Filtered out 0 of 407 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3c5rpxns) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 31307... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▆▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▅▅▃▃▃▃▃▂▂▂▂▂▄▂▂▁▁▃▄▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>76</td></tr><tr><td>best_val_loss</td><td>0.35827</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.28098</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.36157</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">astral-glitter-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/3c5rpxns\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/3c5rpxns</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_182808-3c5rpxns/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3c5rpxns). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036a/runs/2gpt89di\" target=\"_blank\">clean-morning-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-18:31:32] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:31:33] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:31:33] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 504ms/step - loss: 0.8018 - val_loss: 0.7309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.5712 - val_loss: 0.5436\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.5214 - val_loss: 0.6007\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4638 - val_loss: 0.5071\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.4280 - val_loss: 0.5073\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4163 - val_loss: 0.4534\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.4093 - val_loss: 0.4415\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3949 - val_loss: 0.4627\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3920 - val_loss: 0.4610\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3989 - val_loss: 0.4262\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3860 - val_loss: 0.4229\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3760 - val_loss: 0.4232\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3663 - val_loss: 0.5023\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4025 - val_loss: 0.4568\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3903 - val_loss: 0.4273\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3512 - val_loss: 0.4136\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3529 - val_loss: 0.4246\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3537 - val_loss: 0.4261\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3509 - val_loss: 0.4123\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3458 - val_loss: 0.4254\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3500 - val_loss: 0.4203\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3460 - val_loss: 0.4130\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.3437 - val_loss: 0.4172\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3471 - val_loss: 0.4144\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3442 - val_loss: 0.4143\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3500 - val_loss: 0.4149\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3484 - val_loss: 0.4127\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-18:32:05] Training the entire fine-tuned model...\n",
      "[2022_04_21-18:32:13] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4926WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0998s vs `on_train_batch_end` time: 0.1285s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0998s vs `on_train_batch_end` time: 0.1285s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 663ms/step - loss: 0.4926 - val_loss: 0.6485\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 267ms/step - loss: 0.4510 - val_loss: 0.4383\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 268ms/step - loss: 0.4121 - val_loss: 0.4234\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3797 - val_loss: 0.4533\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3510 - val_loss: 0.4249\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3426 - val_loss: 0.4834\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 269ms/step - loss: 0.3421 - val_loss: 0.4375\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 268ms/step - loss: 0.3464 - val_loss: 0.4224\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3069 - val_loss: 0.4619\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 290ms/step - loss: 0.3311 - val_loss: 0.4187\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.2971 - val_loss: 0.4146\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.2977 - val_loss: 0.4310\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 290ms/step - loss: 0.2820 - val_loss: 0.4120\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 268ms/step - loss: 0.2848 - val_loss: 0.4136\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 289ms/step - loss: 0.2861 - val_loss: 0.4237\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 266ms/step - loss: 0.2713 - val_loss: 0.4122\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 289ms/step - loss: 0.2687 - val_loss: 0.4257\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.2589 - val_loss: 0.4255\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.2540 - val_loss: 0.4167\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.2535 - val_loss: 0.4170\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 2s 267ms/step - loss: 0.2548 - val_loss: 0.4195\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "[2022_04_21-18:32:58] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-18:32:58] Training set: Filtered out 0 of 677 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:32:58] Validation set: Filtered out 0 of 245 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.2852WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1042s vs `on_train_batch_end` time: 0.1283s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1042s vs `on_train_batch_end` time: 0.1283s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 618ms/step - loss: 0.2852 - val_loss: 0.4154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36a/2022_04_21_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-18:33:32] Test set: Filtered out 0 of 369 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2gpt89di) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 31922... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▁</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▃▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▄▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▅▃▃▂▂▂▁▁▃▂▁▁▁▁▁▁▁▁▁▁▆▂▁▂▃▂▁▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>12</td></tr><tr><td>best_val_loss</td><td>0.41198</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.28523</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.4154</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">clean-morning-16</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036a/runs/2gpt89di\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036a/runs/2gpt89di</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_183116-2gpt89di/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2gpt89di). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2036b/runs/22m4nr6p\" target=\"_blank\">prime-dream-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2036b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-18:33:53] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:33:53] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:33:53] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 647ms/step - loss: 0.9000 - val_loss: 0.8228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.6629 - val_loss: 0.5852\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.6289 - val_loss: 0.4651\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5335 - val_loss: 0.4341\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4941 - val_loss: 0.4170\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4819 - val_loss: 0.4253\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4474 - val_loss: 0.4131\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4535 - val_loss: 0.4274\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4199 - val_loss: 0.4057\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4152 - val_loss: 0.4651\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4279 - val_loss: 0.3911\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3973 - val_loss: 0.3893\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3921 - val_loss: 0.3974\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4064 - val_loss: 0.3857\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3850 - val_loss: 0.3916\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3794 - val_loss: 0.3833\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3729 - val_loss: 0.3821\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3788 - val_loss: 0.3933\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3845 - val_loss: 0.3836\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3496 - val_loss: 0.3928\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3612 - val_loss: 0.3834\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3691 - val_loss: 0.3796\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3688 - val_loss: 0.3915\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3752 - val_loss: 0.3761\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3580 - val_loss: 0.3760\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3601 - val_loss: 0.3790\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3530 - val_loss: 0.3755\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3588 - val_loss: 0.3770\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3369 - val_loss: 0.3771\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3599 - val_loss: 0.3743\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3409 - val_loss: 0.3774\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3653 - val_loss: 0.3732\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3616 - val_loss: 0.3823\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3553 - val_loss: 0.3729\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3459 - val_loss: 0.3787\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3503 - val_loss: 0.3730\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3513 - val_loss: 0.3726\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3485 - val_loss: 0.3756\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.3522 - val_loss: 0.3724\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3492 - val_loss: 0.3751\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3453 - val_loss: 0.3739\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 0.3420 - val_loss: 0.3712\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.3462 - val_loss: 0.3766\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3364 - val_loss: 0.3701\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3399 - val_loss: 0.3697\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3422 - val_loss: 0.3758\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3424 - val_loss: 0.3697\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3475 - val_loss: 0.3690\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3347 - val_loss: 0.3707\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3426 - val_loss: 0.3685\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3410 - val_loss: 0.3731\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3447 - val_loss: 0.3686\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 0.3463 - val_loss: 0.3694\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3352 - val_loss: 0.3798\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3308 - val_loss: 0.3719\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3297 - val_loss: 0.3672\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3406 - val_loss: 0.3672\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3300 - val_loss: 0.3679\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3378 - val_loss: 0.3688\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3291 - val_loss: 0.3678\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.3407 - val_loss: 0.3678\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3394 - val_loss: 0.3678\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3433 - val_loss: 0.3680\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3335 - val_loss: 0.3681\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_21-18:34:53] Training the entire fine-tuned model...\n",
      "[2022_04_21-18:35:03] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 749ms/step - loss: 0.6547 - val_loss: 0.5117\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.4722 - val_loss: 0.4105\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.3751 - val_loss: 0.3876\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.3492 - val_loss: 0.3951\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.3493 - val_loss: 0.3757\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 327ms/step - loss: 0.3281 - val_loss: 0.3689\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.2937 - val_loss: 0.3671\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.2993 - val_loss: 0.3735\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.2823 - val_loss: 0.3664\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.2482 - val_loss: 0.3990\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.2488 - val_loss: 0.3815\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 306ms/step - loss: 0.2297 - val_loss: 0.3777\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 309ms/step - loss: 0.1940 - val_loss: 0.3916\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.1795 - val_loss: 0.4096\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.1615 - val_loss: 0.3938\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 329ms/step - loss: 0.1523 - val_loss: 0.4019\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.1448 - val_loss: 0.4114\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_21-18:35:37] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-18:35:37] Training set: Filtered out 0 of 614 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:35:37] Validation set: Filtered out 0 of 270 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 739ms/step - loss: 0.2592 - val_loss: 0.3655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/36b/2022_04_21_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-18:36:25] Test set: Filtered out 0 of 407 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:22m4nr6p) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 32279... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▅▄▄▃▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▆▃▃▂▂▂▁▁▂</td></tr><tr><td>lr</td><td>███████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.36551</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.25918</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.36551</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">prime-dream-16</strong>: <a href=\"https://wandb.ai/kvetab/Split%2036b/runs/22m4nr6p\" target=\"_blank\">https://wandb.ai/kvetab/Split%2036b/runs/22m4nr6p</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_183334-22m4nr6p/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:22m4nr6p). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/2vxn1701\" target=\"_blank\">stoic-wind-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-18:36:44] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:36:44] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:36:44] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 641ms/step - loss: 0.9095 - val_loss: 1.0534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.7291 - val_loss: 0.5396\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.6043 - val_loss: 0.5293\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.5412 - val_loss: 0.4230\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5103 - val_loss: 0.4205\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4966 - val_loss: 0.4405\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4904 - val_loss: 0.4126\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4749 - val_loss: 0.4512\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4619 - val_loss: 0.4020\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4455 - val_loss: 0.4046\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4413 - val_loss: 0.4182\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4299 - val_loss: 0.3972\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4330 - val_loss: 0.4265\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4207 - val_loss: 0.3901\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 188ms/step - loss: 0.4277 - val_loss: 0.3909\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4227 - val_loss: 0.4467\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4142 - val_loss: 0.3957\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-18:37:10] Training the entire fine-tuned model...\n",
      "[2022_04_21-18:37:22] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 771ms/step - loss: 0.4230 - val_loss: 0.3943\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 310ms/step - loss: 0.4208 - val_loss: 0.3974\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.4130 - val_loss: 0.3983\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 0.4126 - val_loss: 0.3985\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "[2022_04_21-18:37:36] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-18:37:36] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:37:37] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 727ms/step - loss: 0.4255 - val_loss: 0.3942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-18:38:13] Test set: Filtered out 0 of 395 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2vxn1701) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 32790... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██▁▁▂▂▁</td></tr><tr><td>loss</td><td>█▅▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▁▁▂▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>13</td></tr><tr><td>best_val_loss</td><td>0.3901</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.42553</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.39425</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">stoic-wind-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/2vxn1701\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/2vxn1701</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_183627-2vxn1701/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2vxn1701). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/1sh57rzi\" target=\"_blank\">fresh-wildflower-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-18:38:37] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:38:37] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:38:37] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 490ms/step - loss: 0.9082 - val_loss: 0.5462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4669 - val_loss: 0.6345\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4247 - val_loss: 0.5916\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4445 - val_loss: 0.5395\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4110 - val_loss: 0.6021\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3937 - val_loss: 0.5246\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3659 - val_loss: 0.5189\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3674 - val_loss: 0.5131\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3691 - val_loss: 0.5321\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3502 - val_loss: 0.5760\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3562 - val_loss: 0.5196\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-18:38:56] Training the entire fine-tuned model...\n",
      "[2022_04_21-18:39:04] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3513WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1141s vs `on_train_batch_end` time: 0.1265s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1141s vs `on_train_batch_end` time: 0.1265s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 623ms/step - loss: 0.3513 - val_loss: 0.5184\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3432 - val_loss: 0.5272\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3521 - val_loss: 0.5275\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 259ms/step - loss: 0.3512 - val_loss: 0.5204\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "[2022_04_21-18:39:18] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-18:39:18] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:39:20] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3461WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1010s vs `on_train_batch_end` time: 0.1224s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1010s vs `on_train_batch_end` time: 0.1224s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 629ms/step - loss: 0.3461 - val_loss: 0.5179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-18:39:56] Test set: Filtered out 0 of 380 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1sh57rzi) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 32999... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▅▅▆▇▇█▁▂▂▃▁</td></tr><tr><td>loss</td><td>█▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃█▆▃▆▂▁▁▂▅▁▁▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.51313</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34613</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.5179</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fresh-wildflower-1</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042b/runs/1sh57rzi\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042b/runs/1sh57rzi</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_183815-1sh57rzi/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1sh57rzi). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/3kcrbrps\" target=\"_blank\">still-durian-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-18:40:15] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:40:15] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:40:15] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 614ms/step - loss: 0.9520 - val_loss: 0.7470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.6438 - val_loss: 0.4999\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5981 - val_loss: 0.4387\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5750 - val_loss: 0.4239\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5543 - val_loss: 0.4281\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5348 - val_loss: 0.4690\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4800 - val_loss: 0.4057\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4628 - val_loss: 0.4103\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4587 - val_loss: 0.4217\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4546 - val_loss: 0.3998\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4565 - val_loss: 0.4295\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4405 - val_loss: 0.3941\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4465 - val_loss: 0.3925\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4300 - val_loss: 0.3976\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4207 - val_loss: 0.3886\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4184 - val_loss: 0.3924\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4067 - val_loss: 0.3906\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4029 - val_loss: 0.3906\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-18:40:40] Training the entire fine-tuned model...\n",
      "[2022_04_21-18:40:54] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 759ms/step - loss: 0.4173 - val_loss: 0.3929\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 310ms/step - loss: 0.4083 - val_loss: 0.3902\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.4094 - val_loss: 0.3944\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.4012 - val_loss: 0.3869\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.4104 - val_loss: 0.3850\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.4004 - val_loss: 0.3813\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 334ms/step - loss: 0.3872 - val_loss: 0.3789\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3946 - val_loss: 0.3860\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3864 - val_loss: 0.3749\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 0.3755 - val_loss: 0.3968\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 335ms/step - loss: 0.3831 - val_loss: 0.3741\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3765 - val_loss: 0.3773\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3554 - val_loss: 0.3721\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3614 - val_loss: 0.3799\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3542 - val_loss: 0.3743\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3507 - val_loss: 0.3670\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3332 - val_loss: 0.3777\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.3268 - val_loss: 0.3699\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3115 - val_loss: 0.3809\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_21-18:41:32] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-18:41:32] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:41:32] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 744ms/step - loss: 0.3301 - val_loss: 0.3699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-18:42:07] Test set: Filtered out 0 of 395 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3kcrbrps) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 33188... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇█▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██▁</td></tr><tr><td>loss</td><td>█▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>15</td></tr><tr><td>best_val_loss</td><td>0.36697</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33011</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.36988</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">still-durian-2</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/3kcrbrps\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/3kcrbrps</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_183959-3kcrbrps/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3kcrbrps). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/2a3ne97r\" target=\"_blank\">dandy-monkey-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-18:42:27] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:42:27] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:42:27] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 512ms/step - loss: 0.8397 - val_loss: 0.6918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.5949 - val_loss: 0.7813\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.4684 - val_loss: 0.5383\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4298 - val_loss: 0.5433\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.3879 - val_loss: 0.6076\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.3921 - val_loss: 0.5720\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-18:42:43] Training the entire fine-tuned model...\n",
      "[2022_04_21-18:42:56] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4039WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0983s vs `on_train_batch_end` time: 0.1256s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0983s vs `on_train_batch_end` time: 0.1256s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 628ms/step - loss: 0.4039 - val_loss: 0.5731\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.4021 - val_loss: 0.5429\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3925 - val_loss: 0.5357\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3887 - val_loss: 0.5374\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3862 - val_loss: 0.5437\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3748 - val_loss: 0.5446\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_21-18:43:14] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-18:43:14] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:43:14] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3954WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1232s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1232s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 876ms/step - loss: 0.3954 - val_loss: 0.5369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-18:43:48] Test set: Filtered out 0 of 380 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2a3ne97r) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 33481... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▄▅▇█▁▂▄▅▇█▁</td></tr><tr><td>loss</td><td>█▄▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▅█▁▁▃▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>2</td></tr><tr><td>best_val_loss</td><td>0.53566</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.39544</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.53695</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dandy-monkey-2</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042b/runs/2a3ne97r\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042b/runs/2a3ne97r</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_184209-2a3ne97r/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2a3ne97r). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/17jkrkbz\" target=\"_blank\">lyric-sun-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-18:44:08] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:44:08] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:44:08] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 649ms/step - loss: 0.9174 - val_loss: 0.7073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5703 - val_loss: 0.4567\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 167ms/step - loss: 0.5518 - val_loss: 0.4283\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.5433 - val_loss: 0.5040\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5061 - val_loss: 0.4201\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.5091 - val_loss: 0.4108\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5052 - val_loss: 0.4849\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4838 - val_loss: 0.4118\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4639 - val_loss: 0.4263\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-18:44:27] Training the entire fine-tuned model...\n",
      "[2022_04_21-18:44:36] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 761ms/step - loss: 0.4693 - val_loss: 0.4138\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.4552 - val_loss: 0.4038\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 310ms/step - loss: 0.4539 - val_loss: 0.4148\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 0.4511 - val_loss: 0.3934\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.4370 - val_loss: 0.4120\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.4292 - val_loss: 0.3891\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.4275 - val_loss: 0.3836\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.4104 - val_loss: 0.3960\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.3973 - val_loss: 0.3771\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 0.3921 - val_loss: 0.4074\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3745 - val_loss: 0.3737\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3501 - val_loss: 0.3754\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3389 - val_loss: 0.3777\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3135 - val_loss: 0.3859\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_21-18:45:06] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-18:45:06] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:45:06] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 763ms/step - loss: 0.3603 - val_loss: 0.3739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-18:45:40] Test set: Filtered out 0 of 395 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:17jkrkbz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 33646... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▄▅▅▁▂▂▃▃▄▄▅▅▆▆▇▇█▁</td></tr><tr><td>loss</td><td>█▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▂</td></tr><tr><td>lr</td><td>█████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▄▂▂▃▂▂▂▂▂▁▂▁▁▁▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>10</td></tr><tr><td>best_val_loss</td><td>0.37369</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36028</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.37388</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">lyric-sun-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/17jkrkbz\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/17jkrkbz</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_184350-17jkrkbz/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:17jkrkbz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/932o8hbh\" target=\"_blank\">driven-elevator-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-18:45:59] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:45:59] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:45:59] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 519ms/step - loss: 0.9414 - val_loss: 0.5450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.5655 - val_loss: 0.9106\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.5090 - val_loss: 0.7022\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4808 - val_loss: 0.7454\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-18:46:13] Training the entire fine-tuned model...\n",
      "[2022_04_21-18:46:26] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4582WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0966s vs `on_train_batch_end` time: 0.1257s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0966s vs `on_train_batch_end` time: 0.1257s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 634ms/step - loss: 0.4582 - val_loss: 0.5515\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 259ms/step - loss: 0.4414 - val_loss: 0.5622\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.4401 - val_loss: 0.5430\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.4319 - val_loss: 0.5380\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.4315 - val_loss: 0.5320\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.4185 - val_loss: 0.5242\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 259ms/step - loss: 0.4052 - val_loss: 0.5261\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3914 - val_loss: 0.5222\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3775 - val_loss: 0.5444\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 259ms/step - loss: 0.3694 - val_loss: 0.5350\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 282ms/step - loss: 0.3738 - val_loss: 0.5505\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_21-18:46:53] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-18:46:53] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:46:54] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3797WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1001s vs `on_train_batch_end` time: 0.1242s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1001s vs `on_train_batch_end` time: 0.1242s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 636ms/step - loss: 0.3797 - val_loss: 0.5218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-18:47:27] Test set: Filtered out 0 of 380 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:932o8hbh) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 33865... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▁▂▂▃▄▅▅▆▇▇█▁</td></tr><tr><td>loss</td><td>█▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▁█▄▅▂▂▁▁▁▁▁▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.52179</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37974</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.52179</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">driven-elevator-3</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042b/runs/932o8hbh\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042b/runs/932o8hbh</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_184542-932o8hbh/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:932o8hbh). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/2u60kb6c\" target=\"_blank\">prime-darkness-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-18:47:47] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:47:47] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:47:47] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 629ms/step - loss: 0.8745 - val_loss: 0.7669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.6838 - val_loss: 0.4440\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5636 - val_loss: 0.4401\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5508 - val_loss: 0.5045\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.5299 - val_loss: 0.4168\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5027 - val_loss: 0.4103\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4723 - val_loss: 0.4212\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4556 - val_loss: 0.4034\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4580 - val_loss: 0.4317\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4588 - val_loss: 0.4051\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4474 - val_loss: 0.3955\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4302 - val_loss: 0.4383\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4333 - val_loss: 0.3920\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 185ms/step - loss: 0.4396 - val_loss: 0.3940\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4355 - val_loss: 0.4187\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4125 - val_loss: 0.3869\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4199 - val_loss: 0.4070\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4069 - val_loss: 0.3939\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3993 - val_loss: 0.3915\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-18:48:13] Training the entire fine-tuned model...\n",
      "[2022_04_21-18:48:26] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 771ms/step - loss: 0.5626 - val_loss: 0.4533\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.4762 - val_loss: 0.4600\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.4467 - val_loss: 0.3956\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.4237 - val_loss: 0.4325\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.4143 - val_loss: 0.3852\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.4098 - val_loss: 0.3885\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 336ms/step - loss: 0.3758 - val_loss: 0.3806\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3575 - val_loss: 0.3778\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3532 - val_loss: 0.4078\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 316ms/step - loss: 0.3285 - val_loss: 0.3820\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 340ms/step - loss: 0.3307 - val_loss: 0.3835\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_21-18:48:51] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-18:48:51] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:48:51] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 10s 1s/step - loss: 0.3478 - val_loss: 0.3780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-18:49:24] Test set: Filtered out 0 of 395 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2u60kb6c) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 34034... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██▁▁▂▂▃▃▃▄▄▅▅▁</td></tr><tr><td>loss</td><td>█▆▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▄▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▃▂▂▂▁▂▁▁▂▁▁▂▁▂▁▁▂▂▁▂▁▁▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.37776</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34779</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.37795</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">prime-darkness-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/2u60kb6c\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/2u60kb6c</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_184730-2u60kb6c/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2u60kb6c). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/30v7u8yf\" target=\"_blank\">cool-lion-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-18:49:43] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:49:43] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:49:43] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 528ms/step - loss: 0.9368 - val_loss: 0.5745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.5960 - val_loss: 0.8453\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4779 - val_loss: 0.6088\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4444 - val_loss: 0.5967\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_21-18:49:58] Training the entire fine-tuned model...\n",
      "[2022_04_21-18:50:11] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4681WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0996s vs `on_train_batch_end` time: 0.1215s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0996s vs `on_train_batch_end` time: 0.1215s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 631ms/step - loss: 0.4681 - val_loss: 0.5721\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 259ms/step - loss: 0.4572 - val_loss: 0.5432\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.4400 - val_loss: 0.5482\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.4338 - val_loss: 0.5483\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 281ms/step - loss: 0.4254 - val_loss: 0.5402\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.4133 - val_loss: 0.5162\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 282ms/step - loss: 0.4054 - val_loss: 0.5173\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 259ms/step - loss: 0.3861 - val_loss: 0.5888\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3771 - val_loss: 0.5032\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 259ms/step - loss: 0.3344 - val_loss: 0.5035\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 265ms/step - loss: 0.3164 - val_loss: 0.6246\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 259ms/step - loss: 0.3005 - val_loss: 0.7117\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_21-18:50:39] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-18:50:39] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:50:39] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3643WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0992s vs `on_train_batch_end` time: 0.1234s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0992s vs `on_train_batch_end` time: 0.1234s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 635ms/step - loss: 0.3643 - val_loss: 0.4960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-18:51:13] Test set: Filtered out 0 of 380 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:30v7u8yf) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 34289... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▁▂▂▃▄▄▅▅▆▇▇█▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▂</td></tr><tr><td>lr</td><td>████▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃█▃▃▃▂▂▂▂▁▁▃▁▁▄▅▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.49603</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36426</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.49603</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">cool-lion-4</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042b/runs/30v7u8yf\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042b/runs/30v7u8yf</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_184926-30v7u8yf/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:30v7u8yf). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/29h4h3yg\" target=\"_blank\">earnest-music-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-18:51:32] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:51:32] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:51:32] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 623ms/step - loss: 1.0407 - val_loss: 0.9931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.7794 - val_loss: 0.6543\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.6924 - val_loss: 0.7277\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.6273 - val_loss: 0.4744\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5455 - val_loss: 0.5108\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.5276 - val_loss: 0.4169\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4996 - val_loss: 0.4191\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 187ms/step - loss: 0.4828 - val_loss: 0.4121\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 187ms/step - loss: 0.4629 - val_loss: 0.4061\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4629 - val_loss: 0.4248\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4487 - val_loss: 0.4060\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4399 - val_loss: 0.4042\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 185ms/step - loss: 0.4549 - val_loss: 0.4109\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4766 - val_loss: 0.4826\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.4476 - val_loss: 0.4207\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4632 - val_loss: 0.3933\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4295 - val_loss: 0.4454\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4323 - val_loss: 0.3914\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4249 - val_loss: 0.3923\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.4133 - val_loss: 0.4129\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4121 - val_loss: 0.3932\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4160 - val_loss: 0.3930\n",
      "[2022_04_21-18:52:00] Training the entire fine-tuned model...\n",
      "[2022_04_21-18:52:08] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 11s 782ms/step - loss: 0.4164 - val_loss: 0.3948\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.4117 - val_loss: 0.4010\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.4159 - val_loss: 0.4031\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.4175 - val_loss: 0.3970\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.4152 - val_loss: 0.3961\n",
      "[2022_04_21-18:52:26] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-18:52:26] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:52:27] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 735ms/step - loss: 0.4112 - val_loss: 0.3948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-18:53:02] Test set: Filtered out 0 of 395 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:29h4h3yg) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 34482... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▅▄▃▂▂▂▂▂▂▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████▃▃▃▃▃▃▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▅▂▂▁▁▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>17</td></tr><tr><td>best_val_loss</td><td>0.39135</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.41115</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.3948</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">earnest-music-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/29h4h3yg\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/29h4h3yg</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_185115-29h4h3yg/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:29h4h3yg). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/4rrp0uge\" target=\"_blank\">whole-sunset-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-18:53:27] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:53:27] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:53:27] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 854ms/step - loss: 0.8402 - val_loss: 0.5921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4978 - val_loss: 0.5583\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4145 - val_loss: 0.5498\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3940 - val_loss: 0.5530\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3968 - val_loss: 0.5654\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3896 - val_loss: 0.5235\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.3677 - val_loss: 0.5300\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3695 - val_loss: 0.5935\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3998 - val_loss: 0.6103\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3690 - val_loss: 0.5220\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3526 - val_loss: 0.5254\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3429 - val_loss: 0.5325\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 140ms/step - loss: 0.3408 - val_loss: 0.5197\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3426 - val_loss: 0.5193\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3565 - val_loss: 0.5190\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3375 - val_loss: 0.5209\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3371 - val_loss: 0.5285\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3460 - val_loss: 0.5197\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3319 - val_loss: 0.5143\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.3375 - val_loss: 0.5140\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3371 - val_loss: 0.5212\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3356 - val_loss: 0.5268\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3322 - val_loss: 0.5194\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3316 - val_loss: 0.5175\n",
      "[2022_04_21-18:53:59] Training the entire fine-tuned model...\n",
      "[2022_04_21-18:54:07] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3477WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0980s vs `on_train_batch_end` time: 0.1232s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0980s vs `on_train_batch_end` time: 0.1232s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 632ms/step - loss: 0.3477 - val_loss: 0.5154\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3276 - val_loss: 0.5200\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3341 - val_loss: 0.5190\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3258 - val_loss: 0.5151\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3274 - val_loss: 0.5155\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3261 - val_loss: 0.5176\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3250 - val_loss: 0.5186\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.3351 - val_loss: 0.5172\n",
      "[2022_04_21-18:54:28] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-18:54:28] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:54:28] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3354WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0991s vs `on_train_batch_end` time: 0.1249s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0991s vs `on_train_batch_end` time: 0.1249s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 636ms/step - loss: 0.3354 - val_loss: 0.5151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-18:55:02] Test set: Filtered out 0 of 380 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:4rrp0uge) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 34726... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇▄▄▄▅▂▂▇█▂▂▂▁▁▁▂▂▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>19</td></tr><tr><td>best_val_loss</td><td>0.51396</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3354</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.51515</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">whole-sunset-5</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042b/runs/4rrp0uge\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042b/runs/4rrp0uge</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_185304-4rrp0uge/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:4rrp0uge). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/1e9h5ft9\" target=\"_blank\">earnest-butterfly-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-18:55:23] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:55:23] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:55:23] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 652ms/step - loss: 0.8076 - val_loss: 0.7902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.6794 - val_loss: 0.4389\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5696 - val_loss: 0.4447\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5389 - val_loss: 0.5577\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.5258 - val_loss: 0.4249\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4854 - val_loss: 0.4472\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4757 - val_loss: 0.4100\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4537 - val_loss: 0.4303\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4472 - val_loss: 0.4032\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4385 - val_loss: 0.3984\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.4396 - val_loss: 0.4167\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.4302 - val_loss: 0.3974\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4335 - val_loss: 0.3964\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4204 - val_loss: 0.3909\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4227 - val_loss: 0.3933\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4232 - val_loss: 0.4362\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4238 - val_loss: 0.4156\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4109 - val_loss: 0.3851\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4004 - val_loss: 0.3834\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3967 - val_loss: 0.3966\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3952 - val_loss: 0.3860\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3897 - val_loss: 0.3834\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3886 - val_loss: 0.3856\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3965 - val_loss: 0.3876\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3998 - val_loss: 0.3878\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3961 - val_loss: 0.3867\n",
      "[2022_04_21-18:55:55] Training the entire fine-tuned model...\n",
      "[2022_04_21-18:56:22] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 1s/step - loss: 0.3962 - val_loss: 0.3837\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3914 - val_loss: 0.3874\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.4018 - val_loss: 0.3784\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 0.3904 - val_loss: 0.3927\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3785 - val_loss: 0.3787\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3896 - val_loss: 0.3795\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3759 - val_loss: 0.3828\n",
      "[2022_04_21-18:56:42] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-18:56:42] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:56:42] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 781ms/step - loss: 0.3928 - val_loss: 0.3795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-18:57:23] Test set: Filtered out 0 of 395 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1e9h5ft9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 34992... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▆▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▄▂▂▂▂▁▁▂▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>2</td></tr><tr><td>best_val_loss</td><td>0.37837</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.39278</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.37955</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">earnest-butterfly-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/1e9h5ft9\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/1e9h5ft9</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_185504-1e9h5ft9/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1e9h5ft9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/2x2lzc6l\" target=\"_blank\">ancient-hill-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-18:57:42] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:57:42] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:57:42] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 488ms/step - loss: 1.0130 - val_loss: 0.5424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.5419 - val_loss: 0.7595\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4618 - val_loss: 0.5345\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4104 - val_loss: 0.5468\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3912 - val_loss: 0.5415\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3809 - val_loss: 0.5471\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3682 - val_loss: 0.5340\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3721 - val_loss: 0.5481\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3672 - val_loss: 0.5304\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.3587 - val_loss: 0.5302\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3643 - val_loss: 0.5293\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3706 - val_loss: 0.5216\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3662 - val_loss: 0.5410\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3591 - val_loss: 0.5199\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3539 - val_loss: 0.5303\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3527 - val_loss: 0.5167\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3470 - val_loss: 0.5222\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3518 - val_loss: 0.5188\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3519 - val_loss: 0.5140\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3479 - val_loss: 0.5345\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3499 - val_loss: 0.5148\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.3373 - val_loss: 0.5289\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.3450 - val_loss: 0.5196\n",
      "[2022_04_21-18:58:11] Training the entire fine-tuned model...\n",
      "[2022_04_21-18:58:21] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3422WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0986s vs `on_train_batch_end` time: 0.1231s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0986s vs `on_train_batch_end` time: 0.1231s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 617ms/step - loss: 0.3422 - val_loss: 0.5248\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.3414 - val_loss: 0.5193\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 259ms/step - loss: 0.3324 - val_loss: 0.5299\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 259ms/step - loss: 0.3484 - val_loss: 0.5209\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.3426 - val_loss: 0.5114\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 282ms/step - loss: 0.3331 - val_loss: 0.5379\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3318 - val_loss: 0.5145\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3378 - val_loss: 0.5136\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.3250 - val_loss: 0.5195\n",
      "[2022_04_21-18:58:44] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-18:58:44] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:58:48] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3298WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0967s vs `on_train_batch_end` time: 0.1262s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0967s vs `on_train_batch_end` time: 0.1262s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 612ms/step - loss: 0.3298 - val_loss: 0.5133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-18:59:21] Test set: Filtered out 0 of 380 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2x2lzc6l) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 35264... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▂█▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁▂▁▁▁▁▁▂▁▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>4</td></tr><tr><td>best_val_loss</td><td>0.5114</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32977</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.51327</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">ancient-hill-6</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042b/runs/2x2lzc6l\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042b/runs/2x2lzc6l</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_185725-2x2lzc6l/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2x2lzc6l). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/7xeh9nki\" target=\"_blank\">youthful-cherry-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-18:59:41] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:59:41] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-18:59:41] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 625ms/step - loss: 0.8829 - val_loss: 1.0177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.7493 - val_loss: 0.5174\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5941 - val_loss: 0.4755\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5224 - val_loss: 0.4223\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4954 - val_loss: 0.4219\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.4787 - val_loss: 0.4339\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4695 - val_loss: 0.4064\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4599 - val_loss: 0.4513\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4435 - val_loss: 0.4037\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4481 - val_loss: 0.4282\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4499 - val_loss: 0.3965\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4377 - val_loss: 0.4078\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 0.4243 - val_loss: 0.3894\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4307 - val_loss: 0.3875\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4162 - val_loss: 0.4701\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4221 - val_loss: 0.3913\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 0.4151 - val_loss: 0.3906\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 168ms/step - loss: 0.4056 - val_loss: 0.3864\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4102 - val_loss: 0.4005\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4070 - val_loss: 0.3876\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3983 - val_loss: 0.3893\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4030 - val_loss: 0.3893\n",
      "[2022_04_21-19:00:09] Training the entire fine-tuned model...\n",
      "[2022_04_21-19:00:19] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 762ms/step - loss: 0.3988 - val_loss: 0.4006\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.3942 - val_loss: 0.3886\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3925 - val_loss: 0.3774\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.3872 - val_loss: 0.3899\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 339ms/step - loss: 0.3861 - val_loss: 0.3833\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 335ms/step - loss: 0.3749 - val_loss: 0.3707\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3605 - val_loss: 0.3855\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3584 - val_loss: 0.3688\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3518 - val_loss: 0.3770\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3274 - val_loss: 0.3653\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3501 - val_loss: 0.3771\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3495 - val_loss: 0.4106\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3220 - val_loss: 0.3679\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3174 - val_loss: 0.3746\n",
      "[2022_04_21-19:00:50] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-19:00:50] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:00:50] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 750ms/step - loss: 0.3319 - val_loss: 0.3698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-19:01:24] Test set: Filtered out 0 of 395 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:7xeh9nki) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 35533... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▄▅▅▅▁</td></tr><tr><td>loss</td><td>█▆▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▂▁▂▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>9</td></tr><tr><td>best_val_loss</td><td>0.36527</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33194</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.36982</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">youthful-cherry-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/7xeh9nki\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/7xeh9nki</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_185924-7xeh9nki/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:7xeh9nki). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/1gvn7s18\" target=\"_blank\">winter-jazz-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-19:01:43] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:01:43] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:01:44] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 516ms/step - loss: 0.9458 - val_loss: 0.5712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.6454 - val_loss: 1.0067\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 139ms/step - loss: 0.5933 - val_loss: 0.5863\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4254 - val_loss: 0.6357\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.4196 - val_loss: 0.5500\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3996 - val_loss: 0.5361\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3912 - val_loss: 0.5721\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3989 - val_loss: 0.5636\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3872 - val_loss: 0.5325\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3826 - val_loss: 0.5307\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3799 - val_loss: 0.5370\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3794 - val_loss: 0.5449\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3786 - val_loss: 0.5265\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.3693 - val_loss: 0.5355\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3662 - val_loss: 0.5345\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3646 - val_loss: 0.5357\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3656 - val_loss: 0.5343\n",
      "[2022_04_21-19:02:09] Training the entire fine-tuned model...\n",
      "[2022_04_21-19:02:18] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3832WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0973s vs `on_train_batch_end` time: 0.1259s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0973s vs `on_train_batch_end` time: 0.1259s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 612ms/step - loss: 0.3832 - val_loss: 0.5321\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 259ms/step - loss: 0.3780 - val_loss: 0.5828\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3910 - val_loss: 0.5138\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3541 - val_loss: 0.5102\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3532 - val_loss: 0.5202\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3410 - val_loss: 0.5210\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3341 - val_loss: 0.5210\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3234 - val_loss: 0.5214\n",
      "[2022_04_21-19:02:38] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-19:02:38] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:02:38] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3572WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1243s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1243s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 620ms/step - loss: 0.3572 - val_loss: 0.5110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-19:03:19] Test set: Filtered out 0 of 380 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1gvn7s18) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 35841... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██▁▁▂▂▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▅▄▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▂█▂▃▂▁▂▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>3</td></tr><tr><td>best_val_loss</td><td>0.51019</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35719</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.51095</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">winter-jazz-7</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042b/runs/1gvn7s18\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042b/runs/1gvn7s18</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_190127-1gvn7s18/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1gvn7s18). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/3ohoj59e\" target=\"_blank\">fallen-pyramid-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-19:03:37] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:03:37] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:03:37] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 627ms/step - loss: 0.9310 - val_loss: 0.9595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.7175 - val_loss: 0.5804\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.6257 - val_loss: 0.5604\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5384 - val_loss: 0.4285\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4960 - val_loss: 0.4542\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4822 - val_loss: 0.4155\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4745 - val_loss: 0.4144\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4708 - val_loss: 0.4070\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4567 - val_loss: 0.4300\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4500 - val_loss: 0.4156\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4356 - val_loss: 0.3991\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4589 - val_loss: 0.4296\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.4441 - val_loss: 0.3987\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4209 - val_loss: 0.3903\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4263 - val_loss: 0.3899\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4174 - val_loss: 0.4090\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4152 - val_loss: 0.3926\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4177 - val_loss: 0.3859\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4091 - val_loss: 0.4111\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4170 - val_loss: 0.4117\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.4170 - val_loss: 0.3786\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3936 - val_loss: 0.3957\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4007 - val_loss: 0.4027\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3912 - val_loss: 0.3793\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3873 - val_loss: 0.3876\n",
      "[2022_04_21-19:04:07] Training the entire fine-tuned model...\n",
      "[2022_04_21-19:04:15] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 749ms/step - loss: 0.4915 - val_loss: 0.3964\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.4370 - val_loss: 0.3909\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 309ms/step - loss: 0.4345 - val_loss: 0.4219\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3940 - val_loss: 0.3803\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3742 - val_loss: 0.4108\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 309ms/step - loss: 0.3556 - val_loss: 0.3735\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3336 - val_loss: 0.3791\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 310ms/step - loss: 0.3220 - val_loss: 0.3802\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 309ms/step - loss: 0.3524 - val_loss: 0.4040\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3478 - val_loss: 0.3800\n",
      "[2022_04_21-19:04:38] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-19:04:38] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:04:38] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 765ms/step - loss: 0.3453 - val_loss: 0.3974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-19:05:14] Test set: Filtered out 0 of 395 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3ohoj59e) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 36073... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▆▄▃▃▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████████████▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▂▂▂▁▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.37354</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34529</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.39744</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fallen-pyramid-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/3ohoj59e\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/3ohoj59e</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_190321-3ohoj59e/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3ohoj59e). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/2emdtoac\" target=\"_blank\">serene-universe-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-19:05:33] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:05:33] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:05:33] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 491ms/step - loss: 0.8601 - val_loss: 0.6190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.5689 - val_loss: 0.6303\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.4756 - val_loss: 0.6575\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4794 - val_loss: 0.5776\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.4238 - val_loss: 0.6495\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4015 - val_loss: 0.5324\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.3905 - val_loss: 0.5213\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.3622 - val_loss: 0.5427\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3574 - val_loss: 0.5127\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3490 - val_loss: 0.5275\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3523 - val_loss: 0.5188\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3551 - val_loss: 0.5321\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3377 - val_loss: 0.5108\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3302 - val_loss: 0.5143\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3484 - val_loss: 0.5241\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3363 - val_loss: 0.5068\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3347 - val_loss: 0.5106\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3348 - val_loss: 0.5103\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3340 - val_loss: 0.5181\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3285 - val_loss: 0.5161\n",
      "[2022_04_21-19:06:00] Training the entire fine-tuned model...\n",
      "[2022_04_21-19:06:10] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3868WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0977s vs `on_train_batch_end` time: 0.1251s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0977s vs `on_train_batch_end` time: 0.1251s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 611ms/step - loss: 0.3868 - val_loss: 0.5846\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3468 - val_loss: 0.5054\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 260ms/step - loss: 0.3377 - val_loss: 0.4870\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3121 - val_loss: 0.4949\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3153 - val_loss: 0.5017\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3046 - val_loss: 0.5550\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.2914 - val_loss: 0.4948\n",
      "[2022_04_21-19:06:29] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-19:06:29] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:06:29] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3189WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0998s vs `on_train_batch_end` time: 0.1215s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0998s vs `on_train_batch_end` time: 0.1215s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 620ms/step - loss: 0.3189 - val_loss: 0.4889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-19:07:06] Test set: Filtered out 0 of 380 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2emdtoac) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 36353... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▆▇█▅█▃▂▃▂▃▂▃▂▂▃▂▂▂▂▂▅▂▁▁▂▄▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>2</td></tr><tr><td>best_val_loss</td><td>0.48704</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3189</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.4889</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">serene-universe-8</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042b/runs/2emdtoac\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042b/runs/2emdtoac</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_190516-2emdtoac/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2emdtoac). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/1c8j6zsu\" target=\"_blank\">true-spaceship-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-19:07:27] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:07:27] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:07:27] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 644ms/step - loss: 0.9054 - val_loss: 0.7978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.6560 - val_loss: 0.4522\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5490 - val_loss: 0.4241\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5221 - val_loss: 0.4663\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4967 - val_loss: 0.4096\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4874 - val_loss: 0.4080\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4754 - val_loss: 0.4439\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4740 - val_loss: 0.4007\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4568 - val_loss: 0.4031\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4411 - val_loss: 0.4108\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4327 - val_loss: 0.3964\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.4388 - val_loss: 0.3910\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.4466 - val_loss: 0.4401\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.4588 - val_loss: 0.4079\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.4506 - val_loss: 0.4537\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 0.5054 - val_loss: 0.3853\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4305 - val_loss: 0.4688\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4328 - val_loss: 0.3865\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4178 - val_loss: 0.3871\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4096 - val_loss: 0.3969\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.4091 - val_loss: 0.3980\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4042 - val_loss: 0.3928\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-19:07:54] Training the entire fine-tuned model...\n",
      "[2022_04_21-19:08:04] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 768ms/step - loss: 0.4045 - val_loss: 0.3896\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 0.4061 - val_loss: 0.3928\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.4010 - val_loss: 0.3930\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 0.4024 - val_loss: 0.3897\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.4039 - val_loss: 0.3897\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.4067 - val_loss: 0.3891\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.4073 - val_loss: 0.3891\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 310ms/step - loss: 0.4066 - val_loss: 0.3882\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 0.4088 - val_loss: 0.3880\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.4041 - val_loss: 0.3881\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.4076 - val_loss: 0.3884\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 0.4126 - val_loss: 0.3883\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.4048 - val_loss: 0.3884\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.4093 - val_loss: 0.3884\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 336ms/step - loss: 0.4033 - val_loss: 0.3883\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      "[2022_04_21-19:08:35] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-19:08:35] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:08:35] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 787ms/step - loss: 0.4067 - val_loss: 0.3882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-19:09:26] Test set: Filtered out 0 of 395 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1c8j6zsu) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 36596... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▁</td></tr><tr><td>loss</td><td>█▅▃▃▂▂▂▂▂▂▁▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▂▁▁▂▁▁▁▁▁▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>15</td></tr><tr><td>best_val_loss</td><td>0.38527</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.40668</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.38825</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">true-spaceship-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/1c8j6zsu\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/1c8j6zsu</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_190708-1c8j6zsu/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1c8j6zsu). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/351gp8ct\" target=\"_blank\">vague-night-9</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-19:09:45] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:09:45] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:09:45] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 474ms/step - loss: 0.8122 - val_loss: 0.8014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.5830 - val_loss: 0.8328\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4926 - val_loss: 0.5355\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.4938 - val_loss: 0.5632\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4031 - val_loss: 0.5729\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3939 - val_loss: 0.5419\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.4033 - val_loss: 0.5355\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3837 - val_loss: 0.5802\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3733 - val_loss: 0.5268\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.3703 - val_loss: 0.5249\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3604 - val_loss: 0.5412\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3682 - val_loss: 0.5321\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3618 - val_loss: 0.5223\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3611 - val_loss: 0.5327\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3613 - val_loss: 0.5285\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3645 - val_loss: 0.5207\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3572 - val_loss: 0.5488\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3480 - val_loss: 0.5202\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3459 - val_loss: 0.5226\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3496 - val_loss: 0.5211\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 0.3533 - val_loss: 0.5215\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.3425 - val_loss: 0.5235\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3415 - val_loss: 0.5248\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3476 - val_loss: 0.5225\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-19:10:15] Training the entire fine-tuned model...\n",
      "[2022_04_21-19:10:42] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3566WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0980s vs `on_train_batch_end` time: 0.1235s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0980s vs `on_train_batch_end` time: 0.1235s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 616ms/step - loss: 0.3566 - val_loss: 0.5260\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3479 - val_loss: 0.5215\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3412 - val_loss: 0.5214\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.3411 - val_loss: 0.5203\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3526 - val_loss: 0.5223\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.3492 - val_loss: 0.5264\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3462 - val_loss: 0.5281\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3429 - val_loss: 0.5272\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3396 - val_loss: 0.5269\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 282ms/step - loss: 0.3485 - val_loss: 0.5262\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_21-19:11:06] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-19:11:06] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:11:06] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3446WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1005s vs `on_train_batch_end` time: 0.1228s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1005s vs `on_train_batch_end` time: 0.1228s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 626ms/step - loss: 0.3446 - val_loss: 0.5205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-19:11:42] Test set: Filtered out 0 of 380 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:351gp8ct) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 36906... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▅▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇█▁▂▂▁▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>17</td></tr><tr><td>best_val_loss</td><td>0.52016</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3446</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.52052</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">vague-night-9</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042b/runs/351gp8ct\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042b/runs/351gp8ct</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_190928-351gp8ct/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:351gp8ct). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/2ceb3k4q\" target=\"_blank\">honest-elevator-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-19:12:00] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:12:00] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:12:00] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 627ms/step - loss: 0.9814 - val_loss: 0.9044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.7361 - val_loss: 0.5348\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5920 - val_loss: 0.5477\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 171ms/step - loss: 0.5698 - val_loss: 0.4308\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.5242 - val_loss: 0.4254\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4916 - val_loss: 0.4214\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4727 - val_loss: 0.4082\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.4789 - val_loss: 0.4154\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4663 - val_loss: 0.4151\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4542 - val_loss: 0.4021\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4461 - val_loss: 0.4466\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4523 - val_loss: 0.3979\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4288 - val_loss: 0.3916\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.4301 - val_loss: 0.3995\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4299 - val_loss: 0.4141\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4202 - val_loss: 0.3894\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4170 - val_loss: 0.4065\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4123 - val_loss: 0.3954\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4053 - val_loss: 0.3827\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.4093 - val_loss: 0.3846\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4117 - val_loss: 0.4394\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3935 - val_loss: 0.3786\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3971 - val_loss: 0.3767\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4081 - val_loss: 0.3946\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3697 - val_loss: 0.3767\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3919 - val_loss: 0.3846\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3687 - val_loss: 0.3766\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3799 - val_loss: 0.3779\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3819 - val_loss: 0.3763\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 0.3754 - val_loss: 0.3757\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3853 - val_loss: 0.3814\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3789 - val_loss: 0.3764\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3911 - val_loss: 0.3779\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3769 - val_loss: 0.3791\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3729 - val_loss: 0.3768\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3832 - val_loss: 0.3770\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-19:12:39] Training the entire fine-tuned model...\n",
      "[2022_04_21-19:12:57] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 757ms/step - loss: 0.3785 - val_loss: 0.3714\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 310ms/step - loss: 0.3862 - val_loss: 0.3953\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3742 - val_loss: 0.3716\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 317ms/step - loss: 0.3758 - val_loss: 0.3774\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3677 - val_loss: 0.3783\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.3637 - val_loss: 0.3768\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3655 - val_loss: 0.3749\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_21-19:13:16] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-19:13:16] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:13:16] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 761ms/step - loss: 0.3778 - val_loss: 0.3714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-19:13:51] Test set: Filtered out 0 of 395 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2ceb3k4q) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 37205... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████████████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▂▂▂▁▂▂▁▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.37138</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.37776</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.37138</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">honest-elevator-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/2ceb3k4q\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/2ceb3k4q</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_191144-2ceb3k4q/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2ceb3k4q). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/3lxkksrh\" target=\"_blank\">brisk-fire-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-19:14:10] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:14:10] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:14:10] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 508ms/step - loss: 0.8199 - val_loss: 0.6776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.5135 - val_loss: 0.6016\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4603 - val_loss: 0.6012\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.4034 - val_loss: 0.5354\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3817 - val_loss: 0.5501\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3825 - val_loss: 0.5452\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.3696 - val_loss: 0.5263\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3676 - val_loss: 0.5198\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3615 - val_loss: 0.5704\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3664 - val_loss: 0.5208\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3544 - val_loss: 0.5207\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3382 - val_loss: 0.5437\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3454 - val_loss: 0.5269\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3374 - val_loss: 0.5260\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_21-19:14:32] Training the entire fine-tuned model...\n",
      "[2022_04_21-19:14:47] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3602WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0966s vs `on_train_batch_end` time: 0.1248s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0966s vs `on_train_batch_end` time: 0.1248s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 617ms/step - loss: 0.3602 - val_loss: 0.5231\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 259ms/step - loss: 0.3509 - val_loss: 0.5137\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 259ms/step - loss: 0.3602 - val_loss: 0.5127\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3553 - val_loss: 0.5394\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 281ms/step - loss: 0.3417 - val_loss: 0.5105\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3478 - val_loss: 0.5115\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 280ms/step - loss: 0.3321 - val_loss: 0.5183\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 260ms/step - loss: 0.3350 - val_loss: 0.5248\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3236 - val_loss: 0.5214\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 259ms/step - loss: 0.3239 - val_loss: 0.5204\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3194 - val_loss: 0.5199\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_21-19:15:12] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-19:15:12] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:15:13] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3420WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0991s vs `on_train_batch_end` time: 0.1246s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0991s vs `on_train_batch_end` time: 0.1246s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 645ms/step - loss: 0.3420 - val_loss: 0.5109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-19:15:48] Test set: Filtered out 0 of 380 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3lxkksrh) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 37526... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▄▅▅▆▆▇▇█▁▂▂▃▃▄▄▅▅▆▆▁</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▂▂▂▁▁▁▁▂▁▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▅▂▃▂▂▁▄▁▁▂▂▂▂▁▁▂▁▁▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>4</td></tr><tr><td>best_val_loss</td><td>0.51054</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34195</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.51092</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">brisk-fire-10</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042b/runs/3lxkksrh\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042b/runs/3lxkksrh</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_191353-3lxkksrh/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3lxkksrh). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/29soegta\" target=\"_blank\">decent-wood-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-19:16:07] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:16:07] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:16:08] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 615ms/step - loss: 0.9058 - val_loss: 1.1613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.8160 - val_loss: 0.6446\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 181ms/step - loss: 0.7003 - val_loss: 0.6850\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5535 - val_loss: 0.4772\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5445 - val_loss: 0.5140\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4920 - val_loss: 0.4182\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5021 - val_loss: 0.4213\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4914 - val_loss: 0.4276\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4673 - val_loss: 0.4039\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4590 - val_loss: 0.4110\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4465 - val_loss: 0.4044\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4426 - val_loss: 0.3934\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4371 - val_loss: 0.4236\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4359 - val_loss: 0.3906\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4306 - val_loss: 0.3886\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4192 - val_loss: 0.4131\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4279 - val_loss: 0.3978\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4158 - val_loss: 0.3855\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4073 - val_loss: 0.4071\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4089 - val_loss: 0.3841\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4052 - val_loss: 0.3793\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4012 - val_loss: 0.4417\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.4108 - val_loss: 0.3919\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 186ms/step - loss: 0.4278 - val_loss: 0.3801\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3801 - val_loss: 0.3744\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3972 - val_loss: 0.3851\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3864 - val_loss: 0.3830\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.3867 - val_loss: 0.3737\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3698 - val_loss: 0.3841\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3862 - val_loss: 0.3809\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3811 - val_loss: 0.3736\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3792 - val_loss: 0.3872\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3749 - val_loss: 0.3785\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3830 - val_loss: 0.3759\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3755 - val_loss: 0.3772\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3860 - val_loss: 0.3791\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3792 - val_loss: 0.3808\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-19:16:47] Training the entire fine-tuned model...\n",
      "[2022_04_21-19:16:55] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 740ms/step - loss: 0.3847 - val_loss: 0.3881\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 0.3769 - val_loss: 0.3691\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.3632 - val_loss: 0.4072\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3710 - val_loss: 0.3733\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3699 - val_loss: 0.3873\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 341ms/step - loss: 0.3572 - val_loss: 0.3724\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.3568 - val_loss: 0.3673\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3539 - val_loss: 0.3747\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3361 - val_loss: 0.3709\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3520 - val_loss: 0.3677\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 317ms/step - loss: 0.3375 - val_loss: 0.3676\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3401 - val_loss: 0.3671\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3359 - val_loss: 0.3686\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.3334 - val_loss: 0.3682\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3512 - val_loss: 0.3676\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 336ms/step - loss: 0.3292 - val_loss: 0.3680\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 317ms/step - loss: 0.3377 - val_loss: 0.3683\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3427 - val_loss: 0.3681\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "[2022_04_21-19:17:31] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-19:17:31] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:17:41] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 771ms/step - loss: 0.3306 - val_loss: 0.3663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-19:18:14] Test set: Filtered out 0 of 395 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:29soegta) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 37757... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▃▃▃▃▄▄▄▁</td></tr><tr><td>loss</td><td>█▇▆▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▄▂▁▂▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.36631</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33063</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.36631</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">decent-wood-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/29soegta\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/29soegta</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_191551-29soegta/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:29soegta). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/ut6pw42e\" target=\"_blank\">good-water-11</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-19:18:35] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:18:35] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:18:35] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 499ms/step - loss: 0.7653 - val_loss: 0.6423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4785 - val_loss: 0.6650\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.4479 - val_loss: 0.6278\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4301 - val_loss: 0.5348\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3817 - val_loss: 0.5297\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3701 - val_loss: 0.5619\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3657 - val_loss: 0.5420\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3680 - val_loss: 0.5523\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.3653 - val_loss: 0.5480\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3739 - val_loss: 0.5702\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3638 - val_loss: 0.5197\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3562 - val_loss: 0.5254\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3509 - val_loss: 0.5248\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3498 - val_loss: 0.5259\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3437 - val_loss: 0.5330\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.3433 - val_loss: 0.5307\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3446 - val_loss: 0.5233\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-19:18:59] Training the entire fine-tuned model...\n",
      "[2022_04_21-19:19:11] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3552WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0977s vs `on_train_batch_end` time: 0.1229s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0977s vs `on_train_batch_end` time: 0.1229s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 630ms/step - loss: 0.3552 - val_loss: 0.5149\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 280ms/step - loss: 0.3441 - val_loss: 0.5197\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3331 - val_loss: 0.5116\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3350 - val_loss: 0.5239\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3338 - val_loss: 0.5186\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 281ms/step - loss: 0.3286 - val_loss: 0.5228\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3197 - val_loss: 0.5240\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 281ms/step - loss: 0.3119 - val_loss: 0.5143\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 259ms/step - loss: 0.3171 - val_loss: 0.5188\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_21-19:19:36] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-19:19:36] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:19:40] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3310WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1253s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1253s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 603ms/step - loss: 0.3310 - val_loss: 0.5162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-19:20:14] Test set: Filtered out 0 of 380 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ut6pw42e) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 38150... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██▁▁▂▂▃▃▄▄▅▁</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇█▆▂▂▃▂▃▃▄▁▂▂▂▂▂▂▁▁▁▂▁▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>2</td></tr><tr><td>best_val_loss</td><td>0.51157</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33095</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.51615</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">good-water-11</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042b/runs/ut6pw42e\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042b/runs/ut6pw42e</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_191817-ut6pw42e/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ut6pw42e). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/3jl6j5yd\" target=\"_blank\">good-sea-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-19:20:36] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:20:36] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:20:36] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 625ms/step - loss: 0.9033 - val_loss: 0.8952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.6571 - val_loss: 0.4786\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5604 - val_loss: 0.4377\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5468 - val_loss: 0.4684\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5002 - val_loss: 0.4191\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4800 - val_loss: 0.4381\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4671 - val_loss: 0.4036\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4578 - val_loss: 0.4022\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4455 - val_loss: 0.4175\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4505 - val_loss: 0.3987\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4496 - val_loss: 0.3939\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4396 - val_loss: 0.4030\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.4217 - val_loss: 0.4021\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4271 - val_loss: 0.3918\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4449 - val_loss: 0.3966\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4261 - val_loss: 0.3969\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4118 - val_loss: 0.4108\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4051 - val_loss: 0.3869\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3992 - val_loss: 0.3871\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4014 - val_loss: 0.3954\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3955 - val_loss: 0.3866\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3973 - val_loss: 0.3902\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3968 - val_loss: 0.3908\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3942 - val_loss: 0.3857\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3905 - val_loss: 0.3871\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3951 - val_loss: 0.3878\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3937 - val_loss: 0.3852\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3994 - val_loss: 0.3854\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3853 - val_loss: 0.3867\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3856 - val_loss: 0.3848\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3910 - val_loss: 0.3809\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.3777 - val_loss: 0.3887\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3976 - val_loss: 0.3807\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 187ms/step - loss: 0.3784 - val_loss: 0.3794\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3792 - val_loss: 0.3945\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3846 - val_loss: 0.3792\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3832 - val_loss: 0.3808\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3684 - val_loss: 0.3888\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3874 - val_loss: 0.3797\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3797 - val_loss: 0.3803\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3871 - val_loss: 0.3830\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3863 - val_loss: 0.3852\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-19:21:20] Training the entire fine-tuned model...\n",
      "[2022_04_21-19:21:35] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 774ms/step - loss: 0.5478 - val_loss: 0.5104\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.4408 - val_loss: 0.3842\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.4129 - val_loss: 0.3780\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 0.3791 - val_loss: 0.4128\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.3932 - val_loss: 0.3689\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3631 - val_loss: 0.3886\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3300 - val_loss: 0.3676\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3310 - val_loss: 0.3887\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3123 - val_loss: 0.3867\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.2957 - val_loss: 0.3838\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.2696 - val_loss: 0.3747\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.2708 - val_loss: 0.3798\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.2603 - val_loss: 0.3787\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_21-19:22:04] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-19:22:04] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:22:04] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 747ms/step - loss: 0.3279 - val_loss: 0.3699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-19:22:37] Test set: Filtered out 0 of 395 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3jl6j5yd) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 38394... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▂▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▄▃▂▂▂▂▁▁▁▂</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▂▂▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>6</td></tr><tr><td>best_val_loss</td><td>0.3676</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32786</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.36985</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">good-sea-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/3jl6j5yd\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/3jl6j5yd</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_192016-3jl6j5yd/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3jl6j5yd). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/2zcxl8he\" target=\"_blank\">lilac-energy-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-19:22:57] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:22:57] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:22:57] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 490ms/step - loss: 0.9351 - val_loss: 0.5483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.5703 - val_loss: 0.9161\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.5530 - val_loss: 0.5882\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.4273 - val_loss: 0.6496\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4146 - val_loss: 0.5360\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.4003 - val_loss: 0.5361\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3886 - val_loss: 0.5673\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3889 - val_loss: 0.5380\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3799 - val_loss: 0.5385\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3760 - val_loss: 0.5403\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3778 - val_loss: 0.5364\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-19:23:16] Training the entire fine-tuned model...\n",
      "[2022_04_21-19:23:36] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4187WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1254s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0985s vs `on_train_batch_end` time: 0.1254s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 623ms/step - loss: 0.4187 - val_loss: 0.7372\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.4542 - val_loss: 0.5414\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.4190 - val_loss: 0.5630\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3944 - val_loss: 0.5089\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 259ms/step - loss: 0.3819 - val_loss: 0.5074\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 283ms/step - loss: 0.3629 - val_loss: 0.5301\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3456 - val_loss: 0.5077\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 0.3371 - val_loss: 0.5079\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 259ms/step - loss: 0.3053 - val_loss: 0.5125\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.2973 - val_loss: 0.5164\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.2997 - val_loss: 0.5226\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_21-19:24:02] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-19:24:02] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:24:02] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3598WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1224s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0994s vs `on_train_batch_end` time: 0.1224s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 640ms/step - loss: 0.3598 - val_loss: 0.5070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-19:24:35] Test set: Filtered out 0 of 380 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2zcxl8he) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 38773... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▅▅▆▇▇█▁▂▂▃▄▅▅▆▇▇█▁</td></tr><tr><td>loss</td><td>█▄▄▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▁▁▁▁▂</td></tr><tr><td>lr</td><td>████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▂█▂▃▁▁▂▂▂▂▂▅▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.507</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3598</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.507</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">lilac-energy-12</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042b/runs/2zcxl8he\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042b/runs/2zcxl8he</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_192240-2zcxl8he/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2zcxl8he). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/r5hs63q5\" target=\"_blank\">deft-water-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-19:24:55] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:24:55] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:24:55] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 650ms/step - loss: 0.9337 - val_loss: 0.9286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 188ms/step - loss: 0.7416 - val_loss: 0.4972\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5931 - val_loss: 0.4595\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 188ms/step - loss: 0.5567 - val_loss: 0.4191\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5066 - val_loss: 0.4133\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4860 - val_loss: 0.4417\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4789 - val_loss: 0.4086\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4650 - val_loss: 0.4055\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4730 - val_loss: 0.4539\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4740 - val_loss: 0.4032\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4696 - val_loss: 0.4083\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.4345 - val_loss: 0.4120\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4338 - val_loss: 0.3934\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4388 - val_loss: 0.3899\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4185 - val_loss: 0.3943\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4136 - val_loss: 0.3937\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4066 - val_loss: 0.4002\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4149 - val_loss: 0.3830\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3896 - val_loss: 0.4084\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4029 - val_loss: 0.3808\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4067 - val_loss: 0.3796\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3907 - val_loss: 0.3769\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.4096 - val_loss: 0.4442\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4300 - val_loss: 0.3791\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.4002 - val_loss: 0.4014\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4255 - val_loss: 0.4482\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3901 - val_loss: 0.3732\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3972 - val_loss: 0.3741\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3860 - val_loss: 0.3960\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3774 - val_loss: 0.3738\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3857 - val_loss: 0.3775\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3821 - val_loss: 0.3827\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3699 - val_loss: 0.3846\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3837 - val_loss: 0.3784\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3847 - val_loss: 0.3770\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-19:25:33] Training the entire fine-tuned model...\n",
      "[2022_04_21-19:25:43] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 770ms/step - loss: 0.3739 - val_loss: 0.3752\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3695 - val_loss: 0.3898\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3877 - val_loss: 0.3866\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 337ms/step - loss: 0.3819 - val_loss: 0.3765\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3657 - val_loss: 0.3729\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 310ms/step - loss: 0.3749 - val_loss: 0.3754\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.3716 - val_loss: 0.3767\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3741 - val_loss: 0.3760\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3827 - val_loss: 0.3753\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.3687 - val_loss: 0.3754\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3749 - val_loss: 0.3753\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3652 - val_loss: 0.3751\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 316ms/step - loss: 0.3600 - val_loss: 0.3749\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_21-19:26:11] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-19:26:11] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:26:11] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 766ms/step - loss: 0.3623 - val_loss: 0.3731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-19:27:03] Test set: Filtered out 0 of 395 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:r5hs63q5) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 38989... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▆▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>4</td></tr><tr><td>best_val_loss</td><td>0.37291</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.36234</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.37311</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">deft-water-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/r5hs63q5\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/r5hs63q5</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_192437-r5hs63q5/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:r5hs63q5). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/qulm16ic\" target=\"_blank\">summer-rain-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_21-19:27:21] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:27:22] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:27:22] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 11s 512ms/step - loss: 0.8249 - val_loss: 0.8420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.5911 - val_loss: 0.8546\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 139ms/step - loss: 0.5147 - val_loss: 0.5322\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4844 - val_loss: 0.5420\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4116 - val_loss: 0.5841\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3876 - val_loss: 0.5628\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4171 - val_loss: 0.5781\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3786 - val_loss: 0.5242\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3687 - val_loss: 0.5299\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3615 - val_loss: 0.5351\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3621 - val_loss: 0.5204\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3647 - val_loss: 0.5351\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3579 - val_loss: 0.5180\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3722 - val_loss: 0.5174\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3556 - val_loss: 0.5554\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.3550 - val_loss: 0.5150\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3506 - val_loss: 0.5170\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.3526 - val_loss: 0.5325\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3443 - val_loss: 0.5134\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.3535 - val_loss: 0.5256\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3466 - val_loss: 0.5150\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.3519 - val_loss: 0.5288\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3491 - val_loss: 0.5295\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3430 - val_loss: 0.5217\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3407 - val_loss: 0.5176\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3418 - val_loss: 0.5158\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3516 - val_loss: 0.5141\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-19:27:56] Training the entire fine-tuned model...\n",
      "[2022_04_21-19:28:04] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3398WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0968s vs `on_train_batch_end` time: 0.1265s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0968s vs `on_train_batch_end` time: 0.1265s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 620ms/step - loss: 0.3398 - val_loss: 0.5175\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 282ms/step - loss: 0.3569 - val_loss: 0.5244\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 260ms/step - loss: 0.3408 - val_loss: 0.5211\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3433 - val_loss: 0.5142\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 282ms/step - loss: 0.3460 - val_loss: 0.5132\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 260ms/step - loss: 0.3382 - val_loss: 0.5156\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3508 - val_loss: 0.5193\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3425 - val_loss: 0.5246\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.3403 - val_loss: 0.5188\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3360 - val_loss: 0.5185\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3360 - val_loss: 0.5176\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 260ms/step - loss: 0.3360 - val_loss: 0.5172\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 260ms/step - loss: 0.3408 - val_loss: 0.5176\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_21-19:28:33] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-19:28:33] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:28:33] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3471WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0993s vs `on_train_batch_end` time: 0.1214s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0993s vs `on_train_batch_end` time: 0.1214s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 637ms/step - loss: 0.3471 - val_loss: 0.5132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-19:29:24] Test set: Filtered out 0 of 380 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:qulm16ic) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 39337... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▅▄▃▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>██▁▂▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>4</td></tr><tr><td>best_val_loss</td><td>0.51316</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3471</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.51319</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">summer-rain-13</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042b/runs/qulm16ic\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042b/runs/qulm16ic</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_192706-qulm16ic/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:qulm16ic). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/scrl4g8v\" target=\"_blank\">mild-hill-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-19:29:44] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:29:44] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:29:44] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 635ms/step - loss: 0.9151 - val_loss: 1.0253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.7241 - val_loss: 0.5701\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5681 - val_loss: 0.7302\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5828 - val_loss: 0.4797\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5438 - val_loss: 0.5251\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.5096 - val_loss: 0.4225\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4985 - val_loss: 0.4180\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4798 - val_loss: 0.4297\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4709 - val_loss: 0.4101\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4575 - val_loss: 0.4513\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4440 - val_loss: 0.3989\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4342 - val_loss: 0.4162\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4386 - val_loss: 0.3981\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4249 - val_loss: 0.3925\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4177 - val_loss: 0.4006\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4144 - val_loss: 0.3982\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4098 - val_loss: 0.3936\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4302 - val_loss: 0.3914\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4086 - val_loss: 0.4011\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4112 - val_loss: 0.3977\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3878 - val_loss: 0.3851\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4243 - val_loss: 0.4374\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4108 - val_loss: 0.3777\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4117 - val_loss: 0.3866\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4238 - val_loss: 0.4373\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4005 - val_loss: 0.3769\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4065 - val_loss: 0.3734\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3785 - val_loss: 0.4145\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3942 - val_loss: 0.3731\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3871 - val_loss: 0.3720\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3759 - val_loss: 0.4233\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3812 - val_loss: 0.3738\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3714 - val_loss: 0.3853\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3667 - val_loss: 0.3775\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3645 - val_loss: 0.3764\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3699 - val_loss: 0.3674\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3708 - val_loss: 0.3701\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3632 - val_loss: 0.3707\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3460 - val_loss: 0.3763\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3565 - val_loss: 0.3715\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.3538 - val_loss: 0.3695\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3546 - val_loss: 0.3703\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3584 - val_loss: 0.3768\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3753 - val_loss: 0.3754\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-19:30:29] Training the entire fine-tuned model...\n",
      "[2022_04_21-19:30:41] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 754ms/step - loss: 0.3785 - val_loss: 0.4138\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3601 - val_loss: 0.3688\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3680 - val_loss: 0.3785\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.3475 - val_loss: 0.3709\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3556 - val_loss: 0.3667\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 0.3431 - val_loss: 0.3802\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3388 - val_loss: 0.3594\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3347 - val_loss: 0.3795\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 335ms/step - loss: 0.3216 - val_loss: 0.3589\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3159 - val_loss: 0.3788\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3158 - val_loss: 0.3605\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3148 - val_loss: 0.3763\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3160 - val_loss: 0.3606\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3058 - val_loss: 0.3618\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.2942 - val_loss: 0.3717\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 0.3096 - val_loss: 0.3693\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.3009 - val_loss: 0.3611\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_21-19:31:15] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-19:31:15] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:31:15] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 774ms/step - loss: 0.3268 - val_loss: 0.3590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-19:31:50] Test set: Filtered out 0 of 395 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:scrl4g8v) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 39646... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇███▁▁▂▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▆▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▃▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_val_loss</td><td>0.35895</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32677</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.35905</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">mild-hill-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/scrl4g8v\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/scrl4g8v</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_192928-scrl4g8v/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:scrl4g8v). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/3bepydfv\" target=\"_blank\">quiet-sea-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_21-19:32:09] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:32:09] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:32:09] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 9s 503ms/step - loss: 0.9444 - val_loss: 0.6373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.6697 - val_loss: 0.9320\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.5613 - val_loss: 0.5464\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4569 - val_loss: 0.5693\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4312 - val_loss: 0.5875\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.4301 - val_loss: 0.5364\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3798 - val_loss: 0.5639\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3782 - val_loss: 0.5375\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3705 - val_loss: 0.5225\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3559 - val_loss: 0.5275\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3425 - val_loss: 0.5334\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3545 - val_loss: 0.5471\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4008 - val_loss: 0.6557\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3919 - val_loss: 0.5257\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3412 - val_loss: 0.5197\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 0.3484 - val_loss: 0.5303\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3400 - val_loss: 0.5422\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3430 - val_loss: 0.5159\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3343 - val_loss: 0.5208\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 126ms/step - loss: 0.3325 - val_loss: 0.5209\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.3306 - val_loss: 0.5154\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 124ms/step - loss: 0.3395 - val_loss: 0.5189\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 124ms/step - loss: 0.3288 - val_loss: 0.5126\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 0.3334 - val_loss: 0.5170\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 123ms/step - loss: 0.3356 - val_loss: 0.5165\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 125ms/step - loss: 0.3309 - val_loss: 0.5134\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 124ms/step - loss: 0.3302 - val_loss: 0.5097\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3310 - val_loss: 0.5114\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3289 - val_loss: 0.5087\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3364 - val_loss: 0.5076\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 151ms/step - loss: 0.3314 - val_loss: 0.5090\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.3188 - val_loss: 0.5074\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 0.3229 - val_loss: 0.5096\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3135 - val_loss: 0.5162\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.3266 - val_loss: 0.5090\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3355 - val_loss: 0.5114\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3191 - val_loss: 0.5140\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 0.3194 - val_loss: 0.5138\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3119 - val_loss: 0.5087\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3162 - val_loss: 0.5081\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-19:32:51] Training the entire fine-tuned model...\n",
      "[2022_04_21-19:32:59] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3158WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0999s vs `on_train_batch_end` time: 0.1242s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0999s vs `on_train_batch_end` time: 0.1242s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 623ms/step - loss: 0.3158 - val_loss: 0.5371\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3308 - val_loss: 0.5086\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 259ms/step - loss: 0.3258 - val_loss: 0.5087\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3148 - val_loss: 0.5087\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3121 - val_loss: 0.5233\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 259ms/step - loss: 0.3202 - val_loss: 0.5068\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3082 - val_loss: 0.5117\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 259ms/step - loss: 0.3002 - val_loss: 0.5074\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 282ms/step - loss: 0.3028 - val_loss: 0.5069\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 260ms/step - loss: 0.2953 - val_loss: 0.5195\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 259ms/step - loss: 0.2928 - val_loss: 0.5133\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 260ms/step - loss: 0.2944 - val_loss: 0.5084\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 0.2927 - val_loss: 0.5087\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 2s 259ms/step - loss: 0.2879 - val_loss: 0.5099\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_21-19:33:29] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-19:33:29] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:33:29] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3044WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0988s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0988s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 619ms/step - loss: 0.3044 - val_loss: 0.5091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-19:34:05] Test set: Filtered out 0 of 380 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3bepydfv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 40065... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃█▂▂▁▂▁▁▂▃▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.50683</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30438</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.50909</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">quiet-sea-14</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042b/runs/3bepydfv\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042b/runs/3bepydfv</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_193153-3bepydfv/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3bepydfv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/1xp38pda\" target=\"_blank\">cerulean-leaf-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-19:34:24] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:34:24] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:34:24] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 657ms/step - loss: 1.0670 - val_loss: 0.5943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 169ms/step - loss: 0.6912 - val_loss: 0.5151\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5786 - val_loss: 0.5809\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5349 - val_loss: 0.4404\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5141 - val_loss: 0.4840\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5272 - val_loss: 0.4167\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5120 - val_loss: 0.4179\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4892 - val_loss: 0.4439\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4714 - val_loss: 0.4069\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4441 - val_loss: 0.3996\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4394 - val_loss: 0.4218\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4365 - val_loss: 0.4042\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4314 - val_loss: 0.3947\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4254 - val_loss: 0.3918\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4314 - val_loss: 0.4236\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4195 - val_loss: 0.3884\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4192 - val_loss: 0.3965\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4056 - val_loss: 0.3970\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4092 - val_loss: 0.3952\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4081 - val_loss: 0.3835\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4097 - val_loss: 0.3822\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4081 - val_loss: 0.4384\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4012 - val_loss: 0.3809\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4071 - val_loss: 0.3761\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3901 - val_loss: 0.4327\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.3814 - val_loss: 0.3782\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3967 - val_loss: 0.3801\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3899 - val_loss: 0.3886\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3736 - val_loss: 0.3839\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 187ms/step - loss: 0.3883 - val_loss: 0.3735\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3808 - val_loss: 0.3861\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3748 - val_loss: 0.3774\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 168ms/step - loss: 0.3687 - val_loss: 0.3741\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3616 - val_loss: 0.3844\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.3661 - val_loss: 0.3819\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.3676 - val_loss: 0.3772\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.3632 - val_loss: 0.3772\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.3834 - val_loss: 0.3792\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-19:35:05] Training the entire fine-tuned model...\n",
      "[2022_04_21-19:35:21] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 752ms/step - loss: 0.3806 - val_loss: 0.3798\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 0.3564 - val_loss: 0.3909\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 0.3681 - val_loss: 0.3734\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3624 - val_loss: 0.3699\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3462 - val_loss: 0.3882\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 0.3508 - val_loss: 0.3682\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 316ms/step - loss: 0.3513 - val_loss: 0.3796\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3269 - val_loss: 0.3633\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3243 - val_loss: 0.3665\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3348 - val_loss: 0.3661\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3198 - val_loss: 0.3742\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.3089 - val_loss: 0.3618\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3007 - val_loss: 0.3626\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.2838 - val_loss: 0.3895\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 316ms/step - loss: 0.2953 - val_loss: 0.3694\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.2657 - val_loss: 0.3855\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.2636 - val_loss: 0.3691\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.2423 - val_loss: 0.3709\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.2511 - val_loss: 0.3751\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 2s 336ms/step - loss: 0.2463 - val_loss: 0.3749\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_21-19:36:00] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-19:36:00] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:36:00] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 764ms/step - loss: 0.3024 - val_loss: 0.3633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-19:36:37] Test set: Filtered out 0 of 395 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1xp38pda) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 40446... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▃▄▄▄▄▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂</td></tr><tr><td>lr</td><td>███████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆█▅▃▃▂▃▂▂▃▂▂▂▂▂▁▁▂▂▁▁▁▂▁▂▂▁▁▁▂▁▁▁▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>11</td></tr><tr><td>best_val_loss</td><td>0.36182</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3024</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.36333</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">cerulean-leaf-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/1xp38pda\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/1xp38pda</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_193407-1xp38pda/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1xp38pda). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/2x2xcopl\" target=\"_blank\">deft-brook-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_21-19:36:55] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:36:56] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:36:56] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 534ms/step - loss: 0.8745 - val_loss: 0.5757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 125ms/step - loss: 0.5332 - val_loss: 0.7066\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.4560 - val_loss: 0.5458\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4050 - val_loss: 0.5576\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4126 - val_loss: 0.6541\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3943 - val_loss: 0.5467\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3889 - val_loss: 0.5325\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3798 - val_loss: 0.6434\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3819 - val_loss: 0.5463\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3634 - val_loss: 0.5662\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3549 - val_loss: 0.5407\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3392 - val_loss: 0.5329\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3457 - val_loss: 0.5320\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3390 - val_loss: 0.5337\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3359 - val_loss: 0.5298\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3326 - val_loss: 0.5263\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3321 - val_loss: 0.5350\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3297 - val_loss: 0.5278\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3308 - val_loss: 0.5275\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3236 - val_loss: 0.5332\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3290 - val_loss: 0.5265\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3294 - val_loss: 0.5248\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3333 - val_loss: 0.5237\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.3419 - val_loss: 0.5216\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.3280 - val_loss: 0.5220\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3308 - val_loss: 0.5201\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3388 - val_loss: 0.5196\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3319 - val_loss: 0.5220\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3311 - val_loss: 0.5236\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3255 - val_loss: 0.5318\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3350 - val_loss: 0.5283\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3240 - val_loss: 0.5243\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3281 - val_loss: 0.5206\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 0.3212 - val_loss: 0.5182\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3333 - val_loss: 0.5180\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3286 - val_loss: 0.5184\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3205 - val_loss: 0.5191\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.3281 - val_loss: 0.5199\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3355 - val_loss: 0.5215\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.3299 - val_loss: 0.5215\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.3226 - val_loss: 0.5213\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3269 - val_loss: 0.5212\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3372 - val_loss: 0.5214\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "[2022_04_21-19:37:42] Training the entire fine-tuned model...\n",
      "[2022_04_21-19:37:51] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3467WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0995s vs `on_train_batch_end` time: 0.1245s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0995s vs `on_train_batch_end` time: 0.1245s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 625ms/step - loss: 0.3467 - val_loss: 0.5129\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3182 - val_loss: 0.5579\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3289 - val_loss: 0.5151\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 260ms/step - loss: 0.3193 - val_loss: 0.5182\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 282ms/step - loss: 0.3127 - val_loss: 0.5153\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3109 - val_loss: 0.5169\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 260ms/step - loss: 0.3023 - val_loss: 0.5210\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3032 - val_loss: 0.5174\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 259ms/step - loss: 0.2978 - val_loss: 0.5238\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_21-19:38:13] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-19:38:13] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:38:13] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3245WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0995s vs `on_train_batch_end` time: 0.1241s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0995s vs `on_train_batch_end` time: 0.1241s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 629ms/step - loss: 0.3245 - val_loss: 0.5131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-19:38:49] Test set: Filtered out 0 of 380 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2x2xcopl) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 40845... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███▁▁▁▂▂▂▁</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃█▂▆▂▂▂▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.51288</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32451</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.51309</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">deft-brook-15</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042b/runs/2x2xcopl\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042b/runs/2x2xcopl</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_193639-2x2xcopl/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2x2xcopl). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042a/runs/2y9xai1g\" target=\"_blank\">glad-night-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-19:39:08] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:39:08] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:39:08] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 632ms/step - loss: 0.9911 - val_loss: 0.9805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.7124 - val_loss: 0.6260\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.6559 - val_loss: 0.6948\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.6416 - val_loss: 0.4957\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5657 - val_loss: 0.5162\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4962 - val_loss: 0.4188\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4862 - val_loss: 0.4376\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4771 - val_loss: 0.4225\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 187ms/step - loss: 0.4650 - val_loss: 0.4110\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4594 - val_loss: 0.4025\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4584 - val_loss: 0.4416\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4397 - val_loss: 0.4012\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4451 - val_loss: 0.4305\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4345 - val_loss: 0.3903\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4275 - val_loss: 0.3966\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4176 - val_loss: 0.3956\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4181 - val_loss: 0.3935\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4237 - val_loss: 0.4025\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4138 - val_loss: 0.3872\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4067 - val_loss: 0.3952\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4013 - val_loss: 0.3941\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3896 - val_loss: 0.3844\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.4109 - val_loss: 0.3994\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4052 - val_loss: 0.3860\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.4062 - val_loss: 0.3844\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.4043 - val_loss: 0.3961\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3977 - val_loss: 0.3927\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3977 - val_loss: 0.3857\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4050 - val_loss: 0.3843\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3988 - val_loss: 0.3856\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3955 - val_loss: 0.3894\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.4016 - val_loss: 0.3904\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3993 - val_loss: 0.3883\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.3959 - val_loss: 0.3871\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.3976 - val_loss: 0.3868\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3869 - val_loss: 0.3865\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3976 - val_loss: 0.3862\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_21-19:39:48] Training the entire fine-tuned model...\n",
      "[2022_04_21-19:40:05] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 9s 749ms/step - loss: 0.6206 - val_loss: 0.5269\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.5471 - val_loss: 0.4924\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.4657 - val_loss: 0.4124\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.4254 - val_loss: 0.4614\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.4192 - val_loss: 0.3855\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.4083 - val_loss: 0.3994\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 0.3878 - val_loss: 0.3818\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 334ms/step - loss: 0.3646 - val_loss: 0.3790\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 0.3694 - val_loss: 0.3744\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 0.3397 - val_loss: 0.3763\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 0.3366 - val_loss: 0.3695\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.2883 - val_loss: 0.3771\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 337ms/step - loss: 0.3055 - val_loss: 0.3779\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.2794 - val_loss: 0.4350\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.2749 - val_loss: 0.3878\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 339ms/step - loss: 0.2328 - val_loss: 0.3872\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 316ms/step - loss: 0.2110 - val_loss: 0.3948\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.1993 - val_loss: 0.4019\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 318ms/step - loss: 0.1844 - val_loss: 0.4133\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_21-19:40:43] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-19:40:43] Training set: Filtered out 0 of 633 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:40:43] Validation set: Filtered out 0 of 263 (0.0%) records of lengths exceeding 510.\n",
      "5/5 [==============================] - 9s 755ms/step - loss: 0.3226 - val_loss: 0.3767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42a/2022_04_21_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-19:41:27] Test set: Filtered out 0 of 395 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2y9xai1g) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 41218... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇███▁▁▂▂▂▂▃▃▃▄▄▄▄▁</td></tr><tr><td>loss</td><td>█▆▅▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▅▄▃▃▃▂▃▂▂▂▂▁▁▂</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▅▃▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▂▂▁▁▁▁▁▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>10</td></tr><tr><td>best_val_loss</td><td>0.36954</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3226</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.37668</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">glad-night-16</strong>: <a href=\"https://wandb.ai/kvetab/Split%2042a/runs/2y9xai1g\" target=\"_blank\">https://wandb.ai/kvetab/Split%2042a/runs/2y9xai1g</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220421_193851-2y9xai1g/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2y9xai1g). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Split%2042b/runs/301ucykc\" target=\"_blank\">smooth-wildflower-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Split%2042b\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_21-19:41:45] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:41:45] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:41:45] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 10s 541ms/step - loss: 0.7939 - val_loss: 0.6254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.5133 - val_loss: 0.5438\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4208 - val_loss: 0.5513\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.3945 - val_loss: 0.5284\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3835 - val_loss: 0.5286\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3767 - val_loss: 0.5426\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3696 - val_loss: 0.5150\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.3588 - val_loss: 0.5207\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.3583 - val_loss: 0.5246\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3462 - val_loss: 0.5111\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3360 - val_loss: 0.5805\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3641 - val_loss: 0.5110\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3462 - val_loss: 0.5602\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3764 - val_loss: 0.5225\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3300 - val_loss: 0.4979\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3289 - val_loss: 0.4979\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3135 - val_loss: 0.5043\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3119 - val_loss: 0.4985\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3141 - val_loss: 0.4983\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3307 - val_loss: 0.4993\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3238 - val_loss: 0.5067\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3284 - val_loss: 0.5148\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3369 - val_loss: 0.5031\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_21-19:42:14] Training the entire fine-tuned model...\n",
      "[2022_04_21-19:42:43] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4554WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0971s vs `on_train_batch_end` time: 0.1249s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0971s vs `on_train_batch_end` time: 0.1249s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 612ms/step - loss: 0.4554 - val_loss: 0.5851\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3633 - val_loss: 0.4794\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.3412 - val_loss: 0.5070\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 2s 281ms/step - loss: 0.3334 - val_loss: 0.4875\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3259 - val_loss: 0.4736\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.3107 - val_loss: 0.4760\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.2979 - val_loss: 0.5014\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 2s 284ms/step - loss: 0.2898 - val_loss: 0.5430\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.2960 - val_loss: 0.5621\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 2s 260ms/step - loss: 0.3100 - val_loss: 0.4928\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.2773 - val_loss: 0.4900\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 0.2765 - val_loss: 0.5237\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.2458 - val_loss: 0.4963\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_21-19:43:11] Training on final epochs of sequence length 512...\n",
      "[2022_04_21-19:43:11] Training set: Filtered out 0 of 658 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_21-19:43:11] Validation set: Filtered out 0 of 253 (0.0%) records of lengths exceeding 510.\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3134WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0974s vs `on_train_batch_end` time: 0.1231s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0974s vs `on_train_batch_end` time: 0.1231s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 628ms/step - loss: 0.3134 - val_loss: 0.4747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/42b/2022_04_21_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_21-19:43:48] Test set: Filtered out 0 of 380 (0.0%) records of lengths exceeding 510.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "seq_len = 512\n",
    "tap_data = pd.read_csv(path.join(DATA_DIR, \"tap/tap_not_in_chen.csv\"))\n",
    "tap_data[\"seq\"] = tap_data[\"heavy\"] + tap_data[\"light\"]\n",
    "encoded_tap_set = encode_dataset(tap_data[\"seq\"], tap_data[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "        dataset_name = 'Tap set')\n",
    "tap_X, tap_Y, test_sample_weigths = encoded_tap_set\n",
    "for seed in [4, 18, 27, 36, 42]:\n",
    "    train = pd.read_csv(path.join(DATA_DIR, f\"chen/deduplicated/crossval/chen_{seed}_a.csv\"), index_col=0)\n",
    "    test = pd.read_csv(path.join(DATA_DIR, f\"chen/deduplicated/crossval/chen_{seed}_b.csv\"), index_col=0)\n",
    "    train[\"seq\"] = train[\"heavy\"] + train[\"light\"]\n",
    "    test[\"seq\"] = test[\"heavy\"] + test[\"light\"]\n",
    "    valid_1, test_1 = train_test_split(\n",
    "        test, test_size=0.6, random_state=VALID_SPLIT_SEED, stratify=test[\"Y\"]\n",
    "    )\n",
    "    valid_2, test_2 = train_test_split(\n",
    "        train, test_size=0.6, random_state=VALID_SPLIT_SEED, stratify=train[\"Y\"]\n",
    "    )\n",
    "    \n",
    "    for pat in patience:\n",
    "        for lr in learning_rate:\n",
    "            finetune_by_settings_and_data(pat, lr, f\"Split {seed}a\", train, test_1, valid_1, tap_X, tap_Y, f\"{seed}a\")\n",
    "            finetune_by_settings_and_data(pat, lr, f\"Split {seed}b\", test, test_2, valid_2, tap_X, tap_Y, f\"{seed}b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec00e839-cf6f-400c-9cba-3fc6e08f52b4",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56644fd0-ccff-434a-87a3-6a053d7f1416",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e816cc74-29d5-4c85-9fcc-1db0f5042900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_20-17:24:46] Test set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n"
     ]
    }
   ],
   "source": [
    "seq_len = 512\n",
    "encoded_test_set = encode_dataset(test_data[\"seq\"], test_data[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'Test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ebf1fb3-80bd-423c-a6af-2c51d82675ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X, test_Y, test_sample_weigths = encoded_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76d7099d-ada5-4464-8337-e64f1eaa7fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antibody_ID</th>\n",
       "      <th>heavy</th>\n",
       "      <th>light</th>\n",
       "      <th>CDR_length</th>\n",
       "      <th>PSH</th>\n",
       "      <th>PPC</th>\n",
       "      <th>PNC</th>\n",
       "      <th>SFvCSP</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abagovomab</td>\n",
       "      <td>QVKLQESGAELARPGASVKLSCKASGYTFTNYWMQWVKQRPGQGLD...</td>\n",
       "      <td>DIELTQSPASLSASVGETVTITCQASENIYSYLAWHQQKQGKSPQL...</td>\n",
       "      <td>46</td>\n",
       "      <td>129.7603</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abituzumab</td>\n",
       "      <td>QVQLQQSGGELAKPGASVKVSCKASGYTFSSFWMHWVRQAPGQGLE...</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCRASQDISNYLAWYQQKPGKAPKL...</td>\n",
       "      <td>45</td>\n",
       "      <td>115.9106</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0421</td>\n",
       "      <td>-3.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abrilumab</td>\n",
       "      <td>QVQLVQSGAEVKKPGASVKVSCKVSGYTLSDLSIHWVRQAPGKGLE...</td>\n",
       "      <td>DIQMTQSPSSVSASVGDRVTITCRASQGISSWLAWYQQKPGKAPKL...</td>\n",
       "      <td>45</td>\n",
       "      <td>109.6995</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8965</td>\n",
       "      <td>-4.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Actoxumab</td>\n",
       "      <td>QVQLVESGGGVVQPGRSLRLSCAASGFSFSNYGMHWVRQAPGKGLE...</td>\n",
       "      <td>DIQMTQSPSSVSASVGDRVTITCRASQGISSWLAWYQHKPGKAPKL...</td>\n",
       "      <td>49</td>\n",
       "      <td>112.6290</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.1247</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adalimumab</td>\n",
       "      <td>EVQLVESGGGLVQPGRSLRLSCAASGFTFDDYAMHWVRQAPGKGLE...</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCRASQGIRNYLAWYQQKPGKAPKL...</td>\n",
       "      <td>48</td>\n",
       "      <td>111.2512</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>1.1364</td>\n",
       "      <td>-19.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Antibody_ID                                              heavy  \\\n",
       "0  Abagovomab  QVKLQESGAELARPGASVKLSCKASGYTFTNYWMQWVKQRPGQGLD...   \n",
       "1  Abituzumab  QVQLQQSGGELAKPGASVKVSCKASGYTFSSFWMHWVRQAPGQGLE...   \n",
       "2   Abrilumab  QVQLVQSGAEVKKPGASVKVSCKVSGYTLSDLSIHWVRQAPGKGLE...   \n",
       "3   Actoxumab  QVQLVESGGGVVQPGRSLRLSCAASGFSFSNYGMHWVRQAPGKGLE...   \n",
       "4  Adalimumab  EVQLVESGGGLVQPGRSLRLSCAASGFTFDDYAMHWVRQAPGKGLE...   \n",
       "\n",
       "                                               light  CDR_length       PSH  \\\n",
       "0  DIELTQSPASLSASVGETVTITCQASENIYSYLAWHQQKQGKSPQL...          46  129.7603   \n",
       "1  DIQMTQSPSSLSASVGDRVTITCRASQDISNYLAWYQQKPGKAPKL...          45  115.9106   \n",
       "2  DIQMTQSPSSVSASVGDRVTITCRASQGISSWLAWYQQKPGKAPKL...          45  109.6995   \n",
       "3  DIQMTQSPSSVSASVGDRVTITCRASQGISSWLAWYQHKPGKAPKL...          49  112.6290   \n",
       "4  DIQMTQSPSSLSASVGDRVTITCRASQGIRNYLAWYQQKPGKAPKL...          48  111.2512   \n",
       "\n",
       "      PPC     PNC  SFvCSP  Y  \n",
       "0  0.0000  0.0000   16.32  1  \n",
       "1  0.0954  0.0421   -3.10  1  \n",
       "2  0.0000  0.8965   -4.00  1  \n",
       "3  0.0000  1.1247    3.10  1  \n",
       "4  0.0485  1.1364  -19.50  1  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tap_data = pd.read_csv(path.join(DATA_DIR, \"tap/TAP_data.csv\"))\n",
    "tap_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5cc5b7cb-1cc4-450d-a52d-2a027f00a1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tap_data[\"seq\"] = tap_data[\"heavy\"] +  tap_data[\"light\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b06ed7a5-5e02-48e8-8e69-6355c776ef1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_20-17:25:41] TAP set: Filtered out 0 of 241 (0.0%) records of lengths exceeding 510.\n"
     ]
    }
   ],
   "source": [
    "encoded_tap_set = encode_dataset(tap_data[\"seq\"], tap_data[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'TAP set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e96bc721-e479-400a-8d09-e0b6c9635a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tap_X, tap_Y, tap_sample_weigths = encoded_tap_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49e9d36a-63d9-45b6-bcf8-b66e75fd506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = path.join(DATA_DIR, \"protein_bert/2022_03_30__05.pkl\")\n",
    "model = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1959c14b-c779-4741-9a0a-b2a453fd1fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6486486486486486"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(test_X, batch_size=32)\n",
    "y_pred_classes = (y_pred >= 0.5)\n",
    "f1_score(test_Y, y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c95a893-f467-405f-81b6-59b0277c0a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34285714285714286"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(tap_X, batch_size=32)\n",
    "y_pred_classes = (y_pred >= 0.5)\n",
    "f1_score(tap_Y, y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "345121cc-6ccb-46c2-aef0-5b173263f069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23651452282157676"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(tap_Y, y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "566d023a-b6e2-48e9-b884-c48037cc54bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_20-17:24:54] Test set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "seq_len = 512\n",
    "encoded_test_set = encode_dataset(test_data[\"seq\"], test_data[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'Test set')\n",
    "test_X, test_Y, test_sample_weigths = encoded_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb44bed8-ab18-4ae3-b16e-8c90e38c027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_name, dir_name, x_test, y_test, x_tap, y_tap):\n",
    "    model_path = path.join(DATA_DIR, f\"protein_bert/{dir_name}/{model_name}\")\n",
    "    model = keras.models.load_model(model_path)\n",
    "    parts = model_name[11:].split(\"_\")\n",
    "    patience_stop = parts[0]\n",
    "    patience_lr = parts[1]\n",
    "    lr = parts[2]\n",
    "    \n",
    "    y_pred = model.predict(x_test, batch_size=32)\n",
    "    y_pred_classes = (y_pred >= 0.5)\n",
    "    #print(f\"Model {model_name}\")\n",
    "    #print(f\"Test F1: {f1}\")\n",
    "    metric_dict = {\n",
    "        \"f1\": float(metrics.f1_score(y_test, y_pred_classes)),\n",
    "        \"acc\": float(metrics.accuracy_score(y_test, y_pred_classes)),\n",
    "        \"mcc\": float(metrics.matthews_corrcoef(y_test, y_pred_classes)),\n",
    "        \"auc\": float(metrics.roc_auc_score(y_test, y_pred_classes)),\n",
    "        \"precision\": float(metrics.precision_score(y_test, y_pred_classes)),\n",
    "        \"recall\": float(metrics.recall_score(y_test, y_pred_classes))\n",
    "    }\n",
    "    filename = path.join(DATA_DIR, f\"evaluations/protein_bert/{dir_name}/{model_name}_preds.csv\")\n",
    "    str_preds = [str(int(pred)) for pred in y_pred_classes]\n",
    "    with open(filename, \"wt\") as f:\n",
    "        f.write(\",\".join(str_preds) + \"\\n\")\n",
    "        \n",
    "    filename_sum = path.join(DATA_DIR, f\"evaluations/protein_bert/{dir_name}/all.csv\")\n",
    "    line = [\"protein_bert\", patience_stop, patience_lr, lr, metric_dict[\"f1\"], metric_dict[\"mcc\"], metric_dict[\"acc\"],metric_dict[\"precision\"],metric_dict[\"recall\"],metric_dict[\"auc\"]]\n",
    "    with open(filename_sum, 'a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile, delimiter='\\t')\n",
    "        csvwriter.writerow(line)\n",
    "    \n",
    "    \n",
    "    y_pred = model.predict(x_tap, batch_size=32)\n",
    "    y_pred_classes = (y_pred >= 0.5)\n",
    "    acc = accuracy_score(y_tap, y_pred_classes)\n",
    "    metric_dict = {\n",
    "        \"f1\": float(metrics.f1_score(y_tap, y_pred_classes)),\n",
    "        \"acc\": float(metrics.accuracy_score(y_tap, y_pred_classes)),\n",
    "        \"mcc\": float(metrics.matthews_corrcoef(y_tap, y_pred_classes)),\n",
    "        \"auc\": float(metrics.roc_auc_score(y_tap, y_pred_classes)),\n",
    "        \"precision\": float(metrics.precision_score(y_tap, y_pred_classes)),\n",
    "        \"recall\": float(metrics.recall_score(y_tap, y_pred_classes))\n",
    "    }\n",
    "        \n",
    "    filename_sum = path.join(DATA_DIR, f\"evaluations/protein_bert/{dir_name}/tap.csv\")\n",
    "    line = [\"protein_bert\", patience_stop, patience_lr, lr, metric_dict[\"f1\"], metric_dict[\"mcc\"], metric_dict[\"acc\"],metric_dict[\"precision\"],metric_dict[\"recall\"],metric_dict[\"auc\"]]\n",
    "    with open(filename_sum, 'a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile, delimiter='\\t')\n",
    "        csvwriter.writerow(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ab02fee7-bc99-42f9-8a64-5d7c60f12e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_10-09:53:46] Test set: Filtered out 0 of 654 (0.0%) records of lengths exceeding 510.\n"
     ]
    }
   ],
   "source": [
    "seed = 4\n",
    "split = \"a\"\n",
    "train_data = pd.read_csv(path.join(DATA_DIR, f\"chen/deduplicated/crossval/chen_{seed}_a.csv\"), index_col=0)\n",
    "test_data = pd.read_csv(path.join(DATA_DIR, f\"chen/deduplicated/crossval/chen_{seed}_b.csv\"), index_col=0)\n",
    "train_data[\"seq\"] = train_data[\"heavy\"] + train_data[\"light\"]\n",
    "test_data[\"seq\"] = test_data[\"heavy\"] + test_data[\"light\"]\n",
    "\n",
    "encoded_test_set = encode_dataset(test_data[\"seq\"], test_data[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'Test set')\n",
    "test_X, test_Y, test_sample_weigths = encoded_test_set\n",
    "\n",
    "test_model(\"2022_04_09_3_3_0.0001\", \"4a\", test_X, test_Y, tap_X, tap_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7bddc1b4-70ec-40c7-82b7-86c006cf6412",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [4] # [ 18, 27, 36, 42] # 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b626b4f1-a24a-46ee-8621-3f22949b228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f201d8c-c65c-4ebb-9b78-6f2ff4bf4991",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_20-19:39:32] Test set: Filtered out 0 of 733 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_20-19:39:32] Train set: Filtered out 0 of 585 (0.0%) records of lengths exceeding 510.\n"
     ]
    }
   ],
   "source": [
    "for seed in seeds:\n",
    "    train_data = pd.read_csv(path.join(DATA_DIR, f\"chen/deduplicated/crossval/chen_{seed}_a.csv\"), index_col=0)\n",
    "    test_data = pd.read_csv(path.join(DATA_DIR, f\"chen/deduplicated/crossval/chen_{seed}_b.csv\"), index_col=0)\n",
    "    train_data[\"seq\"] = train_data[\"heavy\"] + train_data[\"light\"]\n",
    "    test_data[\"seq\"] = test_data[\"heavy\"] + test_data[\"light\"]\n",
    "    encoded_test_set = encode_dataset(test_data[\"seq\"], test_data[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'Test set')\n",
    "    test_X, test_Y, test_sample_weigths = encoded_test_set\n",
    "    encoded_train_set = encode_dataset(train_data[\"seq\"], train_data[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'Train set')\n",
    "    train_X, train_Y, test_sample_weigths = encoded_train_set\n",
    "    \n",
    "    for split in [\"a\", \"b\"]:\n",
    "        for model in os.listdir(path.join(DATA_DIR, f\"protein_bert/{seed}{split}\")):\n",
    "            if model.startswith(\"2022_04_20\"):\n",
    "                if split == \"a\":\n",
    "                    test_model(model, f\"{seed}{split}\", test_X, test_Y, tap_X, tap_Y)\n",
    "                else:\n",
    "                    test_model(model, f\"{seed}{split}\", train_X, train_Y, tap_X, tap_Y)\n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc2be9b-108a-4163-a579-07ab6ca4e2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76574dfa-f777-4b64-a8f5-08df816879ab",
   "metadata": {},
   "source": [
    "# Full train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71fbac67-086d-41da-8141-aef3c17a6dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "760d1c1a-68dd-42c8-958b-0cbc2bbb4796",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_train.csv\"), index_col=0)\n",
    "test_data = pd.read_csv(path.join(DATA_DIR, \"chen/deduplicated/chen_test.csv\"), index_col=0)\n",
    "train_data[\"seq\"] = train_data[\"heavy\"] + train_data[\"light\"]\n",
    "test_data[\"seq\"] = test_data[\"heavy\"] + test_data[\"light\"]\n",
    "valid_data, test_data = train_test_split(test_data, test_size=0.5, random_state=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a42ee61a-9dde-446f-8e80-040956e551b8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:rc09kp9o) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 3994... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▄▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▂▂▂▂▁▂▂▁▁▂▁▂▁▁▁▁▁▁▁▂▁▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>13</td></tr><tr><td>best_val_loss</td><td>0.45486</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32702</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.46158</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">rosy-cloud-6</strong>: <a href=\"https://wandb.ai/kvetab/Easter/runs/rc09kp9o\" target=\"_blank\">https://wandb.ai/kvetab/Easter/runs/rc09kp9o</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_143331-rc09kp9o/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:rc09kp9o). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/2icd7p5e\" target=\"_blank\">pious-voice-1</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_15-14:37:59] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:37:59] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:37:59] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 286ms/step - loss: 0.8872 - val_loss: 0.7528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.5899 - val_loss: 0.5730\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.5244 - val_loss: 0.5120\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4718 - val_loss: 0.4757\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4801 - val_loss: 0.4985\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4512 - val_loss: 0.4987\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4273 - val_loss: 0.4444\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4183 - val_loss: 0.4401\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4154 - val_loss: 0.4394\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4138 - val_loss: 0.4271\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4076 - val_loss: 0.4960\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4425 - val_loss: 0.4456\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4032 - val_loss: 0.4281\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_15-14:38:26] Training the entire fine-tuned model...\n",
      "[2022_04_15-14:38:34] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.4165WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1160s vs `on_train_batch_end` time: 0.1431s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1160s vs `on_train_batch_end` time: 0.1431s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 427ms/step - loss: 0.4031 - val_loss: 0.4263\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.4097 - val_loss: 0.4271\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.4015 - val_loss: 0.4251\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.4034 - val_loss: 0.4243\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.4046 - val_loss: 0.4236\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.4002 - val_loss: 0.4234\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3964 - val_loss: 0.4219\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.4003 - val_loss: 0.4215\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3964 - val_loss: 0.4216\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3974 - val_loss: 0.4200\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3905 - val_loss: 0.4197\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3886 - val_loss: 0.4183\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3860 - val_loss: 0.4182\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3920 - val_loss: 0.4167\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3899 - val_loss: 0.4156\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3962 - val_loss: 0.4159\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3902 - val_loss: 0.4143\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3896 - val_loss: 0.4132\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3836 - val_loss: 0.4128\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3864 - val_loss: 0.4123\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3838 - val_loss: 0.4114\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3764 - val_loss: 0.4114\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3784 - val_loss: 0.4100\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3802 - val_loss: 0.4105\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3739 - val_loss: 0.4085\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3807 - val_loss: 0.4078\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3725 - val_loss: 0.4074\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3730 - val_loss: 0.4068\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3686 - val_loss: 0.4061\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 3s 262ms/step - loss: 0.3682 - val_loss: 0.4064\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3684 - val_loss: 0.4051\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3657 - val_loss: 0.4041\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3686 - val_loss: 0.4035\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3720 - val_loss: 0.4035\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3621 - val_loss: 0.4029\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3628 - val_loss: 0.4025\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 3s 263ms/step - loss: 0.3566 - val_loss: 0.4025\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3629 - val_loss: 0.4004\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3578 - val_loss: 0.4002\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3503 - val_loss: 0.3997\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3554 - val_loss: 0.3982\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3515 - val_loss: 0.3979\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3504 - val_loss: 0.3971\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3478 - val_loss: 0.3980\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3442 - val_loss: 0.3976\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3556 - val_loss: 0.4003\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "[2022_04_15-14:40:55] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-14:40:55] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:40:55] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3607WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1116s vs `on_train_batch_end` time: 0.1497s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1116s vs `on_train_batch_end` time: 0.1497s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 442ms/step - loss: 0.3514 - val_loss: 0.3973\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  20  6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36363636363636365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_3_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_3_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2icd7p5e) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 4331... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▃▂▂▂▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>42</td></tr><tr><td>best_val_loss</td><td>0.39712</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35142</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.39726</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">pious-voice-1</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/2icd7p5e\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/2icd7p5e</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_143743-2icd7p5e/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2icd7p5e). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/12ryvo2g\" target=\"_blank\">decent-field-2</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_15-14:41:49] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:41:49] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:41:49] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 292ms/step - loss: 0.7868 - val_loss: 0.5233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.5189 - val_loss: 0.5264\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.4777 - val_loss: 0.4837\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4511 - val_loss: 0.4648\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.4381 - val_loss: 0.4581\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4323 - val_loss: 0.4499\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4276 - val_loss: 0.4476\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4202 - val_loss: 0.4336\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4110 - val_loss: 0.4281\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4066 - val_loss: 0.4312\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4042 - val_loss: 0.4202\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4117 - val_loss: 0.4244\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4000 - val_loss: 0.4191\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4057 - val_loss: 0.4178\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3917 - val_loss: 0.4095\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4015 - val_loss: 0.4206\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4042 - val_loss: 0.4054\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3935 - val_loss: 0.4044\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3775 - val_loss: 0.4023\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3899 - val_loss: 0.4222\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3847 - val_loss: 0.4142\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3762 - val_loss: 0.4094\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_15-14:42:27] Training the entire fine-tuned model...\n",
      "[2022_04_15-14:42:36] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3674WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1171s vs `on_train_batch_end` time: 0.1435s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1171s vs `on_train_batch_end` time: 0.1435s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 424ms/step - loss: 0.3748 - val_loss: 0.3993\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3819 - val_loss: 0.3959\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3636 - val_loss: 0.4047\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3652 - val_loss: 0.4182\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3663 - val_loss: 0.3890\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 262ms/step - loss: 0.3703 - val_loss: 0.3905\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3569 - val_loss: 0.3861\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3584 - val_loss: 0.3878\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3531 - val_loss: 0.3829\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3461 - val_loss: 0.3797\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3426 - val_loss: 0.3802\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3333 - val_loss: 0.3733\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3299 - val_loss: 0.3742\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3307 - val_loss: 0.4043\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3361 - val_loss: 0.3978\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "[2022_04_15-14:43:28] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-14:43:28] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:43:28] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3072WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1135s vs `on_train_batch_end` time: 0.1459s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1135s vs `on_train_batch_end` time: 0.1459s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 427ms/step - loss: 0.3293 - val_loss: 0.3771\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  20  6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36363636363636365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_3_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_3_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:12ryvo2g) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 4749... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>██▆▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▃▃▃▂▂▂▃▂▂▂▂▁▁▁▁▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>11</td></tr><tr><td>best_val_loss</td><td>0.37327</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.32934</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.37711</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">decent-field-2</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/12ryvo2g\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/12ryvo2g</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_144133-12ryvo2g/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:12ryvo2g). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/2kcqor19\" target=\"_blank\">skilled-music-3</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_15-14:44:22] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:44:23] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:44:23] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 11s 410ms/step - loss: 0.8471 - val_loss: 0.8598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 112ms/step - loss: 0.6624 - val_loss: 0.7048\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.5235 - val_loss: 0.5406\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4893 - val_loss: 0.4924\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.5007 - val_loss: 0.4811\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4561 - val_loss: 0.4555\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4313 - val_loss: 0.4444\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4252 - val_loss: 0.4371\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4193 - val_loss: 0.4361\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4121 - val_loss: 0.4511\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.4223 - val_loss: 0.4237\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4149 - val_loss: 0.4291\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4070 - val_loss: 0.4199\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3938 - val_loss: 0.4149\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3858 - val_loss: 0.4216\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4110 - val_loss: 0.4349\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4049 - val_loss: 0.4332\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_15-14:44:55] Training the entire fine-tuned model...\n",
      "[2022_04_15-14:45:03] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3821WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1147s vs `on_train_batch_end` time: 0.1436s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1147s vs `on_train_batch_end` time: 0.1436s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 436ms/step - loss: 0.3912 - val_loss: 0.4092\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3843 - val_loss: 0.4085\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3894 - val_loss: 0.4173\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3941 - val_loss: 0.3999\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3899 - val_loss: 0.4064\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3802 - val_loss: 0.3943\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3711 - val_loss: 0.3874\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3669 - val_loss: 0.3930\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3479 - val_loss: 0.3836\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3405 - val_loss: 0.3854\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3461 - val_loss: 0.3908\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3344 - val_loss: 0.4013\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "[2022_04_15-14:45:46] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-14:45:46] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:45:50] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3296WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1143s vs `on_train_batch_end` time: 0.1446s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1143s vs `on_train_batch_end` time: 0.1446s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 426ms/step - loss: 0.3517 - val_loss: 0.3826\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  20  6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36363636363636365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_3_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_3_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2kcqor19) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 5035... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██▁▁▂▂▃▃▄▄▅▅▅▆▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▃▃▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.38264</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35167</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.38264</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">skilled-music-3</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/2kcqor19\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/2kcqor19</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_144406-2kcqor19/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2kcqor19). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/3og46y6h\" target=\"_blank\">dark-energy-4</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_15-14:46:45] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:46:45] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:46:45] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 11s 292ms/step - loss: 0.7612 - val_loss: 0.6272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.5468 - val_loss: 0.4896\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4684 - val_loss: 0.4755\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4512 - val_loss: 0.4935\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4544 - val_loss: 0.4561\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4271 - val_loss: 0.4412\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4230 - val_loss: 0.4592\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4347 - val_loss: 0.4376\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4187 - val_loss: 0.4279\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4091 - val_loss: 0.4224\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.4093 - val_loss: 0.4206\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4192 - val_loss: 0.4737\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4227 - val_loss: 0.4522\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.4240 - val_loss: 0.4149\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3966 - val_loss: 0.4456\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4198 - val_loss: 0.4099\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3848 - val_loss: 0.4184\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3835 - val_loss: 0.4026\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4016 - val_loss: 0.4089\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3846 - val_loss: 0.4051\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3662 - val_loss: 0.4075\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "[2022_04_15-14:47:22] Training the entire fine-tuned model...\n",
      "[2022_04_15-14:47:31] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.7090WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1136s vs `on_train_batch_end` time: 0.1461s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1136s vs `on_train_batch_end` time: 0.1461s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 431ms/step - loss: 0.6239 - val_loss: 0.4103\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.4389 - val_loss: 0.4141\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.4006 - val_loss: 0.4025\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3978 - val_loss: 0.3925\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 261ms/step - loss: 0.3651 - val_loss: 0.3878\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3655 - val_loss: 0.3916\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3615 - val_loss: 0.3831\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3431 - val_loss: 0.3794\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3294 - val_loss: 0.4244\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3166 - val_loss: 0.3822\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2888 - val_loss: 0.3879\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "[2022_04_15-14:48:10] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-14:48:10] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:48:11] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3102WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1125s vs `on_train_batch_end` time: 0.1470s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1125s vs `on_train_batch_end` time: 0.1470s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 423ms/step - loss: 0.3266 - val_loss: 0.3858\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  21  5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_3_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_3_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3og46y6h) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 5283... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▄▄▅▁</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▃▂▂▆▃▃▃▂▂▂▂▂▁▁▂</td></tr><tr><td>lr</td><td>█████████████████████▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▄▄▃▃▃▃▂▂▂▄▃▂▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.37945</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3266</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.38577</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dark-energy-4</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/3og46y6h\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/3og46y6h</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_144628-3og46y6h/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3og46y6h). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/1td139wd\" target=\"_blank\">fast-donkey-5</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_15-14:49:05] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:49:05] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:49:05] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 289ms/step - loss: 0.7580 - val_loss: 0.6996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.5862 - val_loss: 0.5701\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4914 - val_loss: 0.4816\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4537 - val_loss: 0.4963\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4644 - val_loss: 0.4558\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4447 - val_loss: 0.4844\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4242 - val_loss: 0.4440\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4100 - val_loss: 0.4411\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.4419 - val_loss: 0.4274\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4130 - val_loss: 0.4249\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4055 - val_loss: 0.4190\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4090 - val_loss: 0.4465\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4065 - val_loss: 0.4108\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4002 - val_loss: 0.4094\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3985 - val_loss: 0.4652\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3937 - val_loss: 0.4074\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3829 - val_loss: 0.4118\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3887 - val_loss: 0.4039\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3833 - val_loss: 0.4063\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.4028 - val_loss: 0.4312\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 112ms/step - loss: 0.4020 - val_loss: 0.3983\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3887 - val_loss: 0.3978\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3833 - val_loss: 0.4020\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3829 - val_loss: 0.3955\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3772 - val_loss: 0.3896\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3676 - val_loss: 0.3908\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3584 - val_loss: 0.3864\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3682 - val_loss: 0.4442\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3837 - val_loss: 0.4133\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3937 - val_loss: 0.4148\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3660 - val_loss: 0.3907\n",
      "[2022_04_15-14:49:54] Training the entire fine-tuned model...\n",
      "[2022_04_15-14:50:03] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3714WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1144s vs `on_train_batch_end` time: 0.1462s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1144s vs `on_train_batch_end` time: 0.1462s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 419ms/step - loss: 0.3625 - val_loss: 0.3931\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3573 - val_loss: 0.3862\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3661 - val_loss: 0.3893\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3645 - val_loss: 0.3878\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3721 - val_loss: 0.3858\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3614 - val_loss: 0.3873\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3549 - val_loss: 0.3872\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3484 - val_loss: 0.3872\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3561 - val_loss: 0.3863\n",
      "[2022_04_15-14:50:37] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-14:50:37] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:50:37] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3617WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1146s vs `on_train_batch_end` time: 0.1494s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1146s vs `on_train_batch_end` time: 0.1494s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 424ms/step - loss: 0.3587 - val_loss: 0.3858\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  20  6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36363636363636365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_4_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_4_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1td139wd) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 5544... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████████████████▃▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▃▃▃▂▂▂▂▂▂▂▂▃▁▂▁▁▂▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.3858</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35865</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.3858</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fast-donkey-5</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/1td139wd\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/1td139wd</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_144849-1td139wd/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1td139wd). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/2q37nhq7\" target=\"_blank\">gentle-yogurt-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_15-14:51:33] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:51:33] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:51:33] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 11s 408ms/step - loss: 0.8012 - val_loss: 0.6419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.5295 - val_loss: 0.4864\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4758 - val_loss: 0.4709\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4437 - val_loss: 0.4611\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4418 - val_loss: 0.4907\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4542 - val_loss: 0.4529\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4329 - val_loss: 0.4376\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4194 - val_loss: 0.4333\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4152 - val_loss: 0.4509\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4057 - val_loss: 0.4198\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4258 - val_loss: 0.4440\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4461 - val_loss: 0.4147\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4121 - val_loss: 0.4151\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4354 - val_loss: 0.4109\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3995 - val_loss: 0.4084\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3930 - val_loss: 0.4049\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3941 - val_loss: 0.4222\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3878 - val_loss: 0.4083\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3848 - val_loss: 0.4101\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3743 - val_loss: 0.4068\n",
      "[2022_04_15-14:52:09] Training the entire fine-tuned model...\n",
      "[2022_04_15-14:52:30] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3754WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1142s vs `on_train_batch_end` time: 0.1456s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1142s vs `on_train_batch_end` time: 0.1456s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 432ms/step - loss: 0.3842 - val_loss: 0.4167\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3932 - val_loss: 0.4001\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3817 - val_loss: 0.4084\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3627 - val_loss: 0.3947\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3709 - val_loss: 0.4067\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3854 - val_loss: 0.3902\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3754 - val_loss: 0.3991\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3699 - val_loss: 0.3932\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3685 - val_loss: 0.3862\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3519 - val_loss: 0.3845\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 262ms/step - loss: 0.3502 - val_loss: 0.3916\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3578 - val_loss: 0.4095\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3513 - val_loss: 0.3977\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3365 - val_loss: 0.3794\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3315 - val_loss: 0.3798\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3367 - val_loss: 0.3858\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3336 - val_loss: 0.3798\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3404 - val_loss: 0.3804\n",
      "[2022_04_15-14:53:30] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-14:53:30] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:53:30] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3405WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1152s vs `on_train_batch_end` time: 0.1471s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1152s vs `on_train_batch_end` time: 0.1471s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 434ms/step - loss: 0.3325 - val_loss: 0.3797\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  19  7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4117647058823529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_4_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_4_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2q37nhq7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 5863... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▃▃▂▂▂▂▃▂▃▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▃▄▃▃▂▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>13</td></tr><tr><td>best_val_loss</td><td>0.37944</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33247</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.37972</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">gentle-yogurt-6</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/2q37nhq7\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/2q37nhq7</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_145115-2q37nhq7/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2q37nhq7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/1fg0uvtb\" target=\"_blank\">elated-jazz-7</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_15-14:54:25] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:54:25] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:54:25] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 11s 294ms/step - loss: 0.8363 - val_loss: 0.6832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.5742 - val_loss: 0.5053\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.4685 - val_loss: 0.4820\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.4608 - val_loss: 0.4775\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 104ms/step - loss: 0.4610 - val_loss: 0.5137\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4616 - val_loss: 0.4510\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4408 - val_loss: 0.4732\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4390 - val_loss: 0.4525\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4555 - val_loss: 0.4746\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4265 - val_loss: 0.4324\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4069 - val_loss: 0.4350\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4094 - val_loss: 0.4311\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4021 - val_loss: 0.4284\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4025 - val_loss: 0.4354\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4015 - val_loss: 0.4263\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3965 - val_loss: 0.4304\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3982 - val_loss: 0.4239\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3930 - val_loss: 0.4227\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3922 - val_loss: 0.4219\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 112ms/step - loss: 0.4027 - val_loss: 0.4210\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4029 - val_loss: 0.4287\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3998 - val_loss: 0.4572\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4059 - val_loss: 0.4188\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3888 - val_loss: 0.4178\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3832 - val_loss: 0.4164\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3875 - val_loss: 0.4171\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3898 - val_loss: 0.4309\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3962 - val_loss: 0.4142\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3848 - val_loss: 0.4123\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3848 - val_loss: 0.4111\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3804 - val_loss: 0.4122\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3893 - val_loss: 0.4168\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3906 - val_loss: 0.4121\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3841 - val_loss: 0.4100\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3844 - val_loss: 0.4089\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3821 - val_loss: 0.4107\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3762 - val_loss: 0.4089\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3816 - val_loss: 0.4100\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 112ms/step - loss: 0.3834 - val_loss: 0.4094\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3801 - val_loss: 0.4088\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3735 - val_loss: 0.4087\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3764 - val_loss: 0.4087\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3836 - val_loss: 0.4087\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3728 - val_loss: 0.4089\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3877 - val_loss: 0.4088\n",
      "[2022_04_15-14:55:32] Training the entire fine-tuned model...\n",
      "[2022_04_15-14:56:04] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3976WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1139s vs `on_train_batch_end` time: 0.1454s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1139s vs `on_train_batch_end` time: 0.1454s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 435ms/step - loss: 0.3842 - val_loss: 0.4042\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3736 - val_loss: 0.4150\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3841 - val_loss: 0.3934\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3797 - val_loss: 0.3890\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3703 - val_loss: 0.3852\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3578 - val_loss: 0.3825\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3522 - val_loss: 0.3793\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3520 - val_loss: 0.3773\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3476 - val_loss: 0.3825\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3215 - val_loss: 0.3952\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3173 - val_loss: 0.3780\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3182 - val_loss: 0.3917\n",
      "[2022_04_15-14:56:47] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-14:56:47] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:56:52] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3347WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1144s vs `on_train_batch_end` time: 0.1471s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1144s vs `on_train_batch_end` time: 0.1471s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 428ms/step - loss: 0.3369 - val_loss: 0.3775\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  19  7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4117647058823529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_4_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_4_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1fg0uvtb) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 6154... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███▁▁▁▂▂▂▂▃▁</td></tr><tr><td>loss</td><td>█▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▄▃▃▃▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.37734</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33692</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.37746</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">elated-jazz-7</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/1fg0uvtb\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/1fg0uvtb</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_145408-1fg0uvtb/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1fg0uvtb). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/3aeeekdw\" target=\"_blank\">desert-brook-8</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_15-14:57:46] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:57:46] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:57:46] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 287ms/step - loss: 0.6981 - val_loss: 0.5152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.5291 - val_loss: 0.5591\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4891 - val_loss: 0.4721\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4612 - val_loss: 0.4988\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4604 - val_loss: 0.5260\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4563 - val_loss: 0.4502\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4317 - val_loss: 0.4608\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.4206 - val_loss: 0.4394\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4160 - val_loss: 0.4281\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4078 - val_loss: 0.4230\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4087 - val_loss: 0.4335\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3994 - val_loss: 0.4190\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3960 - val_loss: 0.4178\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.4081 - val_loss: 0.4096\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3842 - val_loss: 0.4086\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3940 - val_loss: 0.4934\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4363 - val_loss: 0.5024\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4307 - val_loss: 0.4430\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3994 - val_loss: 0.4037\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3824 - val_loss: 0.4130\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3767 - val_loss: 0.4019\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3748 - val_loss: 0.4021\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3836 - val_loss: 0.4061\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3721 - val_loss: 0.4047\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3878 - val_loss: 0.4054\n",
      "[2022_04_15-14:58:28] Training the entire fine-tuned model...\n",
      "[2022_04_15-14:58:45] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.6450WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1151s vs `on_train_batch_end` time: 0.1471s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1151s vs `on_train_batch_end` time: 0.1471s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 434ms/step - loss: 0.5680 - val_loss: 0.4705\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.4341 - val_loss: 0.4418\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.4188 - val_loss: 0.3979\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3815 - val_loss: 0.3914\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3696 - val_loss: 0.3928\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3473 - val_loss: 0.3913\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3549 - val_loss: 0.3830\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3263 - val_loss: 0.3739\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3415 - val_loss: 0.3704\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3130 - val_loss: 0.4170\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3102 - val_loss: 0.4432\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.2882 - val_loss: 0.4251\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.2467 - val_loss: 0.3991\n",
      "[2022_04_15-14:59:30] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-14:59:30] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:59:31] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.2886WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1122s vs `on_train_batch_end` time: 0.1484s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1122s vs `on_train_batch_end` time: 0.1484s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 427ms/step - loss: 0.3016 - val_loss: 0.3653\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  91  3\n",
       "1  17  9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47368421052631576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_4_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_4_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3aeeekdw) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 6570... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▄▄▄▅▁</td></tr><tr><td>loss</td><td>█▅▅▄▄▄▄▄▄▃▄▃▃▄▃▃▄▄▃▃▃▃▃▃▃▆▄▄▃▃▃▃▂▂▂▂▂▁▂</td></tr><tr><td>lr</td><td>██████████████████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▆█▅▆▇▄▄▄▃▃▃▃▃▃▃▆▆▄▂▃▂▂▂▂▂▅▄▂▂▂▂▂▁▁▃▄▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.36531</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30157</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.36531</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">desert-brook-8</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/3aeeekdw\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/3aeeekdw</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_145730-3aeeekdw/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3aeeekdw). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/2a15f8bb\" target=\"_blank\">warm-salad-10</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_15-15:00:24] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:00:24] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:00:24] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 295ms/step - loss: 0.8954 - val_loss: 0.8255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.6537 - val_loss: 0.6861\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.5594 - val_loss: 0.5544\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4767 - val_loss: 0.4685\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4627 - val_loss: 0.5020\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4529 - val_loss: 0.4568\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.4414 - val_loss: 0.4820\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4399 - val_loss: 0.4590\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.4370 - val_loss: 0.4494\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4278 - val_loss: 0.4618\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4326 - val_loss: 0.4626\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4117 - val_loss: 0.4239\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.4019 - val_loss: 0.4277\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3991 - val_loss: 0.4339\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3945 - val_loss: 0.4172\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3998 - val_loss: 0.4168\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3989 - val_loss: 0.4120\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3864 - val_loss: 0.4110\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3819 - val_loss: 0.4213\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3894 - val_loss: 0.4083\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3920 - val_loss: 0.4560\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3937 - val_loss: 0.4113\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.3926 - val_loss: 0.4014\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.3939 - val_loss: 0.4356\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4070 - val_loss: 0.4055\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3914 - val_loss: 0.3999\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3701 - val_loss: 0.3977\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3750 - val_loss: 0.4020\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3842 - val_loss: 0.3974\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3688 - val_loss: 0.4273\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3804 - val_loss: 0.3959\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3725 - val_loss: 0.4072\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3642 - val_loss: 0.5025\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3970 - val_loss: 0.3868\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3606 - val_loss: 0.3949\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3676 - val_loss: 0.3964\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3539 - val_loss: 0.4023\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3633 - val_loss: 0.3867\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3569 - val_loss: 0.3872\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3648 - val_loss: 0.4052\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.3540 - val_loss: 0.3861\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 1s 103ms/step - loss: 0.3629 - val_loss: 0.3868\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 1s 101ms/step - loss: 0.3638 - val_loss: 0.3895\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 1s 102ms/step - loss: 0.3590 - val_loss: 0.3874\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 1s 103ms/step - loss: 0.3520 - val_loss: 0.3877\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.3475 - val_loss: 0.3871\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3596 - val_loss: 0.3879\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_15-15:01:32] Training the entire fine-tuned model...\n",
      "[2022_04_15-15:01:41] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3647WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1121s vs `on_train_batch_end` time: 0.1458s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1121s vs `on_train_batch_end` time: 0.1458s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 420ms/step - loss: 0.3745 - val_loss: 0.3893\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3457 - val_loss: 0.3880\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3460 - val_loss: 0.3856\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3441 - val_loss: 0.3858\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3444 - val_loss: 0.3831\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3409 - val_loss: 0.3845\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3548 - val_loss: 0.3814\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 261ms/step - loss: 0.3409 - val_loss: 0.3852\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3484 - val_loss: 0.3814\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3408 - val_loss: 0.3808\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 0.3469 - val_loss: 0.3826\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3519 - val_loss: 0.3858\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3327 - val_loss: 0.3785\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3383 - val_loss: 0.3859\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3528 - val_loss: 0.3798\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3517 - val_loss: 0.3792\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3325 - val_loss: 0.3798\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3296 - val_loss: 0.3797\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 3s 262ms/step - loss: 0.3422 - val_loss: 0.3792\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "[2022_04_15-15:02:44] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-15:02:44] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:03:09] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3311WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1123s vs `on_train_batch_end` time: 0.1457s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1123s vs `on_train_batch_end` time: 0.1457s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 424ms/step - loss: 0.3336 - val_loss: 0.3789\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  91  3\n",
       "1  19  7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3888888888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_6_3_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_6_3_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2a15f8bb) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 6927... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇███▁▁▁▂▂▂▃▃▃▃▄▁</td></tr><tr><td>loss</td><td>█▅▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>██████████████████████▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▂▂▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>12</td></tr><tr><td>best_val_loss</td><td>0.37848</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.33362</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.37895</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">warm-salad-10</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/2a15f8bb\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/2a15f8bb</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_150009-2a15f8bb/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2a15f8bb). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/3mhjtiio\" target=\"_blank\">elated-river-12</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_15-15:04:03] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:04:03] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:04:03] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 294ms/step - loss: 0.8277 - val_loss: 0.7845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.5852 - val_loss: 0.5718\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.5028 - val_loss: 0.4990\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4845 - val_loss: 0.4995\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4431 - val_loss: 0.4567\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4341 - val_loss: 0.4511\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4217 - val_loss: 0.4444\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4198 - val_loss: 0.4376\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4175 - val_loss: 0.4548\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4227 - val_loss: 0.4294\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 113ms/step - loss: 0.4097 - val_loss: 0.4280\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4098 - val_loss: 0.4234\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4161 - val_loss: 0.4662\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4006 - val_loss: 0.4087\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3974 - val_loss: 0.4091\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3816 - val_loss: 0.4395\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3877 - val_loss: 0.4055\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3898 - val_loss: 0.4267\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3884 - val_loss: 0.4626\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3746 - val_loss: 0.4000\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3730 - val_loss: 0.4016\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3875 - val_loss: 0.4031\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3838 - val_loss: 0.3999\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3901 - val_loss: 0.3954\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3968 - val_loss: 0.4305\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4057 - val_loss: 0.4116\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3780 - val_loss: 0.4177\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3757 - val_loss: 0.3966\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3721 - val_loss: 0.4108\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3697 - val_loss: 0.3904\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3514 - val_loss: 0.3874\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3591 - val_loss: 0.3881\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3630 - val_loss: 0.3910\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3619 - val_loss: 0.4010\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3602 - val_loss: 0.3866\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3619 - val_loss: 0.3870\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3630 - val_loss: 0.3927\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3638 - val_loss: 0.3865\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3625 - val_loss: 0.3899\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3662 - val_loss: 0.3881\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3555 - val_loss: 0.3874\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3537 - val_loss: 0.3872\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3541 - val_loss: 0.3874\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3569 - val_loss: 0.3877\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_15-15:05:08] Training the entire fine-tuned model...\n",
      "[2022_04_15-15:05:17] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3664WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1143s vs `on_train_batch_end` time: 0.1450s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1143s vs `on_train_batch_end` time: 0.1450s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 427ms/step - loss: 0.3788 - val_loss: 0.4072\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3651 - val_loss: 0.3821\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3397 - val_loss: 0.3816\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3504 - val_loss: 0.3789\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3407 - val_loss: 0.3826\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3472 - val_loss: 0.3767\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3331 - val_loss: 0.3707\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3340 - val_loss: 0.3751\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3389 - val_loss: 0.3981\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3329 - val_loss: 0.3718\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3258 - val_loss: 0.3702\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3210 - val_loss: 0.3719\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3236 - val_loss: 0.3717\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3273 - val_loss: 0.3692\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3289 - val_loss: 0.3689\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3117 - val_loss: 0.3683\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3140 - val_loss: 0.3687\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3227 - val_loss: 0.3690\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3168 - val_loss: 0.3690\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3101 - val_loss: 0.3701\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3147 - val_loss: 0.3673\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3046 - val_loss: 0.3670\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3154 - val_loss: 0.3679\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3115 - val_loss: 0.3685\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3097 - val_loss: 0.3667\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3086 - val_loss: 0.3671\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3085 - val_loss: 0.3696\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3054 - val_loss: 0.3676\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3174 - val_loss: 0.3673\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3041 - val_loss: 0.3674\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3058 - val_loss: 0.3674\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "[2022_04_15-15:06:54] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-15:06:54] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:06:54] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.2986WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1145s vs `on_train_batch_end` time: 0.1436s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1145s vs `on_train_batch_end` time: 0.1436s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 425ms/step - loss: 0.2999 - val_loss: 0.3721\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  92  2\n",
       "1  19  7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_6_3_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_6_3_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3mhjtiio) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 7461... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▃▃▄▄▅▅▅▅▆▁</td></tr><tr><td>loss</td><td>█▅▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>24</td></tr><tr><td>best_val_loss</td><td>0.36675</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.29995</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.37205</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">elated-river-12</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/3mhjtiio\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/3mhjtiio</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_150348-3mhjtiio/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3mhjtiio). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/1oh19b80\" target=\"_blank\">elated-plasma-13</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_15-15:07:49] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:07:49] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:07:49] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 11s 296ms/step - loss: 0.7848 - val_loss: 0.6447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.5266 - val_loss: 0.4901\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4639 - val_loss: 0.4793\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4496 - val_loss: 0.4714\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4401 - val_loss: 0.4590\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4415 - val_loss: 0.4438\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4460 - val_loss: 0.4397\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4439 - val_loss: 0.4321\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.4228 - val_loss: 0.5120\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4418 - val_loss: 0.4459\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4079 - val_loss: 0.4194\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.4111 - val_loss: 0.4398\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4061 - val_loss: 0.4114\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4012 - val_loss: 0.4212\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.4021 - val_loss: 0.4340\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4003 - val_loss: 0.4336\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3985 - val_loss: 0.4138\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3848 - val_loss: 0.4030\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3795 - val_loss: 0.4018\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3836 - val_loss: 0.4101\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3852 - val_loss: 0.4017\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3764 - val_loss: 0.4018\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3721 - val_loss: 0.4021\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 112ms/step - loss: 0.3702 - val_loss: 0.4016\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3762 - val_loss: 0.4009\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3757 - val_loss: 0.4016\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3715 - val_loss: 0.4011\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3633 - val_loss: 0.4012\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3768 - val_loss: 0.4009\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3726 - val_loss: 0.4004\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3733 - val_loss: 0.4007\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3716 - val_loss: 0.4012\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3693 - val_loss: 0.4010\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3580 - val_loss: 0.4011\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3661 - val_loss: 0.4009\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3659 - val_loss: 0.4009\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "[2022_04_15-15:08:45] Training the entire fine-tuned model...\n",
      "[2022_04_15-15:08:54] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.4098WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1138s vs `on_train_batch_end` time: 0.1460s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1138s vs `on_train_batch_end` time: 0.1460s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 427ms/step - loss: 0.3939 - val_loss: 0.3958\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3863 - val_loss: 0.3933\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3806 - val_loss: 0.3927\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3719 - val_loss: 0.3828\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 261ms/step - loss: 0.3485 - val_loss: 0.3784\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3643 - val_loss: 0.3899\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3532 - val_loss: 0.3831\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3405 - val_loss: 0.3714\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3299 - val_loss: 0.3905\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3236 - val_loss: 0.3765\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3261 - val_loss: 0.3639\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3091 - val_loss: 0.3668\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3033 - val_loss: 0.3779\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2843 - val_loss: 0.3744\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2960 - val_loss: 0.3717\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2838 - val_loss: 0.3687\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2882 - val_loss: 0.3682\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "[2022_04_15-15:09:50] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-15:09:50] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:09:50] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3191WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1147s vs `on_train_batch_end` time: 0.1469s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1147s vs `on_train_batch_end` time: 0.1469s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 431ms/step - loss: 0.3123 - val_loss: 0.3696\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  19  7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4117647058823529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_6_3_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_6_3_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1oh19b80) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 7950... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▃▄▄▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▄▃▃▃▅▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>10</td></tr><tr><td>best_val_loss</td><td>0.36389</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.31229</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.3696</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">elated-plasma-13</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/1oh19b80\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/1oh19b80</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_150732-1oh19b80/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1oh19b80). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/5ve9efob\" target=\"_blank\">pretty-sunset-14</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_15-15:10:45] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:10:45] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:10:45] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 11s 288ms/step - loss: 0.7877 - val_loss: 0.6235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.5436 - val_loss: 0.5044\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4831 - val_loss: 0.4821\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4596 - val_loss: 0.4665\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4459 - val_loss: 0.4518\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4500 - val_loss: 0.4451\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4605 - val_loss: 0.4474\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4533 - val_loss: 0.5045\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4236 - val_loss: 0.4346\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4244 - val_loss: 0.4447\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4135 - val_loss: 0.4258\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4237 - val_loss: 0.4732\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4006 - val_loss: 0.4169\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4058 - val_loss: 0.4391\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4050 - val_loss: 0.4573\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4024 - val_loss: 0.4260\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3860 - val_loss: 0.4114\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3863 - val_loss: 0.4064\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3793 - val_loss: 0.4015\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3701 - val_loss: 0.4032\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3788 - val_loss: 0.4043\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3821 - val_loss: 0.3988\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3860 - val_loss: 0.4009\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3739 - val_loss: 0.3991\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3777 - val_loss: 0.3982\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3805 - val_loss: 0.4031\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3773 - val_loss: 0.3966\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.3785 - val_loss: 0.3992\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 106ms/step - loss: 0.3822 - val_loss: 0.4085\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3720 - val_loss: 0.3979\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3812 - val_loss: 0.3961\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3622 - val_loss: 0.3983\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3699 - val_loss: 0.3959\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3742 - val_loss: 0.3970\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3721 - val_loss: 0.3966\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3720 - val_loss: 0.3963\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3699 - val_loss: 0.3957\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3690 - val_loss: 0.3963\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3640 - val_loss: 0.3968\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3703 - val_loss: 0.3958\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3658 - val_loss: 0.3958\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3764 - val_loss: 0.3959\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3737 - val_loss: 0.3959\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "[2022_04_15-15:11:49] Training the entire fine-tuned model...\n",
      "[2022_04_15-15:11:58] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.7044WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1156s vs `on_train_batch_end` time: 0.1457s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1156s vs `on_train_batch_end` time: 0.1457s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 428ms/step - loss: 0.5852 - val_loss: 0.4028\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.4228 - val_loss: 0.3934\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3777 - val_loss: 0.3874\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3835 - val_loss: 0.4050\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 263ms/step - loss: 0.4174 - val_loss: 0.4378\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 261ms/step - loss: 0.3685 - val_loss: 0.3829\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3653 - val_loss: 0.3788\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3423 - val_loss: 0.3809\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3364 - val_loss: 0.3602\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3167 - val_loss: 0.3619\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.2959 - val_loss: 0.3615\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2796 - val_loss: 0.3648\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2480 - val_loss: 0.3672\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2250 - val_loss: 0.3860\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2092 - val_loss: 0.4218\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_15-15:12:49] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-15:12:49] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:12:49] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3076WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1166s vs `on_train_batch_end` time: 0.1460s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1166s vs `on_train_batch_end` time: 0.1460s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 539ms/step - loss: 0.3049 - val_loss: 0.3639\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  18  8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45714285714285713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_6_3_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_6_3_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:5ve9efob) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 8325... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▃▁</td></tr><tr><td>loss</td><td>█▅▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▅▃▃▃▃▂▂▂▁▁▂</td></tr><tr><td>lr</td><td>███████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▅▃▃▄▃▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▁▂▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>8</td></tr><tr><td>best_val_loss</td><td>0.36023</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.30494</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>val_loss</td><td>0.36395</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">pretty-sunset-14</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/5ve9efob\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/5ve9efob</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_151028-5ve9efob/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:5ve9efob). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/gknzs0q3\" target=\"_blank\">logical-moon-15</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "1e-05\n",
      "[2022_04_15-15:13:43] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:13:43] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:13:43] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 290ms/step - loss: 0.8528 - val_loss: 0.7081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.5688 - val_loss: 0.5597\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4918 - val_loss: 0.4820\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4518 - val_loss: 0.4693\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4398 - val_loss: 0.4552\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.4357 - val_loss: 0.4504\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4283 - val_loss: 0.4570\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 113ms/step - loss: 0.4111 - val_loss: 0.4493\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4102 - val_loss: 0.4356\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4041 - val_loss: 0.4295\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4174 - val_loss: 0.4261\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4017 - val_loss: 0.4257\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4059 - val_loss: 0.4203\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3912 - val_loss: 0.4147\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.4028 - val_loss: 0.4128\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3977 - val_loss: 0.4114\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4027 - val_loss: 0.4207\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3834 - val_loss: 0.4086\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3856 - val_loss: 0.4114\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4393 - val_loss: 0.4075\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4049 - val_loss: 0.4025\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3849 - val_loss: 0.4038\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3771 - val_loss: 0.4191\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4049 - val_loss: 0.3993\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3626 - val_loss: 0.4069\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3798 - val_loss: 0.3966\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3649 - val_loss: 0.4049\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3603 - val_loss: 0.3978\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3747 - val_loss: 0.4434\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3946 - val_loss: 0.3892\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3847 - val_loss: 0.3926\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3778 - val_loss: 0.4162\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3642 - val_loss: 0.3875\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3692 - val_loss: 0.4412\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 113ms/step - loss: 0.3939 - val_loss: 0.3927\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3587 - val_loss: 0.3901\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3494 - val_loss: 0.3898\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3620 - val_loss: 0.3947\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3614 - val_loss: 0.3899\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3620 - val_loss: 0.3920\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3678 - val_loss: 0.3876\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2022_04_15-15:14:44] Training the entire fine-tuned model...\n",
      "[2022_04_15-15:14:54] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3685WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1142s vs `on_train_batch_end` time: 0.1442s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1142s vs `on_train_batch_end` time: 0.1442s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 429ms/step - loss: 0.3500 - val_loss: 0.3962\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3630 - val_loss: 0.3865\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3530 - val_loss: 0.3893\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 0.3555 - val_loss: 0.3874\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3540 - val_loss: 0.3874\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3520 - val_loss: 0.3872\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3469 - val_loss: 0.3866\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3430 - val_loss: 0.3861\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3524 - val_loss: 0.3863\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3558 - val_loss: 0.3864\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3485 - val_loss: 0.3854\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3515 - val_loss: 0.3857\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3515 - val_loss: 0.3876\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3482 - val_loss: 0.3870\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3587 - val_loss: 0.3847\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3582 - val_loss: 0.3858\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3505 - val_loss: 0.3859\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3511 - val_loss: 0.3864\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3469 - val_loss: 0.3871\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3538 - val_loss: 0.3868\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3448 - val_loss: 0.3863\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3538 - val_loss: 0.3858\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3407 - val_loss: 0.3854\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      "[2022_04_15-15:16:08] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-15:16:08] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:16:22] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3404WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1136s vs `on_train_batch_end` time: 0.1473s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1136s vs `on_train_batch_end` time: 0.1473s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 545ms/step - loss: 0.3493 - val_loss: 0.3845\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  21  5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_8_4_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_8_4_1e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:gknzs0q3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 8728... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▂▃▃▃▄▄▄▄▅▁</td></tr><tr><td>loss</td><td>█▄▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████████▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▃▃▂▂▂▂▂▂▂▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.38452</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.34929</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.38452</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">logical-moon-15</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/gknzs0q3\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/gknzs0q3</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_151327-gknzs0q3/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:gknzs0q3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/3j8vlelm\" target=\"_blank\">kind-serenity-16</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "5e-05\n",
      "[2022_04_15-15:17:17] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:17:17] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:17:17] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 287ms/step - loss: 0.8497 - val_loss: 0.9034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.5952 - val_loss: 0.5578\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4940 - val_loss: 0.4785\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4798 - val_loss: 0.5324\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4946 - val_loss: 0.5045\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4619 - val_loss: 0.5121\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4456 - val_loss: 0.4449\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4217 - val_loss: 0.4361\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4272 - val_loss: 0.4370\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4086 - val_loss: 0.4272\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3959 - val_loss: 0.4460\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4005 - val_loss: 0.4193\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3980 - val_loss: 0.4172\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4087 - val_loss: 0.4207\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4005 - val_loss: 0.4172\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.4044 - val_loss: 0.4271\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4002 - val_loss: 0.4126\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3939 - val_loss: 0.4083\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3872 - val_loss: 0.4079\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3833 - val_loss: 0.4152\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3948 - val_loss: 0.4060\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3890 - val_loss: 0.4111\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3750 - val_loss: 0.4401\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4211 - val_loss: 0.5040\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3824 - val_loss: 0.3939\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3975 - val_loss: 0.4189\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.4098 - val_loss: 0.5248\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3833 - val_loss: 0.3936\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3864 - val_loss: 0.4196\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3735 - val_loss: 0.3966\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3656 - val_loss: 0.3854\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3809 - val_loss: 0.4092\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3762 - val_loss: 0.3969\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4207 - val_loss: 0.4420\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4305 - val_loss: 0.3983\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3683 - val_loss: 0.4063\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3556 - val_loss: 0.4052\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3747 - val_loss: 0.3847\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3574 - val_loss: 0.3840\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3582 - val_loss: 0.3975\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3574 - val_loss: 0.3940\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3594 - val_loss: 0.3928\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3611 - val_loss: 0.3839\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3630 - val_loss: 0.3854\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 1s 112ms/step - loss: 0.3550 - val_loss: 0.3871\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3728 - val_loss: 0.3856\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3496 - val_loss: 0.3859\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3494 - val_loss: 0.3857\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3537 - val_loss: 0.3869\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.3525 - val_loss: 0.3867\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3490 - val_loss: 0.3863\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_15-15:18:31] Training the entire fine-tuned model...\n",
      "[2022_04_15-15:18:57] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3447WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1119s vs `on_train_batch_end` time: 0.1476s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1119s vs `on_train_batch_end` time: 0.1476s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 433ms/step - loss: 0.3644 - val_loss: 0.3862\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3565 - val_loss: 0.3843\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3584 - val_loss: 0.3881\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3499 - val_loss: 0.3792\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3436 - val_loss: 0.3864\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3392 - val_loss: 0.3938\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3507 - val_loss: 0.4045\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 261ms/step - loss: 0.3386 - val_loss: 0.3769\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 0.3355 - val_loss: 0.3730\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3278 - val_loss: 0.3742\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.3175 - val_loss: 0.3744\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3267 - val_loss: 0.3727\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3093 - val_loss: 0.3922\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3221 - val_loss: 0.4002\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3112 - val_loss: 0.3820\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3073 - val_loss: 0.3692\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3012 - val_loss: 0.3803\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.2995 - val_loss: 0.3756\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.2960 - val_loss: 0.3662\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.2916 - val_loss: 0.3670\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2917 - val_loss: 0.3735\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2865 - val_loss: 0.3634\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2838 - val_loss: 0.3792\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2913 - val_loss: 0.4171\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2824 - val_loss: 0.4114\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2782 - val_loss: 0.3760\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2699 - val_loss: 0.3659\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.2654 - val_loss: 0.3706\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2574 - val_loss: 0.3665\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2584 - val_loss: 0.3722\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "[2022_04_15-15:20:31] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-15:20:31] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:20:31] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.2816WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1126s vs `on_train_batch_end` time: 0.1486s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1126s vs `on_train_batch_end` time: 0.1486s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 438ms/step - loss: 0.2853 - val_loss: 0.3657\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  90   4\n",
       "1  16  10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_8_4_5e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_8_4_5e-05/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3j8vlelm) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9158... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇▇██▁▁▂▂▂▃▃▃▃▄▄▄▅▅▁</td></tr><tr><td>loss</td><td>█▄▄▃▃▃▃▃▃▂▃▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▃▂▂▂▂▂▂▂▂▂▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>21</td></tr><tr><td>best_val_loss</td><td>0.36335</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.28526</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.36567</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">kind-serenity-16</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/3j8vlelm\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/3j8vlelm</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_151701-3j8vlelm/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3j8vlelm). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/1azrrqg7\" target=\"_blank\">grateful-thunder-17</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_15-15:21:25] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:21:25] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:21:25] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 288ms/step - loss: 0.8104 - val_loss: 0.5915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 116ms/step - loss: 0.5080 - val_loss: 0.4967\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4933 - val_loss: 0.4771\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4499 - val_loss: 0.4649\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4446 - val_loss: 0.4875\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4642 - val_loss: 0.4796\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4450 - val_loss: 0.4536\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4551 - val_loss: 0.4320\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4113 - val_loss: 0.4396\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4132 - val_loss: 0.4250\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.4088 - val_loss: 0.4265\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4210 - val_loss: 0.4349\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4169 - val_loss: 0.4122\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4378 - val_loss: 0.4113\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4294 - val_loss: 0.4850\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4380 - val_loss: 0.4179\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3985 - val_loss: 0.4600\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3916 - val_loss: 0.4074\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3803 - val_loss: 0.4282\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3917 - val_loss: 0.4030\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3769 - val_loss: 0.3994\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3932 - val_loss: 0.4298\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4184 - val_loss: 0.4095\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3925 - val_loss: 0.4070\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3873 - val_loss: 0.4021\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3722 - val_loss: 0.4075\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3646 - val_loss: 0.3956\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3729 - val_loss: 0.3948\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3647 - val_loss: 0.3949\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3659 - val_loss: 0.3952\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.3644 - val_loss: 0.3982\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3618 - val_loss: 0.3984\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3598 - val_loss: 0.3947\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3630 - val_loss: 0.3941\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3644 - val_loss: 0.3969\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3650 - val_loss: 0.3938\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3575 - val_loss: 0.3971\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3601 - val_loss: 0.3948\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3633 - val_loss: 0.3961\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3620 - val_loss: 0.3935\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3606 - val_loss: 0.3981\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3688 - val_loss: 0.3946\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3633 - val_loss: 0.3942\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3662 - val_loss: 0.3946\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3636 - val_loss: 0.3949\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3697 - val_loss: 0.3944\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3602 - val_loss: 0.3949\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3670 - val_loss: 0.3950\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "[2022_04_15-15:22:35] Training the entire fine-tuned model...\n",
      "[2022_04_15-15:22:44] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3987WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1137s vs `on_train_batch_end` time: 0.1469s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1137s vs `on_train_batch_end` time: 0.1469s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 426ms/step - loss: 0.3929 - val_loss: 0.3915\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3789 - val_loss: 0.3896\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3595 - val_loss: 0.3896\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3632 - val_loss: 0.3874\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3423 - val_loss: 0.3825\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3424 - val_loss: 0.3803\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3346 - val_loss: 0.3797\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3455 - val_loss: 0.3730\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3459 - val_loss: 0.3955\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3272 - val_loss: 0.3709\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3203 - val_loss: 0.3693\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3205 - val_loss: 0.3669\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2997 - val_loss: 0.3731\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.3148 - val_loss: 0.3717\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2924 - val_loss: 0.3812\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.2988 - val_loss: 0.3923\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.3008 - val_loss: 0.3754\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2854 - val_loss: 0.3779\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2750 - val_loss: 0.3652\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.2726 - val_loss: 0.3672\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2578 - val_loss: 0.3677\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2669 - val_loss: 0.3677\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2596 - val_loss: 0.3679\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.2458 - val_loss: 0.3712\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 0.2571 - val_loss: 0.3712\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2484 - val_loss: 0.3696\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2618 - val_loss: 0.3736\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "[2022_04_15-15:24:10] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-15:24:10] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:24:12] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.2608WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1141s vs `on_train_batch_end` time: 0.1461s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1141s vs `on_train_batch_end` time: 0.1461s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 427ms/step - loss: 0.2705 - val_loss: 0.3720\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  91   3\n",
       "1  16  10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5128205128205128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_8_4_0.0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_8_4_0.0001/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1azrrqg7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9734... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇██▁▁▁▂▂▂▃▃▃▄▄▄▅▅▁</td></tr><tr><td>loss</td><td>█▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▄▅▃▃▃▂▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>18</td></tr><tr><td>best_val_loss</td><td>0.36521</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.27045</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.37203</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">grateful-thunder-17</strong>: <a href=\"https://wandb.ai/kvetab/Full%20old/runs/1azrrqg7\" target=\"_blank\">https://wandb.ai/kvetab/Full%20old/runs/1azrrqg7</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_152109-1azrrqg7/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1azrrqg7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Full%20old/runs/30skiikj\" target=\"_blank\">apricot-dragon-18</a></strong> to <a href=\"https://wandb.ai/kvetab/Full%20old\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0005\n",
      "[2022_04_15-15:25:08] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:25:08] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:25:08] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 292ms/step - loss: 0.8022 - val_loss: 0.6105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.5308 - val_loss: 0.5151\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4713 - val_loss: 0.4786\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4585 - val_loss: 0.4622\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4384 - val_loss: 0.4639\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.4365 - val_loss: 0.4458\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4301 - val_loss: 0.5041\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4568 - val_loss: 0.4528\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4152 - val_loss: 0.4562\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4044 - val_loss: 0.4256\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4026 - val_loss: 0.4286\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4140 - val_loss: 0.4207\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4122 - val_loss: 0.4189\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3959 - val_loss: 0.4144\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3936 - val_loss: 0.4153\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3846 - val_loss: 0.4568\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3998 - val_loss: 0.4169\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3893 - val_loss: 0.4033\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3821 - val_loss: 0.4016\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3879 - val_loss: 0.4040\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3768 - val_loss: 0.4027\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3793 - val_loss: 0.4098\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3881 - val_loss: 0.4005\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3997 - val_loss: 0.4137\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4236 - val_loss: 0.4698\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.4013 - val_loss: 0.4474\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4242 - val_loss: 0.5761\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4293 - val_loss: 0.3927\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3691 - val_loss: 0.4026\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3652 - val_loss: 0.4107\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3602 - val_loss: 0.3917\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3734 - val_loss: 0.3976\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3669 - val_loss: 0.3893\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3527 - val_loss: 0.3883\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3738 - val_loss: 0.3892\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3562 - val_loss: 0.3877\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3538 - val_loss: 0.3883\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3586 - val_loss: 0.3886\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3484 - val_loss: 0.3883\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3697 - val_loss: 0.3969\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3678 - val_loss: 0.3886\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3523 - val_loss: 0.3947\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3530 - val_loss: 0.3890\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3553 - val_loss: 0.3898\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "[2022_04_15-15:26:12] Training the entire fine-tuned model...\n",
      "[2022_04_15-15:26:21] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 1.0330WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1110s vs `on_train_batch_end` time: 0.1471s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1110s vs `on_train_batch_end` time: 0.1471s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 426ms/step - loss: 0.7918 - val_loss: 0.3976\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.4121 - val_loss: 0.3941\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 262ms/step - loss: 0.4068 - val_loss: 0.4007\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3922 - val_loss: 0.3810\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 261ms/step - loss: 0.3726 - val_loss: 0.3808\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3529 - val_loss: 0.3783\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3494 - val_loss: 0.3975\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3348 - val_loss: 0.3681\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3143 - val_loss: 0.3715\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.3380 - val_loss: 0.3858\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.3073 - val_loss: 0.3953\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.2746 - val_loss: 0.3685\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 264ms/step - loss: 0.2365 - val_loss: 0.3736\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 259ms/step - loss: 0.2225 - val_loss: 0.3818\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2131 - val_loss: 0.3906\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.2003 - val_loss: 0.4062\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "[2022_04_15-15:27:15] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-15:27:15] Training set: Filtered out 0 of 1338 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-15:27:15] Validation set: Filtered out 0 of 120 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3056WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1158s vs `on_train_batch_end` time: 0.1439s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1158s vs `on_train_batch_end` time: 0.1439s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 425ms/step - loss: 0.3067 - val_loss: 0.3806\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0  93  1\n",
       "1  18  8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45714285714285713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input-seq, input-annotations with unsupported characters which will be renamed to input_seq, input_annotations in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_8_4_0.0005/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../data/protein_bert/old_full/2022_04_09_8_4_0.0005/assets\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "for pat in patience:\n",
    "    for lr in learning_rate:\n",
    "        finetune_by_settings_and_data(pat, lr, f\"Full\", train_data, test_data, f\"old_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310c43cb-2358-40c5-b3b6-2027bfc4106a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "559e98e6-c31c-4650-9886-ad9dfc11f0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_15-12:45:47] Test set: Filtered out 0 of 260 (0.0%) records of lengths exceeding 510.\n"
     ]
    }
   ],
   "source": [
    "encoded_test_set = encode_dataset(test_data[\"seq\"], test_data[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'Test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4cc4820c-1011-4813-9e0e-397cfa976103",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X, test_Y, test_sample_weigths = encoded_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c5e115f7-c70f-41df-b3a3-a0da075721c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_04_15-15:27:54] Test set old: Filtered out 0 of 119 (0.0%) records of lengths exceeding 510.\n"
     ]
    }
   ],
   "source": [
    "encoded_test_set_old = encode_dataset(test_data_old[\"seq\"], test_data_old[\"Y\"], input_encoder, OUTPUT_SPEC, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'Test set old')\n",
    "test_X_old, test_Y_old, test_sample_weigths_old = encoded_test_set_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288f2631-ab65-4dea-a81d-871aeea43932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c197e30e-5ddf-453c-ad43-397bc6f1df66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for model in os.listdir(path.join(DATA_DIR, f\"protein_bert/full\")):\n",
    "    if model.startswith(\"2022_04_09\"):\n",
    "        test_model(model, f\"full\", test_X, test_Y, tap_X, tap_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8eedd51e-cb7d-4e6f-aa6d-6f8c6f8daf5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10224... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model in os.listdir(path.join(DATA_DIR, f\"protein_bert/old_full\")):\n",
    "    if model.startswith(\"2022_04_09\"):\n",
    "        test_model(model, f\"old_full\", test_X_old, test_Y_old, tap_X, tap_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ef191c63-a713-48fe-a41e-16f8ab7fe56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"2022_04_09_6_3_0.0001\"\n",
    "model_path = path.join(DATA_DIR, f\"protein_bert/full/{model_name}\")\n",
    "model = keras.models.load_model(model_path)\n",
    "parts = model_name[11:].split(\"_\")\n",
    "patience_stop = parts[0]\n",
    "patience_lr = parts[1]\n",
    "lr = parts[2]\n",
    "\n",
    "y_pred = model.predict(test_X, batch_size=32)\n",
    "y_pred_classes = (y_pred >= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c783e834-4cf9-4049-a9d2-662d35b7bee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15384615384615383"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(test_Y, y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9e3db90a-6ef4-4e06-b6fe-0454584a2dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[200,   6],\n",
       "       [ 49,   5]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(test_Y, y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8543f4a1-2176-4479-b0f7-30fbfd54df04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1291"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e0b6ff60-cf54-4985-8f9e-82fe5a31a5bc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:344ub7bf) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 3731... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█▁▁▂▃▃▃▄▅▅▅▆▇▁</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>████████████▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▃▂▁▂▃▁▄▃▂▁▂▃▂▂▃▁▁▁▂▁▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.45819</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.35927</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>val_loss</td><td>0.48604</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">still-butterfly-5</strong>: <a href=\"https://wandb.ai/kvetab/Easter/runs/344ub7bf\" target=\"_blank\">https://wandb.ai/kvetab/Easter/runs/344ub7bf</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_142948-344ub7bf/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:344ub7bf). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kvetab/Easter/runs/rc09kp9o\" target=\"_blank\">rosy-cloud-6</a></strong> to <a href=\"https://wandb.ai/kvetab/Easter\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0001\n",
      "[2022_04_15-14:33:47] Training set: Filtered out 0 of 1291 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:33:48] Validation set: Filtered out 0 of 130 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:33:48] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 10s 281ms/step - loss: 0.7999 - val_loss: 0.8898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brazdilv/.conda/envs/ml/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer GlobalAttention has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.5859 - val_loss: 0.6600\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.5152 - val_loss: 0.5327\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.4500 - val_loss: 0.5070\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4278 - val_loss: 0.4914\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.4082 - val_loss: 0.5343\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.4290 - val_loss: 0.5193\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4161 - val_loss: 0.5199\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.4100 - val_loss: 0.4779\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.4006 - val_loss: 0.4866\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3921 - val_loss: 0.4959\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3946 - val_loss: 0.4784\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3938 - val_loss: 0.4817\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3883 - val_loss: 0.4907\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3903 - val_loss: 0.4741\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.3859 - val_loss: 0.4760\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3910 - val_loss: 0.4913\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3948 - val_loss: 0.4747\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 1s 109ms/step - loss: 0.3923 - val_loss: 0.4738\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3854 - val_loss: 0.4771\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.3881 - val_loss: 0.4790\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.3888 - val_loss: 0.4770\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3918 - val_loss: 0.4763\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3905 - val_loss: 0.4754\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 1s 108ms/step - loss: 0.3873 - val_loss: 0.4762\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "[2022_04_15-14:34:29] Training the entire fine-tuned model...\n",
      "[2022_04_15-14:34:45] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/100\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.4002WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1148s vs `on_train_batch_end` time: 0.1441s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1148s vs `on_train_batch_end` time: 0.1441s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 418ms/step - loss: 0.4045 - val_loss: 0.4907\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 3s 247ms/step - loss: 0.3969 - val_loss: 0.4714\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3894 - val_loss: 0.4817\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 3s 248ms/step - loss: 0.3789 - val_loss: 0.4876\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3825 - val_loss: 0.4609\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 3s 248ms/step - loss: 0.3693 - val_loss: 0.4934\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3705 - val_loss: 0.5067\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3528 - val_loss: 0.4917\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3563 - val_loss: 0.4596\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 3s 248ms/step - loss: 0.3530 - val_loss: 0.4644\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 3s 248ms/step - loss: 0.3520 - val_loss: 0.4660\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3386 - val_loss: 0.4575\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3348 - val_loss: 0.4666\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3422 - val_loss: 0.4549\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3419 - val_loss: 0.4625\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 0.3339 - val_loss: 0.4622\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 3s 250ms/step - loss: 0.3254 - val_loss: 0.4636\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.3271 - val_loss: 0.4599\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 3s 250ms/step - loss: 0.3225 - val_loss: 0.4595\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.3249 - val_loss: 0.4624\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "[2022_04_15-14:35:49] Training on final epochs of sequence length 512...\n",
      "[2022_04_15-14:35:49] Training set: Filtered out 0 of 1291 (0.0%) records of lengths exceeding 510.\n",
      "[2022_04_15-14:35:49] Validation set: Filtered out 0 of 130 (0.0%) records of lengths exceeding 510.\n",
      " 6/11 [===============>..............] - ETA: 1s - loss: 0.3351WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1126s vs `on_train_batch_end` time: 0.1469s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1126s vs `on_train_batch_end` time: 0.1469s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 11s 418ms/step - loss: 0.3270 - val_loss: 0.4616\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"Easter\", entity=\"kvetab\")\n",
    "model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "    get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)\n",
    "training_callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(patience = 3, factor = 0.25, min_lr = 1e-07, verbose = 1),\n",
    "    keras.callbacks.EarlyStopping(patience = 6, restore_best_weights = True),\n",
    "    WandbCallback()\n",
    "]\n",
    "epoch_num = 100\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "wandb.config = {\n",
    "      \"learning_rate\": lr,\n",
    "      \"epochs\": epoch_num * 2,\n",
    "      \"batch_size\": batch_size\n",
    "    }\n",
    "print(type(lr))\n",
    "print(lr)\n",
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, train_data['seq'], train_data['Y'], valid_data['seq'], valid_data['Y'], \\\n",
    "        seq_len = 512, batch_size = batch_size, max_epochs_per_stage = epoch_num, lr = lr, begin_with_frozen_pretrained_layers = True, \\\n",
    "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 512, final_lr = lr / 10, callbacks = training_callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a4565283-afc4-452e-a7bf-edf3343d243f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>104</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  104  2\n",
       "1   21  3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_data['seq'], test_data['Y'], \\\n",
    "        start_seq_len = 512, start_batch_size = 32)\n",
    "print('Confusion matrix:')\n",
    "display(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "44a664fb-2244-4ead-836c-849c0c33179f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20689655172413793"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_fp = confusion_matrix.loc[\"0\"][1] + confusion_matrix.loc[\"1\"][0]\n",
    "f1 = confusion_matrix.loc[\"1\"][1] / (confusion_matrix.loc[\"1\"][1] + 0.5 * fn_fp)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "26b31976-7e75-40d7-bfcc-aeb4a46995ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1291"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "377ee2c0-1ef8-42c7-a8c6-494d0514daf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "49746582-eab0-4f59-8411-ca38d6f35c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8e26aa-4435-49dd-958d-0318b364b644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
